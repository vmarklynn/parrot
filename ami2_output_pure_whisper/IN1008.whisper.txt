 Okay. Okay. Should I start? Yeah. Maybe by presenting the project a little bit. Okay. Historically it was last year, Prof. D. Lambo, in his CSCW classes asked the student to design a table on all the students and Guillermo was a student of this class. I was very excited to design such a table. So he decided to make more research on this topic. So the general topic is about interactive table, disappearing computer. Okay. That is the other one. So in the general field of computer supported collaborative learning. Okay. So we have different people collaborating in a pedagogical purpose. And we know that when we put a computer in front of them, it doesn't help focusing on collaboration. Yeah. So if we make the computing functionalities disappearing in the furniture, in the wall, or other things like that, we may more efficiently support collaboration than the learning. So I arrived in June, something like that a couple of months ago. Guillaume and Michael are two students that are working for months to the master project. Okay. And they start working on two different sub-projects. So Guillaume is working on the noise sensitive table. It's what we are discussing about today. Okay. So the idea is, okay, we have in the lab a notion about root mirroring. The idea is if we can provide a feedback to people working together, it may help them self-regulating their activity, their collaboration. So it could be normative or not. In our own case, we want to have a table that would be accessible by students. At the PFL we are... Oh, hello. Okay. At PFL, the learning center will be built in a couple of years. So the idea is we will have plenty of rooms, like in libraries or meeting rooms, where students will arrive and walk. So if we have this table that is accessible to students, if they arrive, if we provide a feedback of who is talking instant at one moment or through time or what is the turn taking, maybe they will just by seeing an explicit feedback of that, they will take profit of it on the... So do you know which way you give a feedback light or something like that? Okay. For the Nonsense Tftable project, we decided to take the noise as input and give a feedback with lads on the table. So this is one of the possibilities that were offered to us, but we decided to do it this way. The idea is to have very, very, very cool information. We don't want people to focus on what's going on the feedback. So we don't want histogram or very precise information. But we will have lads that will be 6 centimeters between each lads with a blurring glass. The idea is to have more artistic feedback, to have a general feeling of what's going on, something about more. Okay. Well, I guess it's just a restriction we took. So we don't depest the amount of money we have to spend. Otherwise, we could imagine to have a whole table of lads. And then you can really start to assume some reach-ins on the table. Maybe more complex. Yeah. Yeah. The first idea of the project was to have the world table covered by nuts. But for the type of radical reasons, this is not the way we implemented it now. To the border, basically. In fact, we kept one paper sheet. Yes. There was the restriction that if we covered the table with lads, some won't be visible because you have, obviously. So if there is information displayed under the sheets, it won't be very useful. So we decided to have it, yes, 30 centimeters. Okay. So the first shape of the table, the shape of the table, obviously, is one of the factors that will change the collaboration. So, but we don't want to start with too many parameters. So we start with a table that is approximately the table where we are now. Yeah. And we will have a square shape like this that will be here on the second one that would be there. Yeah. So we will have something like this. Okay. Like this. That will be our first real prototype. Right. Okay. Okay. And so these LED modules are being designed now. We should have them in a couple of weeks. And we will be able to start evaluating prototypes. About the CSCW class that will start at the beginning of November. So the student will be asked to design a table. So to find the shape of the table to get hood and put that on table legs. And we will... So at the beginning, we wanted to put these LED modules in the tables. But we won't make it because for solidity region of the table, it's not very strong wood. So we will rather beam the LED on the table. So we come back to a more richer display, but we will restrict it to small to have the same display. Lights. Lights. That will be distant six centimeters between legs. Okay. So we will simulate... This is for practical reason. But don't matter of time because... Yeah. Not you think with a beam, also you have less problems with the sheets? Yes, but there are other problems like... Yes, the information will be under sheets. Okay. And we have the feeling that to have the information and bedgied in the table and make it part of the table, I don't know. We will try. We don't really know what will happen with this prototype. We are very excited about that. Okay. Okay. Maybe before going on this subject, we can talk a little bit about... Yeah, I can just explain my project. It's completely different from this one. Well, it's also about tables. But the idea is if several students come to a table with their laptops, they should be able to share information easily. Okay. And in fact, what we are doing now, we project another screen on the table. Yeah. And different people are able to use their mouse of their laptop to go to the project screen there. Okay. So the project screen is just another Windows machine that is running. Yeah. So you just have another computer more or less you can use, but you can use it with your own mouse and keyboard. But physically, it's just something flat on the table again. Yeah. But in country, to have another keyboard and mouse, the only one person can use now, the different persons can use the screen at the same time with the mouse. Okay. So based on the fact that most of the PFL students have their own laptop, the idea is the similar idea is to, okay, people are coming and should be able to very easily use some resources offered to them. Okay. So the idea is to help them socializing, organizing their own work and not in a classroom setting, but in a social place where they could gather. Okay. Yeah. So it's different, but still, yeah. It's still the same subject. Okay. It's not collaborative work, but different directions. Okay. It has nothing to do with audio processing and LEDs. Okay. Okay. Okay. So maybe we can focus a little bit more on what we have done about the noise sensitive table. Yeah. Tell you about what we're interested in. So Guillaume started by developing the application that get the audio input. So we wanted to have a modular architecture to be able to improve each layer separately. Okay. So the audio input is quite basic now. We use sound cards to get the signals and to detect, and we have a threshold to detect when someone is talking, at least when noise is bigger than specific labels. Then we get this information about people talking, talking to say, okay, this one is talking for x time or has been talking, x percent of the five last minutes or two people are talking together. So this is more semantic layer. We call that interaction models. And so when different conditions are detected, we trigger an interaction event. So a very simple one could be one person is talking. So it's an easy one. There is a lot of problems now about this implementation. It's still in progress. It's still in progress. Almost done. I think the details of the implementation is not that important. It's more important that first one to have some events like two people are talking or one started and other started and more or less broke into the speech of the first person and so on. And then you can start to visualize these different situations. So the last layer is about this visualization. We are thinking about visual grammar, meaning that when we have an interaction event that is fired, we will associate a way of visualizing it. So for instance, if someone is talking, we can just have a light that will center on the point that is close to him. That could be an instant feedback. And if he has been talking maybe 70 percent of the last five minutes or two more minutes, it may be reflected as the intensity or size of a cycle. Okay. Center on the same point. And then if they start taking, we can show waves of light going from me to you or from other people or using the center part of the table to show something more about the more general dynamic. I guess it's first prototype. You see, we start with people, do people talk or don't they? Afterwards, I couldn't imagine that you, for example, you can detect that someone is a bit angry or is yelling and with different colors. With different colors on the table, you could display this. Also, you might have multiple colors. Yes, that's how. Sorry, don't get that. So we have eight by eight, two times, leads. Each lead is actually three leads. N So this table is two different things. It's one object that will provide feedback to the terminal users, people collaborating around the table. But it's also a tool for us as a researcher to test both the interaction event we want to detect and to test the visualization. So we will be able to edit some interaction rules and visualization grammar. And we will be able to process, to process some conversation just to see what happens and to have these models and feedback working when people are using the terminal. Okay. It's quite clear. So I guess you want to know what you could do with an array, maybe? Yes. Yes, everything from a chest range of audio and how you can analyze it, but information you can get out of speech and so on. I have one or two questions. I don't know if you want to begin with the explanation about the arrays. Or maybe we could summarize the question we were thinking about. Yes, it's good idea. Just give some questions. So I have actually two questions that are just practical questions about my implementation now. So basically what we want to know when R is to king, B is to king and the basic information we need. So the first information is about the noise level. Not the noise level, the sound level. Because what you get from the microphone is a curve. The way it is implemented now to detect, yes, I have packets of... Let's say this is not fixed, but I have packets for about half a second or so. In this packet I want to know the sound level. The average sound level. So the way I do it now is to take each top of the curves, bottoms and just making an average of this. But I don't know if it's not really the best way to do it, but I don't know if you made the top of the waveform. Okay. Probably it would be a good idea if you have two things with results like books or webpages that give a lot of information on how this audio programming works. Maybe you know some good book that's dealing with the stuff. Maybe we can go through the different questions and then organize your own. This is the first question. I would just add something. We are not interested so much in sound level, but in the level of the sound that corresponds to voice. Yes, but I think this is two different problems. The filtering must be done before. Okay. After that if you have a clean curve you can just apply it. The second question was about filtering in FFT. What you can get from it. Okay. I would add one more. It's about because what we can record is sound origins, sound or sonic points. But what we are interested in is people. The question is if someone is moving, I'm talking here, I stop talking, I'm going here, I'm talking how to detect that it's the same people. If we want to integrate information to say, these people have spoken 70% of the time in the five last minutes. Obviously we need to know that it's the same people. Right now our practical solution is to have one microphone close to every place where people are supposed to sit. So we can infer that if a sound is coming in this area it's the same, it's a kind of trick. For the beginning it's enough. Yes, I'll just add something. Is there a way to from one stream to be two streams that correspond to the voice of fun people and the voice of the second one? Yeah, it's a good separation. And it's possible to do this in real time. And a last question that is related to last if I don't find another one. But people, if someone is leaving, or if someone, maybe it would be something quite complex for you. If someone is staying here, but it's quiet or leaving, can we detect that someone is here but quiet? Why do you need a camera? I know, obviously. What's the great information? I mean, will you have a camera? Not yet. Maybe later on. Maybe you can detect breathing. Or the weight and the... Yeah, that you could do. Or on the table, there should be... Yeah, but what happens if someone is standing there? Putting his bag on there. Okay, but it was just too. It's a question. I'm not sure we will talk too much about that today. I'll try to answer some questions. Some related questions. Okay. Not directly, but it's related. So, in a couple of days, we will get the firewire box. Okay. So, we have chosen one that has eight line up. So, we will be able to line in. So, we will be able to plug up to eight microphones with jack plugs. Okay. To excel air with the possibility of extending with eight more excel air microphones. So, you... I think you said that's obtic, something like that. Yes, the communication between the extension. So, you need a different microphone with a device inside, right? No, no, in fact... How does it go? No, no, no, it's not a thing, this obtic. So, we have the computer. We have the main box with the eight line in the firewire port. It's very wide. And it's very wide. And it's an extension for this device. Yeah. Disconnected to it. It's an obtic. It's an obtic fiber. To have eight more excel air. If we need it. Okay. We're not sure to buy it now, but we can extend it. Actually, it's not really an extension to the first box. It's just an industry standard protocol between these two... It's another firewire... Well, not firewire, but... Well, there's a special protocol to deliver audio signals and... But quickly, you would use that to get higher quality signals. Yes. That's all. Just to have the XLR... Well, not sure about what kind of microphone we should use. Okay. So, we wanted to... That's a fun thing. So, we wanted to have open possibilities. And secondly, maybe we will use this box, you know, to get that... I don't know if... In another experiment, maybe in another project, if we want to have comments of different people around the table or elsewhere... I think the possibility of using XLR microphone could be needed. Because for now, I'm just sampling the sound at 8000, so just something very... I could even go to lower. But this is just to get the noise to some level. But if I want to record the conversation with a processing... You might add kilo sampling for a currency. Yes, to have a better sound quality. Perhaps it's better to have an XLR. Yeah. Something like that. We use 16, so that's 8 kilo bandwidth. 16 kilo sampling frequency. It's not bad there. It's quite decent. Okay. If you're answering the questions, there's also a point of precision of localization if you use an array. So, if you use higher frequencies, you can get more precise localization. Sure. Maybe you don't need it. What is the gain in the terminal position? That I can't answer the entry. Is it something that's more... It depends on your setup. Okay. You have to test, basically. What you get is the real position or just the angle at which the sound comes. You get the angle. The most precise one is the azimuth in the horizontal plane. Then you also get elevation, but it's not very precise. It's more of an indication. And radius is very bad. Okay. If you use a planar geometry like this one. Yes, sure. Yeah. You can use multiple ones and do some triangulation. Yes. Because there is just four microphones that are... 8. There are only screws. Okay, but they are in a plan. Yeah. Yeah. Because at EPFL there is a land that is, has done almost the same thing. But they have 8 microphones that are at... Yes. They eat a corner of a cube. A cube? Okay. Then you might get a better direction in elevation. But I'm not sure it's really relevant. No, we are not. The elevation? No. But it could be relevant. It's not always the case with the... I have people move forward. Yes. The position, the radius of the... Ah, the radius. Yes. The problem is not the geometry. You need more than one, basically. Yes. With one you will only get direction. More than one... Microfonary. Yes, because... Because the distance between the microphone is too... Is too low to... Well, sorry. You say this one already works, to tell the direction. The direction. Yeah. But it does work if you placed the microphones at the board of the table. Oh. You see now you have it quite essential. Yeah. And what happens if you put them here around the table? Would it work as well? It's a different option, yeah. We just didn't try. We haven't tried that. Because we are not going towards a special kind of table. More... This is not a good example, but we have a box. We just bring the box and put it. Yeah. So we could not consider that. Yes, it could be interesting, yeah. Unless people put something on the microphone. Yes, sure. But... They won't have nice light lighting on in front of them. Yes, they will be. I think the most driven part is the other hardware that synchronize the microphones. Right. But I think after the meeting we can go and talk to Olivier about that. Yes, I'm not the person for hardware. So maybe you could present a different work you have done and I don't know how they are. Yeah, I'll try to present... I guess it corresponds to your questions. So you told us it was different pieces more than one integrated device? So the lowest layer would be the first two questions. You asked about how to detect... activity, currently you are thresholding the maximum of your waveform. Not maximum, the average on the packets. Okay. And you are looking for a peak? Yes, I take the... When I go up to the curve, I'm at a local maximum. Yeah. I take it, then the next local minimum taking this... Okay. What would be the difference between detecting the average or detecting the maximum? Yes. What would be the difference? What is the difference? The difference is that if I just... it's kind of filtering already. Yes. Yeah. And your second question was about FFT, what can be done with the FFT? Yes. So the approach we are taking here is to answer both questions. At the same time, we use FFT to do detection. We don't use the waveform, the row waveform. So you are FFT implementation? No. We split the signal in small frames, like 16 million seconds. Yes. 16. Okay. Each frame is taking 32 million seconds of signal, just to give an idea. Yeah. And there is one frame every 16 million seconds, and they overlap of 50%. Okay. So it's 32 millisecond frames, they can each... Yeah. Each 16 million seconds. Well, these are details, it's just to give... An idea. An idea. Something like that. So why are you overlapping? Well, it's... I have a rough idea, but... Because usually when you do FFT, you're getting into details, but you apply a window to avoid an idea as an effect. So the beginning and the end... Yeah, it's kind of crappy. It's not crappy, it's just not very much represented, because you apply a window which has this shape having window. So you use the first and... Last point to detect what is between. No, I'm saying when you take your signal, you take one frame, you apply the window. And the window is simply coefficients you give, and you give higher coefficients to the meter than to the extreme. So if you don't do overlap... You lose information. You will lose information at the beginning and the end. Okay. But you don't have to do overlap. I mean, it's... Just... Because we are using an open source framework to do this. There is something... I don't know if you know it. I call it Sphinx. That is doing... Sorry. Is it from CMU? Yes. Yeah. Doing voice recognition. Yeah. And I just use it for the framework, for the microphone and the acquisition. Right. So there is already an FFT module implemented in it. Yeah. I didn't test it. Yeah. But... FFT is only a tool. Yeah, sure. What we do really is a phase domain analysis. We only use the phase between the microphones. Yes, for the position. Yeah, but also for detection. Also for detection. Yeah. We have a way. So... I don't think I should get into details. But basically the beginning is your signal, which you slice in two frames. You do a FFT on each mic. And the end is a number of frequency bins, which are used by each person to explain roughly. So you are... So when you speak, speech is wide-banded. So the more active you are, the more bandwidth you use. So you will get a large value of bandwidth for people who speak. And for the others it will be random, it will be a small value. Corresponding to breath on detail noise. Or just background noise. Okay. So... Okay. But you are talking about the microphone array here. Yeah. Because one problem, if you use energy-based methods, which is probably all the things you have been using so far, is that it's not quite related to location. Nope. So you can have a... Strong background noise. For one person, you can have a high energy signal, which is difficult to locate and vice versa. I think in your case you are quite interested in the location. So I would advise to use more phase domain methods. So phase... That's something with FFTs. Once you have the FFT, for each frequency you have the magnitude. And the phase. So you can compare the phase of the microphones. And this is directly linked to the direction of the person. So the magnitude you don't really use. Sometimes we use it, but it's not the first thing we use. I know it's a bit contour and tutile, but it will be good to the bell. Yeah. I did a speech first, I think, cos. I'm not sure if they also learned that we are more used to phase than the magnitude. Right. For a single channel it's not very meaningful. But here it's a relative phase between the microphones. So I'll just try to... You have to detect an event in one microphone and know when it happened in the other one. No, that's what I don't do. Okay. This would be valid for speech recognition with a single channel. That would be a very good idea. But if you choose to use multiple channels... Let's say you have four of them. With FFT you can just, as I said, look at the phase between the microphones at a given frequency. So the phase, the time between audio signals for this microphone. It is linked to this value. Okay. Basically the time you're mentioning is this value. And this is the frequency. And this is an angle in radiance. Okay. So FFT gives you a measure of these values. This, I can point to a paper. I don't think this is the right place to explain everything. But we'll give you, if you look at your table, what we have developed here is an approach where you divide the space in sectors. For example, ten sectors around the table. And in each sector you will get a value. It's related to the number of microphones around. No. It's application dependent. For example... This is not... On this one you could have a large value on those one small values. These are number of frequencies which we estimate with several steps from these measures. So to conclude, what we are doing is we estimate how much of the frequency spectrum you are occupying when you speak. Or I am occupying in this direction when I speak. And it turns out that this is quite good to do detection and localization at the same time. Because you know in a sector of space there is this much activity. Okay, this is a measure of the activity. Yeah. Yeah. More recently... I've worked on... Prolongating this with the more precise direction evaluation to know where in the sector the person is. So that's not much work. Once you have done this, this can be done quite quickly. What happens if two persons are talking at the same time, just get two sectors that are... I use a value, but then you get two large values. Yeah. And do you get something about the distance of the guy that is talking? No, just direction. Just direction. There is a distance that's not very effective. Well, yeah, as you mentioned... It's an in-ear-run problem to the geometry you use. You can, for example, if you use another array, you can intersect lines of direction. Okay. And from this kind of information, you can, let's say, have a fingerprint of the user to know if this is the same user speaking. So maybe... I wouldn't trust it too much. Classically, what you do is you extract the signal of the person. One interesting thing is that these numbers are not arbitrary things. They represent the number of frequencies where a given person is dominant. So when you have two of them, which might be interesting for separation, you know already when you have done this processing, which part of the frequency of the spectrum belongs to this person and to that person. And then it's easy to separate the signals. You mean that each person will have a specific frequency because the way it's talking is localization? No, it's small there. So kind of... Both. No, it's small. It's more an instant use. This is still instant use. This is for one time frame. Yes, okay. If you look at all your frequencies, for example, 0 to 4 kilohertz, you can split it and say all these parts belong to person 1 here. And all the other parts belong to the other person. This is a bit arbitrary because perhaps person 2 will have some frequencies in the range of... That's correct. But then you can look statistically. It will be negligible because of the difference level. And if you reconstruct the stream from this range, you really get something that is... Yeah, you can listen to it here. And then you can do some higher level analysis where you get the pitch of the person. And not really get your last diagram. So it's at one given moment still. Yeah, yeah. And this is... So you can say that this frequency belongs more to this person and this one more to this one. Exactly. So if the same frequency may... maybe are not used neither by the guy 1 or the guy 2. Then it's random. And that's why you get these values which are random. And maybe they are using the same frequency. Yeah. So you need to... Yeah, but this is very rare. Okay. And when it happens, one is always masking the other in practice. Okay. So... And can you detect if someone is laughing or angry or there's kind of signature? Even if we can't really trust it. No, no, but 100% of the time. I've seen quite a few papers. I've not done it myself and really on the lowest level. But once you have done this, as I said, you can separate the signals. And do processing. Do processing. Okay. So you can get pitch, right of speech. That's quite easy. Pitch. I'll show you this afterwards. A tambre, for a second. Right. So for different person, it might be quite different. At least for male, female. And right of speech. If somebody is talking in a very energetic manner, it might be quite fast. Or just energy or so. Okay. This way you can filter perhaps some range that are just very low frequencies for perhaps noise. Yeah. That might be an issue if people are bringing laptops. Yes, the fan. They might be detected as another source. So you would have to classify the sources as human or machine. Okay. But again, once you see the spectrum, if it's just pure and stable. If you don't really detect the pitch, you can say that. Oh, so yeah. It does not reach. Yes. But this is very interesting because this way you can detect if someone is talking, if someone has some noise with perhaps putting his cup on the table. Yeah. Perhaps if there is a laptop in one position. So all will go and be detected there. And you can classify this. Yeah. Yes. So what you're mentioning is the pain for us. But for you it might be, because we are only interested in getting a speech. But for you it might be quite good. So I've tried to avoid filtering and smoothing. Because as soon as you do that, you exclude some of the information. Yes. So it's better to keep it for the latest stage. So on top of this, there's another part which is more linked to the tracking. All this was for one time frame. So in the length of the time frame as an influence of which parameters? Yeah. You can play with it. In speech, it's a stationary of 10, 20 minutes ago. And we make a stationary local stationary T assumption. Which allows you to use FFT and blah, blah, blah. Now in spite of that, I know some people use much longer windows, which is not a problem. And let's say if you use one second window, it's a bit too much I think. In fact the frame length will be very, very meaningful. I'll let you do this. Yes. Some very small worlds, like yes, there might be 200 milliseconds or 300 milliseconds. They won't be visible on the way. There might be blood with silence. Yes. So yes, you might have to play with it just to save processing time. Like you slightly longer frames. But the longest frame you would lose would be 100. Yeah, at most. At most. Okay. Simply because I've used 100 as a minimum length of speech word. Okay. Yeah. So. Now assuming you have done this for each time. If this is your data, I should use a different. If this is your azimuth, at each time you get a direction. What is azimuth? So azimuth is your direction in horizontal plane, to the angle, like north, south, east, west. So where you represent the phase? Yeah. No, no. This is really different. I'm just saying that once you have done this, for a given time frame, you can have a direction of the person. Okay. So at least in terms of sector. And it's also possible to give a more precise direction quite quickly. So this was kind of the lowest layer. Now this is a layer just above it, which might interest you. If you repeat that over time, you will see patterns. For example, at two different locations. Okay. So you can... Two different person will speak. Okay. So you can cluster those. One user, yes. And you will see that if it's long enough, it's some significant event. This can be done quite cheaply. Yes, before I forget it. Yeah. I have one question. You say you do the processing afterwards. Yeah. And what is the cost? Because just the length, one one or a bit longer. Well, I use Matlab, so it's not perfect. It's like three. This is what Matlab is very effective for matrices. Yeah, but not everything is simple linear. Okay. Especially this part, it's definitely not linear because you're looking at the maximum energy in the frequency. Kind of. The most expensive part is here. With Matlab, I have three, four times or eight times. Okay. You can do sub-optimal processing. And for example, here I'm considering all possible pairs of microphones, which is 28. And the processing is directly relative, proportional to that. So you can save on that, but then you lose a little bit on precision. So I'm using all these small frames with 50% overlap. I don't think you really need absolutely to do that. And even if you use four microphones, it's not that good in terms of direction. Okay. Yeah, I would be careful with that. Okay. Like five, six is decent. Six, I would say. And eight is good. Eight, yeah, I get down now to one, two degrees, root mean square, error. So. Yes. You might not need that. That's what I mean. Might not be. Yes, one, two degrees. Very, very small for our application because we really want to detect if this is the same person that is speaking or not. If there is a error of, let's say, 10, 15 degrees, but it is capable of seeing it is two different person. There is not problems about it. Yeah. So it could be sufficient to define enough sectors. Okay. And the number of sectors is not dependent on the number of microphones. No. So it's arbitrary. I use 20 degree sectors because I had to choose a value, but it's up to you. Otherwise, it could also just rely on the frequency band separation detected way, the different persons. Yeah. Yeah. Then you don't really know where around the table is, but there. Two persons in the same sector, you see. Well, it depends on how you do it, but if you place the microphones. So that's another way. Yeah. It's almost like having a lapel. Yeah. That way you can separate where voice louder or loud. It's not necessarily a bad idea, actually. It's quite a different thing and you don't really get an ankle, but... Well, if you are able to calibrate your microphones, the ones who are closer to the person who will get more energy. So you can compare that. It's possible. I think it depends a bit what you want. Yeah. I have experience with that. On the side I've done some single channel work, which can maybe help. Because it seems to me that you are not definite on geometry, right? No. Geometry is not... we have not thought a lot about it. This part is quite specific to a microphone array, where they are concentrated in some place, like in the middle of the table. Yes. Because if we are distributing the microphone all around the table, then we come back to the energy solution. Yeah. But you can kind of come back to that also. With the energy solution, or with the FFT too? FFT or so. Yeah. But I think what is very important here is the synchronization of the microphones. Right. Yeah. It has to be as good as possible. I'm speaking about it because we wanted to do this table in a mid-tech way. It's a cheap way. Yeah. You have not very expensive tables. So I don't know if what I've seen on your website was that this installation was a bit expensive. But I'm training to do many, many things. To see what is very important and what is not... You can also consider a directional microphone. Yes. You know... no. So you can speak between the pattern is not the same depending on the direction where the person is. So for example, if there was a directional microphone pointing there, you would get most of my energy but less from you. Sure. You can also do microphone arrays with directional microphones. Yes. I think you might not be relevant, but... That would be the difference. You will not compare each mic with all the other ones. But just comparing this one with the... More or less. It depends on what you want. I mean... What are the other approach? Yeah. So what are these mic, for instance? Electric mic? Yeah. Unidirectional. Electric? Okay. I guess so. These are... these are some kinds of... It's the same. Exactly the same. Yeah. Expensive? Yeah. Yeah. Quite expensive. Can you say now? Because the quality of the... Yeah. There are high quality mics. And they are plugging in an excellent... I think we can leave this question for Olivier. Yes. Okay. For the... And when you were talking about comparing energy, in the case where a microphone would be distributed all around the table, you're comparing energy and the time delay? No. No. You would... Because the time delay is too short to be compared or can be neglected. You would neglect it. The time delay is very small. It's only used for getting the direction when you have a microfenor√©. Okay. But in that case, it's not relevant. You would assume that they are roughly synchronized, not necessarily very precisely, and compared the energy levels. But I have no experience with that. Okay. Yeah. No, I don't know. You don't want to use... It's almost... Okay. Let's have a break. Yes. Thanks. Cooler. I wanna say that on any individual part. I have to open this Cheeseotherado postglue with cold oil, It's simple, it actually is also nicely Signs clicked.