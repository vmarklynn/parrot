 It's with cameras! It looks quite funny! This is the first look lake... I saw that... Nice. That's the way I see it. Yeah. Yeah, yes. You see, I never get this. What's the point of this is famous? If this trap is supposed to go behind, I'm obviously doing this wrong. Obviously failing this, like, I said, what a calling center. If you have a... Going behind your ears. Like this. Yeah. Which is good. Yeah, you know what this shit has to do now. Yeah. Okay. Okay. Yeah, so, um, let's just realize that I don't have power. Let me just switch on here. I know what you're going to run out. Okay. Okay. So... There's probably not much to talk about at the moment in terms of, like, talking about each other's stuff. I mean, beyond what we've been talking about yesterday. Yes, too. So what I've been thinking is maybe if we try to really make sense together of the XML format, to be sure that we can all produce the data that we are producing in a way, or at least, I mean... I guess my data will probably load in a completely different way anyway because it's matrix, but all the stuff that goes to the notation that we have in the right format, maybe if you just sort of pool what we know about the XML format and try to make sense of it. I don't at all know how to go about this. What I did just now is I downloaded... I downloaded some files, some XML files. And I was thinking if we maybe just should go through some of them and, like, try to see if you understand the general structure and pull together what we think about the structure, what we think the things mean, and then talk about how the annotation for information density and stuff should maybe be structured in a similar way. But I don't really have an idea like what otherwise what we would do today in this meeting. Yeah, you haven't got that much to talk about. Just maybe talk about how you would give your data file. Yeah. Yes, what's that? What's that part? That's the interface between having the topic segment and calculating the information and port and support. So I would need separate files for each segment or just maybe have delimitres in between. Yeah, well, that's the way it works. It's called delimitres and what the boundary is about. Yeah, if you have to do that. And you send you need a label for each segment. No, I don't. You don't need the one. I could give you an ID, but you've got a type of topic title. I just need a title from the file. It doesn't matter what it is. Okay. Yeah, that's no problem. Yes, so does anyone have a good idea where to start? Like which what? So what is it we're talking about getting in at the moment? We're basically just because the two of you will merge data together, right? In some way, like in the end, it will result in one annotation. So the only thing we're trying to tie in is just an additional annotation which is similar to the annotation about what would be most similar to simply to the segmentation information. Yes, I mean they're all very, very similar. Oh, that was ex- I had a look at the, for example, A's. See, that's what I was thinking. I'm not sure what software I have at the moment. Hello? Anyone know any good standard software that displays XML? Sure. Take Mozilla? Yeah, but I'm afraid only Firefox and Firefox doesn't have an i6 node view anymore. I think I don't have Mozilla. I think I'll just do a next thing. So what we have done when we are mixing our values together, is it a value for each expression for each sentence or something? Yeah, I think that would be, I mean, for me it's quite difficult to say on, you know, what scope one annotation would have. I think it would be certainly more than one word and I think would work best with a trans or a sequence. Yeah, I would just end up with values for each word, so I wouldn't have any boundaries for segments at all. I just would have the words and then how we, how long each expression would be, I don't know. But going from a work business wouldn't it be to then go back and calculate for each other and? Yeah, but what is an utterance? The segments as they are segmented and for example, what we would fit in into the XML file would be a value for each utterance. I think so. And you would probably still need your values for the words and perhaps for the keywords that are displayed when you pick up something. So now we would have to fit in more than just one value. Yeah, but that's another representation then I think for the importance. Does anyone know in which file the actual, what's it called, the splitting into the utterance? It's not segmented. It's called dialogue. No, it's not. For example, there's BDB001C point. What does it end with sex? Does that look like it? I can't see anything. I think that's it. You've all got an ID for each utterance. That's actually just a very K-right. It's a bit better to see, I guess, a white white white white white. It's super cool. Okay. So, okay, so, here is for this segment. It's really all segments. It can be words or topics or anything, I think, in these. That's why it's, for example, time provenance. Time provenance segment or time provenance. Yeah, let's go to different files. What is this? This doesn't contain any content like any text. No, it just points to the words. Oh, wait, somebody explained it slowly. So, yeah, some of them point to the words, other points to the dialogue acts. So, that's kind of the noble thing that ties together. What do all these things in this file have in common? What are they? Yeah, they have their night ID at the time. So, they are segments in what sense, what's the... The other answer is, but it also includes things like... The segment is what is displayed in one line. And that's usually dialogue. What's the difference between the words? Yeah, it looks... the segment thing looks into the words. I think it's... In a different... Dialogue act annotations of dialogue act. Is that just another representation of text? No, they're annotation of dialogue acts. And one segment can have one of those word strings that are presented in one line can have several dialogue acts annotated on it. That's why... Just so you can get these files somewhere. So, the stuff that's called segment here, do we know which file this is pulling from? From words, XML is that what it's saying here? Let me see if I have that word, section of file. So, let's, for example, look at... Do we need a value label for each of those segments that are not dialogue acts? Well, although it's their point to the actual words... Let me just try to get the more different screen. Just, I'm trying to arrange them so that we have one above the other, so to make some sense of that. So, not here we had. And I think these segments were supposed to not exactly what we were looking at, because that's just one tying all the others together. And the information, or I mean we're going to create a file that looks more like the words file. I think... Well, I'm just trying to understand what the segment's file is about. Like, it's referencing here, it's referencing to a certain position in the words file. Yeah, it looks cool. So, is that non-vocal sound one here? That's the mic noise, okay? But does it always just... No, it's referencing to several words. So, this would be basically be a sequence, right? Yes. So, that's from W52. Are they not ordered? Yeah. W52 down to W, what are we talking about? Yeah. To this course marker, this F marker, 0.5. Yeah, that's... No, no, up above. Yeah, right. Okay. So, this is somebody saying it doesn't. If we have any confirmation, do we actually see from this file who's doing it just in interest? This is all speaker D. Oh, sorry, in speaker D's file. So, this is where he, like, time stops. And then it only starts at a later time because, sorry, you don't see when I'm pointing at screen interest. So, then it starts here. This time I can speak a D because in between there's somebody else. Yes. You can see whether there's somebody else or not. You can see that in the W point. First it's W point 54 and then it's 76. So, there's a number of addresses of others because of me too. Okay. So, the file with annotations for information density, what would that be like? Would that be like the segments file giving a start, word and end, word or... Or would that... How would that be to best tie in with the system? I think we would also need two of those. Sorry. Yeah, I think we would have to have the same structure, one that points to it where we tie it together all our information. So, we would have to have to... Something like this or where you give sort of a reference to a beginning and an end position. Yes. And then another one that would perhaps give the actual probability value. Or would that be in the same file, wouldn't it would just... Yeah, it could be like... Probably in the same file. I guess we could introduce a different... If you have several layers, then... Kind of within the modern same file. Well, for now, if we just do something which for every segment in here attaches... Actually, can we not tie it to the segments file? Would that not be the way how they would want it to have to do as if we... In the end, you want to attach to each of those segments one number for now. Do they have an idea? Yeah, I guess they do. So, this is the idea for specific segment, right? In segment point one, I'm not a landlord. Yeah, so, wouldn't it... I mean, the problem is they may have looked at the exact inner workings of the engineer, but you'd think that the easiest way, the way how it's intended to be would be just... If we have here a link to the segment, like an ID for that segment, that we just create another file which links to the segment and then have an additional value, which is the number. So, you mean an attribute? Yeah, yeah. Yeah, that's what I'm gonna do. Yeah, sorry. I thought you were talking about having two files. I was thinking... Yeah, I think I understood wrong. I thought you were wanting to have two different XML files. We want the reference and one just the number. I was thinking that's probably what you meant just having, like for each sort of hour segment, having just the ID, which is referencing to these segments here, and another attribute which is the value. So, this whole information we would then store in this... That I don't have access to, because I didn't download it like this. This one meta information file where it describes the structure of all the files and describes which attributes they bring in. So, we would add that to that file, saying that sort of we bring in information density, and then we would create a file of that type which we probably couldn't call it segment. I'm not sure. We probably might have to have a different word for it. We would definitely travel with it repeating. Wouldn't it be easiest to just incorporate a new attribute in this file? In the existing segments file. Yeah, I know that's probably not very nice, but would be quite easy. Yeah, I reckon actually if you make a copy of it. But, I mean, there's not all the information that is in the corpus in the segment file. I think that is just the things that are loaded every time, but we have lazy loading, so it can load more than that. First we could just try to... Now to make our file one of those lazy loaded, but I have no idea how that works. What are you saying about the... Doesn't the lazy loading apply to everything? I mean, that is sort of dynamic loads. Yes, but it has a basic thing that it loads, I think. So, for example, every time it loads the segment things, it can't. It can't not display words, I think. I think it loads the whole of the segments file every time. I think so. That's how I understood it. The segment is split up. Because otherwise in the other things, there's no information about what the participant name is and so on. So, in the segments, it's really very basic that have to be displayed for anything. But they display the segments in... Or that the utterances in their user interface as well, they display the words I displayed. So, they have... Their utterances displayed in their interface. So, they access probably this file and then they display the words. So, why shouldn't they be able to do that? I think it's too early to really discuss that in detail because we don't all understand at the moment how the internal data structure, like how the loading works. Maybe if we go ahead, do you think it would be possible for you to do something like segments? Or maybe just a copy of segments which has an attribute for each segment, which is with a value, with a density value. Yeah, I mean, it wouldn't be difficult to create a file of a red shape. Would it be easier because all your methods are sort of not working with their whole time frame structure there? So, would it be easy for you to tie the things together? Like, if you're doing it on the word basis, here are those words that in the end, you then tie it back into the right segment here. I mean, probably have to do a bit. Yeah, I don't know how about you with your words. But for my segment with the F0 measurements and all those ways I get from there, I always store the big start and end time with everything I calculated there. So, I would just have to put it up. You're doing a time-based development. Yes. So, this isn't directly having time references, but you can get that, oh, it is actually here. Okay. So, if you slice it up by time, you'd probably be able to just like attach, like, just some attribute of info file just to each of those segments. Yeah, actually, but actually my segments are not always the same as their segments, because in their segments, they're a process sometimes. You can just use those ones blank. Yes, but yeah, I mean, one segment of theirs is sometimes two segments of mine. That's just what they meant. Okay. I don't understand enough of the data structures. It's probably would be a lot easier. Would I really understand how they are handling the data internally? Because then I could say, oh, it's easy to just tie it in. If you just have a time stamp, just reference by word, stuff. But at the moment. So, you're doing a word-by-word phase. Yeah, the problem is probably that I extract all the words, and then I don't use an idea or something for it, so it would be difficult to write it back to the right position. But every word, does the word picking, like, does it always have the same information value in your thing? Does it depend on its position? It depends on the position. You're doing this via some software that's like external software, so you can't. I could use another different approach. So, if you get the results from that software, and you go back over it, then with that file sort of, you write an algorithm which then goes back, because they're in the right order still and stuff. So, that shouldn't be a problem. But this is by speaker here, which makes it slightly more difficult. The problem is that actually NightX may provide a lot of the tools that you need to do there. Yeah, the question I made a file that contains just the start times. It could contain end times and words for all speakers of all meetings. I mean, that's just the same thing as I gave to you, but just playing also started end times of every word. So, you could perhaps use it to match with your additional information. What did you use to make that file? That's good. You didn't use our next file password. No, no. Okay. What do you think it would make sense to just take files by speaker? It doesn't matter what input I give to Rainbow. So, I just could use the files as they are. I don't know if that's... The information density algorithm still makes sense if you split them up because I mean the whole thing is that you look at the frequency of a word in that specific how often a word occurs in a certain topic versus how of a middle course over the whole corpus that from that it calculates. No, I think it's not the whole corpus. It's... You have certain categories. And you measure which words have the highest information for one category. So, it's the categories across each other. But would you then not get the typical words for every speaker? If you... Probably, yeah. But because it's more than good ideas. I mean, it can't be that difficult. If you already have in the right order all the words with a score to them, you have a file which has each word and a timestamp. So, those two types together have each word and it's time and its value. Yeah. What the problem is, I don't know, I think the information game in Rainbow is ordered by the value of the information game not... I am not sure if I can get the right order and the values. That's the problem. So, they are at the moment. If the order stays the same, it's no problem at all to just write back again. But if it's ordered by information game, I don't know where the words come from because it has a back of words for presentation. The person with the rainbow was made for something quite different. Yes, it is. It was made for text classification. So, how could we put our information again? Yeah, that's the problem. I actually like... I'm... I actually sure that Rainbow is doing a measure... Like, is returning a measure of what we are trying to measure because it seems to me that it sounds like something quite different in my aspects. Yeah, what it actually does is that you put in some documents and you have several documents per category and you have several categories. And then it measures which words are typical for a certain category. And if you get a new document, it will compare which words are in that new document. And if there are a lot of words that are typical for one category, it will assign it to that category. And if it's typical for another one, it will assign it to that one. But where do you have the original category information from? Yeah, it's because you have different files. If this is your directory, you have a diagram, a directory 1, 2, and 3. And this represents the category. Everything that's in there is in category 1. So, where do you have them from at the moment? They split up? Yeah, just split it up. So, that's the topic. Oh, yeah, that's the topic information. In the topics, in their human topic. So, you've split it up by topic at the moment. No, I've just split them up somehow by... There are several documents for each meeting. So, it's not very sensible at the moment because I'm waiting for the topic statement. Okay, okay. I was just thinking that doesn't make sense at all, but yeah, if that's just while you're waiting. Okay. Have you ever looked into different ways of calculating? Because I was just thinking, I mean, for example, the info, what's it called? The entropy calculation. Is that your box on that simple calculation that you could probably write the script in no time at all? Maybe it's better if I write it myself because otherwise it's too easy to just split things up into bins. It wouldn't be any work at all in terms of programming or something. Yeah, I was just like, I think that probably the entropy value at the moment for a word is closer to what we're at moment looking for. I can just like, I can sit together with you for 20 minutes and just show you the entropy code that I wrote for my other project. And it's probably, we should work together because we have to use the same matrix as I'm using in my latent semantic analysis. You'd use to calculate entropy scores. And then we'd have a score which actually, which would be the same for the word in each position. So in that sense, it's doing something a bit different. And like, basically the score that I'm talking about is a conditional entropy score which just checks how much information. And the fact that there's one word tells you about what would be the next word. But that's a relatively good measure of whether that's a very specific word, in which case, they usually words would tell you quite a lot or a very general word which usually doesn't tell you quite a lot. How do the calculate that actually? It's basically the standard entropy formula. So how sure and sure you are about what's following of what your context is? Yes, yes, and I think the official description before it tells you is how much that the fact that a given word occurs tells you about what's the next word is going to be. Which doesn't sound too exciting, but it just works out in the way that words which are promiscuous and which occur with everything all over the place. And also usually end up being the words which are least expressive and contain less information or a function word. Yeah, a function word such as very general nouns, which is probably like for example the word computer in that context. You could imagine it to like being all in all the contexts. Do we have enough data for that it gives us sensitive things because I mean the words we're really looking for appear not too often and if they appear five times in the meeting and they have each time different. Yes, so we wouldn't do it five words. Sorry, okay, I was I was getting that actually sorry I was getting that wrong I was getting it from what I did my project now in this case, we would do it by per me words per meeting. Yeah, that's almost similar to what I'm doing because words that are in every class that are not very informative, but which that only one class are very informative for that class. Yes, it's probably doing it's probably doing quite the same thing in the end. I'm just saying like with that thing you would easily have an algorithm which at the moment provides you for each word with a score which we can use. Now, I was I was describing the wrong thing in this case we wouldn't be doing how much it tells you about another word in this case we would be doing given that you know a word how good is it predicting from which a specific topic that was. So that would yeah, and that's the same thing. Yeah, I bet both give the same value for one word every time I mean for specific word. Yeah, yeah, first then it would be quite easy to integrate it. Yes, the same word. The word yesterday would be would have the same score all over the place. But in that category with the same value at each place. So, the category thing depends on that we not just have topic segments but also that these topic segments we have them in categories. That would be best. Will it work without that at all. Works with several categories with each word on the limit each. Yeah, I mean, so in our case basically every every topic would be its own category. The question is does the algorithm still make any sense. I don't understand the algorithm enough for that. Because the entry because I am so simple. Maybe we should look into making that score just as a preliminary score that we have. It's a very it gives you like I've looked at the result it gives you basically something in the end which vaguely tells you just whether words are very specific word. Or a very general word. And I like there is some hope that probably having just sentences with us lots of very specific words if you mark them as being more interesting than the words which are only very general words that they would get us somewhere. And what or what score would work get that just across once in all the corpus for example. Probably 1.0 is very high information value. Even though there are all the other same topics where doesn't occur. Yeah, because this would. Yeah, in a sense, I mean, this is a bit like the what like document frequency over total frequency measure is sort of just going by. So would it be higher scored than a word that occurs in every sequence of the same topic. Or do you mean every sequence is the same. I mean, we have several topics. For example, we have 20 topics and for one of these topics, there are 5 occurrences. And within within the topics are like topic, when you say topic, you mean like just like from from a beginning to end point like within one meeting, there are several topics. Yes, but across meetings, there will be the same topic several times. But we don't have that information anywhere. Okay. I thought that was what you were doing. Well, when it splits the topics up, it does do it on regular words that. Yeah, but that doesn't tell you what they are. But also segment similarity, I think that was. I'm doing one on segment similarity in the end. Yeah, I'm doing like finding similar segments based on semantic analysis. But like for now, like your segmentation is just splitting a meeting up into different blocks. And they are not related then to our blocks of. Not from what columns to what I know. No, it's only that I'm writing an algorithm with it, which then tries to also again based on word occurrence patterns try to link together. Maybe different ones of those. Okay, so yeah, how would it work? I mean, what would his information then be useful for in Rainbow? I thought the point about that was that we would put into several categories of the segments of the same topic. Something else that we just split, I need somewhere to split and that splitting a topic boundary is a nice thing to do other than just splitting somewhere. Yeah, suppose it's quite an entity. Yeah, I'm also a bit like I'm not 100% sure about Rainbow being the right thing because it seems that Rainbow does in its structure quite rely on having different examples of the same categories. Yeah, but I think it should work for just one document because it compares between the categories and if you just have one document, it still can find out which words are informative for that category which are not. So if you have just one document in each category and there are a lot of occurrences of the weather in each one, so this will be not very informative across the categories. So it should work for one, but I'm not sure how exactly it calculates everything. I think it's not possible to look that up. You know what, as a byproduct of my LSA, I'll provide a vocabulary like sort of a dictionary which for each word gives an entropy score, entropy score, which just tells you of how much information the presence of a word tells you about which topic it is, not which category like I'm not lumping together separate topic segments into categories, but just like how much this word tells you about how likely that with the occurrence of that word makes it that it's a specific segment, topic segment, which is some measure already of how widespread this word is versus how specific to certain segments that is. And I'll just provide that because that's just not much more work than just the usual thing and then we can see how we can tie that in with the other stuff. So if you keep on working on rainbow meanwhile and try to find a way how to tie your rainbow stuff into some way that we can attach it to a certain time segment. Maybe it's possible to have a list that is ordered by. Just thinking, is each word completely unique, like sort of does it treat each word, each occurrence of words, a completely unique event or does it, I mean, no, it has to mean, basically the form of the word is important, right? We can't just replace the word pen arbitrary string. Because it looks if the same word occurs again and stuff. Yeah, it has to work. I think it's a stupid question. It has to work on the word. I was thinking whether if we replace the word by something uniquely identifiable, then it would make the difference which order it is, but that wouldn't work because it needs the word because that's all it's working on. It's the word and it looks if that word occurs again and versus how often that word occurs in other contexts. So we can't attach some type of information to the word, just to the word string itself, making it underscore and making the time or something that wouldn't work. You can use some kind of truncation. Maybe if you attach a number to each word and say that it should omit the last part. If you wanted to have that in the untrancated version, then would the output be the untrancated version? Probably not. No, I don't think it would. Yeah, for now, I mean really just like, yeah, I think if we work together on an entropy-based score, let me see if I can demonstrate. Let's just keep on talking me and I'll try to start that up. It's a very simple thing, but it basically just does something which tells you how specific a word is. In a sense, it's basically just to a high degree really telling you how rare a word is or how common a word is. But as the first step, that's probably for a prototype for next week, that's probably not a bad thing. I mean, if it's just that, if you have segments where lots of rare words occur, highlighted in darker red, then segments where all the very common words occur, that's just that somewhere to start from. And it's a bit more sophisticated in that, but then the factor just ends up doing that, obviously, from what I figured out. Actually, I think I'm not going to start that now. That's probably the best way to take too long. So, if we get it on a word-by-word basis, whatever you do, it'll probably appear on a word-by-word basis, what you hit us on a sort of segment but not quite segment-based. Yes. Yeah, I mean, I haven't really decided on how to really get the information out of this now because when I have, for example, the increased speaker overlap that applies to several terms, of course. But I could give the information about how important this is to each of our segments. Okay, what about the following model? I mean, this is a very unscientific way of doing it in some sense, but what if we take time as the standard unit for now and sort of like make a massive one-second split super array because everything you're doing can in one way or the other be tied down to actual time. So, if for each one second time slot, we could attach a value. And then it would be easy to then go back if we have the time marks here and remap that on to the length of a segment. You know what I mean? So, if you have, for each word, say, and we know that word starts at this second and ends at that second, and you have for some time period, the overlap in that period or the F1s in that period or something. So, you also have a value which can be tied down to a time. And then we could just, in MATLAB or in something, just create some massive super array of things for each fridge, like sort of time sampling slot and calculate the value for this. And once we have this array, we can go with the script and sort of go for each segment to the starting and end times and say, okay, this is from our time segment here to this time segment there. So, we take the sum of those and divide them by the number of something, create the average and put it in as the value for the segment. You know what I mean? But how can we get to know what makes sense as a function for joining everything together? Oh, that's a fiddling plane. The end for that problem we'll always have. Because we don't have a useful way of automatically evaluating... You don't have a way of fusically evaluating automatically what's good and what's bad. So, it's probably always going to be a question of looking at it and saying, okay, running it with different factor loadings and saying, okay, this way works that well. First, what you proposed? We should break that down to have the smallest unity of time duration should be one word. I mean, that would be because that would be what naturally came out of her thing. But it would probably be more difficult for map to map for you to map it on. Yeah, sure. But I could then, if I have the value for a segment, I would perhaps just give all the important words. You could always find out how many words there are. So you say that having an array where each cell is one word, and then you would map your information on two individual words. Yeah, someone would have to break it down. I would probably somehow have to break it down to whatever. Would you be able to find out which word that is? Yes, sure. Okay, and then we could have some type of point in the end where the one scores from all the individual word cells get multiplied all with, like, for each utterance or whatever you have, get all multiplied with the same value, all the ones that are within that utterance, that's sort of the combination of the two scores. Okay, so we'll just try to check that out. And then we'd have to go back again and then put that back into that segment mode here. So that we, because in the end we don't want it on a per word basis, but probably on a per segment base. Yeah, then, okay, but at that moment it would be better for me to just make it on a per segment basis right away and that's where I adapt it also to do. Oh yeah, actually that's true so that it's easier if you are able to, I think that's probably back to where we started with this. But you said it's more complicated because your segments aren't those segments exactly. Yeah, I mean mine are only more, but they have the same stud and point. Sometimes there are two segments and one with the gap in between that and they have, they don't overlap. And their segments do overlap. No, no, they don't, but those two of those don't. I mean, my segments overlap in the same way as there are two, but sometimes I have split one of the segments into two segments, but it's easy to match the start and end times. So you could, on their granularity, you could on their granularity create a score for each of their segments. Well, I guess I mean for you, if you know for each word, if you find that out, then it has to be possible because if you know sort of this is going from word to word or if this is going from time to time and then there has to be a way then for you say, okay, this concerns these words and then just make a simple mean over them. I think an interesting thing is if we don't combine your two scores in the XML file yet, but if we do that in the software, then we can probably make ways of playing with it in the software and sort of, you know, like, adapting some sort of control, like playing around with different weightings for the utterance based one versus the word based one and sort of look at it dynamically. You know what I mean? Like, playing around sort of figuring out what's the best way of combining them by playing around looking at the results. We could look at different measures if they come up with the same kind of... Yeah, we could probably make a graphical display initially at least for experimenting, which displays them in different ways and then see how they interact with each other. Yeah, I mean, that's what I'm already doing with my... Okay, that's it. So, do you think, like, both of you then can map something onto their segments? Just each of you provide one value, like double value or whatever, like one decimal value or whatever onto exactly their segments? I think if I can provide something for the words as they are, it's stated from where to where the segments go, which is it? It's stated both... It's stated even which words it is. It's done both in terms of words and in terms of sections. It's a bit sad sort of that we do this before we've truly figured out how the night egg smell works because now we're doing it all by hand and, like, parsing and unpartsing and it's all part of the framework. Yeah. How much easier would it be if we truly understood this? Well, it should be quite straightforward similar to us. It's good. You know, and really understands that I do what I smell. Parsing. I might just change my order of which I do things and, like, forget my latent smell analysis stuff until the weekend and try to really make sense of the night data system now so that maybe as soon as I've understood that we find ways of doing that within the night framework already so that we don't manually have to parse times and entire things together. You mean by matching strings? Yeah. Well, at the moment what you would do, like, to solve this problem is you would sort of, like, write some Perl script or something that gets this time value out of here. What I've done is parse it and make some out of the way you can get the start times. Okay. So that's quite easy because it's an attribution. You just say that you want values of those attributes. So provided that you get your words in the right order, do you think it is an easy task for you? It's a relatively feasible task for you to get just a single value per segment. I don't know how long it takes me. Okay. And you say you think you're able as well to map onto those segments. Yeah. And if you're both able to map onto those segments, then we should be able to get one file where we have, like, whatever two values value, A and value B, both as attributes for. So, for this. And that we could load into a prototype and see what types of ways of displaying this information are there. Yeah. I hope to have some value quite soon that I just work. I mean, I just calculated the values for the average of the rules. And they're what's, I mean, I didn't have a look at the data yet, but they vary quite widely, even for the same speaker cross meetings. So, one speaker had an average of about 100, one meeting and 160, another meeting that's really strange. I mean, perhaps it's because of laughter or something. That's what I was thinking, but I didn't have. There's probably also social interaction factors in that they sometimes just a meeting, like, if people adapt their series to each other, then they sometimes. Yeah, that's what I read in one of those papers. Yeah, that could mess up the square, because it would be. Can you do something like just like not measuring the F0, the absolute robot, like the variance of F0 within a certain timeframe and like sort of like just have some part where it's very low variance with a more or less talking the same F0 and one where there's a lot of more variance. I don't know about that. Yeah, but all this is quite, you know, data-intensive. When I am led, they just calculate the average F0 levels. I think it made it more than half an hour, considering more than half an hour to do it for our meetings. Yeah, we have 75 hours per average of six speakers, and they measure every 0.016 seconds, and that gives us quite a lot of variance. Okay. So at the moment, you're just measuring the F0's relative to the average for the speaker. Yeah, what I did at the moment is, I got the F0 values from for each speaker for his headphone, and I only take those that were required at the time where he was actually speaking. So I calculated the average. So that gave me now one value per speaker per meeting. Okay. So what you would be feeding in would be just like one value per speaker per meeting. No, no, that's just the average. That's your average baseline. Okay. So that would show you how much relative to how he's performing generally in that meeting relative to that, how he's in a specific segment. Yes, I mean, I just needed to have this value now to relate. Yeah. Yeah. And for you, that would be quite easy to relate to those segments. Yeah, I guess I passed in 2015. It's after I'm not able to take it. I'm going to take that and I'll maybe take it after after. I mean, I actually have to look at the data what causes these, because it's quite funny to have a male speaking at 200. Oh, they do exist. Yeah, sure. The castra, the core of the international computer science institute. I'm afraid I have to go soon. No, I'm not quite sure I'm interested in anything more. We have to talk about it anyway. Maybe we need the weekend. Yeah. See, as soon as I'm halfway through my LSA, basically, as soon as I have the matrix built of the document by Word stuff, it's very easy to then calculate for each word a score. And I can just give you those scores and you can do them whatever you want. So, how about the prototype? If we want to show him something on Monday, we definitely have to work together. Some of us at least have to work together to get her running. Yeah, I think Colin Dave and me will actually work on the Java stuff in it. And we'll just see whatever you would, whatever you'd supply us, we'll try to tie in and visualize in some way or another. I'll ask Jonathan if we can postpone the meeting to one o'clock. So that would give us a chance of meeting for an hour before that to discuss the questions that we had. Yeah, you just send us an email. What's happening? Yeah, I haven't gotten my confirmation that Wednesday is fine, I'm not sure if I'm supposed to expect a confirmation from my confirmation from him now. But I'll just email him again and ask him if we can maybe make it one, sorry, reset 12 and I'm asking if we can make one. This is a bit frustrating at the moment, as much as possible, it is so difficult to get to the point where you understand enough to really feel that. I'm not feeling that I'm really working at the moment, I'm not just trying to make sense. It's a bit too far into the meeting for that. And I'm at the moment feeling around with my data, I'm not quite seeing how I get to a sense of abstraction level. From my very... I guess as soon as we have a framework in the type of the prototype where each of us can tie in their stuff and see how it looks like and how it performs. It probably makes it a lot easier then, but if it's a bootstrapping problem, for the prototype we need some type of data, but to develop the data it would be a lot easier to have the prototype. Anyway, I've got to go. So I mean, you will be happy with some data even if it doesn't make much sense. Yes, but if it's in a form which is easy to read in at the moment, that would be fine. Basically, if we have something like this segments file, but for each of you, just have one attribute, I think it's really easy if we don't merge them beforehand, but if we let them... If we combine them in the prototype or don't at the moment, because then we can easily display them individually, contrast them to each other and play around with how to combine them. And also we don't have to recalculate it. Just one. Yeah. And I mean, computationally multiplying two integers or doubles or whatever shouldn't be the thing that... If it's already on segment space, that's not too much. Yeah. Alright.