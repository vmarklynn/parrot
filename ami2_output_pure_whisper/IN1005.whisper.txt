 I'm going to set this round to fit these in. So, is it correct or not? You never know whether you've got these things on right or not? You never know whether they're on right or not? Yeah, well, so we hope. It feels wrong. Okay, so do it like that. Okay, so David came in to me to say something we'd been talking about for a while was some stuff I'd been working on about using these topic models with networks, with things with links between the documents that are linked. It's actually related to Ripple, but it's not actually that this is something I haven't actually been working on for a couple of months now, just because it came up with something more interesting that I've been working on. At the time, basically, I'd been trying to use PLSA and those types of models on web pages. And it came up with a big problem in that most web pages have very, very little text on them. But you can tell what they're about based on the other text on the site, the text and the things they link to. So what I wanted to know was can you combine the topic models that describe the pages that have lots of text on them with something else that tells you what a page is, no or very little text on it based on the links. Yeah, so quite simply if you have a page one, which is like a home page and it'll maybe have five links and a picture on it. And then because it links to page two and page three, can you tell what this is about? Yeah. So I started looking at Google PageRank, which kind of does this by saying where the links go the other way. It says if this page is about fish, or in Google they say if this page is good in some way and it links to this other page and we don't know what this is, then that page is more likely to be good. And it's just page rank, just a simple algorithm for doing this over a whole big network. So I wanted to combine the topic models that tell you beforehand what you think this page and this page are about with this to maybe get a better idea. So the idea is, say you've got some text on a page, that's only part of the picture of what the page is really about and the links pointing to that page are also part of the picture. So I basically came up with an algorithm that's just a plugs page rank into PLSA. So page rank you basically just say you basically take the how good this page is, which is X and how good this page is, which is Y. And you then see this page is based on how you do this. Based on the links command to it, it's just like a sum over all the links of X, I guess, of the value. But it's not... So if this is page rank of X, page rank of Y, I think this is roughly it, or fan, page rank of Z. So you say page rank of Z is equal to one over a constant to normalize it, sum over all the links into Z of page rank of the link. Yeah, it's like that, yeah? So similar idea. If you've got a vector that says what the topics are, so you've learned from PLSA distribution of topics for every page. So if you say probability of topic one for page Z is equal to... And you can use the same sort of thing. So nicking a whole bunch of math from how you work out page rank, it comes up with just an iterative algorithm, which is just the current probability. Probability of topic one given Z. So it's probably old. Plus... Yeah, plus that, and then you normalize between them, so you say alpha and one minus alpha. Yeah. So basically what you're seeing here, one minus alpha say link contribution, and call this, I guess. So what you're seeing is that a page is partly from its text that you've got on the page, and it's partly from what the link say it's about. So the alpha thing is how much do you trust the text on the page, and how much do you trust the links. So that alpha could be different for every page. So what I did then was try this on a big database of web pages. I had some results from that, but they were all on temp too, so that's not important right now. But it's not really important. I can regenerate that. The problem was that this gives you a new vector of topics for every page. So you've got two vectors, you've got the one that you just got from PLSC or whatever model, and you've got the one that you get from this. How do you tell questions better? I had no way to evaluate this well. How do you tell that this page, because you don't know what the topics are, because they're learned automatically. So you have two representations, one which comes from this link stuff, and one is not based on PLSC too? It is, because... It's an initialization. Yeah, you initialize it with... Actually, this is not old, but start. This is how you start off. But then you just iterate. So this is what PLSC goes into start with, and then you iterate this and it becomes something else. So you should first train the whole PLSC stuff, then you initialize this link spreading thing, and then you iterate. That's it, exactly. And it converges, and so you have the both the initial PLSC and the PLSC plus the link thing. Exactly. So you've now got... PLSC gives you two probabilities, right? So it gives you a probability of a topic given a document, and a probability of a word given a topic. So this is the same for both models. Yeah? Oh, this is... What you do is you're taking this and iterating over the probability of the topic given the document. So I've made that clear. I'm not sure if I'm explaining this. No, no, no. Okay. So... So... Yeah, you were saying that you would like to evaluate that now. Yeah, so we were struggling. How do you evaluate this? Because... And if you look at the way that PLSC and all those kind of models have been evaluated, they never quantitatively do anything. They always say, well, look at the clustering. There's... Here we've got some documents that all appear to be the same thing. And here's where we'll get some other documents that all appear to be the same thing. And you can't really tell exactly how good that is. You can just get an idea that it's... Or they use the purple city... Yeah. The likelihood, they say, oh, this likelihood is smaller than the other. Yeah, so that's the first thing that I think you want to spoke to you about this. You suggested you use the likelihood. And Pedro came up with a really good example of why that could be bad. So you have... You have the... Oh, let me think of what this was. It was to do with cars, basically. You have a topic that's representing cars. And you can tell that that's representing cars by looking at this distribution. And it's the same for both. Now, you want to tell which... This page, Pedro's example of what it was, is that it says on the page, lotus, elise. Okay? And... That... Ah, no. I can't... I just remember it, but really, can you remember what it was? Oh, no. I don't remember that. But you said I would tell you that evaluation should be done based on likelihood or perplexity. Yeah, I didn't really consider doing it on perplexity, actually. Well, it's the same. Yeah, it's basically the same. But maybe you could have some other... Yeah, another task. Or also to ask, if you imagine you have some blabbling of the pages, like, for example, you look at a set of pages and you have the directory entries of those pages. Okay. So, like, Yahoo! or the most entries, so you have categories and entries. So, for example, you have cars, SUV, etc. So, what you would like would be that two pages in the same category should be closer than two pages being spread across different categories. It's that fair, though, because I think one of the things with these topic models is that you don't know... You would not use this. You would not use the Yahoo! directory or... The directory in the training step of the model. Okay. This is something you have just for evaluation. And then, for example, your task would be to say, I have some page and I know where it is in the directory. And I have another page. I don't know where it is. And I would like to know whether it is in the same category as the first one, just by comparing their aspect. It's making the assumption that this human categorization is related to the one that is discovered. If they have the same, yeah. So, basically, what you would like to check is the similarity in the aspect distribution tells me about the fact that they could be in the same category in the end-label categories. Okay. Is that really fair, though? Can you say that if a set of documents, if the clustering of these documents is closer to the effect... What you've got there is a clustering of documents as your evaluation, yeah. This labels the Yahoo! category. That's a clustering. So, by saying that this clustering is closer to that clustering, does that necessarily mean that it's better? No, no, no, it doesn't have to be. If you use the latent representation, so distribution of our aspects, you could use this to save two documents, and the same directory or not. And this you should be able to evaluate them. So, what you would have, basically, would be to have a directory which gives you labels for some part of the documents. Okay, so this is a directory, and this is some web page. So, the label, the one. And you have some page which are unlabeled, but you have links between those two. So, what you would do is that you would try and use PLSA and the other model over that. Then you would like to, for example, what could be very simple would be just to match each page like that. So, let's say you have a page which is labeled, and you say, okay, the label page two should get, should be the one which it is the most similar to, or any kind of classification rule you can imagine. And so, you say, okay, what if I iterate from this page which have the similar, so the criterion would be to compare those two distribution. And then, you would like to assign to this document to where you don't know the labels, the same one as this one. And here, that you use only for test. So, this is your grand truth. Then you can evaluate all wrong you are or close you are from the real assignment to the right directory category. In this way, I think it's fair if you know only that and that, and what you would like to discover is this. This is a like a real task. Like if this directory company would like to extend this directory to new pages without the hassle of having human labeling. But in this case, you have to use the label one. You use this one. You only have the labels for some small part of the... How does it enter in that? Well, I think your point was you train this without labeling first, yeah? So, you would train it, this is... You are training it? All those links are known. The links between some label document and unlabeled document. The point is that you should change in a way that to take the label into account. I think you can do it as a two-step process. First, you train... Well, maybe it could be better if you take it into account. But maybe you can do just a two-step process. First one, you have this unsupervised task over both PL and PU. And then you, from this unsupervised task, you can know which document of PL is closed from the document of PU or the reverse. And you can infer some label. Clearly what we have done is you can train after that an SVM, saying this is category one or two on the label document. And then you have a non-labeled document and you just say this is directory one or two. And if you say this feature extraction process is better, then you're going to have a better cleaner separation between those two. The thing that worries me about this approach is that, say, the really, really simple example of this, where your directory had two categories in it. Good and bad. And you run PL, I say, no, that doesn't do good and bad. It does English and French pages. So they don't match, but the clustering was still very good in one respect. But you can extend that up even if you have a hundred categories, PL, I say can find a hundred clusters that are a good clustering but are completely unrelated to the directory. Well, I mean, if you have to judge, but in good, it's a respect to a given task. You can say, of course it's always good because it's maximizing the likelihood. What I want to know is that which of them has the better representation of a given page. So a given a random page in these two models. And the words that these two models give which is more likely to... Yeah, you can, you say, given a random page of PU, which of those two models is more likely to say, for example, that the page of PL, which is the most similar. So you take any distribution comparison thing. So offman in his paper was doing just cosine similarity, but you can do k or anything. So you just compare those two distributions of D1 and D2. And so with that, you can determine, for example, for a document of the set 2, the most similar. So you select D1. So in this set, which is the most similar to the two according to the distribution from one audio or model. And then for this document, you automatically assign to it the label of the document 1. And you check with this, which is your ground truth, if you are right or not. This is very related to what's planted with the keyword image annotation. So you would have a set of key words, the set of image labeled with key words. And you have a set of unlabeled image with some ground truth. And you would check whether the label you assign, so in this case annotation word, are correct. And I think it's fair setup. So maybe this classification rule is not the best. There, there might be some work to do, but the general idea, I think it's fair evaluation. It's a task. Okay. I'm just concerned that that's... It's answering a question that evaluates in some way, but maybe not in the way that... Well, it's maybe not in the same way I was thinking, but maybe it's a battery. So what I mean by that is that... So I'm trying to process all this. Yeah, it's the same thing. It does tell you whether that's... Which clustering is better for this task, but it doesn't tell you which clustering better represents the pages necessarily. No, but you could say that it probably doesn't. I think there is any way to say it generally. This is better or worse. There is not a perfect clustering. What is the... How do you judge what's in the clustering? It has to be related to... with respect to this clustering. What's good is the direct choice that you have a good coverage of biostopic humans can level. But then if you want to detect different languages, this is another task. I think the task of this latent model is to discover some semantic, like some topic proximity between documents. And here this directory is doing the same thing, like associating some topics with some documents manually. So comparing them is good setup. And you're not the same compare... you're comparing them. I compare those two, like the PLSA and the one over the same task of assigning good directories. I don't know if it will provide good result or not, but it is clearly a benchmark for those two... Girls. You can even keep the bag of words and do the cosine comparison. Also... Just showing how... just first how better this is with respect to bag of words and hopefully then how... Like bag of... Regularized P of Z given D is... Yeah, you can have three other apps. Just... It's basic. Basic cosine over the P of T given D. So, wrote this empirical distribution. And the two hidden aspect models. I think... You still not combine. No, I'm coming round to... We have to repeat over and over. I think it probably is fair. I've got an uncomfortable with it, but I think that will go, if I think about it enough. But yeah, it just seems a lot suspect, but I think that's just from the whole point that you can't tell which clustering is better. But what I saw before was, could you get people to evaluate this by hand basically? Could you give people a set of pages and say you did it with five topics, for example. Now you give people five sheets... You give a user five sheets of paper each with words and varying sizes representing the probability of that topic, for example. Try... Give the user these pieces of paper to get an idea of what these topics are. That's this P of W given Z. And then you give them some pages and say which one would you... Which piece of paper would you put that with? And then you get like an intuitive idea of how would a person cluster pages given what these topics are supposed to be? What these topics are supposed to layer? Because you've got something that's fixed, the P of W given Z with respect to both models. So, suppose you could use that. The hidden aspect like the Z are unsupervisedly. It's more like if you were giving the people, you said to them, okay, divide me this bunch of documents into five topics, and you don't give them the topics are priori. So the two person could come up with very different... One is very interesting. In fact, I've seen a paper in CML where the guy was trying to learn what were the aspects of clustering. So I give several users, a bunch of marbles, and they begin to say, okay, and have to divide it in five groups. And in the end, it's always different to clustering each people has because it's completely subjective. Yeah, okay. But there is some characteristics that are transparency, color, which are features that make the clustering that can be extracted. Even if the clustering is always different. Okay. So I think it's something that's saying that it can be really infinity of... See, the thing about web pages, I thought, as well, in getting humans to evaluate it in some ways, is that you have most of the web pages that are... A lot of the web pages that are important in some way are home pages. So google.com doesn't actually say the word search on it, I think. It maybe says go, but you only know it's about search through the thing. But someone looking at it can tell that it's a search engine because of the way it's laid out, because of the pictures on it, and because of all of that. There's a lot of information that's completely lost in this bag of word representation, and I was hoping that in some way you could use that to evaluate this. No idea how. But... Say another example, apart from this search, I don't get exactly what you mean. Don't use a Google page because it's too rude. Even someone would provide very different labels. Like someone would say it's a page in English, someone would say it's a web page which has very few information or anything. So I think there's no art clustering. And what's good with those models is that they are intended to provide you some kind of way to compare to documents, rather than to give you predefined sets of documents. This is not art clustering. I don't think one aspect or one latent topic has any sense by its own. It's when you have the whole set and the whole distribution that you can tell something. If you just take one of them alone, so you take all the P of Z given D and all of the P of W given Z, you can't do anything with that. It's the whole set. I would say. And this is all it should be evaluated. Meaning that, like for example, the Michel example about Marvel is that the point what you can learn from all these sets made by people is that those two Marvel would be considered as more likely to be in the same set because they share some common properties which are not seen as the same for everyone. But they share some common properties that people will notice. And if you look at the two, the all possible pairs of Marvel, you would have some much more agreement between people than if you look at the predefined sets people would just cut at some precise point. You know what's... Okay, I think anyway. But yeah, this is effectively what we have here though and that directory is one clustering of the marbles. And you're hoping that... So what you're saying is that I have two clustering of marbles from my two models and I have a test clustering of marbles. It's saying that one of these is more close to the test than the one that we've been getting. What is that actually saying? It doesn't... It's better for this past but it doesn't really say anything else. No, no, you don't have too much. You have one clustering of marbles. What you have is you have one clustering of marble, which is this one. And then you have lots of people telling you, I consider those marbles to a very salient feature that they share in common. I would like them to be in the same cluster. And those two... Okay. So one of them is labeled and the other is not labeled. And look at that you say, oh, someone told this one was very similar and this one is said to be blue, for example. I don't know if it's the label. So I put it in the blue bag. Okay. But you don't... You never use the blue bag. You don't ask the viewer to look at the blue bag. You just look at the people. You just ask the viewer to compare those two marbles. And then it happens that some of them have been colosted. People are in your algorithm. That's the point. Yeah. Yeah. Yeah, this is a reverse possible. Yeah. It just... So I forget that right. What the algorithms are still coming up with is, in this way, looking at a clustering of marbles. Yeah? Each of the algorithms comes up with a definite... No, no, no, no, no, no. It comes up with, not with a clustering, but with some way to compare two marbles. Okay. So my question would say... Okay. That makes sense. The two ones are completely dissimilar, and the other ones say they are very similar. And if you find that in your unlabeled clustering, those two marbles happen to be in the same cluster. You said the other algorithm might be wrong. But you average that over lots of documents. Okay. Because the latent aspects really doesn't have too much the final directory. Exactly. It can be anything, but it's just a way of saying, yeah. Those documents are similar in some sense. So... We're in a drink. Okay. It's just sometimes literate to say this latent aspect means this. Okay. I'm convinced. I'm convinced. All right. Good. Good. All right. Well, thanks. Another trying to think of anything I haven't said about this stuff. I did. From looking at the results I got, it looked quite good. Just hand reading them. And one of the things I did was I started a web crawl at the EDAAP homepage. It did not very many pages, only like 2000 or something like that, but enough to get the pages that were close to EDAAP to have a lot of links between them. So that there was enough to be able to run this linked clustering algorithm. So what you find is that the pages around EDAAP, there's a lot of English speaking pages and a lot of French speaking pages. So of course this PLSA completely separated those out into categories. So when I looked at the word likelihoods, there were two topics. One was very clearly English words, the most likely word was there, and one was very clearly French words, most likely word was there, I think. You had two topics? No, I had many topics. The others looked like garbage. Couldn't tell what they might be. But anyway, that's often the case when you look at it. You can tell very clearly what one or two topics are, but not the rest. The others usually don't mean anything. The next thing is only the global optimization, the likelihood is optimized globally over the whole aspects, etc. So you never ask the model to do out clustering of documents. You just ask it to find a distribution of aspects such that a document is more likely according to the restriction that the number aspect is limited. And so it just makes sense on looking at the whole set. So I think all evaluations should take that into account and look at the whole aspect set. Like, here you look at the whole aspect set because you look at the similarity of two distributions. Any kind of thing where you unfold the model and look at it manually looks weird, like in those often papers where you show you columns with words or things. Anyway, when I ran it on that, what I found was looking through some of the sites. I looked for the sites that the models differed on, where one said it was probably an English page and the other said it was probably a French page. I found a few examples where you had a site that looked. The one that I remember offhand was, it was a page and it was a big picture and links down the side. But the links were all pictures. So the words were in French in the links but there were no words picked up by the when I did the crawl for the model. And there was a small bit down here. And in this, there was a picture in there but it didn't load, there was meant to be something else. And it said, in English it said 404, this page cannot be found. So all the words in this page were in English but it was very, very clearly a French page. So the first model said it was an English page because all the words were in English. The second model said it was very definitely a French page because all of these points were in French sites and it was something.FR as well. So it was a good point for you. Yeah, for your model. So I found a couple of examples that were kind of like that. The problem that I found when I ran it, the thing that was a bad point was that if you have, see, I was trying to see if you could evaluate this with search, given some keywords, the quench model would more likely find you a good document, that sort of approach. What I found was that say you had, that's not too relevant necessarily but if you have, say, a topic distribution for one page and it very, very highly scores on the P.I.S.A. for one of the topics in that's right. This model, because it's averaging out over all the pages nearby, all of the topic distributions become more flat. It becomes more uncertain about everything. So I don't know if that's necessarily a good thing or not. In some cases it's going to be a bad thing because every page is linked. By the way, the algorithm works. Every page is linked eventually to everything else. So every page has information incorporated in it. The distribution land for every page has information incorporated in it. From every other page's distribution as well. So it averages out, flattens all the distributions, depending on what this alpha is. But also it depends on that could be a good effect because if you can be, if you are highly reliable about one page, but all the page looking to it are actually about very different topics. So you could say maybe I was wrong from deciding only due to the text that this page was about, this aspect distribution because all the other pages are voting in a different direction. Well, I think that the thing is... Well, there's a balance. For example, take a page like Flash. It's a news site for geeks. And you have... News from now, stuff that matter. Oh, like hundreds of thousands of pages point to that. But they're about completely different things. So you, in effect, are losing information about what that page is about because there's so many things pointing to it in the all average out. And this alpha, can it be dependent of the document? Yeah. So how many texts is in it? That's my first idea. How it set this. I remember what the lotus thing was now. So I thought you want to probably set this alpha per page. There's probably some heuristics. For example, this page, there's no text in it. We'll set it very low and use information for the links. Problem is, this is what Pedro immediately said, what if you have a page that looks like this? It's got a big picture of a car. Pedro's always picked some of the cars. Yeah, cars are not. Anyway, and it says, lotus, Elise. So you know that it's definitely about a car, but it's only got two words. So you can't just do it by words because those two words completely specify it into something or other. You suggest doing likelihood. But at that time, and I can't remember what my problem was with that. It might not have been a good thing. But you've got two likelihoods for it as well. But I think for any kind of way you have to select this alpha, you have to look at the effect on an evaluation task. And then maybe you can find some patterns that are. Then once you have the task, you could look at which pages are completely missed by one model, because those are successful with one model. And you can maybe infer some rules afterwards. But if you have no task, you can blindly select good alpha. Okay, that's... Like if you look at your task and you see that, I don't know, the long pages, for example, have good PRSM model without using links. Then you can infer some heuristics like, okay, the alpha should be proportional to the links or anything like that, or to the number of images in the pages or anything. But you should have some tasks where you can individually look at the performance on every page. And as a second step, you could infer some kind of rule about all two select alpha with respect to the task. Okay. Maybe it's very simple rules. But select them blindly. No. It might be odd. Yeah, so I think that there's two ways you can do it with some heuristics, and you can learn it from the task results, or combine it to. So that's kind of straightforward, I think. Probably heuristic will be easier to compute. Yeah, the thing about doing it through cross validation or something like that with the task is, the problem with doing a page rank style algorithm is that it gets better, it converges better, and it gives you better results. The closer a network is to being a small world network. It's a small world network, basically some far links, but generally very clustered locally. Now the thing about doing a web crawl is, the bigger your web crawl, the closer it will be to the overall structure of the internet, which is a small world network. The smaller your web crawl, the less small world network like it's going to be. So the more pages you get, the better your results. And people running this style of algorithm, the page rank style stuff, generally run out on about 20 million pages or whatever. The Stanford web base, the exit that I was using for this, for a bit of this, they have, I can't remember offhand, they have maybe over a petabyte worth of web pages in this bunch of servers. But still, like the work I did on hyperlinks, like I used the Wikipedia. Yeah, maybe Wikipedia is a good thing to do. I think it's because it has fairly high level of inside links. What was concerned about Wikipedia though was that it's, while it has a high level of inside links, it has a lot of directory style links as well that might mess up actually being a, so you can't get the guys down because they are, there's a structure on the webpage. So you can select only the links which are within the article or within the category. Okay, that's good. It's very structured, it's easy to manipulate. Plus, I guess it's actually straightforward to say to start with, is this a small world network and then, kind of not. You can even isolate the directory from the categories of Wikipedia. You were running that on, what was it, like half a million pages, something like that? Well, yeah, I divided in three equal size parts, which of 150 documents. Okay, that was all those documents. And, well, but... Need you manage to divide the link properly? Well, what I did was the link, well, but this is not exactly the same setup you would have. So in this setup, the whole link structure could be known and the only thing you would not know would be the assimilation to a category. In my setup, it was different because I couldn't use the links between different sets, but this is not... I don't want to miss you up with that, it's not the same kind of experiments, but you could just use the World Corps push like this. And for some of them, you assume you don't know the category assignment. And then you can run this set of experiments over Wikipedia. I think offhand, thinking about this model, would have to work better because two things in the same category are going to be linked. Yeah, but this is the... Yeah, this is the hypothesis between these... in this PLSA with link stuff. Okay. Yeah, I think that it's going to be more true in some networks and in some tasks than others, and I think in Wikipedia. You can also add those tasks like that. Like for example, you can take proceedings and assume the categories are the keywords of the documents. And then if you have the archive of a journal, you can use the links between the citation links. So if you want to have a bigger stuff, so Wikipedia would be kind of very small with lots of English. And then if you take a journal, you will have less links and more documents. And then you would be likely to say that PLSA might be better over that because you don't have enough links. So you can find examples which are not the web. I think it's good to work in closed set. Wikipedia is kind of closed set, even if there's lots of outside links, but there's lots of inside links. You have to think at database where there's lots of in-links. Wikipedia might be one. Citation, for example, if you take the NIPS archive which is available, NIPS author always sits on NIPS author, which often themselves, etc. Well, it's so cold. I won't say that. So, but yeah, I think this kind of setup, maybe web pages, it's hard to find sets of web pages which have good links, stuff. If you start from web page or something. Okay. Yeah, maybe have a look at the Wikipedia stuff. Anyway, I'm stuck doing something else for the next month or two, anyways. This is always the case. There's another thing. This was new. I haven't thought that this too much, but it's on the same lines. Have you guys seen this correlated topic models? What? correlated topic models. David Bly the guy that came up with LDA. He's got a new model that's published in NIPS this year, but he's co-authored on the website, a month or two ago. So, you know, LDA, the old model LDA? Yeah. Okay. So, LDA, you basically draw a Dirichlet, which says what type of document this is effectively, and then you draw a topic given the Dirichlet distribution, and then you draw a word given that. All right. Well, that's for every word, and that's once for the document, and that's, you've got a parameter coming at this globally. So, that's roughly what the model is, plus a little bit. But anyway, so what they did was they reworked this because one of the problems with LDA, and I think POSA has this problem as well, is that the more topics you have, the less expressive it can be because the topics, it pushes a document. If it is pulling a document into one topic, it pushes another topic away from that. That makes sense. So, the topics are necessarily trying to be different from each other. So, the more topics you have, the more that they're sort of squeezing into the space of what things can be about, and things are kind of pushed out. So, once you go over, they try that in one corpus, and once you go over about 100, maybe 200 topics, the new, you add more topics and it doesn't get better in any way. The model doesn't, the new topics are effectively going to just be garbage. All right, so what they did was they stopped using the Dirichlet, and they're starting to use a Gaussian there. So, to spare the details basically, they've got a variable here, I think we call it eta. And they, from this, generate a distribution of topics in some way, but it's generated from a Gaussian space, from a space that's given a mean in covariance. But it's discrete, no. Yeah, so what it does, this is the bit that took me forever on the stand actually. Same model, but this is, given a Gaussian space, it maps that down into the simplex of what a topic can be about. So, the simplex basically, if you've got, see you've got two topics. No, see, you get three topics, that makes more easier to draw. These are the probabilities that a document is about, this is the probability that this document is about topic one. So, these guys all have to add up to one, yeah? So, that basically means that what a document is about lies on this space between them. Yeah, so this is the simplex. So basically, a distribution generated by this guy is basically a point on this simplex. So, what they do is, they've got a distribution that maps a Gaussian in some space into a distribution on this thing. So, it maps it into Gaussian in that. The cool thing about that is that if you get two Gaussians, you can tell how close they are. So, you can now tell how close two topic distributions are, right? So, you can have as many as you like, and you can tell that they're close to each other. So, there's loads of ideas that I had about this, in that if you can tell that two topic distributions are close to each other. So, for example, if you split a document in the two parts and said that the two parts have to be close in the distribution, you can see how the document changes. For example, stuff based on that, with this idea, what if rather than working with this P of z given d thing that you get from PLSA, but what if you worked with these Gaussians that you get for every document? And you said that, in some function that said that the Gaussian represent this should be similar to the one's linking to it. I haven't thought about any of the maths for this, but it should be straightforward, I guess, to formulate it basically in terms of the meaning should be similar in the covariance, maybe should be similar as well. So, it might actually be a better model again. I don't know. I'll send you guys the link for that correlated topic model stuff if you want, but I don't know if it's too interesting to you. OK. Thanks for hearing me out in the idea, especially. Maybe I can do more with this. In three months. Well, there are lots of good ideas that are recorded and someone with... Oh, yeah, it's too late. Three months. Yeah, because the data will be very similar. Bastian, you can't release this data for four months. That's cool. Thanks, guys. The move is useful to you. Yes, but... At some point I'm going to have to go to David and convince him that he spent the month until working on this again. You don't have to convince you have to say I'm going to work on this one. Can you make me... No, he's still going to work on it.