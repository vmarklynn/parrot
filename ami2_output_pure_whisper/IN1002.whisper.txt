 Okay, so should I start? Yeah. Okay. So I think it's better to first have a presentation of my work and then we can discuss with this crash in temp in temp what we can do for my future to continue my research because apparently I will have serious problems. So we'll start with the presentation of my work. You can be sitting here, let's open, do the presentation. Okay, I will start. Okay, so my work is generally about using posterior in speech recognition systems and somehow enhancing the estimation of posterior getting new posterior which are more informative by somehow integrating some extra knowledge like prior knowledge and contextual information related to the problem. And then use this posterior estimation method for designing hierarchical speech recognition systems. So in this hierarchical architecture I will be able to integrate extra knowledge at the proper level of hierarchy and I will be also able to combine different kind of knowledge or sources of knowledge I have. Well, as you know, generally, posteriors are estimated using neural networks and more specifically MLPs in a speech recognition system. So time limited window of speech signal is represented by some capsule features which are then processed by an MLP and it gives us some evidence in the form of phone posteriors. When we use this phone posteriors as features or as scores for either just decoding or training and inference training and decoding. But we know that in this case we are not able to integrate contextual information, long contextual information and we are not able to integrate the prior knowledge we can have about phone use or about words in the posteriors estimation. So the main motivation is to see if we can integrate this kind of extra knowledge in the posteriors estimation. So what is proposed to solve this problem is to use what we formally use in HMM formally which is called gamma state posterior as a posterior for training or decoding. So as you know, it is a state posterior probability in HMM which is defined as the probability of being in a specific HMM state at a specific time having the whole observation sequence and the model. So whole observation sequence means having complete contextual information and the model can encode some kind of prior knowledge for us by means of topological constraints. Well you know this gamma posterior is written in terms of forward and backward HMM recursions and actually one way to use this gamma posterior is using this posterior as some kind of new scores for decoding. So in this case it will be similar to hybrid method. The difference is that we do one more step. Maybe I can explain it better there. So we do one more step here and we here by means of another HMM we introduce some kind of prior knowledge and contextual information. We get better evidences than what we initially had here and then we do the decoding and more informative posterior instead of this one. So we are expecting the system to perform better and more robust to noisy cases. Here you see some results and how what is the effect of introducing reasonable prior knowledge. So different rows in this table are showing different level of noise and also clean speech and different columns are showing different kind of posterior or gamma posterior. So the first column is showing our baseline system which is getting estimated of posterior out of MLP and then using these posterior for decoding and the other columns are showing the performance of different kind of gamma. So gamma which is estimated using information about minimum duration of phonemes about possible transition between phonemes and some more prior knowledge like which word is composed of which phonemes and grammatical knowledge about transition between words and things like this. So as you see introducing more reasonable prior knowledge will help to have better performance in almost all the cases. And here I compare the performance of the gamma based system with the standard HMM-GMM system which is based on doing decoding using likelihoods at the scores and the difference is that our system is using posterior for decoding. So as you see it's working better than the standard HMM-GMM system. And finally the conclusion is here we saw how we can get some more informative posterior by introducing extra knowledge like prior and contextual information and then using this posterior estimation method to design hierarchical speech recognition system which the goal is to have better, more efficient and more robust system. So if you want now we can discuss about the crash and the loss of data. So well I lost all the posteriors. I was, I used as input for my system. So the gamma are somehow estimated or computed on top of posteriors. Now I don't have any posterior and probably the script generating that posterior. So what I can do. You go to the table you have there like the big table with all that then we can discuss. So which I think I have all the gamma, especially gamma full models I have. Gamma full model but what about other ones and what about the posteriors? The posteriors because I cannot stop my research. I may do something else in future with this and well I need posteriors. Now you need the different methods from how from gamma you can tell. Posteriors. Even you have those three state performance gamma. You have all these. No, no I don't have them. They are all under 10. We have that because you have stored them. You know you can write another paper if you can. Maybe some other PhD students should be hired to work on gamma. Gamma is supposed to be generating gamma. I was just, let's talk a little about this presentation itself. The slide number 6. How you feel it is important that you are taking the context of the whole utterance. How important it could be. Is it not a possibility that you are having less context? Less context. We mean. Instead of having context from one to capital T which is for the whole utterance. Which doesn't make much sense. Yes. In general. So what is the possibility that you can cut the context? Instead of having a context from. You mean a smaller context instead of this. It might be left to write everything there but not the whole context. Because it's a fact that when you are having one utterance. So one utterance is like a full sentence or something and there might not be much breaks in it. So in that sense it doesn't make sense that you are going too much in the past and then you are trying to cut something which is not related. Well, even they can have some information which are not so meaningful or which are not related. Well, but then I will have the problem to find the optimum amount of context. How much context should I take? Kind of. Two hundred. That's a Phoenix idea is usually 100 milliseconds. Two hundred milliseconds. One second. So but for numbers I think it may be okay. Numbers hardly. Yeah. Numbers it might be okay because most of the sentences might be three seconds or something. Even less than that. Yeah. Just few words. Yeah. It is or something like if it is really long sentence of ten seconds or something you may not really need. So that's that much of context to computer governments. And it's really computationally very intensive also like if you. Okay. Yeah. That's a good idea. That's reasonable. Okay. So when we are talking about them then my stories are the worst I think. So because yeah, all the post videos are from you only. I cannot generate post videos. Yeah. And so there is. And you don't have a script for. I don't have the C files also. Oh, okay. You don't you have just your thesis. Yeah. No. God has stopped my my work. He has said that enough is enough. And what could happen if you don't even have your thesis? Yeah. Then I would have taken two years extension. Oh, you know. Yeah. It was it was pretty scary. And let's see what happens. But for your case, you don't need really complicated post. You just need some PLPs and then. Yeah, you can then read and read it. No, because it's more did it on what? Um, and one of couple of. On C experiments, right? Yeah. For CTS Peter still I think has both there. He didn't lose anything. No, but he's he may be linking to Matthew. I guess because Matthew got the way. Probably might. But you got the all the way file. So yeah, but the way file is the same, the temple. Right. So that's what I'm saying. That I can recover it back from. Yeah. Exactly. Yeah. So if he got features, then it took otherwise if he has to generate again, he can't generate. Because when, uh, path has lost a lot. Yeah. He's having only the results with him. So he doesn't have the files. Yes. The real. Everything, see, if they say that we cannot recover it, then I can quickly get it back from it. See a couple of days I can put all the system back. Yeah. Okay. But everybody has features in temple and most of the time all the CTS features or CTS we find. Yeah. So forget about CTS at least now. I think we will see it again. Yeah. But he's going to download that much of data. It takes at least one or two weeks. No, no, I can now see downloading is another thing. One thing you have to do. That they have data in one particular structure. So I have to first of all, you know, organize, create our files and then I can get it back here. But you said you need to do some segmentation also not to put that. And then you have to get their PLP features and. Yeah. That I can get all those things. Okay. Because those files are also very big. They are not single files. Yeah. But they should not be any big. They should not be any big. The only thing is that now if they have to say that we cannot recover it. Yeah, the we can wait at least one or two weeks for this. One day, one, one, two days like this week I can wait and see if they cannot recover it, then I can just get all the data from XC and put back the CTS. But still I think we can't really do these things before I guess. No, we can. So that's like maybe we can concentrate what especially on numbers now and then we can go to CTS. Yeah. So in numbers case what you really need. Just put your estimated using PLP features for number. Yeah, we can. Yeah. We need to generate again PLP. But we files are there. Yeah, PLP numbers 95 databases. Oh, PLP's are also last. Yeah, PLP's. Everything is last. Everything is last. Everything is last. And if you really asked me even the structure which was there real every point by point how PLP's were generated, that information is also not there. But you're generating PLP's from FICO. Yeah, but the parameters which are exact. But at least hopefully 95% I know but that 5% that you can't do. And then we need to train the new weights because MLP training needs to be done and then only the next step will happen. So it's pretty whole thing is stupid whether we can replicate the results, whatever results you are showing whether we can replicate them or not. Presently it's not known. But at least you have something to verify there. So PLP's these are all 13 just only PLP's not Delta's in Delta. No, there is not Delta. And there is another problem for verifying this results because well for this results you know the configuration should not be well. I also saved the configuration somewhere in the temp. So exactly how many states for decoding and which kind of language model and things like this. I think I can remember almost all of them but it's a bit. But if you got something from Hammond or Maitre then I have those number of states like we have there it's only for only close DCL, KCL they have. No, no, no, no. He's using different setup I think. No, he got from you. Do you use our setup for the number of states? For the confirmation? Yes. I think so then I have those number of states. No, those things I'm also having because they were done two years back. Anything which is done in the last two years is the one which is. So you didn't have any PLP files. So what about these noise levels? We have noise wave files in the COM database. So we have directly with wave files with particular noise added. No, those things are not disturbed. They are all in the database section. Nothing related to the database has been disturbed. No, but I think the switchboard. Yeah, but also we have it but one thing is that we don't have all the data within it. But the problem is like all these SNRs I think it may not be there in databases. No, no, they're there. You added already called the noise. No, no, it was pre-written there. It's in the database itself. Even with SNRs. That's right. Then it's okay. Even an EVFOM we have it. Yeah, so that is not a problem. So it's not that you're added all the... No, I didn't do it myself. With SNR, different SNRs. That part is less scary. So at least we have wave files. So now we can generate anything in a PLP or so at least. So the things which are related to your PLP can be done I think. It's not such a major problem. So as a conclusion there is no major problem except... No, I think that's like a problem. So how much time does it want to spend again? MLP needs to be trained and the PLP features so it's kind of one and a half days work. Kind of to generate the new points. But for all these SNRs, so that's also... Yeah, that means it'll add up, that's it. But all of it is doable in three, four days time. I also had stored a set of generated posterior or gamma or CX like this which I should regenerate. I have the scripts but it takes time. It's CPU and regenerating all these things. Okay, so there is no major problem except time. Deadline of two weeks for ICAS. I don't have to generate any new results. I'm also on the other side then. But if I need to generate new results then... Anyway, there is emotional loss, so much of scripts and codes. No, the thing is like you can't really write this script. Especially the code is real problem. Script is okay. Yeah. Even scripts also the configurations or if you forget something, some even preemphasis factor or something it's really... Means losses are emotional as well as whatever it is there. Manar and blah blah. But... Stretchity. So, rate five you have told to the... It looks like if you get three disks gone then it's very difficult to get it. Which disk? If it's one disk it's fine. Three disks. Because at least one is having the... No, one does there is no problem. Meadows will be causing the mirror. But if the three disks are gone it's very difficult. So, I don't know how it went. Usually as soon as one disk fails it should be replaced. Actually... It should be replaced by... Yeah, otherwise it will go on destroying other disks. So the one... So I don't know how it happened. You know? Such a thing. Like usually as soon as one fails it actually notifies the system administrator or... It makes certain kind of noise. You know, mirror is into the sky. To make sure that it has to be replaced. So if the disk is not replaced then it will fail. That's an internal recovery. If you just replace the disk then it can internally it can quickly recover everything into that disk. So it's like, you know, if you have... If you have... It's kind of like essentially if it is like 120, 100, 100 GB there are data space then it essentially will have 200 GB only visible data to you. But then it can... As soon as you replace one of those hard disks it can... It can from the other one it can recover everything on that. Ah, it will have mirror also. Inter internally it can do that kind of thing. But if the three disks fail then it's... It's a really difficult job. I don't know how it can be recovered. Probably the one way that they extend the hard disks to the people who are experts and they do like sector, basic or block by block with either the cover. But that's very expensive. But the two LEDs. Yes, a simple thing might cost you minimum 10,000 euros. But even this may be really huge data also. Was it insured? Any idea? I don't know. I don't know if it was insured. I don't think insured is coming in here. No, no. Insurance can take care of means. Anything can be insured in this word-lown is. I think maybe some companies differently they will insure obviously. You can't really have so many backups. But this term means one thing which is mistreat to me. Why 10.1 and 10.2 are residing on the same hard disk. No, it was because it was not 10.1 and 10.2. But they have the same... It has three GPs. Three disks hard disks category. So you can go up to 14 or something like that. But it can be on the same thing. Yeah, so it means... It's just partitioning. But so that... So it tries to put... It tries to quickly put things in this way. But only thing is that only one swipe has backup. Keep taking up the backup. So it puts everything quickly. It can decide wherever it wants to put. So it's faster to do that way. No, no, no. What I'm saying means... 10.1 and 10.2 could have been different systems. No, I think that is a soft name they have given the... I'm saying they could have kept differently. Yeah, but then what is the use of RAID then? No, RAID on both the systems. What's the problem? No, but that is what the RAID can allow you to keep up to 14 hard disks. Hard disks? There are already five hard disks growing. Yeah, so they have five. And you can go up to 14 if you want to keep there. And it can still work. And it has... And the way it looked like is still for Talva and you know... The way it is. Usually it is just that if you want these crashes before... If it is replaced quickly, then no problems. Otherwise, unsafe data will start. The first thing is get lost. That's the thing. Yeah, now everybody is going to be scared of temps. Yeah, no temps. The amperes, no, it's anyway. I think they can do because some of these people are experts. There are a couple of experts in Switzerland itself. So if they can easily recover it, probably, do this kind of thing. But is it really costly? This system is... Yeah, it depends on the job. You know, sometimes it's a small hardware thing. They might do it within, you know, 5000 euros and all. But if it's very much detailed work, they will just... The whole thing starts like this, that you have to give your credit card number and sign a form. And then only they will start taking your hard disk. No, no, no, no. I checked over the net, it's not the way. You send the things to them. They will see if they can recover, then only they'll ask you that this is the quotation. No, no, no. But if there is no... If they cannot recover, they will send it back. No, no, no, that's what they say. Like, you do it, then send it the hard disk. Then they will say, okay, we can do it. And then if you do it, we can do it. No, no, no. First they'll give you a quotation. They won't go without that. It's not... It means it's a wrong information. Maybe at least they will give you some number, otherwise they'll... Yeah, yeah. Because in the beginning, what they say that send us the thing, we look at the things. If it is recoverable, we will tell you what is the quotation. What is the estimate of that? And if it is not recoverable, we will send it back to you. That means... and if they cannot recover it, then what they won't charge anything from you for the inspection. No, I think that's it. Yeah. At least they will know beforehand. That's right. Oh, for they can go... Yeah. It means looking at how much failure has happened. Accordingly, they will tell you whether they can recover or not. And in fact, if they can go a lot, but they can go block by block and look into it and try to recover. But that much you will want to go or not is a question. But even how much important data... Sometimes it's just MP3 or something, so you don't really need to... And it's like 40 tera-al. For 4 tera, you know? 4 tera, I guess. There was somewhere else to like... So how much is 4 tera? 4000 gigabytes? Yeah, 4000 gigabytes. Yeah, 4000 gigabytes. Okay. 4000 gigabytes? Not much. Yeah. That's a big... But now we got something like... Isn't it? 8.1 is more fault-out-length as compared to 8.5. No, 8.5 is better. Exactly. 8.1 is having exactly 50% capacity is utilized. It's copy. Okay. Two copies of the same thing will be existing. That is the 8.1. But 8.5 is 10, 20% copy only. That's right. 8.5 is 20%. It's not the other way. So, 8.1 is exactly... It means that's a lot of wastage of resources, but it's more fault-out-length. And there is no parity kind of or anything. Nothing exists there. Yes. So, the disk which has crashed, that cannot be recovered. The disk which has not crashed, it can be recovered completely. It means if both the disk have crashed for the same data, you cannot recover anything. If one disk has crashed, then you can recover everything. It will be the other disk. It will be in the other disk. So, that is the 8.1. Anyway, Hamid, how do you want to go about it? If you are given a choice, you want to take red one or red five or a simple laptop. I would say how would you like to go about with ICASP? Well, I think after this meeting, we can try together to regenerate this. Well, PLPs, I think they should be first regenerated and posterior, then if by the end of week we could have posterior, it's not that difficult. I can easily regenerate the commas and do all the experiments again. So, what are the experiments you need to do? What are the new experiments you are having in your mind? Mainly, there will be some experiment to check how the system is robust against this tuning factors like language modeling. It's a language- It's a language-based technology. This will be the main new issue. And do you feel that there's paper material? Yeah, the new thing in the paper. And also maybe like this noise is noisy case. It was... But the one which you have reported means you want to report something more than that? Well, this is not in the paper. Actually, this was not in the paper. I put it in the presentation, but it's not in the paper. The paper was just for clean speech. But since I had the results before, and I cannot copy these things, because I talked with Helvet and he said, okay, actually, the numbers are not very reasonable because they are just 13 PLP features and we have to have at least something which is known for people. I mean, they are expecting something around 1987 or something like this. So anyway, I had to renew all the experiments, but now it's a little bit more difficult because before I had posteriors, and now we have two more steps, PLP's and then posteriors. At least now he can know full system setup from him. Why be what I can do? I can give you the setup. Yes, I can generate from bottom. Yeah, train the MLP test. Thank you very much. So now mainly like 39 PLP, you are always using 39. I was using 39. But then why these features, you gave me like they are 13? I wanted to extend this. Well, actually the paper we wanted to write was, for ICAST, was something related to multi-stream gamma. No, no, no, no. These ones like that, I will explain. So my plan was to have two streams. And I think that only one stream. So I thought that one stream will be PLP and the other will be delta PLP. So this was one part of experiment, so single stream, and then I wanted to combine delta with this and have another table. So that was why. But actually because the numbers and studies, it's not showing strong results. Anyways, results are not very important, but they should be within a range of acceptable numbers. So then for now I think it's better to instead of doing PLP and delta PLP, I can have single stream of 39 PLPs and then combine them with traps. Yeah, those results only, you reported an interest speech, like 39 PLP results. Yeah, 39. Yeah, that's why they were less than 10% more or less. No, even like clean speech. What is this result? This was related to your MLMI? No, no, it's interest speech. No, no, this is just... Okay, these numbers are full PLP 39. Wow, okay. But just to make things clear in the presentation to show more steps. What is the tri-phone numbers? Here is the tri-phone MLP. Oh, okay. Your MLP. No, he gave scale likelihoods to me than I generated gammas. Okay. Even those gammas will be same as normalized scale likelihoods. No, no, no. They are passing through the data from that. Yeah, because I have tri-phone, GMMs and HMMMs. Okay. So he gave me scale likelihoods of tri-phones. Not scale likelihoods. Scale likelihoods. Scale likelihoods. Scale likelihoods. From? From tri-phone number. Yeah, they gave me a chip. Oh, MLP scale likelihoods. Yeah. Well, you are having post-tators there, right? Yeah, so I divided by the pliers. Pliers, yeah. That's how you do it. Okay, so it's the next place. Then again, I will pass through full network. Then I can generate tri-phone gammas. And full network, what was the architecture? It's like Word. No, it's just GMM Word models. You have all the connections to between all the words. Okay. So... What are the other things we need to discuss? So then first thing is like we need to generate PLPs. And then all this SNR's PLPs. Yeah, it's amazing that everything was under there. No, but I think what I think there is still some data which is the cover of, well, at least they could give us time to, I think that the most of those things should be there. I can copy it already. No, it was there on like, yesterday morning, also even. So I was running some experiments and then it was okay, but it's readable only, so I can't write anything. No, you don't have to write, but it can be. Yeah, so but we don't know, like, we thought like, I thought it's recovering, so maybe by tomorrow I'll get everything. So, because if you want to copy again, you will have... That's all because of you. You came on wrong day from Iran. You should have come on, Jumah. It would have been okay. Yeah. Anyway. But yeah, still some people are in vacation, so they will really... They'll get a shock of their life. See, they will be back and... With you? But he's in it. With you, I'll be there once you come back and you'll have to write their rights. He flies to Senegal directly. Sir, in Daniel's wife in Italy? Yeah. But even all over the recent iteration, iterative training set up is intense. We was like, how much we can copy because it's really so many iterations and... Even without really completing many more experiments also. Yeah, but it's a major loss, no doubt. But at least, yeah, scripts are there. We're like, like, scripts are there, but... So if... The guy told me, he wanted to run the scripts, he's not there. Yeah, if they can record at least Hammond's P-Files then... They can... Yeah, if they can give me my C-Files then also, it's okay. Yeah. But C-Files, yeah. Even if they give you P-Files, it's really bad, even you don't really need to run all the... Yeah, because I don't need to generate features also. Yeah. Yeah, it's amazing. And I don't need all my P-Files. Just a few of them. Hey, just only like, recent P-Files at least. And what if we have someday a fire in India? We will lose all the data. And that is... Anywhere fire will lose all the data. I mean, but they have... Well, I think... Not in... That's not the case. No, they physically also separate the... Yes, they can, you know, the story somewhere. Yeah, physically it is up with this. But, you know, here we can easily have a fire because everything is made of food, I think, and it's small and... Yeah. It's sprinkler something around here, I hope so. Still I think I... Oh, it's sprinklers. I needn't worry, they had that big fire and then... I mean, what if they're in sprinklers? Don't worry. Even even so, several of that buildings, I'm not all the computers in that building, they're all burnt, but... They have like three backups in the university, so... At least people, they didn't really affect much. Whereas universities, maybe it's really big. Yeah. So they can afford... And they can't... They're shifted the whole thing around. Yeah, they... They won't keep it at one point. They'll have backup at different... Physically at different places. And also in some underground, so that like... Five proof or something. Yeah, then you'll kill farmers out, you know. Yeah, maybe. Yeah, anyway. The good conclusion is to have a backup and on CD at home. No, CDs are not enough like... CDs are not enough. They're home. Because one CD is home, which is not even one GB. So then CDs are not enough. Phone book, I can take care of everything on phone book, because I have it as a CD. I wrote it once upon time to copy it to my home, and so I have all the data of my CD. I don't know if I copied all the numbers that are on that. You're talking about data, so you're talking about... Data itself. The whole feature to everything. Feature, segmentation, every label file, everything is there. I can recover it back from there actually. But whatever it's a make-bulse, home directory, so... He also kept everything in... No, no, he was having a lot of many things in the proper places. But in temp he was having a lot of important things. All the experiments which he was running. But he did mostly on numbers. No, so at least if we can recover, we can read some of his stuff or something. Did you look at it? Is home, big ball, home speech, big ball or something? Look what? Look for any features or anything out there. What we do with the switches? No, no, he might be using the same PLP features or something. No, no, no, no. That way. Might be but the situation is that he was generating his own PLP features from his own scripts. He was not using doctor speech. Okay, he got his own. So he was like... So I borrowed few of the things from him. The FFT routine. And reading the signal waveform. These things have borrowed from him and I changed. And then I put it in my all programs. So... I kept the backups but those backups are themselves on them. I didn't know that this way it can crash because I thought it might be... I can do a mistake and wipe out the things. And then there is no backup. So I cannot blame anybody but I didn't know that. The whole backup can itself crash. Yeah. So now in my life I will never believe anything which is temperature. Whether it is temperature or whether it is temporary. No. Hey. Because just few days back I took a backup of my laptop feeling the same thing. But then I didn't realize that I should take a backup off. Only good thing is that the numbers I think I pretty much can recover all the even the iPhone X-Men's. Because I think I have kept those labels all the things that are pretty directly. A P-Pile of it. Then it's good that we don't really need it. You don't need to back all those labels and everything. So but your PLP is also there. PLP should be there. Should I have kept the PLP also somewhere there? Then we just need to run neural not only. Yeah but PLP is hardly a few minutes. PLP is hardly anything. The extra work is like you know converting the segmentation from monophone to triphon. This kind of thing. Then all the scripts are there in the temp for that kind of thing. Which scripts? From Post-T-R-S to Post-A-L-M-N. What you do is like you have the segmentation to train that for monophone right? You can convert the same thing into triphon segmentation because there is monophone and tribal there are no different except that you're just going to give a different label to them depending on the context. So it's easier to convert that segmentation directly here. Rather than now you see the point that if you lose one. So the thing is that I have to recover means I have to recover through the monophone segmentation the whole thing. But you still need to run all the nets. Yeah it's okay. That's your number that's very fast. No but it's not. I think at least it takes two hours or three hours. Three hours I think I end. You can't parallelize also. The thing is that right now you know almost what is the burn which will give you the best results. You can convert. Not in all cases. That's the problem which people are facing. Because everything was written then and there itself. It means the optimization has been done. They did the optimization there and they they worked on that started working on that. They didn't write it with pen or something on a paper or saying that okay this is the best optimization. So for example if you want to replicate all these results then he needs to use all my hammers. PLP configuration then. Because if I change my configuration and your results will be. The results are somewhere else. Then you don't know. Anyway. Okay let's recover it. Let's hope the best. So how much time we have spent? I think we have spent 35, 40 minutes. Okay. So this is good enough. Thank you. Okay. Thank you. Fine. Have a good day. Okay. Good. Good. So we had to leave first. Okay.