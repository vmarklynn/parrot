 Okay, we start waiting for people coming late. I can say this because in this moment you cannot answer because you don't tell the microphone so I can say whatever and you cannot reply. Okay, I don't reply coming. Yeah, it's okay. Well, okay, so ready. We can maybe get a head start. Have you been talking to Matthew about what you've been doing? A little bit but I'm not sure he knows everything. Well first we try to explain why this end-best list is scoring from the slides to enhance the speech recognition on the meetings data is not working. So that's one thing and so what we did with Alessandro is performing some statistical tests to see whatever, if the words, the appearance of the words during the meeting is independent of the appearances of the different slides. So in the case if it is dependent that would mean that certain words tend to appear during certain slides. Yeah, which was the interest. And then there will be a reason for believing that this will work. And the result was no. The result was no. But the question is is that result no because it's no or is it also just because there's not really enough data to be sure about anything. In my opinion it's not because of the nature of the language. I mean intuitively of course you tend to use more of the words that are on the slide. But the mass of the words that actually use are words that are common. Just think that 50% of the words on average, whatever corpus you take are stop words. Yeah, exactly. So in terms of recognition 50% goes away. But the rest remain in 50% are all words that appear one, two, three times. So in any case even if actually they tend to be related or statistically related to a single slide. In any case in terms of recognition, do not have a tone. It's not going to. Yeah, very little. So that's the kind of thing, I mean that's the kind of measure in statistical independence is not on single words overall. Which basically means in terms of recognition that doesn't have. This is sort of a feeling that we had but hadn't shown. I think from an application point of view it might be interesting to make sure that your vocabulary contains all the words on the slides. But from a research perspective it's not interesting. You know even if actually I remember you just made some measure saying okay let's look at how many words I don't have in the dictionary and are in the slides. That be very few. It was 2%. Yeah, almost no. This is how the dictionary is calculated, it's calculated such that if you don't have the word it doesn't affect to a right degree the word error right. Exactly, so in some sense intuitively it sounds very good but in practical terms in terms of recognition here we have to be clear. If you want to improve the recognition that improves very slightly. It is different if you want to have other tasks where the only words that actually play are all are two words. In that case you have the slight improvements yes at that point that can make the difference. But does the recognition? No, well this is yeah. And some way the statistical independence I think it can be a good explanation to show why this happened. Even if it is counterintuitive but that's what happens basically. So I'm sure we were just quickly divided. Yeah, sorry about that. We were talking that we did some experiments to see if the word during meeting the words appear independently of the slides in the sense that if certain words tend to appear during certain slides. Okay. Or no. And the answer is no. Okay. And so we did a statistical test for that and I will add also that we did that on the words after filtering them from the removing the stop words. Of course. Yeah. So it's not going to make any difference. So because yeah with the stop words okay there will be a reason for independence because there is such a huge mass of them. But even after removing them it remains independent. And actually so when I was doing the scoring experiments I was doing that on one meeting that I selected as being like the best looking one. It had the most slides. And now I did the same on the other meetings which were available where it's only three meetings because well there are four of them which are from the test set from the ami recognizer but only three of them have slides. So it's total of three meetings and for the others the tendency because you know I was when I was doing the scoring I was taking into account one given slide also the neighboring slides and as I was increasing the number of slides which were affecting the utterance I was scoring the performance was improving slightly. Very very little. No no wait it's going very fast for me. That is the thing actually it's very fast. So first point the first thing you told me is that you took the slides and you had a big dictionary and you were seeing what words are the inside the slides right. What the first statement I could not understand. When you said the first thing we were talking about takes the experiment to see the if it's dependent or independent. Could you please tell me a little bit more. How we did? Yeah. So we used Pearson key square. No no no don't go there. No I want to understand what did you mean by the thing like saying that I took a dictionary and then a word from the slide and what you are looking for I cannot understand clearly there. So you have to see if certain words have a tendency to appear during if they are more likely to appear during certain slides. Okay but how do what's happened words like how do you expect that that and what. Be careful it's not exactly this because for sure there are words that tend to be. So the point is you want to verify whether you can improve the recognition rate by using as an information the words that are contained in the slide. So basically the idea is that in the moment you are in the slide somewhere your language change accordingly to the slide. Exactly. So the fact of having that information somewhere can help you to improve the recognition. Exactly. But actually it's not what happens. Yeah. And this does not happen for many reasons. First for example 50% of the words in any kind of text are stop words so everywhere. And the reminding appear so little that in the case cannot really improve that much. Most of the words we use actually whatever we talk about are common words. So how to verify this I mean this is something that has been measured etc. but still is a bit qualitative. To have a quantitative measure we simply did a measure of statistical independence between the words in general. So not some words. In other words you can see that if a word appears once basically it is 100% related to one slide. But you have to if you want to consider in terms of recognition performance you have to make it overall. What is important is not the word that appears once. It appears in almost if it appears in all the slides it says 100% correlation for me. If there is one word that appears once all over the meeting it appears in correspondence of one slide. Of course it seems to be very related. But very few words. I like that. They represent a very little part of the word mass. So we simply use the very old test. Statistical test that basically measure the hypothesis that some way the probability of having one word in correspondence of a certain slide is simply the product of the probability of the word. Okay. That's it. Okay. But in general the language I'm not certain words. Certain words. Because certain words for sure are strong dependencies. But they are a few. Okay. Fine. Now I'm okay. I'm into the law. And the answer to that was no. And so when extending those experiments to the other meetings, when I was observing an improvement on the first meeting I was using. So I was observing an improvement as I was increasing like the context. But the improvement is on the ASR. On the recognition. So the improvement was also increasing as I was increasing the context. Well on the other meetings it's really fluctuating and it looks more like, wow, what's the word for that? Statistical fluctuations. The fluctuation. I mean it's probably very dependent on the speaker as well. So people have a habit of just reading what they've got in their slides. And some people. In terms of quite purposefully talk about different things. So that they've got multi-modality. So I mean I guess that's not a surprising result then. It is not in terms, I mean for me for example it was not in the sense that after working a little bit on language you realize that this kind of thing do not help. Simply because most of the words have nothing to do specifically with the subject use. Most of the words we use are strange. But are simply necessary to build the sentence. There are very few content words. But it is true that intuitively as we are driven in our attention. I mean in our understanding we are pretty much driven by attention that we tend to spot the words. Yeah sure. Which goes straight to the semantics. Exactly. And in some sense intuitively it seems that it can help. I mean this is not the first attempt to do things like this and it never works. Actually in terms of recognition. For other tasks. Maybe. It can be helpful. So you mean to say that like when one case you were saying that if you increase the context and all those things it helps in your ASI improvement but it doesn't. It's more subjective I think this whole. Well I mean it just comes down to the mere fact that word error rate is just take the words. How many of them did you get right? If plus or minus two or three or five or ten words doesn't really make any difference. Even if those words at the end of the day would be quite important in any sort of search of the transcript. But at the end of the day why wouldn't you just use the slides to search the transcript. Yeah. If you have the slides then. Yeah. So that's why if we look on the relationship that exists between the speech and the slides the task of improving the recognition. The overall speech recognition using the slides seems to be not very good. You can't expect much from it. So have you been doing other things then? Have you been thinking about what you would like to do in place of this or leading on from this given what you've learnt? Well for now the most effort was on showing why it will not work especially those experiments and these statistical tests. But we were thinking for example of a task like trying to see if a speaker is talking I mean if the speech is actually correlated with the slides which happened during the not correlated but if the careful collet is a dangerous one. It seems to be the opposite of the slides. Well if the. Associated. If the speaker is actually talking about what is in the slides or not. Okay. We're off lit. Basically one thing it can be done and here on the contrary for example the few words that are or may say recognize more can really make the difference. It is in the case if you want to see if I mean what you've seen in the meeting in the okay the representation then yeah still the slide is there but actually the people talk about other things or not really other things but the slide is no longer a support for the discussion. There are moments that actually this is the support because the people describe etc. And there are moments where it is not it's just a background thing and we are after estimating it very quickly but it was one third of the time at least in the meeting we have seen. The slide were just there I mean but they were no longer used as a support it was a discussion with the between people. So in that case the presence or the absence and especially the frequency which you observe the words that are on the slides can be an excellent clue indication a clear indication whether actually that's a support of the discussion meeting action. It can be the easy it can be interpreted as a kind of focus of interest it can be simply interpreted in the sense of saying okay I mean you have this channel open there we have to take into account or not. Or also in terms of action yeah when you see that the discussion is completely disconnected with respect to this. Some way it means that it's happening something different before. So I mean it's a kind of feature that in my opinion can be easily detected and that can be interesting to do relatively easy to do and in that case for example the few words that you get more can make the difference. So it's a double way to show how so it depends on the task. So maybe it can be something like in a meeting it can be like an unusual scenario for you in a presentation with the slide and the initial scenario that you're not using the slide at all. Yeah or for example what happened for example in one of the meetings is that people were summarizing the results of the previous meeting also in the beginning. So the meeting started with the summary of what happened the previous time and this was not related to what was on the slides. Yeah okay so I mean you could begin with seeing the simple task of just determining whether or not the speech is related to the slide content which is sort of building upon what you know Dong and others have worked on in terms of you know is it discussion monologue from the original M4 data collection. Are you also considering that you could actually look at the relationship between meetings for instance. What do you mean? Well okay well I mean if these similar sort of phrases or words were discussed in this meeting and were also discussed in the previous meeting then you sort of have a link between meetings for instance. Yeah some way. I don't know how this you have to check the statistics. This is the worst thing. I mean because obviously you've got to think towards what you know this one occupied for three years or whatever just unless I guess. I have to look at all the features. Yeah I know this we are just talking about a very short term if you want but things just to use the things I mean all the work that you just done basically which is a huge work. It was recycling. And the data we have then you know for the for a thesis of course it must be much better. But I've got there's only two different kinds of works some of them you can do it on a single meeting so this kind of feature extraction because basically that's what it is saying yes now this channel is good now is background now is foreground etc. Yeah okay. Put it in that way this you do it on a single meeting. And then there are corpus based on the C works. So yeah that is one possibility for example finding the connection between different meetings. And I don't know I mean at that point it becomes pretty much crucial the kind of data you have in the sense that for the way we've collected the data some way I don't know if it can be at the same time too easy in the sense that some way you have little groups of meetings extremely correlated. Sure. So I mean it makes it easier I mean I don't know how much significant can be at that level. Yeah I mean it's and because we don't have all the data collected or annotated yet it's very difficult you know. In a sense if you want this kind of first become interesting when the corpus is really big. Yeah. Yeah. When you consider each meeting as a single item so it's interesting when you have tens hundreds thousands of meetings which is not the case. But I mean you can do other work if you consider in terms of speaker terms or in terms of the same kind of Yeah. Yeah. Yeah. I think that's quite interesting I mean because it's an awful lot of speech activity. Yeah. So it's going to be one more feature that helps in. But it is very short term I mean just you know it's just a few few few times we talked together and it was just very quick thing you can have like this. Yeah so I mean as far as that you've got everything you need to do that I presume. You don't need any extra stuff from us in the immediate. Oh no. I mean that's good. No you have plenty of things to do. But for the other thing you know the alternative measures or maybe we should finish with this first. Yeah I wouldn't get too distracted I guess. If you've got two tracks I'd like to. Yeah this would be another subject I would like to discuss. But yeah yeah it's no problem. But basically just know if we finish discussing this. Okay. So well so ultimately now the next step what is the other one like kind of a. So first you said that okay now you did this correlation studies and you showed that there's a very less correlation now in fact no correlation kind of thing. And so what next and on top of it what you're going to build the question now. But I think it has since in my opinion what are two things. When this statistical test it can be we have made an experiment we get to result somewhat counterintuitive we give an explanation. And that's something that's some way closer here. So it gives you the kind of answer you get is that okay to go in that direction maybe it's not the best thing maybe to improve the recognition that way is not something you can expect so it gives you the possibility to decide to go in a different direction that can be more on this. So I mean to the different direction that's what I mean to say what you like to take from there. So it's not too easy to find yet. It's not defined yet okay. But something using this relationship between the slides and other even other take textual support and the speech. But what kind of problem we need the definition of kind of problem right. And the question is also are there other sort of more sensible ways of doing it for instance gaze tracking. I mean if people are looking at the slides. Well I think the advantage eventually with this kind of things is that are much easier. Gaze tracking is still. Yeah sure. And it has specific to the environment and all that sort of thing. The environment is a difficult thing and it requires cameras with pretty good quality etc. But I think his question is more general and he says I mean based on this work do you figure out a direction if I get the direction. That is the thing. What is the next direction? Is it not that? I don't know in a direction for your thesis. It's not happened that you're doing so many things and ultimately it doesn't go into a thesis. Exactly. That should not be the problem. The thesis should be in one direction, one problem. And it should go. One direction or one direction or one direction. What are the things completely not related to the future? It doesn't make sense. It may be good as a CV for you but then as a thesis you will have problems. Defending it and all those things. So what is that? What would be the next possible direction you want to take? Well this is the question I'm working on. But I mean for instance it should be linked to the other stuff that you've been doing on call routing for instance. And you know error measures. Yeah, that's good. Yes. I mean I think it's possible to draw some sort of relationship between the two as we were saying. I mean maybe if you don't measure things in terms of where they're right maybe this sort of information does actually mean something. Given that 50% of the words are not actually interesting. So I mean. Yeah that could be interesting to try to merge those two things. But still doesn't solve the problem. What is that? You are sure what you were going to do for your thesis. As I always say I mean basically if you have to explain in four lines. Yeah exactly. What would be the topic of my thesis? The subject of my thesis. And it is something that is not just saying I'm going to do this and that. I mean because you're not. But yeah in a very broad sense I mean what you're going to investigate. See that is going to tell something like the thing is that you can totally go into the text domain and be there you know. It can be no problem but as long as it is okay. But if you want to do with like something with the speech and all those things together then it's a different scenario. You may not want to work on all the problems. Someone may work on some other problem but then you make use of that to extend your work in something. And as I also it depends like also I think if a boss is interested in working in both speech and text if I understand very well. Yeah that part is also there. Sure. They are interested in that. So it was like heavy like suggesting like you should like work on both sides you know the text and the speech aspect of it. I don't know what Patrick's thing I suggested in the name is thinking about. I mean he's exactly this there are all these scenarios that can be meeting that can be presentation where you have a relationship between speech between things that are saved and some textual documents. Why not try to study this relationship to improve this relationship to use this relationship for whatever it is indexing or annotation or whatever metadata extraction. And if you want for example this little work about seeing when the slight channel is background and foreground is not in the next and beginning of this kind of thing. So a relationship between two channels that provide some kind of information. I think it may be an interesting test scenario and we know that you're sort of basing something around speech and text. I think there needs to be one sort of extra higher goal above that so that you can motivate to do the research. One thing interesting is what we came up in the discussion also you were telling that detecting such scenarios where there is no relation between the slide and the present talking. That may be one good starting point for you actually. And then at that point you do take something like that and then you show something like what he was saying that if there is if there is out of discussion the words are no way are going to help your recognition anyway. Yes sir but if it is more on the slide information is not going to help you but if it is related to the what he is giving a presentation on the slide then you show your answer. I mean if you detect this even then you can show that how where you can really help or not help because you say there is no correlation now but then here it comes at some point there might be a correlation there. Don't be careful. For the recognition in general there is no correlation in the sense that even if you are talking about what is on the slide most of the words you use most of the words you use are not really correlated to that. They are simply words and consider also that we all speak English here and a part of native English speakers we have a very limited vocabulary in general. So we tend to use all of the same words but to average you know linguistic research shows that to average the people use 500 words. That is what we have a conversation. Yeah for English it is especially lower. We say everything with that and probably we not a native English speaker we use even less. So that gives you an idea of why basically in terms of recognition it doesn't really help because you always use anyway the same words and the few occurrences of words that are there in terms of recognition make 1% so maybe it is not enough to justify an effort to improve the recognition. Then you can change of task. And use the task we were mentioning about the Bechannel or Fergram channel. In that case the only words that really are important are those words that appear on both slides. Right and they are all. At that point I mean a little improvement of that it becomes a big improvement. I mean maybe in terms of recognition it is 0.5% but in terms of that task it is maybe 20%. Okay another thing you have to remember is these models have been tuned to include Ami data. So I mean perhaps if you had slides and presentations to do with something that was off topic you didn't have you haven't seen any prior data then you might see a bigger contribution as well but I mean essentially you've actually sort of included the information from the slides in the language model. That's another interesting point I mean we are using data that are fake in any case there are simulation, no real data. And especially the slides I think I've seen the slides and they are pretty much artificial. Okay okay they're pretty prepared. Exactly. Most of the time you have the title to do not the Ami data was that like that only. Most of the time you know the header what are the three slides you are going to present everyone is presenting the same. Yeah I mean everything is pretty much artificial that's what I mean. So you mentioned for example this fact that sometimes speakers tend to read. Especially through when you have pallet list and sometimes people really go through. It didn't happen in the data simply because it was not the same thing they were prepared exactly. Okay. So for example it would be possible in a case because everything for structure is ready now we are going to collect the temptation there maybe we can see. What about MLMI the one we collected last year. The audio is very bad right. I think it's really bad. The dinner time is one single channel at least closed channel collection no. You know that was collected a little bit like this without too much care it was it was personally a part of myel that decided to do it. It was good. It was good. It was very good. But basically it was made without any specific you know without thinking we want to recognize just to have them. So especially the audio channel is absolutely. So they are still using that desktop microphone or whatever. Yeah they had like they you know they had to set up with the microphones. So someone every there was one microphone shared between every two people. But they had deals with people forgetting to turn the microphone on forgetting to turn it off. It was it was meant I think they they chose that lecture theater possibly with the idea that it would be good for collecting data. I mean I I'm sure they could have probably had university of Edinburgh hosted or whatever. But I think the you know the HCOI part of it to be quite work out to how they planned. Okay that's that's that's that is one of the reasons for example why would Shamark with point pretty much no slides as a mean of indexing. Because at that point there is no interaction between humans and capture. Because whenever you have microphones that you have to move it sits. It's just completely. It's I mean the same as us putting on the headphones and whatever I mean even that minor thing I mean any any audio captured during that period is is rubbish. But yeah there's no sort of system that's really built to be able to say when you're. So now the without me sometimes what is we should be getting a time recording for this camera. We are building this slowly and it's it's important that and it's taking time for the material and so on. But yeah that will be the thing. Okay what will that incorporate will incorporate like an SMI and it will be one camera pointing very general but just for display purposes essentially. One microphone and we are thinking to use a little microphone you know this this really thing and so that potentially helpful hopefully helpful to the recognition and the slides. Okay through the projector. Okay so no microphone arrays or anything like that. I guess that's quite a that's a quite a big deal for setting up the recording. Let's say that in any case. It's quite a big problem to do that. First of all we are trying to do something that can be easily the idea is really it's something that you take you bring somewhere else and it works. So it must be as easy as possible. But in any case I mean the device we are doing has input channels. So some way you can add as many inputs as you want. So we are dealing now with three but there are other input lines open. Yeah but probably within the you know the post the thing is somewhere over project resupposed there. The whiteboard or whatever the presentation is over there. The speaker is more probably going to this way or that way. So probably you may not need a very big microphone either probably one or two. Yeah less pretend it's not going to happen in any foreseeable future. I mean I'm just thinking from practical perspectives you'd have to the the AME recognition system that we have now is either for these microphones or for the microphone array. There's no lapel system trying to instance. I mean that in itself it would I mean you'd have some fun training some more speech recognition models. The point we can use this and I was mentioning the the level microphone just like this but if you tell me this that's very interesting because at this point we can use this kind of things rather than why not suitably other data other test data and especially for this kind of problem I think it can be a much more useful kind of because there is real presentations I mean it's sure it's reality you know it's it's what you should be using for this type of task. But it's more realistic and probably it can help. I think going this direction of saying well let's see what happens between speech and slides. And every time we we generate a new corpus and try to carry out new research we discover sort of areas that are too controlled. I mean the AME ones are a lot better in terms of that than them four or whatever but still I mean especially when you get into higher level sort of interpretation it's. I'm just curious do they like the Ixsee meetings do they have anything other than the audio do they have slides or anything. I don't think they would. Okay I'm just thinking whether or not there are any other sources I mean maybe chill. Chill may be interesting for you but it may be interesting for your problem I think. Because I think it's going to be more like the TAM type situation. Yeah it's more like TAM but yeah that their collection is much like they have an audience and all those things people say it's a lecture meeting collection. Are they going to release it through LDA or something. I don't know but I mean given that it's coming from the same basis as AME I expect there has to be some sort of relatively liberal sort of participation policy. Yeah yeah it's a European project anyway so it's probably available. Chill may be interesting actually. The one thing about that and this was something that came up in the last NIST evales was the fact that it's almost too uni modal. You've just got one lecturer talking so once again you don't see the complex sort of levels of interaction. I mean there's no way of getting around it. I mean you just have to think of a task a challenge that is relevant to that type of recording which might you might use the same methods as what we've been talking about today but the actual sort of what you're trying to imagine might need to be slightly different I guess. You know in terms of say measuring whether or not they're talking about the slides or the channel you might find 98% of your data is directly referring to your slides in a time. But now we were measuring one third it's not. That is in terms. At least in... No I'm talking now about the... I mean if you move on to the use presentation of course and presentation you might want to be doing something else basically. Of course. No that is just something that in my opinion with the work has been already done is something we can get pretty quickly. It's an interesting task. It's more specific on meetings. Certainly not for presentation. It represents a nice task that can be measured that is clear and... Yeah and at the end of the day you're probably going to be using learning about the same techniques that you'll need for doing other things. In terms of action recognition can of course help. But of course it's not going to be a kind of... This is about this. It doesn't make any sense. It was to make a proposal basically. I feel. Yeah. That's in a sense if you want. I mean that's at least the way I see that I've made for my own thesis. I mean you just find that the main which is large which can be very general so interaction between speech and slides. Cool. And then you see how all these things, these two little works or the statistical independence and detecting when it is are two little works that some way fits in that very general framework. Okay. Shown for this point of view. The other things for example what I really would like to do which is kind of fun. So when you have a ballot list can I click on one of the ballot and get the piece of speech where the guy was talking about that. Sometimes people just read. Sometimes people improve is more... For example that's another thing that fits in that kind of framework. And that's interesting in terms of... First of all again is measurable in terms of browsing, in terms of retrieval, in terms of index in annotation, let's add a text to action. That's to be a binary. Do automatically speaker rating if they're good speakers or bad speakers depending if they just read. Wow. Look at this. You know that something that's maybe joking now. Something that can be done for example. The police spot if they forget to talk about something and it won't let them continue on to the next slide. And it's like those are all the points. And so why there are parameters that can be measured that tell you that following certain criteria. There is one very easy for example. There are speakers that never look at the audience. Those are bad speakers in general. It's very wrong. You have to look at the audience. This is one. There is for example, and this is something that eventually can be done. If you have a certain number of words on your slides, you need a certain amount of time to read them. The speakers that take a time which is too close to that, they are not good speakers because some way they're talking about... I had a problem. I had a problem. I had like five years of health on my own. They are not good. Well, that was just through a sheer wide of content. They are not giving you the time. There is a certain number of things that can be some way detected and measured. They tell you whether the speaker is actually respecting some form of... It's a bit difficult and I will not go that much in that direction. You're going to be possible, Samhli. So what about the figure things? What happens if there are figures coming on the slides? What do you want to do with that? Figure... I think there are five. There are tables, paper, yeah, visual things. Well, for example, we submitted a project with Shamark to use that thing as metadata. And basically, one thing, for example, we are trying to do, and again, this is something that can fit to be not... After maybe that, but if you get the project, etc. And it is something that Shamark and everyone want to do. Based on the kind of graphic object you have, so tables, questions, figures, plots, etc. The side one, for example, it is a result. Try to get. So that's again, in terms of structuring, browse and structuring, retrieval. Okay, when it is a result, when it is an introduction. Although I remember discussing this, because when the bill was working on his stuff, which was just based on the slides, he asked me, could I pre-segment the slides for him? Because you wanted to have the topic segmentation. And I said, well, just look at the headings. Because anybody, generally, who writes slides. It's not really true. It's not true for me then. Yeah, it's not really true. That's one of the most funny things. You can see when you work in this thing, each one of us has an idea. And then it's not true at all. Some people actually are stick very carefully. If they put in the outline, let's say, title one, title two, then you find title one, title two. But many other people, and I am one of those people, for example, are much more generous. So my outline is something like introduction and experiment and results, conclusions. You don't find the same title in the slides. Okay. It's really a different style of the time. It's a stylistic thing. It's not just the name. People that don't put, for example, outline. We were, for example, in the corpus of MLME. So the one is on the demo. 50% of the presentation have no outline in the corpus of the term 2004 term presentation. 25% of the presentation have no outline at all. Which whether or not that's a good or a bad thing. It depends on the tone. Yeah, well, it is. And basically that's, again, the outline is typical, maybe in scientific presentation. In that case, the majority has. Other kind of presentation probably no. It's a nebit we have, but... Something we taught at university from a very young age. There is nothing true. Yeah, sure. It's all subjective. Nothing true. It's everything. It's changed pretty much. So that was some... Okay. Okay. So what are you going to do? Between the data when you collect and this. What do you want to do now? By the time you get the data, it's going to take another two months, if I am right, the whole capturing system and it's going to come. So what is the next step? Well, trying to really determine the subject, something for my proposal, I mean. Yeah, you have to submit it in like two months also again. Yeah, around February or March. Yeah, I have to submit it. So really... It's funnier, right? Yeah, yeah, it's frustrating. And pause now. So to find something for that, also will the thinking maybe turning this work into publication, the one about... Not maybe. For sure. You did it bad. And also, well, that depends a little on you, but try to finish with the... All of them. measures and the poll routing. That's interesting. Because I was talking with that about that to Ale. Since I was... I will recycle this poster for... I am too, you know, about this. And that could be cool to finish it. Maybe fuse it with the E.M.s work and do a general paper he was suggesting. After the hard drive crash, we took the time to actually rebuild the system and actually train... Because originally we sort of had the idea of, I will use a recognizer that's been trained on a large amount of telephone speech data and that should give us a better result. Just as a contrast, I trained one specifically on this data. Results are almost identical. Which says to me that something wrong with the data rather than... And it's nothing serious. It's simply that it hasn't been chunked or segmented in a way that's typical of speech resources. In terms of you can have like a two second utterance credit cards and then 20 seconds of silence coming after. Which when you first think about it, you just think, oh, silence, whatever, nothing will happen. But a lot of the normalization techniques that we use... Yeah, CMS, CVN. So we needed these in Endpoint Detection system running before we can do that. And we converging on having one now in tandem with this development of this web based recognizer. But unfortunately all these things take time. But I think we should see... No, no, I think... Yeah, it's decent sort of improvement. Not just in terms of no longer having recognition errors whenever silence appears. Because you get that immediate benefit that you don't have any insertion errors. But also it should actually affect the recognition on the speech segment as well. So essentially you say, what are the best risks? This silence time which tend to be even more than... It's better to have a kind of silence detection or something that really takes this and then recognize only that part. This will improve the phoneme recognition, right? That's what we expect. Oh, I might... Given the trouble we've had with this call for a second, make any promises. At that point even the call router that Janja Yves made can improve its performance. Yeah, that's the intention. Exactly. And it can be used after for the measure work. So what you are saying is that you notice that when you tried other features and... We're trying to system just on this data. There were 6,000 utterances that weren't used for the core adding side of things. I think there were 13,000 in Tycorpus, half of them roughly were thrown away. So use that for acoustic training. And the results from that were almost the same as what we got for CVS. And initially it was a new answer. No, no, no. But one can imagine a commercial system that Stephen Cox used from nuance probably had all these things built in endpoint detection, etc. So... There should have had that actually. So maybe we have to do this endpoint detection and probably a little bit of linear filtering. And then I think it should be pretty much converging to the system. But the reason with that, like what are systems we train on databases are they are chopped neatly. That there is not too much talent beginning, no too much further than... That's not so neatly further. It's like custom made database. But here it's not like that. I see. There's also just things like receiver, noise at the beginning and or at the end of... And I mean these are things you can get rid of. There are several things. One thing is we can do this beginning and end detection. As we said, or even we can try to use Franz Sellecom's approach actually. And they have this front end which can do a spectral subtraction. And plus it can give you a voice activity detection. Okay, this is... Part-frame basis. It can give. Okay. So we can just run it quickly and see and probably you will have to run it. It will give you the idea that is it okay, like... Well, I don't mind running it. I've got all the scripts. No, no, no, not running. I mean to say probably he should run it or like we can help him to run it actually. Because at some point he has to get into... I mean just... I'm not sure I said the poor hate actually. I mean to say that it's okay. We have all the scripts but probably he should run it. I mean to say... I don't know. Yeah. Whatever I don't know. I think... I don't know what exactly. All these experiments... I don't know whether that's... For the co-roaching itself or the recognition. Because probably maybe we shall do... To make it faster we should run... If we run it, we have a server. It's very fast. We might as well. Because I mean up until now you haven't been running any recognition. And neither did John Eve on that data. Yeah. People join the scripts and the programs for performing the co-roaching. I think starting from the new sequences. But I mean to say that if we give him the scripts and ask him to run... Oh sure. I mean the scripts are so automated. It's just a matter of submitting a job to the classroom. No. But it's... Yeah. I don't know how to do this last time. I don't think the point of that. I don't mind running it. It would be like half an hour's work in a day to run the recognition scripts. I mean I... I know what you're saying. Our team should familiarise himself with the ASR systems or whatever. Fine. But I don't... I don't think this is a point that's worth sort of worrying about at this stage anyway. I mean... Yeah if he's just running a script. Because I'm sort of assuming that we're going to have an endpoint detection system regardless of this... Yeah. Working on this nuance. No. So... The endpoint system is different thing now. That's... We need it for Ami. We need it for Ami. We need it for... I am the last thing. I am the last thing. Yeah. I am the last thing. But this is basically detecting speech and non-speech. It's a lot more difficult than you would think. No, no, no. It's not a big no. It's this problem here every time on the speech meeting. How do we detect speech and... Well it's speech and any other class of sound. That's the problem. Yeah. Anything nonsense. It's not just speech and silence. It's speech and any other class. It's not that. It's just silence that would be easy. And in controlled cases you can't just assume it's any energy below a certain threshold. And this is what everybody sort of claimed the problem was solved with years ago. But who really can do when somebody will really be able to do that, I mean detect... Well I mean this is once again... Yes from non-speech it will be a little revolutionary. Yeah, sure. But I mean this is once again a very constrained task again. I mean it's... you're almost assured it's only going to be a single speaker on the microphone. So I mean the only sort of noise you're going to have to deal with is... It can be everything. It can be something. It's so impulsive I'm going to say. Or that would be a good start. Yeah. Anyway, so we... Okay, so we'll finish this quality stuff. What do you say? This is okay with you, right? Because I think it has not been even submitted outside also. What journey did you have? No, no. Basically, no done that. Because it was not that... At the level it could be. Anyway, it was good I think with the better... For me we're going to show right, definitely it could improve also the performance. It was showing that. You haven't heard from him I take it either. No. No, okay. Well it would be nice to get the thesis I think. There's a sort of just a research report sort of basis. Yeah, yeah. Yeah. But we can some way find a guy and he should be at multi-tails so it should be relatively easy to... At least there's this last news I had and then we can try to check him back. I think it's less pressure. So yeah, it's pretty much good time also. And when do you intend to start writing the proposal? Yeah, well this is basically a three-year main jobs proposal. No, you should not wait till the end. That's what I mean. I should be looking towards the start of next year. I assure you. You should make it clear before on what I will write. And even a first submission using this work will make things clearer for me writing them down. Okay. That will go very pragmatic. There is a submission deadline for example 31 December for confidence on multimedia and Expo where I think this tool works on... Yeah, it can go there. And it's nice to get some feedback on... I mean that feedback will probably come after your proposal. I think it's good. If it's a research report, you put it here, you can submit it for the conference. It's quite good for the proposal itself. Yeah, yeah, yeah. Exactly. Because as far as I understand this work on the Fonim anyway still takes some development time and the sense that... I think we may have to... It's kind of like... maybe take like two or three days to sit it out and say one... Run was to one approach and then just do it as... Yeah, so that any bugs or whatever. Yeah, and probably that is the thing. We have to sort of the bug kind of thing and see how reliable is this endpoint detection for us. I'd prefer to go back to the CTS models I think as well. Yeah, I think then we can go back to CTS. I think they did work a little bit better. Anyway, we use the trial results. You have just to decide what you have to work first and finalize. I think you can really... Yeah, well I think first this submitted and this will help for a proposal and since we cannot start immediately with the measures, I think that's the best thing to do. Okay. So you wanted to talk something more than that. Well, we have talked to you with that. You wanted to talk something different also. Well, no. No. For me it's a pretty... Oh, maybe another meeting. And it's another one I said. And then I say another one. No, so you're looking at the watch before? No, I was just looking at this. Okay, we are not out of time, you know. I think we thought this like the night time. Yeah, and I think that will be exactly one hour and... So, you're looking at the watch before. No, I was just looking at this. Okay, we are not out of time. You know. The design we thought was like the night time. Yeah, and I think that will be exactly one hour and... So the tape is... The tape is... The tape is 60 minutes. Yes, it happened with my friend. Like he was taking pictures, pictures, pictures and it was going 37, 38, 40 even. At the real level got over. So I said, there's some problem with you. The way you put the role inside. There is some problem with it. It has got struck somewhere. It can never go to 40 or 40. Not normally. Yeah. You got a 37, 38. 38 is 5, you know, 40, 41. That's it. He has already told us. Yeah, okay. Okay. Fine. So we are thinking pictures and pictures and I'm sure they are going to get maybe 5 pictures out of this. If you get 5, you feel lucky. Yeah. So, next week... So, we are going to get a picture. So, next week... So, we have to be... We have to be a photographer. We have to be a photographer. We have to be a photographer. We have to be a photographer. That's getting that much respect now. No, it's not. So, what's that good? I'm not? To do it. Oh, it's a business. I think that's fine. I think that's fine. I mean, the point of the position is just to provide information to people. I'm going to be interested in the discussion. I think we have to be a photographer.