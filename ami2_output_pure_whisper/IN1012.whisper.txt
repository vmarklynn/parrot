 From now on, you will be quiet. I think, didn't I seem to talk a lot? No. I'm here to take care of the QR-O speaker. I never know how to do it this. It's always the other way around that, how you put it on. I think the most of the bad words are at the beginning. At least precise time, yeah. So you start, I think, right? So we all attended inter-speech, no? Yeah, so we are going to talk about the papers from inter-speech. So how many sessions you attended? It's not a good question, I think. I mean, did you attend any session? All of them, of course. You can't attend multi-position at the same time. Yeah, that's true. That's true. Okay, bad answer, right? So any general impression or feedback? So for any of you, it's a first conference, or maybe for course at all? I mean, in the sense that it's a inter-speech is really big conference for speech. Yeah, it's not very direct. Did you attend before? Well, Geneva, that's why I'm here. Oh, okay. Yeah, Geneva, your speech. Yeah, so then I think you have attended before. Yeah, we met in Iceland. Iceland. Iceland. It was the first time. So how did you like the conference anyway? Right. When you compared to the others, the previous one? I think it's quite interesting, but only annoying thing is these multiple sessions. And sometimes you can't able to go to Warhol Tech. These warhol presentations most of the time. Why? Because posters, you can spend a lot of time posters looking at many posters. And sitting 20 minutes for one horror presentation. So in 20 minutes you can see at least two, three posters. And you can directly talk to people. Yeah. So even I found like very few people in some world presentation. Right. I think most of the people are, they can own posters. Okay, so we attended mostly posters, I guess. But how many presentations? Yeah. Presentations are hardly handful of like five or... I like the invited speaker about the implant in the year. Oh yeah, it was quite good. It was from Austria, right? The guy? I think so, yeah. Yeah. He's very famous, I think, for that, right? He's the inventor of the implant and... Oh, okay. For the year, I think. Yeah. Even some panel discussions on the human speech recognition, reducing gap between ESR and HSR. It was a bit interesting, a lot of arguments and... Definitely, it's mainly the differences, different approaches of engineers, versus linguists or phoneticians. I realized that there were panel discussions only at the place where I could see the... You know, there were some boards saying that there is some panel discussion. I couldn't get... No one knew, where is it, what it is, what it happened and why and so forth. Yeah, even the panel discussions, I think one is really held in a small room to... Yeah. People were... Really cold. Really cold. Wow, this speech has not restricted on it to papers and these things. So how did you like this up on the... No, quite good, yeah. Yeah, it's a bit hot, still. Yeah. Yeah. The beach nearby is quite nice. Yeah, if you try... I don't know. Did you go around? Yeah, before or after? Yeah, it was too. Gustavo Carpatica there. Okay. At the past, we were supposed to take the direct one, but we didn't. So you went to a beach? Yeah. Like Atlantic Sea coast? Yeah, I stayed two days before, so I went... You just take the train, maybe that's the same and that's true. The train knows so far from... Half an hour train. Yeah, north? Yeah. West on north, I don't know. Okay, so you stayed at the same coast, not... You didn't go to the island, cross the river, where it's the bridge. No, no, not very far. No, no, no. Okay, so... It's different. We went to like far south to Lagos. Yeah. And... Yeah, with the... Oh, Emma. Yeah, yeah. Really good, those beaches are really good. Yeah. More south to... Yeah, even though the... The hot through the water. Yeah. But weather was really hot in the south, it's more than 35. Yeah. But I think, unless one, those days were pretty good. This one was good. It seemed to be quite... Not cold, but still okay, like reasonable. Yeah, even local transport. Oh, yeah. It's fun, like... But yeah, most of the time the buses are really cold. Yeah, I was happy to be back in Swiss. After a few days, it's a better to be here, I think. Yeah, because it's really big city now. Yeah, it's big city, many people. I was very surprised that even the trams did have AC there. Yeah. I couldn't see it. Those new only, right? Yeah. Not the old one, but... Right, but can you imagine something like in Chequi? Yeah. Yeah, it's not necessary to have it, maybe. Well... Sometimes, yes, but that's so major. If you would have to spend an hour and a half that they might and the other spent while going with the bus to the conference center from the hotel that we were, that was the direct bus. Yes. And you should be there now. No, but that's not good. That's not good. That bus was not good. You can go to the center place and then... Yeah, it's a good trams and a train. Okay. Yeah. That's all the right choice, Baba. So, it's like something which was direct. Yeah. It was not the......it was not the direct bus. Anything else? Yeah. Oh, what is that place? Keshe. Keshe. Keshe. That the center, where we changed the bus to... I don't remember the name. Keshe. It's very nice. Yeah. Keshe. Keshe. Keshe. Sure. Yeah. Maybe you can check the book. Yeah. Oh, you got this. It's still like I used to find bus number from book. Originally, I brought it because I marked some papers, but... One. Yeah. Only one which surprised me a bit. It was a speaker recognition, I think, but it doesn't really matter. They were trying to model covariance of different components for GMM instead of using a diagonal. But then if you use a full, it's too much. So, they used some approach where they tied them automatically. And after that... Proning....of covariance. Well... It's like... It's more tiring. It is still full, but different components are tied. Different components of the matrix are tied. What does it mean tied then? Equal. Like... Yeah, but then you have to store the whole matrix and plus some extra information about it tied to it. No, no, you have to store a minimum number, a reduced number of parameters. And they are tied in linear, we're actually sorry, not equal, but... It doesn't. It's like all the semi-tite covariance... Yeah, exactly, that's it. So, they reduce... Yeah. It's in between the direct covariance, which is full covariance. So, you try to reduce some parameters by tying. The interesting thing is that they applied it on speaker recognition. And on the features, before they had three different ways to decorulate the features. And this showed it without this... Yeah....with diagonal GMM or with this. And with this, with semi-tite covariance matrices, it was much less sensitive to which decorrelation procedure you choose. So, they obtained better results with that? Or so better. But of course, it depends on the number of parameters you put. Right. In your tail... But if you compare it with some baseline, let's say you use just GMM with diagonal... No, I think they got good results. I think just using diagonal, so definitely. I think it was slightly better, yeah. Yeah. And no decorrelation for like... I can find it. Maybe decorrelation, again, they do DCT or KELT or D but still there is still even. That's why people will come. So that means that the decorrelation is not optimal, right? Yeah, definitely it's not optimal. So it's also like doing along the diagonal. So, again... So it's completely impossible to do it with covariance matrices. Why? It's really computationally, really... Yeah. I think it's completely... Of course. So from the point of view of competition, but otherwise it's... I think there's no problem. No, there is no problem. No problem but DPC is so many models and so many mixers. So many mixers. I have many things to store and huge computations. You can't really... You can't... The models itself is like 38 times more than that. So if you have like 5 megawatt or 10 megawatt sub-modepens. But even I think it's really... Yeah, it's this one. It's like impossible for the really big system to do it. Yeah, it's an improved covariance matrices. Almost impossible for like LBCSI, it's impossible to use it, right? Yeah, it's really tough. You've got the dimension of 39 times 39. Yeah, for every year. So you have to store all this... Yeah. So there are no results there. And you cannot even train it. No, but more. In the proceedings you can find them. They are better, yeah. They're from PCA, LDM, LRT. Oh, so... Interesting. It's not really anything new but... They just applied that to speak your ID. Yeah, different task. Yeah. Yeah. So I found one paper interesting getting you know with it. This is VFX, paper on F. Variable scale feature extraction. Normally we used to fix it window for feature extraction like 20 family seconds or 30 seconds. Here he's proposing variable scale. Because the fixed scale is non-optimal for... It's non-optimal because for vowels you can have... Much longer. Much longer and for plosives and these things. It's really shorter like even around less than 20 milliseconds or so. Okay. So he's proposing like this variable scale window kind of online for each segment. Why this to be online? Yeah. You can do like you can measure and you can do... But he's basically doing some likelihood ratio testing. So the main idea is like suppose you have one segment. So he's trying to find the stationarity, cautious stationarity of that segment. Right. Like extended as much as possible. As much as possible. Yeah, as much as possible. Okay. But yeah, definitely you have to assume some minimum and maximum sizes of your own. So he's using minimum... What is the minimum? Minimum is I think around 12.5 milliseconds I think. Maximum is 60 milliseconds. So is it... Isn't it smaller with minimum? No. Minimum to 0.5 milliseconds. No, I think a minimum is 20 milliseconds. 20 milliseconds. 12.5 milliseconds is kind of 50 or this. 20 milliseconds minimum is... I don't know. No, not as small as it should have been. No, you can use small... Other processing after. No. Mainly because of MFCC computation because it becomes really noisy. Like if you use... No, it's... Yeah, 10 milliseconds means you have only 80 samples for... And then... Right. You have if you use 24 filters, the filters won't get any samples. Okay. But it's not an interesting limitation. It's just because of... Yeah, because of computation. So even... I think even he can find less than 20 milliseconds. Okay. And the shifting is still the same or not? Shifting... Yeah, he's using... Yeah. 12.5 milliseconds. Yeah, shifting is almost same. So... But the problem is like... How do you mean... Like... No, how many frames per second you do have? Is it again like... 100 frames or it's less or... Even if you keep variable length of the frames, right? You can still keep the same... Yeah, he's keeping same number of frames. He's using... 12.5... Yeah, 12.5 milliseconds. I understand this would have sense for... I don't know, speech coding where you want to preserve... Or you want to encode it into less frames, right? Yeah. Why do you do it for speech recognition? It means that... Number of frames? Yeah, you want to keep variable... Variability in length of the frames. Why not to keep the same length? Actually, the problem is again... You see the 50s, the Nyquist frequency, if you compute modulation spectrum. Yeah. So again, if you change this shift to the Nyquist... The modulation spectrum, Nyquist frequency changes for each window. So if you want to do again, another high level feature extraction again, it will be a problem. So... But he's... Like, he's here discussing... I mean, describing this because... Even this itself is a problem, keeping the fixed frame size. Because you're analyzing your... This segment many times. Right. So this shift will... Right. Suppose if you find one segment of 60 milliseconds... And then you're doing this 10 milliseconds... So every... Almost, I think, five frames... You're analyzing the pre... This already segment. Right. So again, this may blur some frequency transitions... Or... But the problem is again... This bottom link is here. Like, you can't change your Nyquist frequency modulation. Right. Yeah. I have two questions. First is... Essentially how he's doing it. Yeah. Well, once he gets the frame, like from here to here... Yeah. Then where he advances to start with... Computation over the next frame. So the main idea is like first you take the... Some window. Some samples. Then he assume somewhere... Some variable end point. There is a change. Okay. So it's not symmetric. It's not symmetric. It's not symmetric. You can see... Basically, our tool, like again... Do for all the samples in that 0 to n. So then what he does is like... He proposed... Like... Like-lute ratio test based on maximum likelihood. So that is really simple. So what he does is like... So he... He compute the residue. He first he does the LP analysis and then he computes the residue. And he takes the residual energy of full signal. Like these 0 to n samples. Okay. So the full mean full. So maybe some sub window or the whole... The whole... The whole means... The second sub speech or what? No, the... Like he assumes... He also some... Yeah, 60 milli seconds or something. He starts with something. Okay. So then he can... Something longer window than the one we are speaking about. Yeah. Then he can split that window at point n. Say point n. So then you will have 0 to... No, you can segment it. So you have a window and then you split it to two parts and you shift the... So where you split it or... Yes, see, so this is your window. So then you can move your... Like this. You can move your point. Okay. So then this will be like one and this will be second. Okay. But again he assumes some... Initial sizes for these windows. Okay. So basically we have... He don't start with... He don't... Yeah, yeah. He doesn't start with zero sample or something. Right. Starting with... Yeah, okay. So the left window starts at 20 milli seconds. So you already hear like... Suppose this is full signal you are already here. So right window is... Okay. Should then the 12 point. So you have to set basically... In this ring. So... Okay, so once you have say one hour of speech, you start at the beginning... Yeah. And first you try with 20 milli seconds. Yeah. And then the maximum is 60 milli seconds frame for him. Okay. So if you don't find anything... Where... where... where... where do you stop actually? 60 milli seconds if you... Yeah, I know but... You started 20 and go to 60 but where... which point do you choose? No, the point is depending on the likelihood ratio. So he got... Okay. So he supposed this... So this is supposed to say segment one and this is segment two. So he compute this error here. So... and then he give some kind of... in terms of this residue error. So this residue error is for full 0 to n samples. Then divide by suppose this point is say n. So 0 to n and then... Like... So this is the basically the main equation. So here... So he is maximizing that? Yeah, he's finding the likelihood for full frame and then it's basically likelihood ratio test. So he's comparing the likelihood of the full segment. Divide by the likelihood of the sub segment. Like the hoot. That's something and something. What is the likelihood? Like the hoot. Like this likelihood is... this error estimate of the residual. Okay. So residual power he can compute. So after doing LP he can get a residual and then he can compute the power of the residual. So that... He's proving that power is... So again maximum likelihood estimate of your LP parameters. So that way it's interesting. You don't really need to do a lot of computation. You just need to take the error of the residual of this full window and then residual of energy of the sub windows and then you can just divide them. And then the only thing is like again he has to use some threshold to decide that's only the... He got... he's finding something around 3 or 3.5 is the optimum threshold. But the... another advantage is like it's not really changing speech recognition. Whatever it's not really changing because of the threshold. Okay. You don't really need to like fiddle with threshold a lot. So... but why the main value which you are comparing to the overall value or overall likelihood or whatever you call it? Why he's using the whole signal for this? Because if the whole signal was steady somehow and it would be able to be modeled by LP model well then the residual will be very low. So he's supposing that the whole signal won't be able to be captured by LP model and that the residual would be high enough or... Yeah, this is... Yeah, I understand. This is the... actually basically some theoretical proof like maybe if you're interested you can see and he's quoting from... You know, I like that. I see the point of dividing over both stuff. Yeah, statistical signal processing, what they say here is like suppose if you analyze two distinct pth order, this LP analysis in the same stationary analysis window, the coding error will always be greater than the ones resulting from this analysis in two windows, two stationary windows. Mm-hmm. Well, some... So he's basically based on this theorem. So... So what he's saying is like if you do this error, it will be always greater than the... these errors of two stationary windows. Okay, that's reasonable. Because actually if the whole signal would be able to be modeled by LP C, then anyway he has to design some framing. Yeah. So even if it's very stationary by dividing over this whole stuff, he is able to find some reasonable boundaries. Yeah, probably. It makes sense. Yeah, yeah, yeah. Yeah, this may be... I think he's getting some improvement, but... How did you cope with what you mentioned some time before that if it's a very long stationary period for each 12.5 frame, you will actually get the same segment now. Yeah. So how... See, he's just... he's normalizing the energy coefficient because energy is really affected a lot. Okay. So he's... Power, not energy, definitely, you know. Energy, like... He's C0 component, like... See, he's using MFCC. Yeah, but it's... He's normalizing the C0 component by this... For the length. Yeah. Than power instead of energy. Power is a little... And then it's over, the hands are... Yeah, it's time, like, you know, it's okay, like samples. Or you can say root mean square energy or something. Yeah. Yeah. Okay. So, but this... So two things I still don't understand. This is supposed to be only for MFCC's and these easy features. One question I wanted to ask before. Does he preserve within the feature vector the framing he's using, finally, because... Does he somehow put the information about what's the size of the frame to the feature vector itself? Or is it throwing away this framing? No. No, you've got what you mean, like, the window length? Something I need to... No, it doesn't... It doesn't tell you it could be good for the recognizer to know what was the chosen framing, by the way. But I think it's... Yeah. It doesn't use, I guess, because at the end, like, you just... You take these features and you try and model. So, it really matters what frame shift you're using for models. Like, because he needs to use... I'll save that for the back and it might screw it up to... No, why? No, he will get like every ten milliseconds. He'll get one for one feature. Like, it doesn't really depend on... Because suppose if you take your case, like, your window, it may be longer. So, it doesn't really matter, like, what size window you use or not. But at every ten milliseconds, whether you give some features or not really matters. So, see, you can use 30 milliseconds or 50 milliseconds window. Oh, probably I didn't get it. So, the framing is... So, the equidistant. Just the windows are larger. Yeah, framing, I just like it's really... Yeah, it's crazy. Yeah, if you change framing, it's really... It might be difficult. I think even... It's really a problem. So, not like if you try even models or something like this, if you change. It's not only with this modulation spectra and shifts in deltas because... No, maybe it's possible to use it somehow. But then there is information about... Temporal information is somehow included. Yeah, but it's really complicated. I think if you use... Yeah, that because... Can I imagine the transformation to get the modulation spectrum out of such and stuff. Yeah, yeah. But there is kind of interesting... Also, some... like not paper, but I saw the algorithm. I have been using him that called Temporal Decomposition. I don't know if you know that. Atelstemporal Decom... I don't know who proposed it. I just had it from Bimbot, Fereggo Bimbot. You know, this guy, he's French. Somewhere now in Talaecom, French, I don't know where he's working. But we were using it for speech coding. So, we just had a speech and you had decomposed speech into such segments, tempo segments. Which were stationary inside. And then you can more or less quantize those segments somehow and use it for encoding. Yeah, first this is proposed by Atel, even we read one paper in our reading group, you remember? I have implemented it quite well. They were using like SVD stuff, Cin' Girl, value decomposition for that. And it worked pretty well. Yeah, exactly. He was using it. So why don't you still use it? No, he was quoting... It has been used for speech coding. Nobody used it for recognition stuff. I think I never heard that. Yeah, but the speech coding, so do you still use it? No, I don't know. Why? Yeah, that's... good question. Maybe. Yeah, he was quoting that paper also. He was telling... This is kind of optimization criteria. What this temporal decomposition is doing. You're trying to segment your signal into discrete windows and then... Right. So but he was telling like this relationship between optimization and then quasi-stationarity is not really obvious. So stationarity is again different. No, stationarity is... This is optimization. We want to see some few segments which can really represent more. So maybe that's where this may not be really... That's true. Yeah, this one. He's getting some improvement. That's important. Well, there should be... After each conference, there should be something like... Ten people should sit down, read the papers. And then write them. You know, all the similar papers within one session say... Focus to this topic and say this is the best. This works slightly worse than this one. And throw away what is not that good. And... I think people would like it. Many people would like it. The best should be implemented and used from... Again, I'm on a worse. Like, forget all the MFCs because it's too old and then start building the story on the new stuff. But again, getting that ten purpose is really difficult task. It takes maybe years. Again, it's definitely like who will choose the ten purpose? I know. Exactly. If you ask. Yeah, but those things are practically kind of really... But I think these kind of things... I mean, what you said is really good. Like, if people start implementing... Like, some people propose something in feature levels, some propose in... Something in model levels. These two are really independent. No one really combine these two. Right. And everybody is using different training and testing data and... Especially features. If somebody comes with different good features, again, people... They show you works for Timit, but then you try to use some different databases. You see, it doesn't work. No, it's not really sure. Suppose people... You can't really force people to use same features. So they will be happy with their own features and their own scripts. So they are a bit reluctant to change features every time. So a little bit what NIST is trying to do now. Yeah, exactly. So NIST evaluations. Yeah. Because... But even NIST, you can... Everyone is still comparing to PLPs or MFCCs. And there are so many new systems and everything is new. Yeah, but nobody... Still a little bit. 20 years old. Those new systems are still not general. Like those PLPs and MFCCs, because everybody knows that it works somehow. Yeah. Right. For any kind of data. That's why they are comparing to that. So, once somebody will come with something new, okay, he is showing it works for some data. But... Especially LVCs are in really big systems. Yeah. Then people... First it's difficult to show that it works for all the data. Right. Because... It's a game you don't know. And... Every paper then should contain all the results from NIST, evolution or some standard task. No, there will be only one conference in three years or something for speech. Yeah, but then we all optimize on the same task. Ah, yeah. So... You cannot get anything new without that after some time. But again, it could be... You would have to publish the results on this standard task. And then you could say, well, I also tried on this and this... Yeah, but then you run out of time. Yeah. That's why you can publish only once in three years now. But then only it makes some progress. No, actually you are correct. But... But I think people are doing... Suppose in the inter-speech there was some challenge for speech synthesis. So... So what's... Like, a challenge was really good. So, they give the database. So you have to work on that database. What technique you use is... Yeah. It's your choice. But the database and then the results analysis is... They decide. So... Even speech recognition also, some tests are coming for phone segmentation or something. People give some database and then you have to... You can use whatever you want and you have to produce. Even for feature cells. Can you use from... Some data... Features also, I think... From the outside world. Yeah, they designed the data such a way that it's... It's really like... Really real data. They give you also training data? No, first... The next speech synthesis is... They give you some data. So whether it's used for training or... You can do data-driven or model-based approach or whatever. At the end, they will ask you to synthesis some sentences and you have to synthesize them and then... They give you only the training or development data. Training data. No development training. How do they evaluate? I don't know. It's second subject to speech synthesis. They ask people to sit and listen. Listen. Yeah. Even... It's mostly they use native speakers only because they can really judge them. Okay. But they don't have any measure or like... You have an original speech. Yeah, yeah, yeah. You transcribe it. Yeah. So even they propose some kind of word error rate. We have the original speech. So if they synthesize speech, is there any words which are not matching? Even speech synthesis, they also introduce this word VR term. Okay. But this was really quite successful in this conference into speech. So a lot of people part-spattern. Right. I think even they are continuing this for next year. It's just DTS, right? Text to speech. Text to speech. Right. So even for ICAS, next year there is some compilation by Martin Crook, Sheffield, and on this feature extraction stuff. So at least if you make tasks simple focus, then it may be good to compare these features. But if you use some large vocabulary system, it takes six months to build and then at the end you don't know whether your features are really... Right. So maybe that's why people are always using PLP or MFCC. It's such a lot of time involved, so you can't really check. So maybe you... Well, but then if somebody like NIST will have one recognizer and you just plug a different feature. Yeah. But at least in ADF we have these numbers recognized. It's almost kind of free. So everybody is using... Right. So that's what we are doing. No, no, we are almost like we are putting different features. I think yeah, numbers recognition is not really different from... At least in ADF we have almost the same. But maybe you are using 29. That's right. It's different. We are a couple of people at ADF but they are already through two different setups. But at least... Are they different? 27. Am I OJI? They are all using 27. For OJI numbers? Yeah. There is a number. There are different setups. Like some people use these setups and something to that. Yeah. We use these things trained in tests that even... The different sets of funny years. No, but the problem is you are using only digits. So maybe that's why you... Well, I don't use only digits. My MLPs are trained on... Who are you saying? No, but test set is digits. Test set is... We task is digits. Yeah, that's why like it's... But OJI numbers are... Like now we are a little bit converged. Like we are using 27 phones and before it was like 24 or 25. But it's better like once you have like whatever the back end then you can put features like whether they are Gamas or like Spectral and Ropeys or MFCs or whatever. Then at least you can see... Would be nice for OJI maybe. Yeah, yeah. But Aurora is... Oh, Aurora? Aurora. Yeah. Oh, yeah. So but Aurora is a really big database or how much time it takes to set up the data. No, but it's fast. It's fast. I don't know. I've been working with that. But the problem is Aurora again, these models are world-based models. Right, yeah. So again, here we use tripons and... So... But definitely I... If you want to show noise, I think it's... You have to show on Aurora. For noise conditions, it's pretty good. Aurora is real. Well, those recognizers are made there. You just use them. That's it. You don't have to play with it. Even if it was forbidden in that time, right? So you have a recognizer, just use it. Don't play with it. We just play with the features. Yeah, yeah, yeah. But you didn't become forever. Right. Yeah, at least I mean, those studies, I think people already did for... Even in ISC, I said, it's a little bit 2000 to... Yeah....special session on... Right....features for Aurora. Yeah, I remember. With David Pierce, he was... Yeah, yeah. This call-com mixy features and... Yeah. Yeah....the OGI features and all this. So there are already people... But again, at the end, for LVCSR, people are using PLPs or MFCs. I think all this... Yeah, who is the one who makes......who actually uses what we try to do here? These are companies because it's not scientists. Scientists just start from PLP and......design new features, new backend or new......anything, but they start from the old stuff. But who is the one who is using these results, which we publish and try to make? But maybe mine. We are using those two calls. For publishing another pair on. Yeah, but once you finish your PhD, you go and... But not everybody, girl. Not everybody. But in your space. And those who don't......they stay and publish another paper, but they... But maybe......you turn into a life, no? You can tell......you work with these Siemens people, no Siemens or these......what kind of features they use? I don't know. You know, they are extremely secretive. I had a hard time just to get the signals. So I asked other questions, but......it's quite tough. Yeah, I didn't discuss anything with Sunil, or like......not technical discussions. Yeah, I see these kind of things are built-in. Yeah, they have to. But do you have any feeling that they go for all these new features? Or they usually use only... I wonder which level they are using? Yeah, they have to do some tests in their massed desk, some speech recognition test. But it seems now that at least they mostly work on dialogue management. But not much on features and recognition. But even they don't believe, like, with one paper or two, I guess, because......at least for them, no PSO or MFCs, they know that......okay, these things work on every tag, so we can put hands on these things. I doubt they are trying right now. I doubt that at them level they are trying right now, but maybe other companies. Yeah, yeah, yeah. Maybe like, because Sunil is working in Nokia, he is a... Okay, I think so. Nokia will be using them. But you are not sure. You know, Sunil won't tell us. Yeah, you know. Well, many people play with tandem, right? Like, MIT or who is using that? No, no. So, not MIT. No, this......cohand, you mean like... I don't know, Enoch told us that somebody is using that very actively, like, okay, it works for them. And they are close to like industry, like, you know, there is......it's a research group. It's not related to Ixit. I know, I don't think it's......is somebody else or......I don't know, somebody knew us, but I think... But this was Sunil company, like AT&T or... What? This was a signal company here, Enoch always mentioned......cohand company. Yeah. The word there is in you. You know, you wrote that in coin? I don't know, he is even a......quoco? No, no. They make a recognition for the Samsung phones. Right, right. No, I don't know. But they have like really many recognizers like this. Yeah, I think so. Maybe definitely they may be using PLPs or... Maybe they are using PLPs and......but I believe that there is still some energy stuff......that's something......or a star or whatever else, like......all those decorrelations and......transformations. I think it's there, but......more like it's again based on MCC or something like that. Yeah, but whatever is certain to work......and they use it. They don't use anything which is too new......which is not......the photo is really......definitely. Yeah. We always let doubt at least four or four years of results of......some new features and some constant results. It's like a NASA......you know, they use 4 or 8......4 or 8-6 machines to put to......no......space or spaceships......because it's known to work perfectly well......or Pentium, not any......before or whatever, because......it's safe, it has been working for 20 years......and then it relates to......the true one on 20 years. But that's different situation because......they have, right? Yeah. They need......it doesn't crash that machine. Cool. Yeah, it's......something happened with the machine and......I don't......the allerks system, it doesn't matter, right? Somebody will fix it and......change it, but... Yeah. Another issue is, like, again, the......competitionally system. If they want to make on mobile......they don't want to really use some fancy, really expensive......competitionally expensive features, so......all these. And I also thought about this......Cohen's cell phone, which he was showing......to work with the LVC, all right? He was saying that there is something really, really......simple, the decoder, I mean, not the features, but he was mentioning......something to the decoder. There is just... It's just phone recognition or something like... We will not......viter, we'll just see. Yeah, something like......no DTW or something. I don't know, I'm not really sure about it. But most of the day, mobiles, they use VTW kind of thing, you know? All these... I don't know. So, it's voice calling... Yeah, voice calling at this stage. Oh, yeah, yeah, yeah, yeah. That's true. Because that's easy, like, because they don't really need to put a lot of......memory and these things. Otherwise, if they want to build a real......reg But there, you could send an aside. You could send an SMS, I think, or something like that. You could dictate text. Yeah, yeah. So... Even on a phone. Yeah. Some new one or... Yeah, the phone that... From the......cohen, cohen, yeah. Voice in nose. That's a similar one to what Erware has, for example. He was showing it live on MLMI, last MLMI confirmed. You can look at the video and play it. But which phone it is. It's Samsung. Samsung. Samsung. Some new, really new phone. Whatever, some......which you can buy, right? Yes, yeah. Commercial phone. But the software is always available or... You can train it, like, on your speech or... What do you mean, the software is... I don't know that's......again, on top of mobile or, like, it comes with your mobile......well, they just both mobile and based on the OS it's using. No, the thing is usually these kind of......these kind of facilities are, like, not normal. So, they may charge more......or, like, yeah, yeah, yeah....phone. This is not the commercial stuff. Yeah, definitely they may charge for......more money, again, some extra bill for this using this recognition engine or something. Or you can just buy that application, right? Yeah. I don't know, like, it's like... What? Like, you've got some mobile phone and then you can buy from another company some such application to can dictate your SMS or whatever. I don't know. If it's already the case. I don't know if it is done now. But maybe, like, if you can't really recognize you will be really... But have you ever used, like, dictation system for Windows? Yeah, I have used ones like this Dragon. That was kind of a good link. And I heard that it worked very, very well. Yes, even when I was working in Edinburgh, like, one guy, he got some problem with hands and then he was always using this dictation. Even he used to write a lot of C++ code with dictation. It's really good, like, so... But you have to really try and then you have to kind of... Right. Your speech. Yeah. You have to get used to this. You get used to this system. Yeah, you have to get used to this system, like, how to say all the call lines and this system, how... But it's really funny, like, he is able to use it for, like, years and he is to write a lot of code. It was really great. By the way, you should be the one to ask... Is there anything... if I am just a private person and I want for free any software, which is able to read me a book in English, for example, that festival, am I able to put the text to the system and to... In real time to listen to the... Yeah, you can listen. The festival is able to read the... to read just the normal plain text in reasonable, comprehensive English. So that I can understand it. I think you can do the festival, you can just call this... There are different kind of, again, how you call the festival. So you can just give the full interactive is that there is an interactive. So then you will get the all the call and other things. If you want to, like, read full text, then you can say, like, the full text, those syntax, and then you will speak till it finishes. And it's in real time on a great time of the question. It's based on the iPhone's defense, right? That's why it's real time. The carways, it's a bit difficult. It's okay. You can easily understand. So it's intelligible. Yeah, I think it's very good. The festival is good. Now it comes with all the Linux boxes also. It's already in this. Because this is the iPhone. I was trying to install it and there are other voices, like, cat voice and these voices. This is not... No, there are so many voices. Because I don't know which voice is running or if it's the iPhone or which approach this is. No, most of the festival, what you get on Linux missions is the iPhone based only. So they supply a few photos of the iPhones. Yeah, yeah. Just a concrete... You got it. You can't do it. You can't do it. Yeah, if you just put a festival right? Festival, right? Because it comes with direct Linux software on the nowadays open source. Okay. And have you tried it? Yes, I've tried. And there are some built in, like, easy voices which you can... There is a default one completely synthetic which you cannot understand at all. Then there is a iPhone synthesis. I think it is the iPhone synthesis from something like cat voice. Cat voice, like, they have so-so but this doesn't sound very nice. And we were playing at the summer school with them, actually with the authors. And they were playing as some other new voices. Yeah, new voices. And it sounded like a human. It was great. Yeah, that's the difference. In CSTR we use this unit selection. It's not headphones. You have really large inventory of the speech. So this is not... Those voices, they are not a release. No, no, it's free. They will be releasing soon, I guess. Maybe you can just... It's not here so far. No, it's here on the web. Then it won't come with your Linux software. So you have to again download from... Okay, I'm fine with this. But I can. Yeah, you can download. Make it run on my machine. Yeah, yeah. The voices are called multi-syn. Okay. Multi-syn, some voice. So you can download from the voice. From the web festival. And the engine is in the festival. Yeah, yeah. So it's just the voice. Yeah, yeah. So voice is like the database of the speaker who speaks in this text home. If it's really a large database, so the quality will be better because you can find similar... It's still real time. Yeah, it's more or less. But not exactly. It's two times or one point for time. But you can prove how much you want. Yeah, okay. So this is again like, that's kind of my PhD work. How in this system like how you choose the units and how you concatenate. So there are again cost functions and better me. Yeah, it's fine. So you can always optimize. You can put some thresholds and pruning. Yeah. Then you can make it real time. So then again, compromise between the quality and... Yeah. But still it's good. Now, you can just check multi-syn and then voices. That's right. Yeah, it's really good. And you just have chosen some text and it takes you just put it in. Yeah, just use the regular settings. That book, you know. Okay. Oh, it's cool. But only sometimes it kind of like... Just general English text, book. Yeah. I try. It can read how does it read from which... From which... How to stay in it. Is it possible to read PDFs or just standard text? Yeah, it can be. Whatever. Text D, right? Yeah, ASCII file. Yeah, ASCII file is... You can take a PDF and convert PDF to ASCII. Yeah. But only the problem is again, acronyms and sometimes it expands. Sometimes it may not expand because it was not in the dictionary. It's not in the dictionary. It's not in the dictionary. Like if, again, it's probably like so. But those things you can add, like if you're really familiar with this too. So you can add always this. I'm not and I don't want to go too much in the details. These things, okay. Like it won't really make mistakes so often. Sometimes... Anyway, who's the author of Festival? Helen Black and Portola. They started Festival in 1996. These are the Edinburgh people. Edinburgh? Edinburgh. And I thought that that's the guy from CSLEU from Portland. Who was there? Oh, this Mahajeshi. You want to water? Yeah, but there was... No, what they did, Portland, OJI, they did LPC based synthesis. The festival is already like... It's... Well, first we start with... But that was the guy who is on a wheelchair now. He cannot move even. He's very like in DIC. I'll say that. Like he cannot... Oh, okay. He just can't speak only through some Festival system or something like that. He moved it. No, yeah, he was working, I think, in OJI. He's very famous. But I thought that he was the author. No, actually... It was developed for him. Like he was just first tester, I think, of the system. Oh, okay. Something in there. Yeah, maybe because... It's not text to speech. It's just a synthesis, which means... Like she's more or less the same, but... I don't know. Yeah, first he started in CSLEU with Talon Black and Portola. Then he takes it into many places because somebody... And Simon King is playing with that, right? No, Simon King is one of the others. But Rob Clark is the main human. Rob Boyce. Rob Boyce is the human. Okay, I know. Yeah. I think that's enough. Yeah. Say anything more about Lisbon or Interspoon? No other papers, I've heard of just one paper. Yes, you know. And Festival from Linux. It's very interesting. Okay. Okay. Yeah. Yeah. Okay. Okay. Okay. Hmm. You don't know, don't one......pest you. That's a good one, Simon. Yeah. This is... This is... Yeah, like this paper movie. It's not easy to read this room without breaking anything. Okay. Okay.