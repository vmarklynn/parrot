 Tla B Oh my god. Yeah, still doesn't feel natural. Yeah. I wish you know the room had just attacked when a meeting started. Starting. Well enough. Okay. So, you better start it. Was your idea? No, so I thought that we might speak a little bit about speech coding because nobody is doing here speech coding. And I would like to have some maybe feedback because except me, I'm supposed to do some speech coding stuff here with Hineck, but you know that little bit at least that it's based on some, let's say, Hilbert transforms using longer temporal context and deriving some parameters which would be transmitted to decoder. So it's a little bit different than the stuff which people are using now which is like LPC based. And but we are at the beginning with everything more or less. So still we can encode a speech. But of course there are many problems which we still didn't solve. And the biggest one is quite similar to LPC stuff where people still don't know how to exactly encode the carrier or the source signal. You know what to do with the envelope or how to approximate the envelope, let's say, when you have got. Maybe is the whiteboard working? Or not? You mean that it would be nice to show us. Maybe I have a paper. So what we are doing now, yeah. You mean? Okay, everybody knows our PC stuff how it works. So I don't have to present anything more else. But it's more or less very simple. We have, as I said at the beginning, there is some speech now. What we are, we might see it in a very simple way that we are using some Hilbert transform at the beginning to create analytics signal. The analytic signal is the signal which is a complex which you can create from the real, some sequence. And if you apply Hilbert transform on such a speech like a real trajectory, then you get a complex signal and you take just the amplitude. Then you get Hilbert envelope code. So, I don't know, yeah. So is this different from the original PLP2 stuff then, is that? Yeah, it's not PLP at all. That's what you started with, wasn't it? That's the stuff that Marius. No, but Marius is, it's not late up. No, Marius is also late in Hilbert transform. Okay. You're, I know what you mean with the PLP. They were applying PLP with some iteration with this one approach. They were doing some smooth thing of a spectrogram in two ways. If I'm advertising it both temporarily and a spectrogram. But PLP is using just the spectral direction, let's say you are doing for every 20-minute second processing one feature vector. You are processing one spectral band, frequency band. Yeah, okay. So it's different direction. Yeah, but anyway, this is very simple. So applying that absolute, absolute operation, you get Hilbert envelope. And of course, there is the phase. So it's just the complex operation. And that's more or less all what we are doing. So now what we know, what we know is how to approximate the Hilbert envelope. We are applying that Hilbert, what is Marius stuff. Yeah. So he knows how to encode the Hilbert envelope using... There's some kind of DCT or... Like LSEFS or LPC. You're applying again some linear operation model after that. But it's a... But it's a... It's a different linear prediction. It's a complex linear prediction. It's a little bit different, not applied in frequency domain. So instead of classical or BCL domain where you have a spectrum, let's say something like that, and you're applying some linear prediction model which is trying to approximate the spectrum, right? So this is frequency. In our case, we are trying to approximate kind of instantaneous energy. So if this is a time and you get one frequency band, you can see something like this. And we are trying to approximate that envelope which is the Hilbert envelope. So again, we know what to do with Hilbert envelope. We know that we can apply linear prediction model and it works pretty well. But if you want to really well construct the speed signal or D... You need also the phase signal, which is in some sense quite simple for voice speeds. Because you use linear phase or... The phase... No, it's a phase or you can even call it like a carrier. So the carrier is, if you know, amplitude modulation, right? You might see something like there is really one cosine which is very good. Well, it depends on how... I mean, you're talking... Yeah, just the carrier. It's not necessarily phase, but if you're using some sort of... It's called like the... So it all kind of... Right, right, exactly. So this is the signal that you also have to somehow encode or transmit from a code of the decoder in order to resin-design it. So what sort of bit right do you get for the encoding of the Hilbert transform, the spectrogram? What do you mean? What sort of coding do you get so far for the... For this one? No, no, no, no. Do you listen to linear prediction model? Well, it's just linear prediction. Closical, right. I mean... Oh, okay, well, if you want to see it in more details... No, no, no, no, but what sort of bit right do you get? Is it... Do you have any sort of idea of what sort of bandwidth? Oh, you mean... Mean bitrate... Yeah, bitrate, I mean. Yeah, sure, sure. So, more as we are... What we are using now is, let's say, something like 1.2 kilobits per second for... The Hilbert and well. Okay, and that's with a fully optimized sort of compression, skate mode. That's just the raw data. It's, again, roughly estimated somewhere. Okay. But we still know that we can go down. Yeah, sure. You just cut out fewer bands and just use interpolation or whatever. Exactly. Yeah. We are using like 15 bands. Okay, that's with the full spectrum basically being used. Right. But they are first... They are integrated into 15 bands using... Classical way like in MFCC or in PLP. You just apply some... those triangular windows or something like that in order to... To get really just 15 bands, right? And each band is then processed independently. Yeah. So for one band we more or less have like 100 beats. Okay. That's quite good. So, for a hill band load. And in Ecker was showing me how scalable it is in terms of... You can take half the bands away and still get decent performance. Yeah, you can just... Three quarters of them away. It starts to degrade but it's still intelligible. Right. That sort of thing. Yeah. Okay. And that's without doing any sort of prediction of what the missing bands might be, right? You just leave the band. Right. That's what we did. You could be able to do some sort of code book. Yeah, I'm sure you... You're missing data sort of by stuff. Yeah. There are many things we didn't try at all but... Yeah, exactly. Okay. But now what I'm really trying to do is somehow approximate that......of the carrier signal to do something with the carrier. There are many simple things which you can do like......quantization, scalar quantization, for example. It would be nice to have a scalable scheme now, wouldn't it? Like because you've got a nice scalable scheme for the......coding of the smooth spectrum. Right. It would be nice to have some sort of scalable......coding scheme for the voice signal as well, right? It means something like......that kelp coding system where you are......or... Well, I don't know how this... No, what is exactly a scalable system, what do you mean? Well, a different bit right for the actual......exitation as well. Yeah. Because I mean, it'd be nice to......let's say if you had 1200 bits in there for the smooth spectrum and 1200 bits for the voice... Yeah, but I think this approach can do that because you really can speed it into several......bag......and you can decide how many bands you want from what. I have a question. Is the scale linear or is math scale like in... Yeah, this is a bar scale. Is it bar scale like in......or math scale? So it's being more wider for......our frequencies and of course......the state. Developing is different, right? Right. Yeah. But what I......my sense is that it is not so much important still. I mean, it doesn't matter if you use math scale or bar scale but... No, no. Just like some solute... Right. Yeah. But that's what we are using. Okay. So, actually, we are using just Gaussian windows here instead of those triangular......because they are more......smooth or......no, it's better than for decoding. It's better to play with them because they are not zero anywhere, you know? Yeah, yeah, yeah. They are approaching......or in linear they are in zero but......actually, they are not. So you don't have to care about some zero values here. Okay. But more or less it doesn't matter. Yeah. We are using just Gaussian windows. Okay. So after you're doing LSF, so......like, I mean, how you are finally encoding those... Okay. Those that have about......an envelope is......is approximated by linear prediction. So you have LPC coefficients which you can transcribe into LSFs......which are......and then vector quantiles or......or......or......or......or......or......or......or......or......or......or......or......or......or......or......then you apply those......T Dracula-o-G Wonst Windows. Yeah. So this is done for 1, a second. 8,000 samples for 8 Sisters. You know cells. Some of the 40 Ding used. Okay. Good collaborating. Okay. There you go! Okay. And so you split into 50 Then you applied those dropping intozer bits. If only carbon is diagrammed right there. Like, figure it out or population. Right, exactly, you just apply a piece. So, autocorrelation let's say, or you can do power spectrum, whatever. Yeah, so for each one you get LSS here. So how many LSS you're using? Oh well, I'm using now like 20 LSS per one second, per one frequency band. So, but you have 20, 15. How many bands do you use? 20, 15. 15, you're not skipping any bands or like... No, no, if I just keep... 15 into 20 then. So 20 LSS each one is quantized by four or five beats. Okay. So it's 300... Of what? 300 beats? It's a total of 300 sort of parameters per second. Two. Yeah, yeah. But face your not-em coding, you think face your directly transmitting band? You mean this face the Hilbert carrier or... Yeah, yeah, yeah. No, of course. Face your field burn, he's not transmitting that at all really at this point in time. Yeah, you're not transmitting at all. Well, there are many things you can do. We don't... First thing what you can already do is just to replace it by Gaussian. Yeah, yes, noise. So you can really use... Like in a... You can whisper in time. Right, it centers in a sense. Or you can replace it by some cosine, one cosine generated in a central frequency of that Gaussian window. Or one of these glottal pulse things. And just use basic LPC10 style. Right. Yeah. Which is probably, I mean, if you were going to aim for like a very low rate code of life, I'm 100 bits per second, I guess. That's... That's probably fine. So what is the standard knowledge? It's 2kV per second. The lowest standard is Malp, I think, which is 2400. Oh, the standard. It's been implemented at 1200. It's per second using more more complicated quantizations, games and stuff like that. So I'm 100? Yeah, I think so, yeah. Yeah, it's for solutions. I saw it. They used matrix quantization on the LPC... On the LSF parameters or whatever. Yeah, yeah. I mean, that's quite... It becomes quite computationally intensive. But I mean, I imagine the same sort of approach could be... Well, I'm just comparing with LPC10 standard, which is like classical LPC. It'll be optimized. So then there is, I think, 2.4 kbps to be trade for that. Yeah, that's the same. 20 milps, a lot better quality than that. Yeah, definitely. But this is good, except... This is... You have good milps. No, I mean, LPC10 is just LPC. Oh, it's not good, it's good. No, no, no. I think... Just noise or just still seen positive... I think sound for camp or however you pronounce it. Yeah, it's about 4,800, I think. Yeah, it's a 1.0, yeah. But the standard's 4, yeah. Yeah, yeah, yeah. That's, I think, even GSM and all this G729. Or based around that. Yeah, yeah, on B7. At least it gives... It's not terribly interesting, though. But still there's code book news, so that's... So it may be better than using just random noise. Well, I mean, you've got a bunch of options, haven't you? You can use a... A poetic side, or you could use... Well, more or less, what we didn't do is analyze Bicentas. Right, so that's what... Sound is better based. No, yeah, yeah, sure, sure. Yeah, so this is what we already do, and that might help a lot, of course. Yeah, but what's more interesting? Yeah, exactly. That's the question. I suspect there's something like... Help, you're going to get a lot higher bandwidth, aren't you? I mean, it's a higher bandwidth standard. It's a little bit, yeah, yeah, sure. For example, there... What I also did was a simple peak-picking algorithm. So if you take a look on the spectrum of the carrier signal, which is more or less like the cosine, if you really take a look on that, but it is kind of frequency shifted cosine, or frequency modulated cosine. Right. So the frequency is really changing. Of course, the amplitude is not constant all the time. It should be theoretically, but practically it's not. So what you can see for one second signal, you more or less, you really might see some like one impulse there, or spectral line for. But usually there are many others, which are quite small, but they are quite important. If you don't transmit them, then the signal is quite roboticosome. So if you really transmit just one spectrometer line. But this is what you can do, and you can understand pretty well. The signal is audible or... Okay, so... If you really just find one line, one spectrometer, the dominant one in the spectrum of the carrier Hilbert carrier, and you just transmit this parameter. Because it's a bit tricky, isn't it? It's not like a standard... I'm just thinking, I mean, what sort of option... It's not like you can't just do standard, you know, inverse filtering for single reconstruction, or anything like that, can you? It's even easier, because what you have to do is just divide... You just multiply that Hilbert and all BIDET Hilbert carrier. That's all. Oh, okay, and then you just do the inverse FFT or whatever. InOSU. Okay, yeah, I mean, you've got... So you just multiply that one Hilbert carrier, which is transcribed by OSFs. Yeah. BIDET, this one, IFFT signal, so... But that's going to be susceptible to having phase problems, is it? Well, you mean the phase of that carrier? How could signal... Of course, I try to play also with that, and I found that it doesn't matter so much, which is the phase. Because we are using just one second window. Yeah. Right. So of course, there might be some disturbance... Disturbed when you concatenate those segments together, but then the phase might be important. Yeah. But otherwise, I mean, I didn't find anything important with the phase. Like, quick, keep the phase zero and it works. And I mean, there is research that's been published showing, you know, speech reconstruction from the... Essentially, the spectrogram without phase information. Right, right, right. Yeah. So... Base, again, is a bit different because they are doing this in a frequency domain. Yes. But this is in time domain. Yes, but I mean, you can reconstruct the entire... Right. The spectrogramming. Spectrogram from that, can't you? So you could do it that way. Because I'm just thinking, there's a heap of options. Like, if you'd exactly the same with sinusoidal coding or something like that, all you need is the reconstruction of the spectrogram in order to estimate the harmonic amplitudes. Exactly. And then, what we can also do very simply is just to get the magnitude spectrogram using this approximation without the carrier, just Hilbert envelope. But you just get it in this way, in the way of routes, right? But you can also get the short time Fourier transform... Yeah. Just by ray-filling. Right. Just classical frame-by-frame algorithm 20 milliseconds. But you keep just the phases instead of magnitude. And you can put this together. Okay. Right. And if it works pretty well, then the signal is not just whispered. And you really hear that there is the voiceness in the signal. Yeah. Okay. But of course, the quiet is not so good still. But you can use it for like two, three kilobits per second. And it works. Yeah. But that's the big thing at the moment, isn't it? If you're... Like, yeah. If the... If the encoding writes two kilobits per second for the... The excitation component of the signal. Right. It doesn't matter how efficient your encoding of the spectrogram is. Right. You're always going to have a high, relatively high bit, right? Yeah, sure. Anyway, so maybe it would be nice if you'd... If you'd... So what you did with the speech coding, because... I didn't see any of PhD thesis of you. I think so. If you did something. So maybe it would be interesting at least to you. Yeah. Yeah. So, well, one could argue it's a field that's quite a sort of a niche these days, isn't it? Yeah. Yeah, sure. Yeah. But yeah, but it's more towards industry than standards. Yeah. Because I worked in a industry for like a small time, but there they used to implement all these coders, GSM and ITU standards. So you were... When did you do that? No, just before my PhD. I worked for like a few months. Yeah, but the industry is really a big deal. Like so they work on a lot of these standards. I think so, yeah. What did you say? Yeah. But mostly they're all based on self-paranalic milli. Yeah. So... Is it Nokia's being sued in the US for their try band usage? Because... You mean using that CDMA? No, no. JSM? JSM because I don't have GSM in the US, or they didn't have GSM in the US. Yeah, it's not GSM right. CDMA is another level, I think. No, CDMA is there, but now slowly they're introducing GSM also. Well, let's try band. Yeah, GSM privately. Frequencies are different. Yeah, exactly. Yeah. And apparently Nokia's a bit of trouble over that one, but... That's interesting. Yeah, now it's really big competition. I think there is a lot of money in that way. Right, there should have been a lot, yeah. Like speech recognition, of course, it's very interesting, but people don't use it so much as... Well, that's... Just coding, just telephoning, right? I mean coding is a much more sort of standard... It's a whole... I mean, it boils down to just mathematics. It's a lot of... Right, yeah. It doesn't... Yeah. It's just sort of... It's... You know, communications. Right, right. Sort of... No, yeah. So where do you think your thing is useful in CDMA or JSM? No, I... I think maybe in CDMA it may be... Well, what they were thinking about is... CDMA, I mean, there is... Or time domain that they're doing only... Well, the problem is this with this application, or it doesn't have to be problem, but one constraint is that we are processing one second of the signal. You can go down, of course, but then you are losing a little bit of the bit, right, let's say. Yeah, sure, sure. So, but there are many applications where you really can use it because you don't care about algorithmic delay. One second algorithm delay, like for... I don't know if instead of SMS you can really have some other channel. Yeah, sure, sure. Because you're talking stuff like that. Right. Yeah. So... And I mean, as you say, I mean, the delay is... I mean, you can squash that delay to some extent. Right, right. Yeah. I mean, I think as far as the delayless coding goes, I don't think you can probably do much more than what's already been achieved, because to get higher compression you have to... You know, take advantage of redundancy over longer times fans. But even though... But even though they... These long telephones, they already have delay, at least half a second. Yeah, sure, but I mean, that's transmission delay. Not algorithmic delay if you add those together, you end up with a... Yeah....night mess. No, yeah. But mobile... We might have someone else joining in on a meeting soon. That's nice. I don't know. I mean, you tried SonyaSoidal coding, those approaches, didn't you? Which one? SonyaSoidal coding. But the SonyaSoidal coding is usually, at least what I think or what I know is, when you are doing SonyaSoidal coding, it means you just take the speech signal at the beginning, right? And you are trying to find... Homonics and... Yeah, to find the harmonic in a temporal domain, right? Yeah, for that. Yeah, but I mean, the thing is, you need... I mean, part of the parameter set for that is the harmonic amplitudes. Right. Now, one way of compressing the harmonic amplitudes is just storing the smooth spectrum. And then... Oh, yeah....you can interpolate. Yeah, you can just pick the harmonic amplitudes on that spectrum when you reconstruct. Right. On that smooth version of the spectrum. More or less I did it, maybe the firm way, but... Yeah, well, I just... Kind of regretting like, okay, trying to find out the strongest or the most dominant peaks in the spectrum, which is more or less the same. But maybe the bit rate is the problem, not like, what can we do? Yeah, why......it's really... But it's quite high, I've been right, but I mean, once again, it may be it's something that's scalable. But, yeah, maybe you can just see how many harmonics... He was using 20, I think, like, maybe you can try to reduce... Although, I mean, you can use as many harmonics as you want. I mean, he was just using the fundamental... Yeah, frequency. Right. And if you're doing a smooth spectrum representation of the harmonic amplitudes, then you only need to store the fundamental frequency. Right. And then all this amplitude. And the maximum voice frequency. And then you do multiples of... Yeah, sure. Yeah, yeah. So... And then the tricky... The only tricky thing that we hadn't figured out was the phase information. You take in. Yeah. But you can still get A, like you get... Basically, if you do phase unwrapping and all of these other tricks, you end up with some sort of phase representation, which is... Some kind of......some sort of monotonically decreasing... Yeah. If it's unwrapping, unwrapping. Yeah, unwrapping. Yeah. So, I mean, it's something that you can, saveably represent in some sort of... Right. Some functions. Low sort of... You know, low bit right representation, I guess. Right. And then maybe you'd need some sort of correction to correct minor phase discontinuities at some point in time. I also play with that. And for example, what I found is that, in this case, we are not trying to encode the speed signal. We are trying to encode some carrier, which is much easier, I would say, because you can see that even. Yeah, okay. So, where I was trying to encode the phases, so I just found that two bits is enough for that, and you really don't hear the difference. So, if you keep all the spectral lines... Yeah....as they were there. Yeah. But you are just quantizing with two bits, or three bits for amplitude, two bits for a phase. So, it's okay. The problem is that you have to transmit many of them. Because there is not just one domain component, but usually more others, which are quite small. Yeah. Right. Yeah. Or what you have to do, then you have to decrease the size of... The size of... The size of... Not one second, but you have to go down, because then you can get just voice part and a voice part. You know, one second is usually mixed, right, somehow, together. But you don't do any voice and voice decision. No, no. No, no. You don't even have advantage, I think. You want to avoid half choices. Yeah. We don't want to do that. Yeah. Because in same subtle modeling, it's really another issue. Well, that's one of the big problems with the whole piece, say 10 frames. Yeah. Yeah. It stinks, because it's hard. Exactly. Yeah. And then you have to combine all these harmonic model and noise models. So again, you may end up some joining. But for example, what we also found is that if you use just the Herbert envelope, that one, which is trying to approximate that in some terrorist energy, in some low band, like around 200 Hertz. And you take a look on the signal on the trajectory. You can see that it works like kind of voice detector. Oh, sure. Yeah. Because for low energies, there is usually noise for high energies. There is kind of harmonicity. Yeah. So it works pretty well for that. So we might use this one. But that 200, this is what they use. Like, they do use this for pitch marking in speech. So this is what Ellen Black's code uses. But it's very speaker dependent on what sort of bandwidth you look at this Hilbert envelope. Right. Depending on the pitch, depending on the person, you'll get a different amount of smoothing at a certain sort of... Yeah. So I mean, if it's a female speaker, for instance... That's true. Yeah. Yeah. So you have to tune a lot of parameters like that. Yeah. Yeah. That's a problem. But anyway, you don't really need hard decisions. Not like voice to voice. Oh. Oh. So maybe it may help everybody. That would be nice. Not to have anything like that. Yeah. But from memory, that basic script that they had wasn't very effective. I mean, maybe if you did some sort of initial first guess, and then... Right. And then chose your smoothing factor based on that. Yeah. Because selecting any particular point of what the pitch is at the point is might be inaccurate. But if you're estimated at over a second, you should be able to get a decent estimate on something. But do you really need it or like voice to voice? Well, I don't know. Maybe if we really want to improve the quality, then we will need it. I don't know. Yeah. But it's not really... You don't need to specify at each point. It's voice. But now, yeah. Well, I'm going to have a time next month or in three weeks. So I'll try to play some examples there. It'll be nice to really see what this... What you're actually trying to reconstruct here. Yeah. What we are trying to reconstruct... This carrier signal, what it really... How does it look like... Yeah. Yeah. And you did say it looks like a frequency modulator... Yeah. How it's not? It's kind of not the frequency, let's say amplitude modulation, where you have just the carrier and the modulation now. And then amplitude. Right. But you're changing even frequencies also shifting, you don't have to make a picture. Yeah. Yeah. Yeah. Exactly. So then, yeah. Then it's not the exact length to the modulation. So it can be... Yeah. It's in between. It's in between. Yeah. So it's not this or that, but... Still, yeah. But anyway, what... Also, the another approach, which is possible to use... The another is one to keep the face like this. One, two spectrograms, one the face, one the magnitude. Yeah. And, well, I can again play it. And it works on two kilobits per second. That's what Mario's did, but he was keeping on the face. No, he was doing... He was using a four-speed recognition, what I... No, but even then he's... For sure. He didn't do anything. He just transmitted it. That's why... He was aiming for... Speech coding, I mean? He's not really kind of doing speech coding, but he wanted to do it on music. So he doesn't really care about the bandwidth. He wanted to have the quality. I don't know. I don't know. But even especially the audio CDs. That's why he was telling me, like, at least even 10k bit, 20 kilobits also is okay. Right. So, yeah. But he's not touching face at all. I think he's just transmitting all the face and then... Well, for example, this face... But he's encoding the envelope. Yeah. Right. Yeah. Yeah. I think so. There are more papers which are using this approach, like trying to encode the magnitude spectrum. And then leaving the face. Leaving the face. Or you just really can... Again, the face can be replaced by the noise, which is pretty good. So that's the bandwidth variable approach. So if you really don't have a bandwidth, you just can replace it by noise and it works. Of course, then it doesn't sound as the original because it's more whisperate or whatever. But you can answer well. And for example, if you mention this one, approach with the trajectory of the face. So, if I'm a face trajectory, I also use because you can do it here. When you get into this domain where you apply, or you are trying to compute power spectrum in order... Let's say that this is kind of trajectory. It doesn't fit this in frequency or time domain. But here you are trying to compute, or doing a... To do linear prediction. So we are just doing FFT, right? Then applying power in order to get... To power domain and then IFFT, right? Yeah. So you get the autocorrelation coefficient. So before you are applying power spectrum, you can again get the... You have the complex... The frequency or complex spectrum, you can get the face, right? And the face looks quite similar like this. So you might see it's more or less like a line. Yeah. And it does start to get a little bit... Right. Nom linear towards the end, but normally that's when the components are not actually probably going to be audible anyway in voice space, right? Right. But this face is different from your career. Yeah, it is very different. Yeah. Because you're just doing on your... You are doing in different domain. Yeah. And those are the main things in time. And it is quite... It's quite sensible to any quantization, what I found. Really, it looks like the line. So you just say, okay, let's put the line there, but it doesn't work. Well, you can understand, but... Yeah, sure. You can hear that there are many... Because any sort of error in that line is... Is transmitted... Is realized in it as a visor, right? Right. And the other problem is that it's not bandwidth variable. So you cannot say that this part will be just the noise. Which you can do here in this error approach. So that's why I didn't play with that so more. Because I found there are more... Design event features than the other entities. Disapproves. So that's why. But even you can use that face directly. You mean this one? Yeah, this is like just the original face. But this is face of your spectrum. Like it's not the time to... Right. Yeah, it's not a face. Of your signal, it's not... Yeah, yeah. So then it's face... It's not tricky, like how... No, you can again put that envelope with this face together. Because I love this. But still you get only the magnitude of your Hilbert envelope. No, you have... No, you get the... Five slasters like... Yeah, you get the complex spectrum with this. No, but you started with Hilbert envelope. No, like magnetic... No, no, I started with the... magnetic... DCT. No, before DCT, you're... No, after DCT, you're trying... This is a kind of, again, real trajectory, right? And you're trying to first find that Hilbert envelope. So, as... So you input to the DCT's, the speech or like your... This is a speech. Okay. Just starting with... No, but here what I want to say is really that... It's not part of Hilbert envelope. It's part of the DCT's, you know. Oh, cool, cool. So... Yeah, maybe it's then... It's enough. If you have just this face and the Hilbert envelope, you can put it together. Okay. You don't need the career. I mean, this is kind of a career signal, but in different domain represented. Yeah, yeah. It's in frequency domain. Right. Right. So, what I think is that we really need to apply kind of an honest, bicentre-essive approach in order to get something... Yeah, that makes sense, because it's a term domain sort of... Right. Approach. And I mean, and in fact, I mean, it's what you're doing is so different to most approaches to speech coding that it doesn't matter that we don't know much about. No! I haven't had much. You mean... I mean, it's quite different to a lot of the traditional sort of... Sort of, you know, sort filter type, and approaches or... Yeah, different types. Right, right. Yeah. Yeah. But I mean... No, I think... That's maybe even the advantage here, because you... It's not speech-based, which means... It's just a matter of minimum distortion type. Right. So you can... When I tried and comparing with RPC-10 on a... twice bigger or higher bitrate, this work works better, it seems to me. Just for unvoiced speech. I'm not mentioning... Unvoiced speech. Unvoiced speech is very difficult to... sort of judge quality compared to the way it's sitting. Yeah, but when you hear and try to hear some examples with the music, for example... Yeah, plosives are very important, I guess. Right, yeah. So maybe it does a better job of plosives. Mm-hmm. That's true, yeah. Why, anyway, it seemed to me pretty good. Okay. It'd be nice if we had some demos here. That's true. That's the first time I'd have to wait your time, I guess. So, in its your time? I should be 13th December. Oh, I think. Oh, it's a little one. Oh, bugger. I just... You'll be coming over here. You won't be here. No, I... You'll practice it before then, won't you? Okay. Yeah. Now, I just swapped it with Mike Pero, I think, because he's not going to be here this time. Yeah, he's going to us. And I was supposed to have it in January, so... Whatever. Anyway, so... But anyway, I wanted to hear something maybe from you, what you were doing with speech coding, because you mentioned that. I did, yeah. But we're doing a more traditional... Bay frequency slices rather than temporal based stuff. Bay was based on, like, you were using kind of regularizer there, right? Yeah, well, that was one of the approaches, sure. Yeah, and the other approach was this HMM based stuff, which is basically just a parameterization of the... You know, the source filter type parameters, and then learning the training like a speech recognition system. But that's not terribly... That's got no relationship to this. Yeah, it's different, but speech coding. Yeah, sure, speech coding. I read some of the other techniques. Or standards, but... I mean, I think you're on the right track with... No, I'm supposed to have synthesis, but... That doesn't tell you how, what sort of bit writes you're going to achieve or... Right, yeah, sure. And because you have to do it separately for H... Band, band, right. It would be the... I imagine that's the tricky part. Yeah, because it would be varying quite rapidly, wouldn't it? Your... Your codebook, like within each... With different... When you go along the one second sort of segment of the band, you... Maybe what we can already do is to decrease this distance. We can do a Hilbert envelope approximation with one second, but then we can split that signal into, I don't know, then segments. We did 100 milliseconds or even less, and we can apply some another technique for that. Well, first you've got to choose your approach and not worry about the bandwidth. Right, exactly. And then start thinking about how to... Because I mean, I guess, especially because this is a more of a commercial-based project, you've got to match the quality before you start worrying about other things, right? I mean, what sort of... More or less, they don't care what will be the quality or the bitrate. They just want to get something new, you know. Oh, I can't. So if you get a very low bitrate with reasonable quality or really high bitrate, which is again different, but with high quality, they will be happy with both. Okay. So, you know... Yeah, in the... On one side, it's pretty good, but on the other side, you don't know which way you should go. Yeah, sure. And at the moment, you're at sort of a point where maybe you have to make that choice a little bit. Right, right. Okay. Well, there you go. Do you want to mention your speech-cleving experience? So you're saying that you're doing something in India with... Yeah, but it's mostly like this self and... Do you know the self? Like... No, I was doing some writing some code for testing and these things, so I'm making some standards for that. Yeah, which is not... So they're like... The thing, these standards, how it works is like they give all the pseudo code kind of thing, and then they put severe test conditions. So you have to pass those test conditions. Oh, yeah. Then you can say, okay, we implemented these 70- Yeah, although standards, depending on what standard you're dealing with, sometimes it can be quite ambiguous, right? I'm in a world like the Sempag standard that basically... This is the container. We don't care how you do it, right? But it's got to fit inside this... Right. This sort of format. Even the standards, they give the test signals also. You can't really do it on whatever database you have. Okay. And then you have to... And then you have to do MOS testing. Yeah, it's more like they define... So I did for some, even a co-cancer also. So they define what is the overlap speech condition, everything they define. So you need to follow all those. I see. Yes. This industry is again different. Yeah, but it's quite interesting even with current speech coding technologies. Yeah, they are still trying to use SALP or those IP... Yeah, I think they're using only... There is nothing new, Morris. And when I was listening to the talk of Milan, Italy, he's the Czech guy, but he lives in Canada. I don't know the name of the university, but they have the pattern, I think, for KELP. Okay. KELP is from Italy. Not in KELP. I don't know how it is exactly, but they got very... The other SALP is famous for this, for itself. What they did exactly, I don't know, if something different, a little bit or not. Or they just keep those patterns or... Okay. I don't have this exactly. Maybe Hinné will know much more, but... When I heard the presentation, they didn't do so much new. I mean, it was last year. I think Hinné was asking, so what's new that you are so famous with that? Then they said, okay, we know exactly how to do that. I mean, you know, not to make any errors, because everybody knows how to do that, but it's not so easy to implement it, right? Yeah. Yeah. There is nothing so much new. Yeah, they've been seeing you in... What's new is in the transmission medium, like... Yeah. You know, new, like, higher bandwidth transmission. Right. Or like, packet-ized-based transmission. And then all that sort of thing. Where they're not so much looking at the algorithm, but so much as... How do we incorporate the algorithm within a new transmission sort of medium? Basically, like, I mean, if you see a lot of papers on, like... Like, voice over IP or whatever, the publication is sort of like... Yeah, right. How do we deal with packet loss? Or how is it affected by packet loss? Right, right, exactly. But then they are still using, like, error signal, which is going to be approximated by codebook, right? Yeah, sure. And linear prediction, which is approximating just the spectrum, four months and everything. That's all. So... And again, a lot of this stuff, like, even when I was working mostly, that company was developing for voice over IP. So, they have to put all these things on, again, DSPs, and also it's mostly... Right, yeah. Out to how many... All the other devices... All the code, all that sort of stuff. Yeah, how many code? Yeah, it's still optimization. And then... But even that's less of a big deal now, right? Because a whole little bit of a lot of... Yeah, you can know... Now even they're also becoming... Even the memories are not really... Not really. You can throw MATLAB on them. What am I about to find these days? Yes, yeah. I'm like an opera. So even I think then you can have... Whatever bandwidth... It's not really... Well, I hope that we still have a lot of time for that. But no, I think... But still then, that's why maybe people are going towards science order modeling, not like... I've always before... I think that... Then the bandwidth conditions were really stringent, then they were using all the LPC base. For example, when I was... You know, harmonic noise modeling, that H&M system, which is using... Yeah, which is very good. And small. It's pretty interesting. I mean, it's good. Yeah, even when you can look at his code and look... Yeah, but don't always cut him for my minutes. It wasn't perfect. It was a little bit of bugs with some of the files matching, but it was pretty hard to tell that it was not perfect, unless you heard the samples sort of next to one another. So that's not too bad. But definitely it's better than code. Or code is speech. No different issues. Oh, sure, really. And I'm like, how much you can get compressed is. Like what you're telling is you can do all these tricks. The only trick is the files. And we've got a student starting in January who's going to start working on how to actually efficiently parameterize the... In sinus rule, modernity? Yeah. And I think we've got a number of ideas for the spectrum. That's easily enough handle just using standard techniques. But anyway, do you know exactly how it is done when you really have those amacicy coefficients, which means manual spectrum? And then you are trying to reconcile the speech. Without phase? Without the phase. I've got no idea. Because I've tried... You know, those... I've used one paper like that. I'm too confused about it. No, yeah. Maybe you can look at dates from... Our streaming coffee. Yeah. Which one? The University of East Anglia. In the University of East Anglia. But it's near sort of... It's near the sort of Cambridge side of England. Yeah. Yeah, I can forward you. There are a lot of tricks there. Like there again, training, Gaussian mixers for different frequencies. And then... Yeah, but there are some opportunities. But they're falling... They're quite... No, much more mathematical ones. Yeah. Yeah. Just basically doing a search through... Or stuff like that. No, no, no. Those things are again based on LPC. No, maybe they're... It seems to me, yeah. Something like that. It's like the pitch. Estimation in X waves or these things. You know what you can do? Dynamic programming based on LPC and all this. What you can do is take just the minimum phase of which you can get from linear prediction, right? You know what I mean, minimum phase signal. And then constraint. It sounds quite reasonable in the two kinds. But of course, it's not original at all. So I don't know how they do that exactly. I have no idea. Yeah, again, like... No, the problem is different for everybody. Like these guys were doing for distributed speech recognition mainly. Right, right. When you've only got the MSCC. Yeah, MSCC is so... I think it was a deal with a raw or something. A CTSA or something European standard. For our only deal with anything. Even it's kind of Aurora framework. Yeah, but we didn't do any speech recognition. But now it's a bit forward. Yeah, no, it was for say you've got someone who has... You know, they want a dispute. That this is what they actually said. Right. Like the recognition. And the recognition network was wrong and UOMI money or whatever. And then they can say, well, we've reconstructed your speech from the speech recognizer input to the speech recognizer. Hey, voila, it sounds like you. I mean, obviously there's a lot more to it than that. There's a lot of sort of... Well, does the quality stand up to legal sort of requirements or whatever. But even at the simple point they're planning to use for standard GSM or this... It's not that... Oh, no, there's a separate set of standards I think for speech recognition. Yeah, ETS, yeah, yeah, yeah, yeah. Come here. Which I think requires, you know, that states the information they can send is this. So I don't know if you can actually tack on any extra phase. You have to deal with what you're given. Yeah. Okay, all the standards are really... Yeah. I don't know. I... It would be interesting. I mean, maybe that's the best approach to look at to begin with. It's just pretend all you have got is... You mean with this phase... Yeah. Sure. Because you can also interpolate the pitch information from the spectrum as well. Yeah, they do some kind of reconstruction of pitch. Yeah. So I mean, maybe try to see how good you can go without transmitting any... Right....of that information. But the quality again, from... The pitch was really good. I thought I didn't think you could really tell. No, no, no. Again, Europe in standard for this, they give the MFCC's and they give the pitch. Then you have to construct the thing. But you have MFCC's then like, again, you have to get the high-row drum MFCC's and then... Yeah, but the thing is, if it's male-scaled, if you have a male-scaled, you get quite fine reconstruction of the spectrum at the lower frequency, which is what you make for reconstruction of the pitch anyway. Right. Right. Yeah. So I mean, if you wanted to transmit with fewer bands, you'd maybe still keep the lower bands or... Yeah. Or maybe you could still transmit the pitch, the pitch isn't that big of the upper. Yeah. It'd be nice if you didn't because then I mean, that's... Well, I mean, it's worth giving it a go. And then maybe saying, well, what information you can add to the data stream to improve performance. Sure. Yeah. I don't know. Okay. Thank you. That'd be cool. So how about you? I've never done any coding in my life, so... I'm just a... I'm just a... One curious, so this is coding then. You call the speech then you're supposed to transmit the speech, right? Yeah. Let's say in GSM or something else. I remember when I was doing some image processing, there were techniques for compressing, and then coding, so adding channel information on the image at the same time in order to save some bits. You mean like... You mean channel and source coding together? Exactly. I don't know. I was just wondering if there's some... Someone was doing it with speech as well. Well, I can imagine kind of like JPEG operations which might be... It was like source coding and we also channel coding because you're just smoothly or just... Well, actually when you compress... Yeah, well, first you compress the signal, right? And then you add some complex signal in order to recover the error. Right, right. So I was wondering if by any chance there was something like this in speech with the speech? I don't know. You don't know? I don't know so much about channel coding like those Hoofman coding stuff. Oh, well, I mean... Half-lands easy enough. Just standard thinking. It's not the most difficult. Right. Yeah, sure. That's where I stop. One length of cutting I'm out of here. But I think again it's different. Not like these standards again, all these Gs standards. Yeah, exactly. They have the compression standards and the... Yeah, sure. Coding standards. But maybe... And this was the problem actually because you were... I don't remember anything like they're doing... You come back. Not in GSM definitely. Even normal G7 to normal speech code. So they have this speech coding standards again. They have the channel code. Then they have some... Source. So, yeah. Okay. I mean, I think they do try to... I mean, obviously the techniques they choose are supposed to be less susceptible to coding. Yeah, definitely. But I mean, that's maybe the limit to that. Okay. Again, yeah. It's okay. That's good. Yeah. Thank you. Okay, thank you. Thank you. Okay, if you find some paper which you... Yeah, I told you that. That was the... Fave. Fave. Fave. Fave. Fave. Fave. Fave. Fave. Now, waveform prototyping is different. I was suggesting like this. But that's again, totally... Did you hear this? Bashing claims. You know really... Now, he... He was... He really... Found you again in speech coding like this. Where he proposed one prototype waveform. Like, they take the signal and then they project into two-dimensional and then they take the faces. And it's like prototypes. Kind of wavelets you can see. Like... But it's not exactly wavelets. So they take slow moving waveforms, slow moving signal, and like fast moving and then they combine. So it's like... So you project signal into two-dimensional space and then separate the slow moving parts and... Wow....fast moving... Kind of tips towards something like that. No, this is a... They operate a... Yeah, it's... So now people mostly use it for like residual and coding. So even... So if you have the LPC then you can use this technique for residual... Oh, that's quite interesting. Yeah, yeah. So... It would be good for me. You can really good qualities with four-loop. So there was the waveform coding which you... You can rotate me, the prototype coding paper. But did... So people can use at waveform level or at residual level. If you have a LPC you can use at residual rate, consider residual as some signal. So it's really... Same residual you can clearly see whether it's fast moving... Yeah, yeah. For your disposal...