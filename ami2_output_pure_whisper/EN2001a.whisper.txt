 Okay, does anyone want to see Steve's feedback from the specification? Is there much more in it than you said yesterday? Not really, just what he was talking about, the duplication of effort. Duplication of effort and stuff. And shame that we should maybe think about having a prototype or a weak sex. Which is necessary. So, this is probably why all the times are... Yeah, I'd say if for the prototype it's just like wherever possible, chunking the stuff that we have pre-annotated and stuff. And for the stuff that we don't get pre-annotated right like a stupid baseline, then we should probably be able to... Basically, that means we focus on the interface first, so that we take the ready-made parts and just see how we get them to get into the interface the way we want them. And then we have a working prototype and then we can go back and replace pieces either by our own components or by more sophisticated components of our own. So it's probably feasible. I think I'm away this weekend. So, we need... We just want to have some data for the user face could even be random data. Oh yeah, no. But also, the similarity thing, just my matrix itself for my stuff. I think I can do that very quickly because I have the algorithm. Yeah, I think today's meeting is really the one where we sort of settle down the data structure and as soon as we have that, probably like after today's meeting, we then actually need to... Well, go back first of all and look at NIDEC smell to see how far that... That which we want is compatible with NIDEC smell offers us. And then just sort of everyone, make sure everyone understands the interface. So, I think if today we decide on what data we want to have now and later maybe even today we go and look at NIDEC smell or some of us look at NIDEC smell in a bit more detail, just trying to make some sense of that code and see how does the representation work in their system. And then sort of with that knowledge, we should be able to then say, okay, that type of NIDEC smell data, we want to load into it and this is how everyone can access it. And then we should be able to go for it. We'll do a job report for now. But look at the documentation and like, seeing enough to make me think that if you want to use the NIDEC smell framework because they have a good event model that synchronizes sort of the data and in every display element. So that takes a lot of work away from us. Sort of that would be a reason for staying within their framework and using their general classes. But beyond that I haven't looked at it at all. Which is something we should really do. Who actually like for this whole discussion, I mean who of us is doing stuff that is happening online and who of us is doing stuff that's happening offline. Like my data is coming. The basic word and thought is a fine as well. The combined measure might not be if we want to wait for the user has typed into the search. Okay. There won't be any process in 10 to 10 to yes. So basically apart from the display module, the display itself, we don't have an extremely high degree of interaction between sort of our modules that create the stuff and the interface. The interface is mainly while it's running just working on data that's just loaded from a file I guess. I don't know if the search function or something like that. It depends how it's going to work. Yeah, the search is sort of a strange beast anyway because for the search we're leaving the 9x smell framework. But that's still sort of that's good that means that at least like we don't have the type of situation where somebody has to do like a billion calculations on data online. That would make it a lot more like that would mean that our interface for the data would have to be a lot more careful about how it performs and everything. And nobody is modifying that data at online time at all. It seems nobody is making any changes to the actual data online. So that's actually making it a lot easier. That basically means our browser really is a viewer mostly which isn't doing much with the data except for sort of selecting a piece of bit and then to save it. Are we still going to go into the database? Are we still going to dump it into the database? Well some parts relevant for the search, yes. I'd say so because if we are I recommend all read-off classes out of the database it'd be so much easier. What if we're going to dump the part of the inter-data database anyway and most of our dump all the fields you want into the database, calculate everything from there. Then we don't even have to worry that much about the underlying XML representation and just query it. But nobody of us is doing much of searching from the data in the online stage. And for altogether like the display itself I think we are easier if it's sitting on the XML than if it's sitting on the SQL stuff because if it's sitting on the XML we have the night XML framework with all its functionality for synchronizing through all the different levels whenever there's a change whenever something's moving forward and stuff and we can just look at their code like how their player moves forward and how that moving forward is represented in different windows and stuff. So I think in the actual browser itself I don't want to sit on the SQL if we can sit on the XML, if we're sitting on the XML we have so much help. And for like the calculations that we're doing apart from the search it seems everyone needs some special representations anyway. Even our results, yeah in the night XML XML format so with the timestamps and stuff so that it's easy to tie together things. So we have to think about is if we go with this multi-level idea like this idea that sort of if you start with the whole meeting series as one entity as one thing that you display as one whole sort of that then the individual chunks are the individual meetings and then you can click on a meeting and then sort of the meeting is the whole thing and the chunks are the individual segments that means sort of we have multiple levels of representation which we probably, if we do it this way like we have to discuss that if we do it this way then we should probably find some abstraction model so that the interface in the sense like deals with it is if it's the same so that the interface doesn't really have to worry whether it's a meeting in the whole meeting series or a segment within a meeting you know what I mean. And that's probably stuff that we have to sort of like process twice then like for example that like the summary of a meeting within the whole meeting corpus or meeting series. This meeting series is a good word that I don't really know how to call it you know what I mean like not the whole corpus but every meeting that has to do with one topic. And so in the meeting series that a summary for a meeting within the meeting series is sort of compiled offline by a summary module and that is separate from a summary of a segment within a meeting. So I don't think we can, we don't even need to do that because we got our information density calculated offline so all we do is treat the whole lot as one massive document I mean no it's not me so big we can't load in information density for the meeting. I mean just summarize based on that. I thought we would just have like one big summary of only different importance levels displayed on it and depending on what our zoom level is we just display a part of it. So are we doing that at all levels are we. I mean we would have one very big thing offline and from that we would just select what we are displaying and just have different like fine greatnessness levels sort of. Yes. So for example you would give a high value to those sequences you want to display in the meeting series summary. So the only thing that would happen basically if I double click let's say from the whole meeting series on a single meeting is that the zoom level changes like the start and the end position changes and the zoom level changes. That was what I saw. I think we can do it online. I thought we couldn't do that. I was under the impression that we couldn't do that because we couldn't load the data for all that. But I don't know I mean that I don't think there's really much point in doing like that when it's just going to feed off in the end the information density measurement basically and that's all calculated offline. So all you're really doing is sorting the list. Is the computation hard part of it. So I'm not sure if I got it. Well like the ideas we're calculating our information density offline first where the options in the whole core. So you do is you say if you're looking at a series of meetings you say well our whole document comprises of all these who's stuck together and then all you have to do is sort them by information density like maybe wake up with the search terms. So I think it's too slow to online to be honest. I was just worried about the total memory complexity of it but I really did. I mean I just sort of like took that from something that Jonathan once said about not loading everything but maybe I was just wrong about it. But I think the difference might be that we would just want to have the words and that's not so much what he meant was not possibly loading everything was. Yeah. Load all the annotation stuff all the sound files. So what we have is we would have a word like we would have words with some priority levels. And that would basically be it already because even the selection would the summaries automatically feed from just how prioritize an individual word or how prioritize an individual utterances or other summaries sort of refined from that and made by a machine to make sentences and stuff. Or are they just sort of taking out the words of the highest priority and then the words of the second highest priority. No, the options. The options is like mean. The whole thing on the afterance level or we're doing it on the word level like the information that we're doing on the word level is if you want the audio to sync up you've got no way of getting in and extracting just that word. I mean it's impossible. I think we have started an end time for words that could be every single word. Yeah but it might sound crazy in the player we should really maybe we can do that together at some point today that we checked out how the player works. I don't think but there's maybe some marriage in altogether doing it on a matter and I'm getting quite lost. Because what's our difference between the the importance measure and the skimming and do we do both or is it the same thing. Well the skimming is going to use the importance. But when you talk about summaries you talk about this about skimming and that. Well mostly skimming. But also about the displays in the in the text body in the in the latest draft that we had sort of became up with the idea that it isn't displaying after and part of it's also displaying a summarized version in you know like below the below the graph apart. Yeah. Maybe. Yeah. Yeah. Isn't that just. Oh yeah it's just like there's like audio skimming and there's displayed skimming. Yeah but it's just same data. Yeah. Yeah. Maybe there's some merit of going all together for our trans level and not even bother to calculate. I mean if you have to do it internally then you can do it but maybe like not even store the importance level for individual words and just sort of rank utterances as a whole. The other thing about that is that we would say that we would say that we would be in sentences more or less. Yeah. So it might more sense than you would just extract it. It might be better skimming and less memory required at the same time and I mean if you know how to do it for individual words then you can just in the worst case if you can't find anything else just sort of make the mean of the words over the utterance. You know what I mean. Not quite. So what did you want to do? You just wanted to assign. What's the smallest chunk at the moment you're thinking of assigning importance measure to is it a word or is it a matter. I thought about words. So we're thinking of like maybe just storing it on a preference level. Because it's less stuff to store probably for days in the audio playing and for in the display it's probably better if you have whole utterances than I don't know like what it's like if you just take single words out of utterances. It probably doesn't make any sense at all whereas if you just show important utterances but the utterances a whole it makes more sense. So it doesn't actually make a difference for your algorithm because it just means that if you work on a word level then we just mean it over the output. Yeah I think we also thought about combining that matter with the matter as I get from hotspots and so on. So that would also be on a short step. So that's good anyway then. Yeah. Because that makes it a lot easier than to put it on an error. But how about those words which don't carry any meaning at all? Because if we ever reach over over whole utterances other words and there are quite unimportant words in there but quite important words as well. I think we should just disregard them. Maybe we should have like a cutter or something. So it's a word that only gets about a certain threshold so anything that has less than 0.5 importance gets assigned to zero. Or we do a pre-filtering of sort of the whole thing sort of like the other. But the problem with that is it's easy to do in the text level but that would mean it would still play the error in your audio unless we sort of also store what pieces we cut out for the audio. Yeah. I think before we can like answer that specific question how we deal with that it's probably good for us to look at what the audio player is capable of doing. I think we'll have to buffer. Yeah. I don't think it'll be very hard. I think it'll be like an hour please work. So what do you mean the buffering like you think the director is feeding another way for it essentially. Yeah. But not but not store it on the heart as can they load it in but load it indirectly from memory. Yeah. So just like there's down to like a media wave object or something like that. But it's probably a stream if it exists each other it would probably some binary stream going in sometime. I have no idea. They must have like classes for dealing with fire is needed to have classes with concaten if I was even doing that. Okay. So I mean so that means that there's probably even if you go on a preference level there's still some merit on within utterances cutting out stuff which clearly isn't relevant at all. Maybe also for the audio we'd have to do so. Let's say we play the whole phrase but then in addition to that we have some information that says minus that part of something. That's okay that we can do. Oh I think I might try and build this basically a class that you just feed it a linked list of different waveforms that will just string them all together with what might be 10th and the second silence in between each one. Yeah. So maybe even I mean that sort of depends on how advanced we get. If maybe if you realize that there's massive differences in gain or in something you can probably just make some simple normalization but that really depends on how much time you have and how much is necessary. Yeah. I don't know anything about audio and I have never seen the player so if you find that the player accepts some input from memory and if it's easy to do then I guess that's fairly doable. So that means in the general structure we're actually quite lucky so we have we loaded to memory for the whole series of meetings. Just the utterances and rankings for the utterances. And some information probably that says well I guess that goes with the utterances who's speaking. Oh yeah. Because then we can also do the display about who's speaking. We also really want the other search by who's speaking as well. Yeah. But I'm still confused because I thought like that's just what Jonathan said that we can't do like load massive document of that size. But yeah. Yeah. Because all the calculations gone offline. The other hand I mean it shouldn't be like 50 megabyte in RAM or something. It shouldn't be massive. Maybe 100 megabytes is quite busy. Just thinking what's this sim. So we do get an error message with the project if we load everything into the project with all the data they load. So we know that doesn't work. So our hope is essentially that we load less into it. Yes. So this is lazy loading things. I'm going to explain lazy loading. It's on the map when it needs a particular type of file. Like when it's being answered. So that is that only by type of file. Like if if the same thing is in different files would it then maybe like you know if if up trends are split over three or 10 or 100 different files. Is that a chance movie that it doesn't try to load the model memory at the same time but just. I think that's idea. Why does it fail then in the first place and it shouldn't ever fail because then it should never. If you would in a search over the whole corpus you'd talk to them all. Yeah but it failed right when you load it right. It's interesting. It's just a different baseline. So it's just a different way to load it. For example it loads all the advances and so on but it doesn't load the discourse acts and. So it's not the most. Not the summaries. Let's check that out. I'll probably ask Jonathan about it. So alternatively if we realize that we can't do the whole thing in one go we can probably just process some sort of meta data you know what I mean like sort of for the whole serious. Chunks representing the individual. Meetings are something. Like something that represents the whole serious. In a in a structure very similar to the structure in which we represent individual meetings. But with data sort of always combined from the whole. Serious so instead of having a single utterance that we display it probably be like. That would be representing a whole. Topic a segment in a meeting. And so that using the same data. You mean that you basically split up the big thing into different. Some of these. That you have a very. Top level summary and. Separate file. I think in the sense of like creating a virtual a virtual meeting out of the whole meeting series sort of. Easy just like. Create a new. Yeah sort of like offline create a virtual meeting which basically treats the meeting series is if it was a meeting and treats the individual meetings within the meeting series. If they were segments and treats the individual segments within. Meetings is if they were. Up fences you know so we just sort of shifted one level up. And that way we could probably use the same algorithm. And just like make like one or two if that say okay if you are on. Whole document a whole serious level and that was a double click then. Don't just go into that segment but load a new file or something like it. But in general use the same algorithm that would be an alternative if you can't actually load the whole thing and. Because also like even if we. Maybe this whole like maybe I'm worrying too much about the whole serious. In one thing display because actually I mean probably users wouldn't. You that one too. It's really that much. But what we can do is just roll the offline stuff and all we can do is just process. Like the summarization say you wanted a hundred options in the summary. Just look at the meeting take your top 100. After it's neat other needs in it. It's goes higher and the one's already in the. Summary so far just replacing and then you only have to process one meeting at a time. Yeah but I'm still worried like for example for the display. If you actually if you wanted to play like for the whole serious. The information density levels based on and the only granularity has individual. That means you have to go through everything. That friends in a series of 70 hours of meeting. So maybe we should. Store a main measure. Yeah. Yeah. And if you make that structurally very similar to. The like one level down like the way how we store in which latrances and stuff. Maybe we can more or less use the same code and just. Make a few ifs and stuff. So but still so in general we having we having. Atrances. And they have a score. And that's as much as we really need. And they also have a time. Of course. And speaker information. Yeah. Top. Yeah so an information which topic they are in. And probably separate to that information about the different topics like that. Yeah. So. So the skimming can work on that as the skimming just sort of sorts the utterances and puts as many in as it needs. Oh yeah. So. Preserve the order when it's displayed. Yeah. Yeah it'll play them in some order in which they were set. Otherwise it's going to be more entertaining. But that that's enough data for the skimming. Yeah. And the searching for what the searching does is the searching leaves the whole framework. Goes to the SQL database. And gets like basically in the end gets just a time marker for whether it's like that utterance that you're concerned with. And then we have to find I'm sure there's some way in the night XML to just say set position to that time mark. And then it shifts the whole frame in the deluxe everything the element of the display and the display updates. You don't want to go to the old tree display as well for multiple results. Yeah. Yeah. Yeah. Yeah but so so if if some. So if in that tree display somebody clicks on something. Each time the train is up. Yeah and then you sort of feed the timestamp to the night XML central manager and that central manager alerts everything that's there like alerts the skim like the audio display alerts the text display alerts the visual display and says we have a new time frame and then they all sort of do the update routines with respect to the current level of soon so how much to display and starting position at where the. Or maybe the mid position of it. I don't know like if you start with the thing was found or if that thing was found is in the middle of the part that we displayed that I don't know what that we can decide about but it general sort of. It's the same thing if like whether you play and it moves forward or whether you jump to a position through search it's essentially for all the window handling it's the same event. Yeah, it's only that the event gets triggered by the search routine which would have pushed it into the text and says, please go there now. So we should basically make our next document in memory. That everyone's module changes that rather than the underlying data and then have that. Why do we have to remember? You can make it. I mean like the information is coming from offline. So we probably we don't even have to change the utterance document right because the whole way like the whole beauty of the night ex melis that it ties together lots of different files we can just create an additional ex-mail file which for every utterance like the utterance of ideas I presume some references. So we just we tie just very short ex-mail file which is the only information it has it has whatever a number for for the wait for the information that we just tie that to the existing utterance and tie them to the existing speaker changes. But there's no idea for that. I think it's just one. So we have to go over it like add some integer that we just increment from top to bottom sort of to every other as an ID. Sometimes or try to understand how not to tell ideas work and maybe that's a special rule to follow when we use these ideas. The girls said the utterance of themselves are not numbered at the moment. I'm not quite sure I have only seen that the individual words have gotten an ID. So I guess that would be solved with the split. You always could have a look at the timestamps and then take the ones that belong together to form an attachment. I think you just take these segments that are already. Yeah, there's segments five and no these are my assignments. I think that's just like to make a list of all the stuff where we can find somebody can do the paper. So the stuff we have with utterances and speakers and wait for utterances. For every utterance, the utterance has a speaker and the weight coming from outside which is tied to it. And there is segments which are the segments are also a topic segment. They are a super unit so the utterances are tied to topic segments. And if the timestamps are on a word level then we somehow have to extract timestamps for utterances where they start. There are timestamps for the segments. What segments are for example when you look at the data what is displayed in one line? Okay. Is that the same about the utterances? Is that the same as utterances that... I think so. It may not be exact at the time. So I compared it with what I did for the post duration extraction. And basically it's words that are uttered in a sequence without pauses but sometimes however there are short pauses in it and they are indicated by square brackets pause. So that's one segment or is that two segments then? Sometimes the annotators decided what was one segment and what wasn't. Okay, but generally utterances is that which is called... Sorry, segments is that which is called utterances now. I think so. It's sort of like one person's contribution at a time sort of meeting. Okay. So we have those and then we have some fields somewhere else which have topics. Yeah, and the topic's basically just an ID probably with a start time or something and the utterances reference to those topics I guess. The topics don't contain any redundant thing of like showing the whole topic again but they just sort of say a number and where they start and where they finish. And the utterances then say which topic they belong to. Yeah, but I think for some annotations an utterance can have several types. Those are the dialogues. Yeah, I know but I was thinking of the topic segmentation. For that there would only be one, right? Because it's sort of like it's just a time window. So if this lazy loading works then this should definitely fit into, I mean not memory then because it wouldn't all be memory at the same time. So if you just have sort of that information like a long list of all the utterances slash segments and like shorter smaller lists which give weight to them. And even though probably if there's a lot of overhead and having two different files you can probably merge the weights into it offline. You know what I mean? Like if there's a lot of bureaucracy involved with having two different trees and where the one ties to the other because the one is the weight for the other then it's probably quicker to just go wider or just ride it. So you can just ride it in the XML file. You can not handle just loading arbitrary, new, like the trib use and stuff. I mean I always thought that it would be able to. Yeah I thought there was the whole beauty that like you can just make a new XML file and sort of tie that to the other end. So why do we need to have two XML trees and then we're on. Oh yeah, so no I didn't mean tree, no, no I meant just like handling two different files internally. So I was just thinking like if the overhead for having the same amount of data coming from two files instead of from one file is massive. Then it would probably be for us easy to just like offline put the weight into the file that has the segments. Yeah, that's left utterances already. But that we can figure out. The other thing is that would be easy to pass as well which means you wouldn't have to pass anything. Yeah, yeah, no we'd be completely using like the whole infrastructure and basically just I mean the main difference really between our projects and that's really is that we load a different part of the data but otherwise we're doing it the same way that they are doing it. So we just we're sort of running different types of queries on it. In a sense I think we are running queries it's not just about what we load and what we don't look but we're running queries in the sense that we dynamically select by by weights don't we. Now we have to check how fast that is like to say give us all the ones that whether that works with their query language whether that's too many results in whether we should. No, because if it let's say I mean if their query language is strange and if it would return the 10 million results and it can't handle it, then we can just write our individual components in the way that they know what the threshold is so they still get all the data and just they internally say oh no this is less than three and I'm not going to play or something. Yeah, I mean process it in chunks. In just process it all in chunks. Yeah, I'm just thinking for this whole thing of like a different level sort of cutting out different pieces whether we do that through a query where we say give us everything that's above this and this weight, or whether we keep the same infrastructure but every individual module like the player and the displays they like they still get sort of all the different differences all the different pieces but they say oh this piece of leaf out because it's below the current threshold level. So, I think that's why I have cool information density as well as like an information density score for each meeting and each topic seven because otherwise we'll be recalculating the same thing over and over again. And that will obviously make it much easier to display. When do we need the one for the meeting? Yeah, I guess for the so when we have to display where we display the whole series then if we have for the individual topic segment within the meeting, if we have ready calculator measures that we don't have to sort of extract that data from the individual Yeah, and that's also fairly it's just or long with our segments, isn't it? Yeah, for the segments are we extracting some type of title for them that we craft with some fancy algorithm manually, are we just taking the single most highly valued keyword And then our friends for the segment heading, well we can start off by that, well I was going to start off off, I've got some halfway through and printing one that does just idea and then just change that to work on whatever It's probably like in the end probably it wouldn't be the best thing if it's just the most highly ranked phrase a keyword because like for example for an introduction that would most definitely not be anything that has any title anywhere similar to introduction And then we can use the way for stuff like the hotspots and the key words for the sections and that. Also like for this part maybe if we go over with named entity in the end, if I mean one of the people doing deal has some named entity code to spare Like at least for the for sort of for finding topics titles for for segments just take a named entity which has a really high what's it called DFIDF whatever, because you'd probably be quite likely if they talk about a conference or a person that that would be named entity which is very highly frequently in that part. Yeah he said they're quite sparse so that basically was don't bother bathing too much of the general calculation but like especially if they're sparse probably individual named entities which describe what a segment is about would probably be quite good There's some name of some conference that could probably say that name of the conference quite often even though he's right that they make indirect references to it anyway. He said you're counting implementing the idea what exactly are you it's not TFIDF it's just a rest of the agency because it's really easy to do basically. There's just like for a baseline. So you're doing that on a on a per word level. Yeah. Okay. And then averaging the idea. Okay. But it's not like related to the corpus at all it's just one of them in arbitrary text. Okay. Okay. I was just wondering where you had to call this from at the moment. So it seems the data structure isn't a big problem and that basically we don't have to have all these massive discussions of how we exactly interact with the data structure because most of our work isn't done with the data structure in memory in the browser but it's just an offline and everyone can represent it anyway they want as long as they sort of store it in the use of like the matter of presentation in the end. It's like being useful to know how to store that thing. Yeah that would mean understanding that I text the metal like metal sort of format in a lot more detail. We should I think we should just have a long session in the computer room together and like now that we know a bit more what we want take a closer look at the topic segments. And then yeah there's a few poem you think and it gives the timestamp and say each one was the actual like utterance segments and the less of them are called and they're all numbered. That's what I just thought. Yeah I haven't looked at this stuff much at all. So I guess I'm going to be segmenting it with the LCC and that's the same format I want to do back out and be equivalent. Who's sort of doing the central coordination of the browser application or like integration? Yeah but also like all these elements like the loading and integration and handling the data loading and stuff. I don't know what to think anyone's been able to do that. I'm sort of like I think I'll take over there to split just because I started with a bit and found it durable. And I think everybody should sort of be the one person who understands most about what's essentially going on with the project like with a browser as a whole and where the data comes in. So it's boring. It's also complicated. Yeah it could be difficult. I can do it like several people together to probably just those people have to work together a lot and very closely and just make sure that they always understand what the other ones doing. And then we'll maybe have to prioritize somebody to enter it here. Yeah but I think actually like at the moment the integration comes first. I mean it's sort of at the moment the building the browser comes first and then only comes to creating new sophisticated data chunks because that's sort of the whole thing about having a prototype system which is more or less working on chunk data but at least we have the framework in which we can then test everything and look at everything because before we have that it's going to be very difficult for anyone to really see how much the work that they're doing is making sense because you just where I guess you can see something from the data that you have in your individual x-mails files that you create but it would be nice to have some basic system which just displays some stuff. Or something like that. Or just adapt like there like just sort of go from their system and adapt it piece for piece and see how we could arrange like adapt the power system. Does anyone want to like just sit with me and like play for three hours with an ad ex-mail at some point? I wouldn't like to be. I'd like to go to the gym I'm theoretically free but if there's any time. Give nothing no for a time on Wednesday Wednesday I've got a 9-12. 9-12 from the nothing you have for you. I've got nothing in the afternoon. Anytime Wednesday afternoon I'd be playing. So we're about just in Afton Tower. Yeah for us to whatever one is easy to discuss stuff I don't know. I'll be in. I'm not 5-12 on any way. Okay what time do you want to do? Well I'll be there for 12. I've got some other stuff that needs to be done on that lab so if you're not there at 12 I can just work on that. Okay so I'll just meet you in 18 India. I guess at the moment nobody critically depends on like the night ex-mail stuff working right now. Like at the moment you can all do your stuff and I can do my other stuff. And I can even do the display to a vast degree without technically having this flying framework working. So it's not that crucial. I would need the raw text pretty soon because I have to find out how I have to put the segments into a bin. Yeah actually I need the raw text as well. I was more thinking of the whole browser framework as a running program now. Yeah I think we all need the raw text in different flavors don't we? Yeah that's what I thought you said but you did extract in the text. I've only just got the notes I have to still have to order everything by the time. Yeah so you should be doing it in Python yeah. Yeah I think it's quite easy. Because yeah I was having a look at it as well. And the speakers are on a separate file. You have to combine them all and then you order them. That's what I thought. Yeah just combine them and order them. Right. So that's a quick time. Well I was going to do so. What I found out was that there are quite a lot of things without time stamps and the beginning. In the word file. Yeah that's just an idea. Yeah everything that's word has a timestamp. But what are the other things that's some kind of number. I'm not sure. I think there are quite a lot of numbers in the beginning where there's no timestamps for the numbers. I think they say quite a lot of numbers and before that there's this number. Number within the XML context. Yeah there are numbers in the W tag but there are no timestamps. Are they spoken numbers? Like do they look like they are utterances. Yeah there's the number task isn't there. That's part of their thing. That's at the end. Yeah in the beginning as well sometimes I think. At least I'm so sorry. We have to be cut that out anyway. Sorry you're going to screw up a lot of our data otherwise. I'm not sure what it does to tell me. I think it wouldn't. Of course I mean it would be because in every meeting. It would probably make the. Yeah if you have segments for that probably. And I think it even has its own annotation like digits or something. I'm just thinking like it probably like the LSA would perform quite well and it would probably find another number task quite easily. A constraint vocabulary with a high co-currents of the same nine words. But what is it actually that numbers? It's just a system I think. So but there are no timestamps. I think it's also something that they said the numbers in order right? They have to read numbers. Yeah I think it sounded like they wanted to check out how well they were doing with overlapping and stuff. Because basically it's like they're reading them at different speeds but you know in which order they are said. I didn't have a look at that. I actually have some reasons for doing it. I have some tips on saying like numbers at the end of the room. And also there are different combinations of letters. Is everything ordered at the timestamps global or are they local? At any point. I thought we'll make both of the pictures on YouTube. Okay. If you're doing IDFs or whatever you call your frequency that will mix up the name. You need some dictionary for that at some point. You need to have some representation of a word as not that specific occurrence of the word token but of a given word form. Because you may count the word on the right. Yes we should work together because I need a dictionary as well. I was just going to use the hash that one and Java because I'm only going to do it on small documents. It's just like until the information densities are from running. Just saying to work with. Didn't you say that? I'm not sure. Right. Is there any one of you for the document frequency or total frequency? You're going to have total frequency to work with that right? Like over the whole corpus sort of word. Why do you need to classify them to a different setting? It's because when you use the type of classification system and I think it's not possible to have just one class that's supposed to be. Can we just fill a second clock with junk that you don't care about? Like copies of Shakespeare or something? Yeah sure we could do that but I don't know if that makes sense. Because if all we're looking for is the frequencies because it's only how that would be changed for a classification. If we need just frequencies maybe we should just calculate them from using code or something. Or maybe another tool. Using which tool are you talking about? Just using a power script. Be careful with that. My experience with the British National Corpus was that there's far more word types than you ever think. Because anything that's sort of unusual generally is a new word type. Like any typo or any strange thing where they put two words together. And also any number is a word type of its own so you can easily end up with 100,000 words when you didn't expect them. So generally dictionaries can grow bigger than you think they do. I don't know how you want many terms you can handle. Or you can probably also pre-filter like with regular expressions even just say if it consists of only digits then skip it. Or even if it consists of any special character then skip because it's probably something with the dot in between which is usually not something you want to happen. I can't remember who's got it. It might be word met but one of these big corks has a list of stop words you can download and they're just basically this is really uninteresting boring words that we could filter out before we do that. And I think that's one of the papers I read that's one of the things I did right at the beginning is they've got this big stop list and they just ignore all those. But I did for my protocol just ignore the 100 most frequent words because they actually end up all being articles and everything. So we need like several of us need a dictionary. And I think that's a very useful way as well. Am I the only one who needs it with frequencies or is that a useful way as well? Frequencies. Well I guess as soon as we have the raw tech we can probably just start with a Java hash map and just hash map over it and see how far we get. I mean you can probably on a machine with a few hundred megabyte RAM you can go quite far. You can run it on beefy so even if it goes wrong and if it doesn't really. You want to look into getting some subset of the XC purpose on the guys machines. So I hate working on the ices awful. Like so it needs my home machine. Oh yeah burning it on a like you should be able to burn the whole purpose. Just dig. Where has a CD. Ah yeah I'll support about that two days ago. In the informatics building there. Sorry in applicant tower five. The ones closest two machines closest to the support office so I presume. I would I have the exact email I think you talk about sort of the ones that. Right hand corner. Yeah if you enter the big room in the right hand corner I think. The thing is like you can only burn from the local file system so if it's from. Well actually I think if it's mounted you can directly burn from there but the problem is I have my data on beefy and so I have to get it into the local temp directory and burn it from there but you can burn from there. How big is it without. Oh we looked at that map. We looked at that map because I could just say it going over a C one night and just leave it going all night. Yeah yeah we should be able to get it. I don't think it was. I don't think it was a gig about. Yeah I mean the way. And then like copy but you know what I figured out I'm quicker downloading over broadband. And using this artist there's something strange about the way how they access the hardest how they mounted. Which is unfortunate. I'll see like a. What operating system do you have. What connection do you have at home. Yeah so if any one of us gets it we can then just use it. I can burn it to CD or put it on hard to spread everywhere. Question is if you're not quicker if you because you should get massive compression out of that. Like 50% or something with a good algorithm. So if you compress it and just put it into a temperature as an off space. The temps usually have a gig about three or really. The temps yeah I don't like I mean there's no guarantee that anything stays there but overnight it'll stay. And I think the temps usually have. But that would have to be the temp directory of the machine you can SSH into directly off SSH. Yeah I can do it from that session down I think compress it from a remote session in the same session. Yeah they probably hate you for doing it. They probably they'd like you more if you SSH into another computer. Compress it there and then sort of copied into the gateway machine. If you SSH in there if it's big warning about doing nothing at all in the gateway machine. I was thinking of SSH and just into some machine. Yeah and then just actually being. I have I haven't figured out how to tunnel through the gateway into another machine. It's not it's not easy definitely. That's why I end up sort of copying stuff into the temp directory at the gate the machine. Sorry if this is boring everybody else. It's just details on how to get stuff home from. We could probably just look at that together when we're meeting. I just wanted so who's when doing the frequencies on the words because I think I will also I could also make use of it for the agreement. I'm not going to do the same thing because I am in my eye and I talked about using the discourse acts first. And then in the chunks of text I found looking for word patterns and so on. So I would for example need the most frequent words. So if you cut off all that I don't see as soon as somebody gives me the raw text of the whole thing. I can probably just implement like a five line Java hash table frequency dictionary builder and see. Yeah but I needed for my chance then I would. Did you not say frequency of words no sorry. I would like to look at the frequency of words in my in the regions of text I found out to be interesting. So I wouldn't need it. It would have to be recalculated. You'd have to count yourself here. But first how big is the chunks. How big is the chunks. I think it. As big as the hotspot annotation. So quite small. That's quite small. So you could just some addresses. You could use just the same thing we used to build a big dictionary. Just do that online. So that would take time to build a little bit through that. I mean just use the same tool. Yes. So I would probably just concatenate all my text chunks and then that same. You don't want to have different counts for each chunk. Just like for for something from all chunks. Yes. Oh yeah no that's yeah so once I write an art like if I write like an algorithm which does a hash table dictionary with frequency from a raw text then the raw text can be anything. So how far are you getting raw text out of it do you think? I can get all the raw text but it has to be on it still. Okay well that's good because for the dictionary the order doesn't make it any more of it. So yes so I'll get that from you and I'll write a hash table which goes over that and creates a dictionary file. So for the dictionary is it okay if I do whatever word blank frequency or something. Just put everybody for the start from that I mean I guess. Well that's it. I use in TF IDF for the information density. It's in what is implemented in Ray's information game. I'm not quite sure how they calculate that. Because frequency would be useful I think. Yeah I need frequency as well. Depending on the context, the size and what we consider a document in the sense of calculating TF IDF is going to change. Which might be thinking about. I think we might have a lot in common what we talked about because I for my latent magic analysis need like counts of words within a document within a segment actually within a topic segment. So that's what Ray was also I think you can just get probabilities for certain words for each document. Can I convert these probabilities back into frequency? I mean we have to look at that. So that's what Ray was. That's what LSA builds on. I get built a document by frequency matrix. I can probably get that even though but I already have my code to build it up. I have my code already. Yesterday you said you need the frequency counts actually for a document but you say not for the whole world. You need the raw frequency as well. You also need how many times things occur within each document. And what we consider a document is going to depend on our context I think. If we're looking at a whole lot of meetings we'll consider each meeting a document in terms of this algorithm and if we're viewing like say just small topic segment you might look at even each utterance. Here's a small document. It more and more appears to me that if we scrap the notion of the meeting as an individual thing and sort of see meetings as topic segments and have sort of like hierarchical topic segmentation instead then it's like you're walking here in framework. So it's only something that's thought of how actually maybe it doesn't actually matter. Maybe if you just do it once at the highest level it will be fine. But I was just thinking it might be difficult to calculate the TFI yet offline for the different levels we might want. So if we're going to allow this joint segment for example and how we're going to know what's going to be in context at any given time. I suppose if you just did it globally treating a meeting with a document it'd probably still be work out fine because you don't even be comparing with everyone within the company. Are we using this for the waiting in the end now this measure of your calculating? I don't know I thought we didn't need that in the end. Because if we're doing I think for the information density we should calculate it on the lowest level not on the highest level. Sorry that's what I mean like yeah for each word or whatever that across the whole lot is what I mean by highest level across the whole process. Yeah but don't you have to like go sort of like for in a document versus the whole thing that how it works. Yeah I really thought it was the H meeting as a document. I don't think that's a good idea because isn't it like that we expect it to change over with the different topic segments more that they're talking about something different in each topic segment. And possibly. Because that's what relative term frequency is about that like in some context they're talking more about a certain word than general. So that would more be the topic segments. And that's what I thought as well that probably the topic segment level is the most informative. Are they big enough to get anything. So I'm just wondering if there's ways to abandon the whole concept of meetings and sort of. Which is not really treating separate meetings is too much of a separate entity. But on algorithmic level whether we actually whether there's some way to just represent meetings as a topic. Yeah you just like whatever you want to look at just jam together into an external flow and that's your meeting even though bits it may have come from all over the place or whatever. I don't see why that's really a big problem. That's not really what I meant but I think I have to think more about what I meant. I'm confused about everything. So basically what you're saying to you take an arbitrary amount of data and process it with the same algorithm. It doesn't matter conceptually what that data is. It could be a meeting. It could be two options. It could be a meeting plus half a meeting somewhere else. I'm not so concerned about them meeting plus something else. I'm more talking about like yeah the keeping keeping the same algorithm in the same way of handling it and just saying. Like just this topic here happens to be like a whole meeting and it has sort of sub topics such as that sort of topics are hierarchical concept. Like the topic where there can be super topics and topics in the super topics are in the end what the meetings are in general. At some level super topics are created like like topics. I think it's very difficult. I mean what you do is you just build an XML file and if you want it to get down to the entrances. You go to the leads and then if you are in the next level up you go to the parents of those and like just go from like the leads in which towards the branch to build up things like you know when you pick on a segment it's going to have like words or whatever are important. As long as like the algorithm is designed with it in mind and then it's very good for them. So just grab that again then. Well like say you had like say for a meeting like you've got like say a hierarchy that looks quite big like this and like the utterances come off of here maybe. Then when whatever your algorithm is doing as long as when you're working with options you go for all the leads. Then if you need something next up so like a topic segment you go to here. But if you're looking at say this one that only went like this. So you say you'd start with the leads and you go I want a topic segment so I go one layer up. And then if you're working with just the topic segment in there because the only thing you have to worry about and like each time you want to hire level you just need to go up the tree. As long as your algorithm respects that then we can just process any arbitrary X and O. So that would be the series as a whole that would be sort of a meeting. Me too. I'm a bit brain damaged at the moment but I think I'll just sit together with you again and go through it again. So I think as long as you build an algorithm that respects whatever structures in the file rather than imposing its own structure. So is this structurally or is this not always identical? Well no. So that we can be treated with this algorithm more. But I mean it could be as many nodes as you want. This one could be deeper maybe. So then you start with all your utterances here. And when you go up to get topic segments you go to here, here, here, here, here. That might be a bit confusing. I hope you have things on different levels. I'm not sure how we can go from bottom up. I'll sort of take more that. This is all too complicated worrying about that at that moment. Anything that we're doing anything. Add their implementation in more detail that I understand what's going on so we'll see if we can get many browser displays two things seen together. Yes, two things from their stuff to make sure that we understand it, understand it enough to modify. You should be not have a good directory or something, you can put all our code in there. Maybe use for that, maybe use for that, maybe use for that. Yeah, how would you do that by just making like, read right for everyone? Okay, who is most free space on there? I don't know how it is. I've probably got a read for now because everything on my website now can actually be deleted or stored at home as well. Alternatively, we can probably just make another directory on the BPH scratch space. That's where I'm having gigabytes and gigabytes of stuff at the moment. Is that guaranteed to stay here? No, no. If not Steve, we can get space. Maybe he should send a support form. But I think if he sent to support he'd probably be a better partition. Yeah, I'm sure he had to deal with it last year. Because that would be really useful if we had a big directory, especially for transferring. Having said that, we allowed to take a copy of the XC process. So I need you to probably ask before we do it. I think it said yes to that. I think that when we were still in the seminar room, I asked that once or asked, is it possible to get it off? And nobody said, like people were discussing about the technical practicalities, but nobody said anything about being allowed to not allow it. I mean, we have access to it here and I guess it probably means that we can't give it to anybody else. But if they give us access to it here, sitting on a dice machine, then they should be very... Well, we shouldn't be able to leave it on our laptop. I personally don't have too many friends who would be too keen on getting it anyway. I have this really excited pirate copy. Annotated meeting data. So shall we stick together tomorrow then? Yeah. Okay. So what is the text you're extracting? Looking like them at the end? It would be best. At the moment it's because just... I think it's actually very similar to what I did for my speaker. I think I would perhaps have to change two lines of code to get you for each meeting, a file that says from this millisecond to this millisecond. That was this sequence of words. And so on. So that's just changing two lines of code. And it would be very good. Okay. Do you expect the words as well? Yeah, so far I extracted the durations, but it's from the words file. So I couldn't just... I think the words instead of the durations, and it should... I can try to do it in. I can't have a little bit of it. It makes sense for which one. So we already expect from all the files. Yeah, I just let it run over the first. So if you also order... Yes. Wait, wait, wait. Sorry. Yeah, sure. I was according to the starting terms and the other. What I just realized we should really keep different series completely separate for virtually all purposes. Just be careful about that. What do you mean by that? The exicopress isn't one meeting series, several meeting series with different people meeting for completely different things. Yeah, I mean, I have one, but I give you one file for each meeting. Yeah, not for each meeting series. I can do that. Let's just be careful that whatever sort of we merge together, the highest level of merging, it's not the whole exicopress, but the original series. Yeah, it might be funny to see what is summarized the whole corpus. I think we might actually... I think it'd be very useful. It's probably somewhere well or something like that. I think we might just get away with for the whole project just like looking at only one series and just doing within one series. I mean, you can do everything you want in one series. Yeah, I mean, there's one series that has just one meeting. Oh, yeah, that's actually... Is the data always clearly split up a different series? Is it like easy to just pick one? The data is of the form you have the pre-identification letter. So B-E-D or B-B-D or something, and that's always the same group. And then after that, there's a number like 001 or 003. So at every level, everyone has to be careful to really just take... Even if at the highest level just takes stuff from one series, not merge stuff from different series together, because there would probably be just majorly messy. So even if you make one single text file which has the whole corpus, sort of our corpus, there would still be from one series only. But what you're producing at the moment is individual text files that sort of have the raw text for a whole meeting as a whole. Yes, but I mean, as the start times, start for each meeting at zero, you could just probably just add the final second time to the next meeting. And so I'm just going to draw it together. And then we would have to change the information about who on which channel it was set to, by which person it was set, and that is actually starting another X and Y. Okay. So is anybody creating a real raw text thing at the moment, which is just the words? That's what I'm going to need. Yeah, but Ben just not print out the style of the end times. But if they are... so it's not an end times just for the file, like is it just the first and the last line or is it for every single thing in... So every single word. Oh, for every single entrance. Yeah, that depends on what you want. So what do you mean by just not print out that? I do this first, it's just string manipulation. I mean, I would just... If you're into it, can you make a text that which just like makes just the words? Sure. Okay. Do you want it straight flowing? Because I would need something that marks the end of... is your segmented by traffic standards? Is there any information you have to do topic to the automatic topic implementation? No, I didn't do so. Then I need something different later anyway. Okay, but for now, if you... That's what in the else you say, that marks the end of segment. Okay, you're going to put that as now to put the segmentation. Okay, so for now, can you create like sort of just a dump which is pure text, just pure text that I can get a dictionary and you can work on that for your topic segmentation? Yeah. And... And you would want that all in one file for all the corpus? For the series. For the series. But I can also deal with separate files. I mean, I can just write the algorithm that it loads all files in every directory or something. Yeah, I can directly put it in. So if you can put it in one single megafile, that would be quite useful for me. So only words for meeting series. If you look for you, wouldn't it be easy if you had different files because any sort of no... For me, it's better for buying. So give me different files as long as... If you could name them in a way that is easy to enumerate over them like whatever, one, two, three, four, five or something. It's just anything that I can... Yeah, they will just take over the names they have anyway. Yeah. Is it something that's easy to enumerate over? Is it some ordered pattern? Yeah. One series has the same pre-starting letters. Okay, cool. Yeah. So only words and words sometimes. No, I don't need the time. So just... Need words but... And the right order. Yeah, you want to order. Okay. Okay. We live. Okay. Orders. I'm hard-based at times. Yeah, and do want... Yeah, sometimes they're contained in one another. So... Doesn't matter too much. Just after. Okay. All of that. All of that. All of that. All of that. When do you think you'll have, like, a primitive segmentation by some ready-made topic segmentation by some ready-made tool ready? Then, no, but... You really need to do it once you've got the barcode. Okay. Just run in the... Okay, cool. Because I'll need that then when it's done. Yeah, hopefully that's neat. Okay. And I think for all the quotas, it's just from what I know from my attempts, it's nine megabytes. Mm-hmm. To have... It should be similar to... Yeah, yes. Words. Let's see. Words, what's nine megabytes? No, um, all the words together. And for... One of the meetings. That sounds quite reasonable. That's nine... Nine characters over... That's what I... Because nine megabytes is what I got for when I said for every......adurance. This goes from there to there. Okay. That is for... Are we picking one particular series at the moment or...? Oh. Yeah. I'm doing it for all of it. Doesn't matter. Okay. Yeah. I guess you can probably process the data for all different series and then check which series is the best for the presentation. It sounds a bit reasonable, nine megabytes to me. If you think if it's roughly a million words......and nine characters, the words sound really... Yeah. I hope this will be the same for the words. It's just with that. Yeah. Yeah. Yes, I'm going to build a dictionary then from that. Like, just a list of the words and maybe a list of the words with the frequencies or a list of the words......sorted alphabetically or numerically. What does anyone want to sustain your wishes for dictionaries? I could just use it with the frequency, I think, until the information density is finished. That would be really useful. Mm-hmm. So I'll create a dictionary. So, I'll probably send just one file of the first meeting to all those who need it so that you can have a look at what you want. Yeah. And then the actual file we can probably copy from your home directory or something like it. Yeah, I mean, if it's just for one meeting, it's really not today. Yeah, but I'm saying for the whole thing in the end, like, a big thing. You probably shouldn't do that, you know. Yeah. How long would it take to make the frequency counts for the Java edge table? From the time I get the file, I can do that in an afternoon, the next sort of the next morning. Well, you mean how long processing time it takes? No, how long you would have to program something? It's in box-ended algorithm. I've sort of written it for a deal, just a bit half an hour or something similar. It's just you put them in a hash table and say, well, if it exists already in the hash table, then you increase the count by one end. I probably implement some filter for filtering out numbers or something. Because it's quite easy in Perl as well. It's just a line of code for counting all the words. Really? How do you do that? Yeah, it's by hashes. Okay, well, I don't know any Perl. I mean, if anyone wants to do a Perl script for that, does it? It's a nice thing. I have no problem with that. But I think I have the Java code virtually ready because for Dilla wrote something very similar, like for Dilla wrote something that counts there. The different occurrences of all the text. If you're doing it in Java, it could be serialized, the output as well as writing it to a file. Sorry? If you're doing it in Java, it could be serialized, the hash table as well as writing it to a file. I've never serialized it. Really easy. Wouldn't that be absolutely massive? I don't see why it would be any more massive than the file. And then write the serialization to a file. So you want a file which is the serialization of a hash table. It's just a C pass in the file representation of it. And now, because I'll be using it in Java anyway. So I'll just be building data structure again. I'll check if I understand how it works. Otherwise, I can give you the code for loading a dictionary. It's a line break separated file. It seems like a bit silly to be passing it over and over again. Yeah. Yeah, I see if I understand how to serialize. There's a serialized command so that gives me one mega-mother. I think all the collections and things implement serializable already. But do they automatically write to the file? Anyway, I'll figure that out if we don't do that. Yes, that's pretty much it. So Dave and me look at how nightx mail works and we're out trying to either work some more on the TF-IDS or the audio thing. I'll build a dictionary as soon as I get the text. And yeah, so that when do we have to meet again then with this? Do you know what's going on with all of this? How are we going to do a demonstration next week? We had to demonstrate something like that. What do we have to demonstrate? No, no, not demonstrate. Didn't we agree that it would be useful to demonstrate or something like some primitive thing working next week? Yeah, I suggested that we could have an initial prototype. There's got to be very prototype. I'm not surprised if we can get anything more than that. Mmm. Wow, let's go. I feel like hanging mid-air and not really finding a point where you can get your teeth in it and start working properly. It's also fuzzy though. I think it's because we have to specify ourselves that it's not as focused as specification of most... Yeah, but at the moment it's also an implementation level. With the data structures, I'm just like over these vague ideas of some trees. Once we start doing it, it will become more obvious. Yeah, it's just our halfway through the project time. That's what just freaks me out. I would give it a chance. you know, like, Evaluationridge C C C C you