 I Yes, we are supposed to speak English. That's true. Oh, yeah. You don't put your headset microphone? Ha, ha, ha. Ha, ha. So, you're sitting there, 10-3. Everything. So, we are here to just make the point. Make the point. Yeah. Make the status about the shot detection. Okay. So, you have applied the shot detector to some of the Sinitis video on this one. And so, we can basically, maybe just comment on this. Maybe quickly remind what the shot detector is. How the shot detector is working. Basically, we might be able anyway to give you some reference. I mean, your report. Yes. It should be described there. Yeah, I have, actually, yeah. You can see the report here and the demo. And the goal of this meeting is to present the video shot detector to Olivier. Yes. Okay. So, yeah. It was my project during my internship at the app, under the supervision of Romach. And as you saw in the email, I sent, okay. I didn't send, this video is the new one. I just processed from your images. Okay, yeah. Yeah, but let's start by the video from Friday. Sinitis demo one. By the way, I was going to ask, I had seen the video. You did a, I did something of the video, right? I mean, for browsing, I mean. Yeah. Okay. Yeah. Okay. So this was, okay. So this is a yaki call normally. So you have scenes, shots. Yeah, here on the left, you have different scenes. But as I was asking myself, we could probably, you know, like group all the initial ones into one scene. And maybe after when there's some things inside, it should be another scene. So this detector detected 30 shots. And if you click on a plus button, you can expand it and see the key frames that are in fact five extracted frames from the shot, which I expected, I think. But I think as it is, it's extracted without using subshots. It's simply five down some, yeah, something. Okay. So I, we can see the video actually, the entire video, and open with your player. I hope it will work. So we don't have the sound. And it won't be grabbed. The video won't, I don't know if, okay. Not good quality. Yeah, the quality piece. That's why I was asking that. Oh, yeah, yeah. It's not this that you process. No, no, no, no, no. Okay. It's in real media, so it's very compressed. So here we see that we also show up at 18th and after 22th and after again. And if we go down, I mean, maybe in terms of shot here, we can see that. All right. So maybe C-shot 7, as it was. Yeah. Okay. Okay. Actually 9 currently is 10, probably 11 or 12. No, 11. 10 is 10 in that, okay. And then there is, yeah, this one is 11, 12 or 14 seconds. On a more case, it is a, yeah. Yes. And this one, yeah, it's a dissolve. Yeah. It's a dissolve, so it doesn't detect. Or is it really like, is it a unique thing? Yeah. When I saw them with them play it, it's a dissolve. Okay. Yeah. But it's not standard manmade movies because I would say more than that. I would say most of the time people didn't do any. Yeah, I could see here. Yeah. And that it was like, yeah, of course it was only one single one. Yeah. I don't know if this one, if we see was correctly, you see? Yeah. Yeah. Yeah. This is the one I had seen. Yeah. Yeah. But as I said, I mean, we have an emotion detector. When it's black, it's, there's not no structure. But there is a moon. Yeah. And then we have an image that's really estimate motion. Okay. Because there is no motion in between the two. Right. There is no structure. There is no texture. Yeah. Okay. This is not so good for the motion. But for instance, of course, between the horse and the, yeah. That is a cue. The cove. The cove. The cove. Yeah. Maybe we could have detected things. It's even clearer because you see the background is quite similar and it's not that sure. Yeah. Maybe to explain this method was using a simple distance between consecutive histograms. Okay. And we didn't use motion features at all for that. That's why you. So it's, yeah. And as you as Instagram, so you get experience with it. Yeah. So it's, it's crampers. So you can't scramble the image. Okay. And you get the same Instagram. Yeah. So there is no structure. In gear. So of course, when you look the horse and then you could see as a cow, the cows. Of course, you have a black in the top and we have also maybe 80% of like green, which is not green there, but grass. You have stones and so there are of course some differences because on one hand you have holes, but it's black. So you have more sky, which is black on the right. So you don't care about this. So it's a global, it's a global Instagram. So yeah. Okay. But it's when for this time with the for this application, I could imagine that it would be more like a sliding window because of course, if you just compute, I can show you, yeah, if you have a report because if you compute the distance between the two Instagrams, I think this was better. Yeah. Right. And so if you take these two with the with the better here, then of course, if you just do a threshold, when there is motion, you will see a lot of things changing. So by using another TTF threshold, if you see that there is a continuous change of the distance, you don't do a threshold. It's only if you have an identified peak, which is 50% or maybe two times higher than the next second value in the window, then you see it's a shot. Yeah. Right. So we want to make use the whiteboard or here it's a it was to this. It's in the this spot about a shot boundary detection is in French and fortunately, but this image shows the results using motion when you use the motion features. And here on the right is when using the Instagram distance, the motion features can better show what's going on when there is such a difficult transition while with the Instagram. Yeah, you just show that there is a shot change at the end, but even for dissolves, I don't have examples here, but there is not a strict transition. It's continuous. It's continuous for dissolves. But dissolves is anyways difficult, I mean, in general, because sometimes there are some dissolves you could imagine they are part of a scene. But of course, when you have cuts, you can see they are easy to detect right? I mean, next. So, yeah, so with the motion, basically the motion tried to estimate what the motion between two frames, right? You can see many images there, but after it's there and it looks at how many points are well matched. Okay. So you can see that like somebody speaking and so on. So before the before the shot, you can see that usually there are like 90% of the points at least right where the motion. So, but then after suddenly, if you go from one frame and you try to match all the points here with like a translation plus a zoom, plus maybe some rotation, you can find anything well. So basically, you see that the number of points, maybe only 10% of the points you can match well. So it's easy to detect. And after you can see that indeed, because it's a long shot and it's almost like a flat plane, you can do a very good registration. So the number of points is very high. I mean, the number of points that are matching. And here you basically here with a Instagram, you can see it's very flat. But of course, you should go from this to this to this, which is essentially green. You have this big measure, which is here, the guide to, I don't know whether this is what you use. You know, now it's for the video you saw, it's better. It's better. Okay. Okay. So. Yeah. And you have maybe one image with an adaptive threshold. I think that's something like this. Right. Just to explain. Okay. So that's really. Yeah. Yeah. Let me see back here. So there is the idea is having a sliding window. We have already all the data that is on the graph. And then to detect the changes, we do a calculation with points under this sliding window, within this sliding window. And we calculate. So what do we do here? In fact, we want to detect that this points in the middle is higher than. Alpha times the maximum of the other ones. Exactly. The other ones. Alpha times the maximum of the other ones. So of course, if it's you are not a maximum, you are not going inside the window, you are not the max. Anyway, you are not going to be above alpha times the max of the other side, because alpha is low or less than 1. But if you are in the maximum already at this point, we check simply if this value is alpha times the max of the other points. Let's say this point here. I see 6, but I don't know if this was for BataSha. It may be for Kait 6, but for BataSha it's for the test, I did it 2.4. 2.4. So it's a bit less because you amplitude this much. Yeah. So it was not so obvious because it was in the shot we've seen, especially with Kait 2 square it, the amplitude is much bigger than with BataSha. You can see that it could go to 700, and when it's very nothing, it can be 0. BataSha is between 0 and 1, so you have not so much amplitude. But still in some of these videos, for instance, even if we're going to go now, if we want to go now, for the kayak case, I don't know. At some point, he's targeting 2 kayaks and he's moving quite fast towards other people. I don't know what happened there if it was detected as a shot. In the first shot? Yeah. Yeah. I thought that it was detected as a shot. It was a shot. It was a shot. You want to see now? Yeah, maybe. But he didn't. Okay. But here, for instance, you understand. So we can show this tomorrow to the people at this. Yeah. I think it's a good meeting. I mean, to have a connection. Yeah. Okay. Yeah. We need a connection. That's right. And then the laptop, or you have your laptop? Yeah, I can bring my laptop if necessary. Okay. Yeah. I'll bring it. Okay. This is easier, a laptop. So here, in fact, after 12, I don't know if it's, I think it's 12 frames. Oh, yeah. But normally, if you, maybe you can play the shot for. Yeah. It's strange. Oh, yeah. Maybe you're burning. Maybe you play, yeah, if you play the video. Well, at the beginning, I don't know what's up. Oh, there is no, uh-huh. I didn't, uh-huh. I didn't have time. I didn't, uh-huh. Okay. Real media. Yeah. Okay. But, yeah, it's complicated. Well, no. Here I can see that it was, it was detected as a single shot. Yeah, here we can see that. Okay. I was, because here you can see that the motion, here is, you know, is doing a zoom on the tool. Yeah. Kayak. And then there is translation. Yeah. And then he's doing quite a fast transition from these to another one. Okay. And I was wondering whether, so if you, well, of course it's true that it's many water, so maybe the color is indeed a quite, but you know, as Kayak is entering quite fast, I mean, you can have quite, maybe. I can see. There is another program that I can see. But I think it seems to be. How many shots do you have indeed? Uh, Deanna. It's, I think it's, there are five, normally. Yeah, the one is, uh- If we would expand all- So you did it with what, by the way? It's with the images I gave you or- Yeah. Okay. And the other one had problems converting the video into Devix and then, so I used a program from software from Sibassia, PPM to API. Ah, to API. Okay. Yeah. Yeah. Because, yeah, one thing maybe we will talk about that is the library. It's, it doesn't compile anymore on the VPN machine. Yeah. So maybe some work has to be done on this. I don't know why this image doesn't appear. This is the, um, the frame that is, uh, uh, extracted to be the shot key frame here. This one is this one, the middle one. So I don't know what, why it doesn't appear, but it's just a simple problem in the get frame. And what I can show you now is a little program if you want to be, um, very precise about, so it's get frames. Okay. And we were interested in around, what do you want? Around six, seven seconds. Yeah. I mean, initially I don't know why there is a shot detected. Okay. Uh, if we do right next. Oh, yeah. Yeah. There is something wrong. So, okay. So the zero zero do here is an image number. Yeah. In fact, yeah. That's what I said. It's a frame number. I don't know why. Yeah. Yeah. It's a firm number. Okay. So then this is the firm number. This is in second, but I, I kept the firm number. So, but zero zero point two sequence. Yeah. It's in second. Sorry. It's 12 frames, zero seconds. Okay. Yeah. Okay. Okay. Okay. Second plus number. Next is 11 seconds. Okay. And this point are other than, okay. So this is normal. Yeah. So if we do every second, uh, the middle frame is, uh, this 1000. Okay. Just, it's another kind of browsing page, but, uh, to be more precise about, uh, to, to check maybe the, uh, this. Okay. Like, yeah. Exactly. Yeah. Okay. Then you, you go back to one frame around 160 for instance, you know. And there is, okay. Yeah. That's it. Okay. And it's, okay. You see that there is, uh, okay. It's not a shot change. It's a motion actually here. Yeah. It's motion. Yeah. Yeah. Yeah. But I was thinking that maybe it could have been, but it's through the, I think, many water when you're looking for some sweets. Yeah. Such a big, uh, but yeah. Sometimes I mean you can have very things. I mean, you just think about more action moving and things like this. Right. Sometimes very difficult. So for cuts usually there is no problem. People say this is a cut, this is not a cut, but like for this solve, even sometimes, you know, people may, people may have some trouble. I mean, saying this is a difficult, this is not, this should be labeled as a transition or not. Some people would say, yeah, but it's part of the thing. For instance, in like, in, um, also in, uh, come on. Advertising. Because the shots are very short. Yeah. And I think usually it's very fast. So sometimes, yeah, when you have an intro station like last time, what, what should you say? It's, yeah, it's a change of, but, uh, when you have, yeah, the, the goal results, then in fact, it's not a short change. Yeah. I mean, so sometimes very difficult. At least what I can see that currently like for sinities, for instance, if it's to do color correction or for instance, if it's too black, too a bit whiter, I think this type of things should be fine. Because basically, even if there are not real shots, they are not so interested if it's not in the shot. Maybe after they have this and they may say, okay, just by looking at least a summary. So maybe for the summary, we could do better keyframe extraction, for instance. But by just looking at the keyframes and so on, they may say, okay, from this shot to this shot, we apply like specific color enhancement algorithm or restoration algorithm or to improve the brightness and something like this, right. So I think for them, this is fine. Because this would be made manually, actually, when you say from this shot to this shot, it's using a graphical user interface, someone would do manually for each video. Well, maybe after, I mean, they would go directly and say select the shots from here to here and they would say you apply this algorithm. So of course, we could try to make sin clustering and so on. But I'm not even sure that this is necessary for them at this point because if they have a six minute movie, if they have a British shot for them, it might be easier to look at from here to here, apply this as they can do it quite quickly and they have to do it anyway because what's going to take time might be to select the type of restoration algorithm or type of thing. So this might take more time having to click two times on two shots for them than having directly a scene, right. So I think for them, this type of things is going to be useful, at least for this task. If they want to do stabilization, it might not be sure of that. This is the right segmentation for some of these. But this is another point, which for motion stabilization, as I said several times, we did not commit to provide anything. But of course, if you can provide things, we'll do it. So do you have more questions? Or you have a... I have a look at the demo film and it's about 40 minutes and about 173 different shots. So this is the one in the DVD? Yeah. Maybe we can see a little bit if you want to play. Yes. And I don't know if this is a real sample DVD because this will make about five seconds for each shot. So I think it's very short. Well, no, it's okay. Well, I think this is to make changes in the color light and they have a lot of shots. Yeah, but after a minute, it might be more like a guy in an interface. I mean, if they can select from shot one, we see them. They may do the grouping by themselves by looking at this and it may take less than one minute. I think it's anyway, if it's too summarized, after they may want to go inside to see whether there's no special things. Because after it depends, I mean, if you give only one keyframe per shot, after I mean, it's 170 minutes, it's quite fast. So I think Andrew Yaki could think it's good because still you can check if you have some ambiguities you're not showing. Okay. In fact, I didn't understand that. But now I understand better that Siniti wants this kind of method to help them to restore and to apply the some algorithms to make a better movie. Okay. It's not for the client, for the user who will maybe. Well, it's huge, literally, I think this is a type of thing. Okay. But, okay. I hope that if it's a big success, they will ask you to provide something like this, of course, some money, at least to be a student or somebody doing it. But yeah, of course, this was ultimately, I think, one way of if you could provide like chapters or something like this, indeed, you could provide like the FBI to the clients and you may ask for special services or even to just towards a movie to have direct access to different parts and so on. But this is, I mean, this tabling might be more easy to do for the client, by the client, because he knows what's there. I mean, he wants to have this and this. Okay. Yeah. Yeah. So, because he did the movie, so he wants to, okay, to archive, to one eye, to one eye, otherwise, I mean, this. Yeah, but of course, it might be easy to do a very simple software, which would allow the user to select just the shots and give names and after it would generate automatically some index. Yeah. And I suppose the client might be interested in such a thing. So, you could generate automatically on the DVD, but for, and you could imagine that before sending him the DVD, you could allow him to watch this type of interface and he would be able to do the selection. So, this is this, this is this, this is it. And when he would receive the DVD, he would indeed get already the chapters that he decided to put there. And this could be done by through internet. Yeah. Yeah. And I think this is good that this is more like this, because I'm sure that they might be interested in it, because I imagine that. From the client point of view and from the preparation of the movie DVD. Yeah. So, I think this is a very good. But, okay, I will play. So this is a kind of service, I don't know, because I give it for free and you had an hour here. Sorry. But you see that sometimes even here it's a bit like, for instance, if people would like to have like some motion compensation, it could be proposed. But for instance here it would be problematic to have motion. So, it's fine to do motion compensation, stabilization rather. But for instance here it's actually difficult to offer. No, that's not. So how come that's there? I think they have made some effects, especially effects with the DVD. There is two or three times. If I go to menu, there is something? You see the same kind of scene, just two or three. Okay, so maybe like here for instance it's just reproduced two times. You know what this is, there is no menu in the DVD. No, there is no menu. Okay. So it's just one chapter, no menu. Okay. So this is the same video, the FBI number one. Yes. Yes. And on the DVD I saw that there are some other stuff. I don't know. This one? I think the video is two videos. One is the real video and the other is just an introduction when you have the menu. Okay. Oops. So this is, okay. This video is about four minutes, five minutes, right? No, it's 13 minutes. Okay, so this is not the one that has been done. Yes, this is the same but there was a problem when I have to the grab from the DVD to the files. Okay. So this is why there are two files. That's two files. Okay, that's right. No, I have one file or so on. Okay. So you can process again. Okay. And it's real time, right? I mean, the processing. Yeah. I would maybe it's a bit less than, a bit more than real time. A bit more than real time? I will have to check but if it's five minutes videos it's like done quickly. Yeah, normally it could go faster. But probably it's probably about that even for the project you would do some of the programming. Yes. Okay. Especially if we need to interface new components, open CV, or for just reading. I mean, the DVD, except thing that we used. So to more anyway, for this we don't know exactly yet because what they want to, what they use as tools for reading DVD and so on. And because I don't think this is so difficult now you have, you also already looked at the Instagram and the computer instances and so on. So if we want to do a special thing that probably runs fast, it may be easier to do it directly. But yeah, the interface is very, so the interface uses the output of your program. Yeah. So I will use the whiteboard. So in fact, yeah, the interface is quite simple to use. For instance, you see on the URL I put, okay, there is mmm.id.ch slash. CGI bin slash video browser, okay, slash video browser three frames, three frames.html. And then this CGI parameter, see, video name equals, CNETT, okay, demo one. In fact, I just explained how to put the data on mmm, but it's quite simple. When you log into mmm with a CSSH for instance. So you just put it working like this mmm data video browser, okay, slash. And then this is the name of the new directory you will create demo one. And there stands, there is a couple of things. It's this name. So I will say this is video name. So video name dot avi, you just, because we need the avi so that we can do the, we can do the, the get frame. But otherwise it's not necessary. Otherwise it's not necessary. And also for the. And this is, no, no, no, this is done on the fly with get frame. Okay, yeah. So you need that. You need the XML. So it's video name. And I could read data that XML. But in fact, it's just a simple convention, I could have put nothing, but I just put data. It's a convention data. And then video name. Okay, it's rm9 dot rm for the stuff. But in fact, yeah, when I think now we could have only with the extension you can, you can see the name of the video file is video name. The name is no, in fact, yeah, yeah, it's the same as the directory. It could have been video name. So, but it's the same as the directory. And that's it actually when you, you do that. I did the same for kayak. You say kayak. In fact, now it's kayak. And then you can see. And this output, yeah, this output, this file is an output from the. The software, the short boundary detection software. Okay. And it's a simple format. I suppose like. It's a very simple format. Like your scenes. Yeah. Yeah. Actually we. Sometimes times. Yeah, I don't know. We cannot. But you. Yeah, it's the video scene shot. Yeah. It's, yeah, it's a video scene shot. It's yarkical. So in XML and key frame timestamp. And from the timestamp and the video location. He knows how to get the frame. Each of these frames. And if we do, in fact, I don't know. Yeah, it's more data for your folks. You're page two. So every. Okay. And every frame is stored into a backup. In fact, every frame. It's MMM who manages automatically. Okay. So that when you ask for this frame, in fact, we can. Okay. I just view from. Why not a view image? When I say view image. When I say view image on this frame. Yes. Up. We go there. And this is a, in fact, it's a per program that outputs in, instead of outputting HTML or text, it outputs a JPEG. Okay. I understand. From the video with the timestamp, he outputs a JPEG. But this is quite nice. And the server, the original is a video frame. Yeah. Okay. And this image is output, I was put it to the client. You can change the number. Yeah, exactly. Yeah. That's what I did. Okay, I understand. Here. In the other. In the other application. Yeah. Okay. I don't know. Who was the right to put that on MMM? Yeah, the MMM developers. So I'm part of that. But we could, we could. Yeah. It's not a problem. Yeah. I wanted to see to see something else. Yeah. And the images are sent to your browser, the client, to this browser, but also kept in a directory on MMM. Okay. There is a local cache. Exactly. A cache. I say backup, but it's a cache. Okay. If I go. That's how it works. So if we, for example, now we have the case that this video is not correct because I have maker from grabbing. So we need to remove the cache for this video. No, no, no, it's okay. For the cache, we have plenty of space on MMM. Yeah. So this is not a problem. What I mean is, ah, yeah, for a new video. If I, if I put the same video with the same name. Exactly. Yeah. Instead of this one. And it detects, okay. It should be defined. Yeah. So you go inside the get frame function. You see where it saves it. And then there is a special naming convention and you can remove the. Okay. But it's, yeah. It's true. It's not a real usage. No. Okay. Because it's in cache. Yes. So that if I put another video with the same name. Ah, okay. Yeah. They, they will happen again. They will appear this. Yeah. First, this one was. Yeah. If you have the same decomposition. Yeah. Yeah. If you have the same decomposition. Yeah. But it may happen that for 90, 90% of the frame, it's different. So you see it's fine. But suddenly you have one of a frame. They don't know what it is. And where it comes from. But they are on MMM. Oh, no. We don't have such many videos. Okay. Yeah. Yeah. But if you put, yeah, in another directory, then it would, or new naming convention with a special, yeah, code. Oh, you mean that? Because it's, it's, it's a MMM cache. Yeah. MMM cache. So this means that when it's done, if I go on my computer and I do the same, it's not going to enter in my cache, but it's going to be already there. Yeah. In an MMM. Okay. But it can go in the browser cache or so. So it's not done online. The first time is done online. Yeah. But after it's, yeah, you can see here, yeah. Okay. Okay. The first time you put it was everything was generated on the fly. You know, you see that. Here it's, it was a cache because I already pressed few, but now it's on the fly. Yeah. Okay. So it's quite quick. Okay. Like this. Okay. So if you come back now, and then if you come, yeah, yeah. Because the images are already generated. And maybe they are on the cache of this browser, this computer. Right. So, okay. That's why it was used with, yeah. Sebastian did the C function. And yeah. The bird is calling this C executable. Yeah. And it was used with the MM server with Pierre at the beginning. Yeah. Okay. So. Okay. And just a question on the. The organization of the MM. So you don't have a part for a specific project. Every, for example, if we make a lot of film for Cinities all around the top. I mean, all the film are. Oh, the movie. You don't have a separation for the film for Cinities of other project. And in fact, where you say video, main equal. We could, you know, you know, all these graphic areas on the face that you see. So could you say, okay. In fact, slash something. Okay. Yes. Yes. Okay. We could do that. Okay. We could put Cinities slash here and then Cinities slash. Oh, yeah. It works. Yeah. And the, the point is this interface is I think six or seven files only XML. Yeah. Well, it's per CGI that I put XML and XSL. So it's, it's not a big deal to, to, to make a new like Cinities browser or, you know, video browser. You can do some special thing. Yeah. We copy all the files and there is just one, one parameter, one string. Yeah. That we need to know where this data is and we could create a MMM data with a browser Cinities or something else. Okay. It's quite extensible and. Okay. Even if you would change the CGI for instance, let's assume that we say we don't do any scene thing. Yeah. Exactly. Yeah. I was thinking that here I see that you have online video structure correction. Yeah. But it's not, it was, you know, this is working the X-Men or yeah, but this is working. This is wish wishable. Yeah, to do this something on a video annotation on a video to come up with correction. But it's not easy on the web, on the web with HTML pages to do such corrections like video structure corrections for instance. For that you need, it's possible maybe on interfaces like flash or Java, but still there is a text based more work. So it's a feature for all of you. Yeah, yeah. I mean, it's a good point. No, because I see if it's there or I see it. Yeah. You know, you can do anything. And all these files are only internal. This means if I go on the C10E, C10E, it's a no, no. From outside, we can do it. From outside of Egypt, we can access this. Yeah. From home I was able to access it. With a password? No. Well, we can make for, for instance, for TSR, TSR videos, Alisson rule or other things. And ask me to do the same thing. And we, I put, we had like a password convention. Okay. So this is possible to make for these videos. Actually, it's been downloaded that several times. And there's some new years that I've never known. Yeah, there is no password production for these videos. Yeah. Maybe what I say, I'm very tired. But yeah, it's demo and one can say you have to guess the name. Yeah. Yeah. Yeah. Okay. Yeah. It's good to... Yeah, a good point about password protection issues. Yeah. What privacy issues? Yeah. It might be good if we start building some experiments like this to other passwords. Okay. Yeah. All the firmware is ready with TSR videos. Yeah. Yeah. Okay. Usually what we do sometimes, okay, just to say something, what password protection sometimes. Instead of having a purely secure directory, we put a code number like so that it's quite impossible to guess. Instead of video, name equals kayak, then we would have 0, 6, 7, 5 or 2. And then this directory from outside, we have to take care of that. Okay. And then only the video and this URL is not accessible, but also we should take care that, in fact, MMM, it's in MMM data video browser and this is accessible from outside. We should take care that this directory or in fact, this directory is not listable. Right. You know, you cannot list from outside with the FTP or HTTP. Okay. Yeah. I don't know if you want to add anything, but I would say that for the C code, I just want, we'd like to stress something is that, yeah, if we want to use the new versions, etc, I think we need to go a little bit through the code and I would be happy to help to, to, to, for instance, the classes, what videos to do, etc, to put them here. And yeah, it was done indeed to be more general than just to do the short detection. So that's why the code is a bit more complex. Yeah, currently. And the code is by the open CV and talk. Yeah, in fact, there is no, it was called vision lib to read to extract one frame, each frame of a video, you know, so it was from Sebastian. It was in the torch library, but there is nothing to do with the torch and it's using open CV and some classes from Jean Mark for a better shot at this time, etc. Okay. Image for the same one, so we can forget the torch and use. Yeah, we can forget the inter reader for the impact we can use open CV. Yeah, yeah, yeah. Okay. So it's 4 30, so we did one hour. Yeah, I think it's good. Yeah, good. Just if you want, yeah, this is the parameters of the, of the C program, but usage video in. Yes. Okay. The output file XML output file. Yes, it's the distance between each histogram so that if the XML you feel there is not enough shots, you can instead of reprocessing the video, you reprocess this file. Okay, that's good. And also some other stuff. For instance, the alpha threshold we talked about is set to 2.4. And yeah, so the seek first, that's the step we always use one and it should be one because otherwise it's step between consecutive frames. You can skip some frames. Yeah, but yeah, it doesn't, yeah, it's better to keep it one. Otherwise after, I mean, you very short, you don't know between, because you dropped one frame, you don't know where it is. Yeah. The detection latency is about the, it's half of the windows, yeah, the sliding window. See, option for keyframe exception again. Sub threshold, yeah. Leave it to one. Yeah. Okay. It's better. Yeah. Maybe I will, I will give you this file in as a, it's a kind of weird mirror. Yeah, I just, for preparing this nice discussion. I'm quite happy to do that because it's good when you do something that there is someone using it. Yeah. I don't know, I'm sure that there will be, well, I hope there will be a theory of something. At least here with the interface, it's quite funny to see that quickly you can do something in, in, yeah, in third, if the video is 10 minutes, in 30 minutes, because it's not a lot of things to change, or even it can do it, it can, we can do it quite automatically. It can be done. So just last question. Yeah. So this makes XML file. Okay. This is so it's a program to do generating to XML file and the, the shots. Yeah. And with the kayak, you have one problem. You don't have the real media. So this is another thing. Oh, yeah. How do you generate a real media? So the real media is a, a parallel thing that from the AVI, you convert it into real media and it is done on Windows. Okay. So you have some special tools from, from your media. Yeah. Okay. There is a, the free one is okay. You can use the free, we have a license here at the app, but the, the, if you want to pay, you pay for options such as resize or cropping. But in fact, maybe we don't need that and it's called a Alex producer. Okay. Alex. Yeah. I forgot. Yeah. Alex producer and that's it. Okay. You can, you can eat it. But you need, you need to have Windows to make this. Yes. So I will ask you. Yeah. Okay. Okay. I understand. And yeah. Okay. With real media. This is make by your program done outside the job. Okay. Well, this is, you can buy or you can all for free one time. It's not the job. No, no, no, no for the real media. And you didn't see, but the, the real media is called via a real server. So it's RTSP, the connection and it's streaming, video streaming. So it's not HTTP. So that it can play a smoothly. But we are still investigating for other video formats because it's not supported for instance in Java, in Java applications, etc. Oh, yeah. You can use Java framework. What am I presented? Yeah. Yeah. Yeah. Okay. So there are many issues about that. Yeah. Yeah. It's another program. It's a format. Yeah. Okay. So I think this was good to see the tool. Thank you. And tomorrow we are going to receive most of the things, but shorter. I believe right. Yes. And I think maybe later we can start doing this. I think for test I can take the real demo and make the full processing. Yeah. That's my question. It's interesting to see tomorrow. Because maybe they would say, ah, 170 shots or it's a lot. But after I'm in a despond we don't have nothing we can do unless they know what they want to do with it. Yeah. Yeah, there are other things such as video structuring but it's other level. Yeah. It's quite a research. Yeah. I think it's going to be very good for them to visualize how you can organize things and maybe what they can do and so this is important. Yeah. Last thing, this is the kind of XML that is outputted. So it's the structure is such as a video segmentation with a video seen beginning and then for each shot you have an ID. Well this is the terms from MPEG-7 convention anyway. Short, begin and end. The short key frame that is the middle here. Okay. In this, in this the beginning and end the number is the, for example the end is zero dot. You're right. So I have these 12 frames. This frame too, yeah. Okay. And in fact that's what I would put. Okay. In the, yeah, I think on the interface. And there is only one subshot because, ah, yeah. Okay. Yeah. There was, there was problems with detecting the subshots etc. But we were. Because you tried to do that. Yeah. Well, not with this one. Yeah. I tried to code some stuff. But I'm not so confident in the way it is. Okay. Good. Yeah. Like for instance, if we need to find subshots usually it, for instance, is better to have the whole shot to decide on how to build the subshots. But it shows that maybe if you store all the distance, the distances we could do some basic stuff. But we, yeah, even we should, ah, for instance, for the videos, ah, for the spectral clustering, for the clustering. Here I store the consecutive, the distance between consecutive histograms. But we could store the matrix. Yeah. Yeah. Yeah. Yeah. Yeah. I think there are many possible things with the spectral clustering. What? But with, there is one parameter that is very, what, I can remember, what is the parameter? The Eigen gap. You mean? No. Yeah. Oh, I mean, it's a better string, at least. No, no, no, no. The Eigen gap to find the good number of clusters is so sensitive. Yeah. Yeah. Anyway. It's a bit later in the report. Okay. So, any time I would be happy to, I'll have to open it up. Okay. Okay. Okay. Yeah, yeah, we can do it. It's not a snack or that. This, when you get, in fact we don't need it. Okay, but everything is stored here. Okay. I see. Thank you.