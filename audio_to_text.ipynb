{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuzzPHd6gIUQ",
        "outputId": "a34fedfd-7340-43a9-90d7-4952f665db1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Test GPU Compatibility\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# My GDrive Path\n",
        "PATH = \"/content/drive/MyDrive/audio_data/\"\n",
        "\n",
        "# For initial testing purposes\n",
        "DUMMY = PATH + \"anjali_zoom_test.m4a\"\n",
        "DUMMY_LONG = PATH + \"50_min_test.wav\"\n",
        "\n",
        "# Verify GPU is active\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# Utility functions\n",
        "def run_with_gpu(function, *extra_args, gpu=True, which_gpu=\"/GPU:0\"):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    if gpu:\n",
        "        with tf.device(which_gpu):\n",
        "            return function(*extra_args)\n",
        "    else:\n",
        "        return function(*extra_args)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7EsxxG36STID"
      },
      "outputs": [],
      "source": [
        "# Taken from https://github.com/yinruiqing/pyannote-whisper\n",
        "from pyannote.core import Segment, Annotation, Timeline\n",
        "\n",
        "\n",
        "def get_text_with_timestamp(transcribe_res):\n",
        "    timestamp_texts = []\n",
        "    for item in transcribe_res['segments']:\n",
        "        start = item['start']\n",
        "        end = item['end']\n",
        "        text = item['text']\n",
        "        timestamp_texts.append((Segment(start, end), text))\n",
        "    return timestamp_texts\n",
        "\n",
        "\n",
        "def add_speaker_info_to_text(timestamp_texts, ann):\n",
        "    spk_text = []\n",
        "    for seg, text in timestamp_texts:\n",
        "        spk = ann.crop(seg).argmax()\n",
        "        spk_text.append((seg, spk, text))\n",
        "    return spk_text\n",
        "\n",
        "\n",
        "def merge_cache(text_cache):\n",
        "    sentence = ''.join([item[-1] for item in text_cache])\n",
        "    spk = text_cache[0][1]\n",
        "    start = text_cache[0][0].start\n",
        "    end = text_cache[-1][0].end\n",
        "    return Segment(start, end), spk, sentence\n",
        "\n",
        "\n",
        "PUNC_SENT_END = ['.', '?', '!']\n",
        "\n",
        "\n",
        "def merge_sentence(spk_text):\n",
        "    merged_spk_text = []\n",
        "    pre_spk = None\n",
        "    text_cache = []\n",
        "    for seg, spk, text in spk_text:\n",
        "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
        "            merged_spk_text.append(merge_cache(text_cache))\n",
        "            text_cache = [(seg, spk, text)]\n",
        "            pre_spk = spk\n",
        "\n",
        "        elif text[-1] in PUNC_SENT_END:\n",
        "            text_cache.append((seg, spk, text))\n",
        "            merged_spk_text.append(merge_cache(text_cache))\n",
        "            text_cache = []\n",
        "            pre_spk = spk\n",
        "        else:\n",
        "            text_cache.append((seg, spk, text))\n",
        "            pre_spk = spk\n",
        "    if len(text_cache) > 0:\n",
        "        merged_spk_text.append(merge_cache(text_cache))\n",
        "    return merged_spk_text\n",
        "\n",
        "\n",
        "def diarize_text(transcribe_res, diarization_result):\n",
        "    timestamp_texts = get_text_with_timestamp(transcribe_res)\n",
        "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization_result)\n",
        "    res_processed = merge_sentence(spk_text)\n",
        "    return res_processed\n",
        "\n",
        "\n",
        "def write_to_txt(spk_sent, file):\n",
        "    with open(file, 'w') as fp:\n",
        "        for seg, spk, sentence in spk_sent:\n",
        "            line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sentence}\\n'\n",
        "            fp.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wv0tmMie_VU",
        "outputId": "a5036866-878e-4234-b7fe-47e2724ad6e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 50.8MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hi, I'm Anjali Sebastian and for my research presentation, I have selected the paper, Predictibility of Web Server Traffic Conjession written by Y. Barashnikov et al. Written in 2005. The location to the reference in the textbook is on page 417, and which discusses dynamically creating replicas when demand goes up. There are often large swings in demand for web server content, and these bursts of traffic are usually called hotspots or flash crowds. So the paper actually looks at investigating the potential for predicting these hotspots in advance. The idea here is to assess the feasibility of a prediction algorithm, not specifically deliver a finished product. The approach taken is predictive versus reactive. So when you're talking about a reactive approach, you are already experiencing the hotspot and are trying to mitigate it by either throttling requests or you know, farming new servers. With this predictive approach, however, you could also use it in conjunction with a reactive approach for better resource management. So first we look at a more rigorous definition of a hotspot, and a hotspot is defined as a point in time when the number of requests, page requests, RT goes higher than H, which is our maximum capacity. But since time is very small and there can be a lot of high variation, we need to smooth it over using an interval, which is called WD. So in this case, what we're saying with this formula here is that when the traffic is experiencing a hotspot, on average, the request rate is higher than H. The paper analyzes request flow data from four well known high traffic servers. The x-axis shows the time in seconds, while the y-axis represents the request number of requests every 10 seconds. All four of the servers show very different traffic patterns. And in order to predict future traffic, we first construct something called an auto correlation function. Auto correlation is simply the correlation between a variable and its future or like time shift itself. So in this case, we are looking at the low values between the time t and the time t plus x. If there's high auto correlation, then you can predict the will load at t plus x given the load at t. An ideal auto correlation would be aligned that is very close to one, nearly very flat. We can see that the workup server has very high auto correlation, while NASA has very low auto correlation. We then use these auto correlation functions and we perform basically linear regression on them. And we do the linear regression for a specific time window, which is called the prediction window. And essentially tells us how much of history do we want to consider. And in addition to this, we also look at something called the advanced notice. And the advanced notice is just how, how ahead we want to predict the hotspot. And this is typically denoted by the Greek letter tau. So the algorithm, we in the algorithm, we set a tau max, which is like the maximum advanced notice you could get. And we assume that the hotspot is going to arrive by tau max. So it acts like an alarm. So if it doesn't happen, we reset the alarm after a tau max. And we keep doing this over and over again. And we also find a set of sequences of advanced notice called tau T that keep predicting how far, how much time is left to find the next hotspot. We'll look at this in the next slide. This is how we can use the algorithm to detect hotspots. The advanced notice sequences or prediction sequences are superimposed on the corresponding request data. The prediction sequence is the tau T that we had discussed in the previous slide. And you can see that it is falling here. The T edge over here is the actual hotspot arrival time, which is indicated by this line. And what we see here is that this is a case of a very well predicted hotspot where the. Prediction sequence is linearly decreasing and can and falls to a near zero when the actual hotspot arrives. So tau max as we discussed is the maximum advanced notice that you can get. And it's obviously good to have a very high tau max because that gives you a lot of time to prepare. But if it's too large, it generates too many false alarms, which we can see on the second image here where there's actually a false alarm. There is actually no hotspot, but an alarm was raised. So going forward, we just look at. At the algorithm assumptions and our ability. The assumption is that it assumes that we have the ability to identify proper values for. For edge for the granularity for the prediction window for team tau max. And all these are much easier said than done. The paper does have significant details regarding how you would select these parameters. But it does not consider the fact that maybe these parameters might have to be dynamically changed. Analysis I felt the algorithm is similar to auto regressive moving average model, which is used extensively in micro economic and financial forecasting. It has become much harder to predict hotspots due to new traffic patterns like social media, video streaming, etc. However, the idea that predictability in internet traffic is possible and it can be used to allocate resources efficiently still relevant. This is my conclusion. Thank you.\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "result = run_with_gpu(model.transcribe, DUMMY)\n",
        "        \n",
        "print(result[\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "Q0fE3IESx8RB",
        "outputId": "f0a122dc-f857-4f06-fb13-a2069a4eecf5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Hi, I'm Anjali Sebastian and for my research presentation, I have selected the paper, Predictibility of Web Server Traffic Conjession written by Y. Barashnikov et al. Written in 2005. The location to the reference in the textbook is on page 417, and which discusses dynamically creating replicas when demand goes up. There are often large swings in demand for web server content, and these bursts of traffic are usually called hotspots or flash crowds. So the paper actually looks at investigating the potential for predicting these hotspots in advance. The idea here is to assess the feasibility of a prediction algorithm, not specifically deliver a finished product. The approach taken is predictive versus reactive. So when you're talking about a reactive approach, you are already experiencing the hotspot and are trying to mitigate it by either throttling requests or you know, farming new servers. With this predictive approach, however, you could also use it in conjunction with a reactive approach for better resource management. So first we look at a more rigorous definition of a hotspot, and a hotspot is defined as a point in time when the number of requests, page requests, RT goes higher than H, which is our maximum capacity. But since time is very small and there can be a lot of high variation, we need to smooth it over using an interval, which is called WD. So in this case, what we're saying with this formula here is that when the traffic is experiencing a hotspot on average, the request rate is higher than H. The paper analyzes request flow data from four well known high traffic servers. The x-axis shows the time in seconds while the y-axis represents the request number of requests every 10 seconds. All four of the servers show very different traffic patterns. And in order to predict future traffic, we first construct something called an auto correlation function. Auto correlation is simply the correlation between a variable and its future or like time shift itself. So in this case, we are looking at the low values between the time t and the time t plus x. If there's high autocorrelation, then you can predict the will load at t plus x given the load at t. An ideal autocorrelation would be aligned that is very close to one, nearly very flat. We can see that the workup server has very high autocorrelation while NASA has very low autocorrelation. We then use these autocorrelation functions and we perform basically linear regression on them. And we do the linear regression for a specific time window, which is called the prediction window. And essentially tells us how much of history do we want to consider. And in addition to this, we also look at something called the advanced notice. And the advanced notice is just how, how ahead we want to predict the hotspot. And this is typically denoted by the Greek letter tau. So the algorithm, we in the algorithm, we set a tau max, which is like the maximum advanced notice you could get. And we assume that the hotspot is going to arrive by tau max. So it acts like an alarm. So if it doesn't happen, we reset the alarm after a tau max. And we keep doing this over and over again. And we also find a set of sequences of advanced notice called tau T that keep predicting how far, how much time is left to find the next hotspot. We'll look at this in the next slide. This is how we can use the algorithm to detect hotspots. The advanced notice sequences or prediction sequences are superimposed on the corresponding request data. The prediction sequence is the tau T that we had discussed in the previous slide. And you can see that it is falling here. The T edge over here is the actual hotspot arrival time, which is indicated by this line. And what we see here is that this is a case of a very well predicted hotspot where the. Prediction sequence is linearly decreasing and can and falls to a near zero when the actual hotspot arrives. So tau max as we discussed is the maximum advanced notice that you can get. And it's obviously good to have a very high tau max because that gives you a lot of time to prepare. But if it's too large, it generates too many false alarms, which we can see on the second image here where there's actually a false alarm. There is actually no hotspot, but an alarm was raised. So going forward, we just look at. At the algorithm assumptions and our ability. The assumption is that it assumes that we have the ability to identify proper values for. For edge for the granularity for the prediction window for team tau max. And all these are much easier said than done. The paper does have significant details regarding how you would select these parameters. But it does not consider the fact that maybe these parameters might have to be dynamically changed. Analysis I felt the algorithm is similar to auto regressive moving average model, which is used extensively in micro economic and financial forecasting. It has become much harder to predict hotspots due to new traffic patterns like social media, video streaming, etc. However, the idea that predictability in internet traffic is possible and it can be used to allocate resources efficiently still relevant. This is my conclusion. Thank you.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Q1jLEgGmx834"
      },
      "outputs": [],
      "source": [
        "result = run_with_gpu(model.transcribe, DUMMY_LONG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yPwWe6TFzBbs",
        "outputId": "85fe7337-85e9-474e-fcf2-64c2a26ce88c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Okay. Okay. Does anyone want to see Steve's feedback from the specification? Is there much more in it than he said yesterday? Not really. Just what he's talking about, duplication of effort. Like duplication of effort and stuff. Yeah, seeing that we should maybe think about having a prototype or a weak sex, which is necessary. So, this is probably prioritized or... Yeah, I'd say if for the prototype we feature like wherever possible, chunking the stuff that we have pre-anotated and stuff. And for the stuff that we don't have pre-anotated, right, like a stupid baseline, then we should probably be able to... Basically, that means we focus on the interface first, so that we take the ready-made parts and just see how we get them work together in the interface the way we want them. And then we have a working prototype and then we can go back and replace pieces either by our own components or by more sophisticated components of our own. So, it's probably feasible. I think it's a way this weekend. So, that... If we just want to have some data for the user-fist could even be random data. Oh, yeah, no. But also, the similarity thing, like just my matrix itself for my stuff. I think I can do that fairly quickly because I have the algorithms. Yeah, I think today's meeting is really the one where we sort of settle down the data structure. And as soon as we have that, probably like after today's meeting, we then actually need to... Well, go back first of all and look at Knight XML to see how far that that which we want is compatible with that with Knight XML offers us. And then just sort of... Everyone, make sure everyone understands the interface. So, I think if today we decide on what day to be want to have now. And later maybe even today we go and look at Knight XML or some of us look at Knight XML in a bit more detail. Just trying to make some sense of that code and see how does the representation work in their system. And then sort of with that knowledge we should be able to then say, okay, that type of Knight XML data, we want to load into it. And this is how everyone can access it. And then we should be able to go... I don't actually look to the Java code for that. No, it makes good. But look at the documentation and like, seeing enough to make me think that if you want to use the Knight XML framework because they have a good event model that synchronizes sort of the data and every display element. So that takes a lot of work away from us. So that would be a reason for staying within their framework and using that general classes. And the one that I haven't looked at at all, which is something we should really do. Who actually like for this whole discussion, I mean, who of us is doing stuff that is happening online and who of us is doing stuff that's happening offline? Like my data is coming. The basic word importance is offline as well. The combined measure might not be if we want to wait what a user has typed into the search. Mine is going to be mostly using the offline, but the actual stuff is doing online. There won't be very processor intensive or memory intensive. So basically apart from the display module, the display itself, we don't have an extremely high degree of interaction between sort of our modules that create the stuff and the interface. The interface is mainly while it's running, just working on data that's just loaded from a file, I guess. I don't know if the search functionality that might be online depends how it's going to work. Yeah, and the search is sort of a strange piece anyway because for the search we're leaving the Nitex model framework. But that's still sort of that's good. That means that at least we don't have the type of situation where somebody has to do like a billion calculations on data online. That would make it a lot more like that would mean that our interface for the data would have to be a lot more careful about how it performs and everything and nobody is modifying that data at online time and all its use. Nobody is making any changes to the actual data online. So that's actually making it a lot easier. That basically means our browser really is a viewer mostly which isn't doing much with the data except for sort of selecting a piece of fit and just playing it. Are we still going to go for dumping it into a database? Are we still going to dump it into a database? Well, some parts relevant for the search, yes. I had to say so because if we are, I reckon we should all read our classes out of the database. It'll be so much easier. Well, if we're going to dump the part of it into a database anyway, we might as well dump all the fields we want into the database, calculate everything from there. So we don't even have to worry that much about the underlying XML representation. We can just query it. But nobody of us is doing much of searching from the data in the online stage. And for altogether, like the display itself, I think we are easier if we, if it's sitting on the XML, then if it's sitting on the SQL stuff, because if it's sitting on the XML, we have the the NightXML framework with all its functionality for synchronizing through all the different levels. Whenever there's a change, whenever some things moving forward and stuff, and we can just look at their code, like how their player moves forward and how that moving forward is represented in different windows and stuff. So I think in the actual browser itself, I don't want to sit on the SQL if we can sit on the XML because sitting on the XML, we have so much help. And for, for like the calculations that we're doing, apart from the search, it seems everyone needs some special representations anyway. So we can do that. We should try and store everything in the next LL format as well. Even our results. Yeah. Yeah. In the NightXML format. So with their timestamps and stuff, so that it's easy to tie together things. So I'm like, what we have to think about is if we go with this multi-level idea, like this idea that sort of, if you start with the whole meeting series as one entity, as one thing that you display as one whole sort of that then the individual chunks are the individual meetings. Whereas, and then you can click on a meeting and then sort of the meeting is the whole thing and the chunks are the individual segments. That means sort of we have multiple levels of representation, which we probably. If we, if we do it this way, like we have to discuss that if we do it this way, then we should probably find some abstraction models so that the interface in the sense like deals with it is if it's the same so that the interface doesn't really have to worry whether it's a meeting in the whole meeting series or a segment within a meeting. And that's probably stuff that we have to sort of like process twice, then like, for example, that like the summary of a meeting within the whole meeting copers or meeting series is meeting series a good word for that. I don't really know how to call it. You know what I mean? Like not not the whole copers, but every meeting that has to do with one topic. And so in the meeting series that a summary for a meeting within the meeting series is sort of compiled offline by a summary module and that is separate from a summary of a segment within a meeting. So don't think we can. We don't even need to do that because we got our information density calculated offline. So all we do is treat the whole lot as one massive document. It's not going to be so big. We can't load in a information density for every author and I mean just summarize based on that. I thought we would just have like one big summary, not only different importance levels. Displayed on it and depending on what our zoom level is, we just display a part of it. So are we doing that at all levels? Are we? I mean we would have one very big thing offline. And from that we would just select what we are displaying and just have different like fine graignness levels sort of. Yes. So for example, you would give a high value to those sequences you want to display in the meeting series summary. So the only thing that. Yeah. So the only thing that would happen basically if I double click let's say from the whole meeting series on a single meeting is that the zoom level changes like the start in the end position changes and the zoom level changes. That was what I saw. I thought we couldn't do that. I was under the impression that we couldn't do that because we couldn't load the data for all that. But I don't know. I mean that. I don't think there's really much point in doing like that when it's just going to feed off in the end. The information density measure basically and that's all calculated offline. So all you're really doing is sorting a list. Is the computation hard part of it? So I'm not sure if I got it. So like the ideas we're calculating our information density all offline first for every utterance in the whole corpus. Right. So all you do is you say if you're looking at series of meetings you say well our whole document comprises of all these stuff together. And then all you have to do is sort them by information density like maybe weighted with the search terms. So I don't think it's too slow to do online. I was just worried about the total memory complexity of it but I really did with it. I just sort of like took that from something that Jonathan once said about not loading everything but maybe I was just wrong about it. But I think the difference might be that we would just want to have the words and that's not so much what he meant was not possibly loading everything was that you load all the annotation stuff, all the soundfights, all the stuff. So what we have is we would have a word like we would have words with some priority levels and that would basically be it already because even the selection would the summaries automatically feed from just how prioritize an individual word or how prioritize an individual utterances or other summaries sort of refined from that and made by a machine to make sense and stuff or are they just sort of taking out the words with the highest priority and then the words of second. And the options level I think the options is with the highest like mean. Are we doing it on the whole thing on the utterance level are we doing it on word level like the information that's the trouble is on the word level is if you want the audio to sync up you've got no way of getting in and extracting just that word. I mean it's impossible. I think we have started in times for words like every single word. Yeah, but it might but it might sound crazy in the player we should really maybe we can do that together at some point today that we checked out how the player works. But there's maybe some merit in all together doing it on utterance level. I'm getting quite lost at the one because what's our difference between the importance measure and the skimming I mean do we do both or is it the same thing. Well, the skimming is going to use the importance. But like that's what when we talk about some areas we talk about this about about skimming and mostly skimming. But also about the display. I mean the displays in the in the text body in the in the latest draft that we had sort of we came up with the idea that it isn't displaying utterance patterns but it's also displaying a summarized version in you know like below the below the graph the part. Maybe. Yeah, isn't that the skimming. Yeah, it's just like there's like audio skimming and there's displayed skimming. Yeah, but it's just same data. Yeah, maybe there's some merit of going all together for utterance level. I'm not even bothered to calculate. I mean if you have to do it internally, then you can do it. But maybe like not even stored the importance levels for individual words and just sort of rank utterances as a whole. The nice thing about that is it also has to be the incidences are more or less. Yeah, so it will make more sense. It might be better skimming and less memory required at the same time. And I mean if you if you know how to do it for individual words, then you can just in the worst case if you can't find anything that just sort of make the mean of the words over the utterance. You know what I mean. No, quite so what did you want to do? You just wanted to sign. What's the smallest chunk at the moment you're thinking of of assigning importance measure to is it a word or is it not friends? I thought about words. So we're thinking of like maybe just storing it on a per utterance level. Because it's less stuff to store probably for Dave in the audio playing and for in the display it's probably better if you have whole utterances than I don't know like what it's like if you just take single words out of utterances that probably doesn't make any sense at all. Where is it just to show important utterances but the utterances are whole it makes more sense. So it doesn't actually make a difference for your algorithm because it just means that if you work on a word level, then we just mean it over the other thing. Yeah, I think we also thought about combining that measure with the matter as I get from hotspots and so on. So that would also be on utterance level. So that's good anyway. Yeah, because that makes it a lot easier than to put it on the internet. I need to be calculated at word level though because otherwise it won't be enough occurrences in terms of like any way. I mean, like how just mean does it internally? I don't know but it's probably you probably have to work on word levels for importance but there should be ways of easily going from a word level to an utterance level. But how about those words which don't carry any meaning at all? Yeah, I think that's something like that. Because if we ever reach over over a whole utterance, other words and there are quite unimportant words in there but quite important words as well, I think we should just disregard them. Maybe we should have like a cut off. So a word only gets a value. It's above a certain threshold. So anything that has less than say 0.5 importance gets assigned to zero. Or we do a pre filtering of sort of the whole thing sort of like. But that like the problem with that is it's easy to do in the text level but that would mean it would still play the air in your audio unless we sort of also store what pieces we cut out for the audio. Yeah, I think before we can like answer that specific question how we deal with that it's probably good for us to look at what the audio player is capable of doing. I think we have to buffer the audio. I don't think it would be very hard. I think it would be like an hour or two's work. So what do you mean by buffering like you think direct just feeding another way file essentially? Yeah, but not stored on the hard disk and the loaded in but loaded in directly from memory. So just like there's bound to be like a media wave object or something like that. And just probably a stream if it exists in Java it would probably some binary stream going in some time. I have no idea. They must have like classes for dealing with files. And it has classes for concatenating files in memory. Okay, so I mean, so that means that there's probably even if you go on a preference level there's still some merit on within utterances cutting out stuff which clearly isn't relevant at all. And that may be also for the audio we'd have to do. So let's say we play the whole phrase but then in addition to that we have some information that says minus that part of something that's okay that we can do. I think I might try and build is basically a class that you just feed it a linked list of different way forms that will just string them all together with maybe one or 10 to the second silence in between each one or something. Yeah, maybe even I mean that sort of depends on how how advanced we get if maybe if you realize that there's massive differences in gain or in something you can probably just make some simple normalization but that really depends on how much time you have and how much it's necessary. Yeah, I don't know anything about audio and I have never seen the player. So if you find that the player except some input from memory if it's easy to do then I guess that's fairly doable. So that means in the general structure we're actually quite lucky. So we have we loaded to memory for the whole series of meetings. Just the utterances and rankings for the utterances. And some information probably that says well I guess that goes for the utterances who speaking. Yeah, because then we can also do the display about who's speaking. We also really want to be able to search by who's speaking as well. Yeah. But I'm still confused because I thought like that's just what Jonathan that that we can't do like load massive document of that size. But yeah, that is all the calculations done offline. The other thing I mean it shouldn't be like 50 megabyte and rum or something it shouldn't be massive. Actually, 50 hundred megabyte is quite big. Just thinking what's this so we do get an error message with the project if you load everything into the project with all the data they load. So we know that doesn't work. So our hope is essentially that we load less into it. Yes. What's this lazy loading thing? Somebody explain lazy loading. It means it loads on demand only loads when it needs a particular type of file. Like when it's being accessed. So that is that only by type of file like if if the same thing is in different files would it then maybe like you know if after this is split over three or 10 or 100 different files. Is there a chance maybe that it doesn't try to load the more into memory at the same time but just. Yeah, I think that's idea. So what is it failing in the first place and it shouldn't ever fail because then it should never. It failed right when you load it right did not make it so that's interesting. So it has different baseline loaded loads for example it loads all the utter answers and so on but it doesn't load the discourse acts and for example not the. What else about that not the summaries on your notes those on demand. Let's check that out. I'll probably ask Jonas about it so alternatively if we realize that we can't do the whole thing in one go we can probably just process some sort of metadata you know what I mean like sort of sort of for the whole serious. Chunks representing the individual meeting system. So I think that represents the whole serious in a structure very similar to the structure which we represent individual meetings. But with data sort of always combined from the whole serious so instead of having a single utterance that we display it probably be like that would be representing a whole. In a meeting. So that using the same data you mean that you basically split up the big thing into different summaries for example that you have a very top level summary and separate five for. So I think if in the sense of like creating a virtual a virtual meeting out of the whole meeting serious sort of easy just like create a new XML. Yeah sort of like offline create a virtual meeting which which basically treats the meeting series is if it was a meeting and treats the individual meetings within the series is if they were segments and treats the individual segments within meetings is if they were. And so we shifted one level up and in that way we could probably use the same algorithm and just like make like one or two ifs that say okay if you are on whole document a whole serious level and that was a double click then don't just go into that segment but load a new file or something like it but in general use the same algorithm that within alternative if we can't actually load the whole thing and. Also like even if we maybe this whole like maybe I'm worrying too much about the whole serious in one thing display because actually I mean probably users wouldn't view that one too often I think it's really that much of a problem because if it's too big what we can do is just well the offline stuff doesn't really matter and all we can do is just process a bit of time like for summarization say we wanted a hundred utterances in summary just look at the meeting take the top 100 utterances and each other meeting. If it scores higher than the ones already in the summary so far just replace them and then you only have to process one meeting at a time. Yeah but I'm still worried like for some of the display if you actually if you wanted to display like for the whole serious the information density levels based on and the and the only granularity you have a individual utterances that means you have to go through everything. So we're using lot rents in a series of 70 hours of meeting. So maybe we should build and store a main measure for the segments and meetings as well. Yeah and if you make that structurally very similar to the like one level down like the way how we store in which lot rents and stuff then maybe we can more or less use the same code and just make a few ifs and stuff. But still so in general we having we having utterances and they have a score. And that's as much as we really need and they also have a time a time information of course speaker and a speaker information. Yeah topics segmenting. So in information which topic they are in yeah and and probably separate to that information about the different topics like that. So so the skimming can work on that because the skimming just sort of sorts the utterances and puts as many in as it needs. Preserve the order when it's displayed. Yeah yeah it'll play them in some order in which they were said because otherwise it's going to be more entertaining. But that that's enough data for the skimming yeah and the searching so what the searching does is the searching leaves the whole framework goes to the SQL database and gets like basically the end gets just a time mark for where that is like that utterance that we're concerned with. And then we have to find I'm sure there's some way in in nightx mail to just say set position to that time mark and then it shifts the whole frame in the alerts everything element of the display and the display updates. If you don't want to do the old tree display as well for multiple results. Yeah yeah that we can yeah but so so if if some so yeah so if in that tree display somebody clicks on something. You just need to find the time stamp yeah and then you sort of feed the time stamp to and the nightx mail central manager and that central manager alerts everything that's there like alerts the skim like the audio display alerts the text display alerts the visual display and says we have a new time frame and then they all sort of do the update routines with respect to the current level of zoom so how much they display and starting position at where they. Or maybe the mid position of it I don't know like if you start with the thing was found or if the thing was found is in the middle of the part that we display that I don't know what that we can decide about but in general sort of. It's the same thing if like whether you play and it moves forward or whether you jump to a position through search it's essentially for all the window handling it's the same event. Yeah it's only that the event gets triggered by the search routine which sort of pushed it into nightx met and says please go there now. So we should basically make our own XML document in memory that everyone's module changes that rather than the underlying data and then have that excellent night XML document tied to the interface. Why do we have to do in memory that stuff you can make it a file if you want. I mean like the information is coming from offline. So we probably we don't even have to change the utterance document right because the whole way like the whole beauty of the night XML is that it ties together lots of different files so we can just create an additional XML file which for every utterance like the utterance ideas have presumed some references. So we just we tie just very short XML file which it's the only information it has it has whatever a number for. For the wait for the information density and we just tie that to the existing utterances and tie them to the existing speaker changes. But there's no idea for an utterance I think it's just for individual words so how do we do that then. And I know it's for utterances as well. I think it's just for one word so we have to have a segment for each utterance. Yeah. Thanks. Well, otherwise we'd probably have to go over it like add some integer that we just increment from pop to bottom sort of to every utterance as an as an ID. Or try to understand how not XM LIDs work and maybe that's some special rules we have to follow when we use these ideas. And press your it's already there. Press your that's already there. The aren't so numbered. The girls say the utterances themselves are not numbered at the moment. I'm not quite sure I've only seen that the individual words have got an ID. So I guess that would be solvable if not. Yeah, you always could have a look at the timestamps and then take the ones that belong together to form an utterance. I think we'll just take the segments that are already there. Yeah, that's the segment. And you know the XM L segments. Is that a board market and actually. Yeah, I think so. Let's just like to make a list of all this stuff over here. Paper. So what so this stuff we have we have utterances and speakers and wait for utterances for every utterance. Sort of like the utterance has a speaker and the weight which is coming from outside. Which is tied to it. And there is segments. So the utterances are utterances. Yeah, that's a topic. Topic segments. I meant like they are super units. So the utterances are tied to topic segments. And if the timestamps are on a word level, then we somehow have to extract timestamps for utterances where they start. So there are timestamps for the segments.\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FGtdk8gtzCIp"
      },
      "outputs": [],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
        "                                    use_auth_token=\"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\")\n",
        "\n",
        "diarization = run_with_gpu(pipeline, DUMMY_LONG)\n",
        "\n",
        "with open(\"audio.rttm\", \"w\") as rttm:\n",
        "    diarization.write_rttm(rttm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1vyNc7CSu6r",
        "outputId": "4ab8655c-014d-4c8a-8613-6df0ae2559f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00 7.00 SPEAKER_05  Okay.\n",
            "7.00 17.00 SPEAKER_04  Okay. Does anyone want to see Steve's feedback from the specification?\n",
            "17.00 21.00 SPEAKER_00  Is there much more in it than he said yesterday?\n",
            "21.00 28.00 SPEAKER_04  Not really. Just what he's talking about, duplication of effort.\n",
            "28.00 33.00 SPEAKER_04  Like duplication of effort and stuff.\n",
            "33.00 42.00 SPEAKER_04  Yeah, seeing that we should maybe think about having a prototype or a weak sex, which is necessary.\n",
            "42.00 46.00 SPEAKER_04  So, this is probably prioritized or...\n",
            "46.00 56.00 SPEAKER_00  Yeah, I'd say if for the prototype we feature like wherever possible, chunking the stuff that we have pre-anotated and stuff.\n",
            "56.00 61.00 SPEAKER_00  And for the stuff that we don't have pre-anotated, right, like a stupid baseline, then we should probably be able to...\n",
            "61.00 69.00 SPEAKER_00  Basically, that means we focus on the interface first, so that we take the ready-made parts and just see how we get them work together in the interface the way we want them.\n",
            "69.00 78.00 SPEAKER_00  And then we have a working prototype and then we can go back and replace pieces either by our own components or by more sophisticated components of our own.\n",
            "78.00 80.00 SPEAKER_00  So, it's probably feasible.\n",
            "80.00 86.00 SPEAKER_00  I think it's a way this weekend. So, that...\n",
            "86.00 93.00 SPEAKER_03  If we just want to have some data for the user-fist could even be random data.\n",
            "93.00 100.00 SPEAKER_00  Oh, yeah, no. But also, the similarity thing, like just my matrix itself for my stuff.\n",
            "100.00 104.00 SPEAKER_00  I think I can do that fairly quickly because I have the algorithms.\n",
            "104.00 110.00 SPEAKER_00  Yeah, I think today's meeting is really the one where we sort of settle down the data structure.\n",
            "110.00 114.00 SPEAKER_00  And as soon as we have that, probably like after today's meeting, we then actually need to...\n",
            "114.00 122.00 SPEAKER_00  Well, go back first of all and look at Knight XML to see how far that that which we want is compatible with that with Knight XML offers us.\n",
            "122.00 124.00 SPEAKER_00  And then just sort of...\n",
            "124.00 129.00 SPEAKER_00  Everyone, make sure everyone understands the interface.\n",
            "129.00 135.00 SPEAKER_00  So, I think if today we decide on what day to be want to have now.\n",
            "135.00 140.00 SPEAKER_00  And later maybe even today we go and look at Knight XML or some of us look at Knight XML in a bit more detail.\n",
            "140.00 145.00 SPEAKER_00  Just trying to make some sense of that code and see how does the representation work in their system.\n",
            "145.00 151.00 SPEAKER_00  And then sort of with that knowledge we should be able to then say, okay, that type of Knight XML data, we want to load into it.\n",
            "151.00 154.00 SPEAKER_00  And this is how everyone can access it.\n",
            "154.00 157.00 SPEAKER_00  And then we should be able to go...\n",
            "157.00 160.00 SPEAKER_04  I don't actually look to the Java code for that.\n",
            "160.00 162.00 SPEAKER_04  No, it makes good.\n",
            "162.00 175.00 SPEAKER_00  But look at the documentation and like, seeing enough to make me think that if you want to use the Knight XML framework because they have a good event model that synchronizes sort of the data and every display element.\n",
            "175.00 178.00 SPEAKER_00  So that takes a lot of work away from us.\n",
            "178.00 182.00 SPEAKER_00  So that would be a reason for staying within their framework and using that general classes.\n",
            "182.00 187.00 SPEAKER_00  And the one that I haven't looked at at all, which is something we should really do.\n",
            "187.00 197.00 SPEAKER_00  Who actually like for this whole discussion, I mean, who of us is doing stuff that is happening online and who of us is doing stuff that's happening offline?\n",
            "197.00 199.00 SPEAKER_00  Like my data is coming.\n",
            "199.00 206.00 None  The basic word importance is offline as well.\n",
            "206.00 220.00 SPEAKER_02  The combined measure might not be if we want to wait what a user has typed into the search.\n",
            "220.00 232.00 SPEAKER_01  Mine is going to be mostly using the offline, but the actual stuff is doing will be online. There won't be very processor intensive or memory intensive.\n",
            "232.00 245.00 SPEAKER_00  So basically apart from the display module, the display itself, we don't have an extremely high degree of interaction between sort of our modules that create the stuff and the interface.\n",
            "245.00 252.00 SPEAKER_00  The interface is mainly while it's running, just working on data that's just loaded from a file, I guess.\n",
            "252.00 258.00 SPEAKER_04  I don't know if the search functionality that might be online depends how it's going to work.\n",
            "258.00 268.00 SPEAKER_00  Yeah, and yeah, the searches, I guess the search is sort of a strange piece anyway, because for the search we're leaving the Nitex Metal Framework.\n",
            "268.00 278.00 SPEAKER_00  But that's still sort of that's good, that means that at least we don't have the type of situation where somebody has to do like a billion calculations on data online.\n",
            "278.00 285.00 SPEAKER_00  That would make it a lot more, like that would mean that our interface for the data would have to be a lot more careful about how it performs and everything.\n",
            "285.00 290.00 SPEAKER_00  And nobody is modifying that data at online time and all its team.\n",
            "290.00 295.00 SPEAKER_00  Nobody is making any changes to the actual data online.\n",
            "295.00 310.00 SPEAKER_00  So that's actually making it a lot easier, that basically means our browser really is a viewer mostly, which isn't doing much with the data except for selecting a piece of fit and just playing it.\n",
            "310.00 313.00 SPEAKER_01  Are we still going to go for dumping it into a database?\n",
            "313.00 316.00 SPEAKER_01  Are we still going to dump it into a database?\n",
            "316.00 326.00 SPEAKER_01  Well, some parts are relevant for the search, yes, I'd say so, because if we are, I reckon we should all read our classes out of the database, it'll be so much easier.\n",
            "326.00 336.00 SPEAKER_01  Well, if we're going to dump the part of it into a database anyway, we might as well dump all the fields we want into the database, calculate everything from there.\n",
            "336.00 343.00 SPEAKER_01  So we don't even have to worry that much about the underlying XML representation, we can just query it.\n",
            "343.00 348.00 SPEAKER_00  But nobody of us is doing much of searching from the data in the online stage.\n",
            "348.00 370.00 SPEAKER_00  And for altogether, like the display itself, I think we are easier if we, if it's sitting on the XML, then if it's sitting on the SQL stuff, because if it's sitting on the XML, we have the the NightXML framework with all its functionality for synchronizing through all the different levels, whenever there's a change, whenever some things moving forward and stuff.\n",
            "370.00 376.00 SPEAKER_00  And just look at their code, like how their player moves forward and how that moving forward is represented in different windows and stuff.\n",
            "376.00 384.00 SPEAKER_00  So I think in the actual browser itself, I don't want to sit on the SQL if we can sit on the XML, because sitting on the XML, we have so much help.\n",
            "384.00 393.00 SPEAKER_00  And for, for like the calculations that we're doing, apart from the search, it seems everyone needs some special representations anyway.\n",
            "393.00 400.00 SPEAKER_01  What if we can do that? We should try and store everything in the next LL format as well.\n",
            "400.00 411.00 SPEAKER_00  Even our results. Yeah. Yeah. In the NightXML format. So with their timestamps and stuff, so that it's easy to tie together things.\n",
            "411.00 430.00 SPEAKER_00  What I'm like, what we have to think about is if we go with this multi-level idea, like this idea that sort of, if you start with the whole meeting series as one entity, as one thing that you display, as one whole sort of, that then the individual chunks are the individual meetings.\n",
            "430.00 459.00 SPEAKER_00  Whereas, and then you can click on a meeting and then sort of the meeting is the whole thing and the chunks are the individual segments. That means sort of we have multiple levels of representation, which we probably, if we, if we do it this way, like we have to discuss that if we do it this way, then we should probably find some abstraction model so that the interface in the sense like deals with it as if it's the same so that the interface doesn't really have to worry whether it's a meeting in the whole meeting series or a segment within a meeting series.\n",
            "459.00 474.00 SPEAKER_00  And that's probably stuff that we have to sort of like process twice, then like, for example, that like the summary of a meeting within the whole meeting copers or meeting series, because meeting series is a good word for that. I don't really know how to call it.\n",
            "474.00 480.00 SPEAKER_00  You know what I mean? Like not not the whole copers, but every meeting that has to do with one topic.\n",
            "480.00 493.00 SPEAKER_00  And so in the meeting series, that a summary for a meeting within the meeting series is sort of compiled offline by a summary module. And that is separate from a summary of a segment within a meeting.\n",
            "493.00 511.00 SPEAKER_01  So don't think we can. Well, we don't even need to do that because we got our information density calculated offline. So all we do is treat the whole lot as one massive document. I mean, no, it's not going to be so big. We can't load in a information density for every author and I mean just summarize based on that.\n",
            "511.00 529.00 SPEAKER_03  I thought we would just have like one big summary, not only different importance levels displayed on it and depending on what our zoom level is, we just display a part of it.\n",
            "529.00 542.00 SPEAKER_03  So are we doing that at all levels are we? I mean, we would have one very big thing offline. And from that, we would just select what we are displaying and just have different like fine greatness levels sort of.\n",
            "542.00 552.00 SPEAKER_03  Yes. So for example, you would give a high value to those sequences you want to display in the meeting series summary.\n",
            "552.00 566.00 SPEAKER_00  So the only thing that could happen, basically, if I double click, let's say from the whole meeting series on a single meeting is that the zoom level changes like the start in the end position changes and the zoom level changes.\n",
            "566.00 576.00 SPEAKER_00  That was what I thought we couldn't do that. I was under the impression that we couldn't do that because we couldn't load the data for all that.\n",
            "576.00 587.00 SPEAKER_01  But I don't know. I mean, there's really much point in doing like that when it's just going to feed off in the end, the information density measure basically.\n",
            "587.00 596.00 SPEAKER_01  And that's all calculated offline. So all you're really doing is sorting a list. It's the computation hard part of it.\n",
            "596.00 606.00 SPEAKER_01  So I'm not sure if I got it. Well, like the ideas we're calculating our information density all offline first for every utterance in the whole corpus.\n",
            "606.00 615.00 SPEAKER_01  Right. So all you do is you say, if you're looking at a series of meetings, you say, well, our whole document comprises of all these stuff together.\n",
            "615.00 622.00 SPEAKER_01  And then all you have to do is sort them by information density, like maybe weighted with the search terms.\n",
            "622.00 627.00 SPEAKER_01  So I think it's too slow to do online.\n",
            "627.00 639.00 SPEAKER_00  I was just worried about the total memory complexity of it, but I really did. I just sort of like took that from something that Jonathan once said about not loading everything, but maybe I was just wrong about it.\n",
            "639.00 657.00 SPEAKER_03  I think the difference might be that we would just want to have the words and that's not so much what he meant was not possibly loading everything was that you load all the annotation stuff, all the soundfights.\n",
            "657.00 686.00 SPEAKER_00  So what we have is we would have a word like we would have words with some priority levels and that would basically be it already because even the selection would the summaries automatically feed from just how prioritize an individual word or how prioritize an individual utterances or other summaries sort of refined from that and made by a machine to make sense and stuff or are they just sort of taking out the words of the highest priority and then the words of second highest priority.\n",
            "686.00 692.00 SPEAKER_01  On the utterances level, I think the utterances with the highest like mean.\n",
            "692.00 702.00 SPEAKER_01  Are we doing it on the whole thing on the utterance level, are we doing it on word level like the information that's the trouble is on the word level is if you want the audio to sync up, you've got no way of getting in and extracting just that word.\n",
            "702.00 704.00 SPEAKER_01  I mean, it's impossible.\n",
            "704.00 707.00 SPEAKER_00  I think we have started in times for words actually every single word.\n",
            "707.00 716.00 SPEAKER_00  Yeah, but it might but it might sound crazy in the player. We should really maybe we can do that together at some point today that we checked out how the player works.\n",
            "716.00 719.00 SPEAKER_00  But there's maybe some merit in all together doing it on utterance level.\n",
            "719.00 731.00 SPEAKER_03  I'm getting quite lost at the one because what's our difference between the importance measure and the skimming.\n",
            "731.00 738.00 SPEAKER_01  Do we do both or is it the same thing while the skimming is going to use the importance.\n",
            "738.00 747.00 SPEAKER_03  But when we talk about summaries, we talk about this about skimming and not the one mostly skimming.\n",
            "747.00 760.00 SPEAKER_00  But also about the displays, I mean the displays in the in the text body in the in the latest draft that we had sort of we came up with the idea that it isn't displaying utterance patterns, but it's also displaying a summarized version.\n",
            "760.00 763.00 SPEAKER_00  You know, like below the graph the part.\n",
            "763.00 766.00 SPEAKER_00  Yeah, maybe.\n",
            "766.00 769.00 SPEAKER_03  Yeah, isn't that the skimming?\n",
            "769.00 774.00 SPEAKER_00  Yeah, it's just like there's like audio skimming and there's displayed skimming.\n",
            "774.00 777.00 SPEAKER_03  Yeah, but it's just same data.\n",
            "777.00 784.00 SPEAKER_00  Yeah, maybe there's some merit of going all together for utterance level, not even bothered to calculate.\n",
            "784.00 793.00 SPEAKER_00  If you have to do it internally, then you can do it, but maybe like not even store the importance levels for individual words and just sort of rank utterances as a whole.\n",
            "793.00 798.00 SPEAKER_01  The nice thing about that is that it will also actually be in sentences or more or less.\n",
            "798.00 800.00 SPEAKER_01  Yeah, so it will make more sense.\n",
            "800.00 804.00 SPEAKER_00  It might be better skimming and less memory required at the same time.\n",
            "804.00 811.00 SPEAKER_00  If you know how to do it for individual words, then you can just in the worst case, if you can't find anything that just sort of make the mean of the words over the utterance.\n",
            "811.00 817.00 SPEAKER_02  No, quite. So what did you want to do? You just wanted to sign.\n",
            "817.00 823.00 SPEAKER_00  What's the smallest chunk at the moment you're thinking of of assigning importance measure to? Is it a word or is it not friends?\n",
            "823.00 825.00 SPEAKER_02  I thought about words.\n",
            "825.00 832.00 SPEAKER_00  So we're thinking of like maybe just storing it on a per utterance level.\n",
            "832.00 844.00 SPEAKER_00  Because it's less stuff to store probably for Dave in the audio playing and for in the display, it's probably better if you have whole utterances than I don't know what it's like if you just take single words out of utterances.\n",
            "844.00 849.00 SPEAKER_00  It probably doesn't make any sense at all, whereas if you just show important utterances, but the utterances are whole, it makes more sense.\n",
            "849.00 856.00 SPEAKER_00  So it doesn't actually make a difference for your algorithm, because it just means that if you work on a word level, then we just mean it over the other thing.\n",
            "856.00 865.00 SPEAKER_03  Yeah, I think we also thought about combining that measure with the matter as I get from hotspots and so on.\n",
            "865.00 869.00 SPEAKER_03  So that would also be on utterance level.\n",
            "869.00 871.00 SPEAKER_03  So that's good anyway, I think.\n",
            "871.00 873.00 SPEAKER_00  Because that makes it a lot easier than to put it on the utterance level.\n",
            "873.00 880.00 SPEAKER_01  I need to be calculated at word level, though, because otherwise it won't be enough occurrences in terms of like any...\n",
            "880.00 890.00 SPEAKER_00  I mean, like how just mean does it internally, I don't know, but it's probably you probably have to work on word levels for importance, but there should be ways of easily going from a word level to an utterance level.\n",
            "890.00 892.00 SPEAKER_01  I reckon you can just...\n",
            "892.00 897.00 SPEAKER_02  But how about those words which don't carry any meaning at all?\n",
            "897.00 901.00 SPEAKER_02  And ears and something like that?\n",
            "901.00 917.00 SPEAKER_02  Because if we ever reach over a whole utterance, other words, and there are quite unimportant words in there, but quite important words as well, I think we should just disregard them.\n",
            "917.00 925.00 SPEAKER_01  Maybe we should have like a cut off, so a word only gets a value, it's above a certain threshold.\n",
            "925.00 931.00 SPEAKER_01  So anything that has less than say 0.5 importance gets assigned to zero.\n",
            "931.00 934.00 SPEAKER_00  Or we do a pre-filtering of sort of the whole thing, sort of like...\n",
            "934.00 948.00 SPEAKER_00  But that, like the problem with that is it's easy to do in the text level, but that would mean it would still play the air in your audio, unless we sort of also store what pieces we cut out for the audio.\n",
            "948.00 958.00 SPEAKER_00  Yeah, I think before we can like answer that specific question, how we deal with that, it's probably good for us to look at what the audio player is capable of doing.\n",
            "958.00 960.00 SPEAKER_01  I think we have to buffer the audio.\n",
            "960.00 964.00 SPEAKER_01  But I think it'll be very hard, I think it'll be like an hour or two's work.\n",
            "964.00 967.00 SPEAKER_00  So what do you mean by buffering, like you think direct just feeding...\n",
            "967.00 971.00 SPEAKER_01  And another way file essentially.\n",
            "971.00 977.00 SPEAKER_00  Yeah, but not stored on the hard disk and the loaded in, but loaded in directly from memory.\n",
            "977.00 982.00 SPEAKER_01  So just like there's bound to be like a media wave object or something like that.\n",
            "982.00 983.00 SPEAKER_01  And just put it in the middle.\n",
            "983.00 988.00 SPEAKER_00  It's probably a stream if it exists in Java, it would probably be some binary stream going in sometime.\n",
            "988.00 989.00 SPEAKER_00  Oh no, there.\n",
            "989.00 997.00 SPEAKER_01  They must have like classes for dealing with files, and if it has classes for concatenating files, you can do it in memory.\n",
            "997.00 1007.00 SPEAKER_00  Okay, so that means that there's probably, even if you go on a peraderence level, there's still some merit within utterances cutting out stuff which clearly isn't relevant at all.\n",
            "1007.00 1008.00 SPEAKER_00  And that may be also for the audio we'd have to do.\n",
            "1008.00 1015.00 SPEAKER_00  So let's say we play the whole phrase, but then in addition to that, we have some information that says minus that part of something.\n",
            "1015.00 1016.00 SPEAKER_00  That's okay that we can do.\n",
            "1016.00 1033.00 SPEAKER_01  Well, what I think I might try and build is basically a class that you just feed it a linked list of different waveforms that will just string them all together with maybe one or ten to the second silence in between each one or something.\n",
            "1033.00 1037.00 SPEAKER_00  Yeah, maybe even, I mean, that sort of depends on how advanced we get.\n",
            "1037.00 1046.00 SPEAKER_00  Maybe if you realize that there's massive differences in gain or in something, you can probably just make some simple normalization, but that really depends on how much time we have and how much it's necessary.\n",
            "1046.00 1050.00 SPEAKER_00  Yeah, if I don't know anything about audio, and I have never seen the player.\n",
            "1050.00 1056.00 SPEAKER_00  So if you find that the player accepts some input from memory, and if it's easy to do, then I guess that's fairly doable.\n",
            "1056.00 1059.00 SPEAKER_00  So that means in the general structure, we're actually quite lucky.\n",
            "1059.00 1067.00 SPEAKER_00  We have, we loaded to memory for the whole series of meetings, just the utterances and rankings for the utterances.\n",
            "1067.00 1073.00 SPEAKER_00  And some information probably that says, well, I guess that goes for the utterances who's speaking.\n",
            "1073.00 1076.00 SPEAKER_00  Oh, yeah, because then we can also do the display about who's speaking.\n",
            "1076.00 1080.00 SPEAKER_01  We also really want the other search by who you're speaking as well.\n",
            "1080.00 1083.00 SPEAKER_05  Yeah.\n",
            "1083.00 1092.00 SPEAKER_00  But I'm still confused because I thought like that's just what Jonathan that we can't do, like load massive document of that size.\n",
            "1092.00 1100.00 SPEAKER_01  But yeah, that is all the calculations done offline.\n",
            "1100.00 1114.00 SPEAKER_00  The other thing I mean, it shouldn't be like 50 megabyte and rum or something, it shouldn't be massive.\n",
            "1114.00 1116.00 SPEAKER_00  Actually, 50 hundred megabyte is quite big rum.\n",
            "1116.00 1118.00 SPEAKER_00  Just thinking what's this.\n",
            "1118.00 1123.00 SPEAKER_00  So we do get an error message with the project.\n",
            "1123.00 1126.00 SPEAKER_00  If we load everything into the project with all the data they load.\n",
            "1126.00 1131.00 SPEAKER_00  So we know that doesn't work. So our hope is essentially that we load less into it.\n",
            "1131.00 1132.00 SPEAKER_00  Yes.\n",
            "1132.00 1133.00 SPEAKER_00  What's this lazy loading thing?\n",
            "1133.00 1135.00 SPEAKER_00  Somebody explain lazy loading to.\n",
            "1135.00 1137.00 SPEAKER_04  It means it loads on demand.\n",
            "1137.00 1141.00 SPEAKER_04  Only loads when it needs a particular type of file.\n",
            "1141.00 1143.00 SPEAKER_04  Like when it's being accessed.\n",
            "1143.00 1150.00 SPEAKER_00  So that is that only by type of file, like if if the same thing is in different files, would it then maybe.\n",
            "1150.00 1161.00 SPEAKER_00  Like, you know, if uptrends are split over three or ten or 100 different files, is then a chance maybe that it doesn't try to load the mollent memory at the same time, but just.\n",
            "1161.00 1163.00 SPEAKER_04  Yeah, I think that's idea.\n",
            "1163.00 1165.00 SPEAKER_00  So why does it fail then in the first place?\n",
            "1165.00 1167.00 SPEAKER_00  And it shouldn't ever fail because then it should never.\n",
            "1167.00 1170.00 SPEAKER_04  But if you didn't search over the whole car, as you thought.\n",
            "1170.00 1175.00 SPEAKER_00  Yeah, but yeah, but it failed right when you load it, right?\n",
            "1175.00 1181.00 SPEAKER_00  The notic smell kids that's interesting.\n",
            "1181.00 1184.00 SPEAKER_03  So just a different baseline loaded loads.\n",
            "1184.00 1193.00 SPEAKER_03  For example, it loads all the utterances and so on, but it doesn't load the discourse acts and.\n",
            "1193.00 1195.00 SPEAKER_03  For example, not the.\n",
            "1195.00 1197.00 SPEAKER_03  What else about there?\n",
            "1197.00 1200.00 SPEAKER_03  Not the summaries.\n",
            "1200.00 1204.00 SPEAKER_03  Only loads those on demand.\n",
            "1204.00 1207.00 SPEAKER_00  Let's check that out.\n",
            "1207.00 1209.00 SPEAKER_00  I'll probably ask Jonas about it.\n",
            "1209.00 1217.00 SPEAKER_00  So alternatively, if we realize that we can't do the whole thing in one go, we can probably just process some sort of metadata, you know what I mean?\n",
            "1217.00 1222.00 SPEAKER_00  Like sort of sort of for the whole serious.\n",
            "1222.00 1228.00 SPEAKER_00  Chunks representing the individual meeting system.\n",
            "1228.00 1238.00 SPEAKER_00  So I think that represents the whole series in a structure very similar to the structure which we represent individual meetings.\n",
            "1238.00 1243.00 SPEAKER_00  But with data sort of always combined from the whole series.\n",
            "1243.00 1251.00 SPEAKER_00  So instead of having a single utterance that we display, it probably be like that would be representing a whole.\n",
            "1251.00 1254.00 SPEAKER_00  A segment in a meeting.\n",
            "1254.00 1266.00 SPEAKER_03  Using the same data, you mean that you basically split up the big thing into different summaries for example, that you have a very.\n",
            "1266.00 1272.00 SPEAKER_03  Top level summary and separate five.\n",
            "1272.00 1278.00 SPEAKER_00  I think in the sense of like creating a virtual a virtual meeting out of the whole meeting series sort of.\n",
            "1278.00 1300.00 SPEAKER_00  That's easy just like create a new XML. Yeah sort of like offline create a virtual meeting which which basically treats the meeting series as if it was a meeting and treats the individual meetings within the series as if they were segments and treats the individual segments within meetings as if they were.\n",
            "1300.00 1318.00 SPEAKER_00  And that way we could probably use the same algorithm and just like make like one or two ifs that say okay if you are on whole document a whole serious level and that was a double click then don't just go into that segment but load a new file or something like it.\n",
            "1318.00 1325.00 SPEAKER_00  But in general use the same algorithm that would be an alternative if we can't actually load the whole thing and.\n",
            "1325.00 1336.00 SPEAKER_00  Also like even if we maybe this whole like maybe I'm worrying too much about the whole serious in one thing display because actually I mean probably users wouldn't view that one too often.\n",
            "1336.00 1351.00 SPEAKER_01  I think it's really that much of a problem because if it's too big what we can do is just well all the offline stuff doesn't really matter and all we can do is just process a bit of time like for summarization say we wanted a hundred utterances in summary.\n",
            "1351.00 1365.00 SPEAKER_01  Just look at the meeting take the top 100 utterances in each other meeting if it scores higher than the ones already in the summary so far just replace them and then you only have to process one meeting at a time.\n",
            "1365.00 1380.00 SPEAKER_00  Yeah but I'm still worried like for some of the display if you actually if you wanted to display like for the whole series the information density levels based on and the and the only granularity you have a individual utterances that means you have to go through everything.\n",
            "1380.00 1384.00 SPEAKER_00  And we think lot rents in a series of 70 hours of meeting.\n",
            "1384.00 1391.00 SPEAKER_01  So maybe we should build and store a mean measure for the segments and meetings as well.\n",
            "1391.00 1405.00 SPEAKER_00  Yeah and if you make that structurally very similar to the like one level down like the way how we store in which lot rents and stuff that maybe we can more or use the same code and just make a few ifs and stuff.\n",
            "1405.00 1415.00 SPEAKER_00  So but still so in general we having we having utterances and they have a score.\n",
            "1415.00 1427.00 SPEAKER_00  And that's as much as we really need and they also have a time a time information of course speaker and a speaker information.\n",
            "1427.00 1437.00 SPEAKER_00  So the topic segmenting.\n",
            "1437.00 1442.00 SPEAKER_00  Yeah so in information which topic they are in yeah and and probably separate to that information about the different topics like that.\n",
            "1442.00 1452.00 SPEAKER_00  So the skimming can work on that because the skimming just sort of sorts the utterances and puts as many in as it needs.\n",
            "1452.00 1463.00 SPEAKER_00  Yeah it will play them in some order in which they were said because otherwise it's going to be more entertaining.\n",
            "1463.00 1467.00 SPEAKER_00  But that that's enough data for the skimming.\n",
            "1467.00 1480.00 SPEAKER_00  And the searching so what the searching does is the searching leaves the whole framework goes to the SQL database and gets like basically the end gets just a time mark for where that is like that utterance that we're concerned with.\n",
            "1480.00 1491.00 SPEAKER_00  And then we have to find I'm sure there's some way in in nightx mail to just say set position to that time mark and then it shifts the whole frame in the alerts everything element of the display and the display updates.\n",
            "1491.00 1495.00 SPEAKER_04  If you don't want to do the old tree display as well for multiple results.\n",
            "1495.00 1503.00 SPEAKER_00  Yeah yeah that we can yeah but so so if if some so yeah so if in that tree display somebody clicks on something.\n",
            "1503.00 1527.00 SPEAKER_00  You just need to find the time stamp. Yeah and then you sort of feed the time stamp to in the nightx mail central manager and that central manager alerts everything that's there like alerts the skim like the audio display alerts the text display alerts the visual display and says we have a new time frame and then they all sort of do the update routines with respect to the current level of zoom so how much they display and starting position at where they.\n",
            "1527.00 1540.00 SPEAKER_00  Or maybe the mid position of it I don't know like if you start with the thing was found or if the thing was found is in the middle of the part that we display that I don't know what that we can decide about but in general sort of.\n",
            "1540.00 1549.00 SPEAKER_00  It's the same thing if like whether you play and it moves forward or whether you jump to a position through search it's essentially for all the window handling it's the same event.\n",
            "1549.00 1557.00 SPEAKER_00  Yeah it's only that the event gets triggered by the search routine which sort of pushed it into nightx mail and says please go there now.\n",
            "1557.00 1577.00 SPEAKER_01  So we should basically make our own XML document in memory that everyone's module changes that rather than the underlying data and then have that excellent night XML document tied to the interface.\n",
            "1577.00 1582.00 SPEAKER_00  Why do we have to do in memory that stuff you can make it a file if you want.\n",
            "1582.00 1588.00 SPEAKER_00  I mean like the information is coming from offline.\n",
            "1588.00 1603.00 SPEAKER_00  So we probably we don't even have to change the utterance document right because the whole way like the whole beauty of the night XML is that it ties together lots of different files so we can just create an additional XML file which for every utterance like the utterance ideas have presumed some references.\n",
            "1603.00 1612.00 SPEAKER_00  So we just we tie just very short XML file which it's the only information it has it has whatever a number for.\n",
            "1612.00 1620.00 SPEAKER_00  For the wait for the information density and we just tie that to the existing utterances and tie them to the existing speaker changes.\n",
            "1620.00 1628.00 SPEAKER_02  But there's no idea for an utterance I think it's just for individual words so how do we do that then.\n",
            "1628.00 1633.00 SPEAKER_02  And I know it's for utterances as well.\n",
            "1633.00 1643.00 SPEAKER_02  I think it's just for one word so we have to have a segment for each utterance.\n",
            "1643.00 1644.00 SPEAKER_04  Yeah.\n",
            "1644.00 1646.00 SPEAKER_04  Thanks.\n",
            "1646.00 1658.00 SPEAKER_00  Well, otherwise we'd probably have to go over it like add some integer that we just increment from pop to bottom sort of to every utterance as an as an ID.\n",
            "1658.00 1664.00 SPEAKER_00  Or try to understand how not XM LIDs work and maybe that's some special rules we have to follow when we use these ideas.\n",
            "1664.00 1667.00 SPEAKER_04  And press your it's already there.\n",
            "1667.00 1669.00 SPEAKER_04  Press your that's already there.\n",
            "1669.00 1671.00 SPEAKER_00  The aren't so numbered.\n",
            "1671.00 1676.00 SPEAKER_00  The girls say the utterances themselves are not numbered at the moment.\n",
            "1676.00 1681.00 SPEAKER_02  I'm not quite sure I've only seen that the individual words have got an ID.\n",
            "1681.00 1684.00 SPEAKER_00  So I guess that would be solvable if not.\n",
            "1684.00 1694.00 SPEAKER_02  Yeah, you always could have a look at the timestamps and then take the ones that belong together to form an utterance.\n",
            "1694.00 1701.00 SPEAKER_03  I think we'll just take the segments that are already there.\n",
            "1701.00 1704.00 SPEAKER_03  Yeah, that's the segment.\n",
            "1704.00 1707.00 SPEAKER_02  And you know the XM L segments.\n",
            "1707.00 1712.00 SPEAKER_00  Is that a board market and actually.\n",
            "1712.00 1713.00 SPEAKER_00  Yeah, I think so.\n",
            "1713.00 1717.00 SPEAKER_00  Let's just like to make a list of all this stuff over here.\n",
            "1717.00 1724.00 SPEAKER_00  Paper.\n",
            "1724.00 1732.00 SPEAKER_00  So what so the stuff we have we have utterances and speakers and wait for utterances for every utterance.\n",
            "1732.00 1737.00 SPEAKER_00  Sort of like the utterance has a speaker and the weight which is coming from outside.\n",
            "1737.00 1740.00 SPEAKER_00  Which is tied to it.\n",
            "1740.00 1743.00 SPEAKER_00  And there is segments.\n",
            "1743.00 1748.00 SPEAKER_01  So the utterances are utterances.\n",
            "1748.00 1750.00 SPEAKER_00  Yeah, that's a topic.\n",
            "1750.00 1751.00 SPEAKER_00  Topic segments.\n",
            "1751.00 1753.00 SPEAKER_00  I meant like they are super units.\n",
            "1753.00 1761.00 SPEAKER_00  So the utterances are tied to topic segments.\n",
            "1761.00 1769.00 SPEAKER_00  And if the timestamps are on a word level, then we somehow have to extract timestamps for utterances where they start.\n",
            "1769.00 1776.00 SPEAKER_03  So there are timestamps for the segments.\n"
          ]
        }
      ],
      "source": [
        "final_result = diarize_text(result, diarization)\n",
        "\n",
        "for seg, spk, sent in final_result:\n",
        "    line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sent}'\n",
        "    print(line)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
