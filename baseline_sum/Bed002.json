{"filename": "./data/Bed002.txt", "transcript": "0:00:00\tNone\n Yes.\n\n0:00:01\tSPEAKER_05\n Okay, we're on.\n\n0:00:10\tSPEAKER_05\n So just make sure that the wireless mic is on.\n\n0:00:12\tSPEAKER_05\n If you're wearing wireless.\n\n0:00:13\tSPEAKER_05\n Check one.\n\n0:00:15\tSPEAKER_05\n And you should be able to see which one.\n\n0:00:20\tSPEAKER_05\n Which one you're on by watching the little bars change.\n\n0:00:24\tSPEAKER_03\n So which is my bar?\n\n0:00:25\tSPEAKER_03\n Number one.\n\n0:00:26\tSPEAKER_05\n So actually if you guys want to go ahead and read digits now as long as you've signed the consent form, that's all right.\n\n0:00:35\tSPEAKER_05\n Are we supposed to read digits at the same time?\n\n0:00:37\tSPEAKER_05\n No.\n\n0:00:38\tSPEAKER_05\n No, each individually.\n\n0:00:39\tSPEAKER_05\n We're talking about doing all at the same time, but I think cognitively that would be really difficult to try to read them while everyone else is.\n\n0:00:48\tSPEAKER_05\n So when you're reading the digits strings, the first thing to do is just say which transcript\n\n0:00:52\tSPEAKER_04\n you're on.\n\n0:00:53\tSPEAKER_05\n So you can see the transcript. There's two large number strings on the digits.\n\n0:01:06\tSPEAKER_05\n So you would just read that one.\n\n0:01:08\tSPEAKER_05\n And then you read each line with a small pause between lines.\n\n0:01:12\tSPEAKER_05\n And the pause is just so that the person transcribing it can tell where one line ends and the other begins.\n\n0:01:16\tSPEAKER_05\n And I'll read the digits strings first so you can see how that goes.\n\n0:01:21\tSPEAKER_05\n Again, I'm not sure how much I should talk about stuff before everyone's here.\n\n0:01:33\tSPEAKER_04\n Well, one more coming.\n\n0:01:35\tSPEAKER_05\n Well, I'm going to go ahead and read digit strings and then we can go on from there.\n\n0:01:39\tSPEAKER_05\n So this is transcript.\n\n0:01:40\tSPEAKER_05\n 3, 6, 5, 1, 3, 6, 7, 0, 3, 3, 5, 1, 9, 0, 3, 6, 4, 0, 2, 3, 6, 5, 1, 3, 6, 5, 1, 0, 5, 1, 0, 5, 2, 1, 5, 6, 4, 7, 6, 7, 8, 0, 2, 1, 4, 0, 4, 6, 0, 5, 5, 8, 0, 2, 8, 1, 6, 8, 2, 8, 2, 9, 3.\n\n0:02:18\tSPEAKER_04\n Okay, well, we can start doing it.\n\n0:02:26\tSPEAKER_03\n Okay, this is transcript 3, 6, 3, 1, 3, 6, 5, 1, 3, 1, 5, 4, 2, 9, 7, 6, 3, 9, 5, 7, 1, 6, 7, 4, 4, 6, 7, 8, 9, 0, 4, 3, 0, 2, 4, 9, 2, 6, 9, 1, 7, 4, 1, 2, 3, 4, 0, 4, 7, 9, 3, 7, 4, 5, 6, 8, 3, 9, 9, 3, 4, 7, 0, 7, 0, 9, 1, 7, 1, 2,\n\n0:03:15\tSPEAKER_04\n 3, 6, 7, 4, 7, 6, 7, 6, 6, 7, 8, 7, 8, 9, 1, 8, 7, 8, 8, 8, 8, 8, 9, 0-0-3129-889-0.\n\n0:03:39\tSPEAKER_04\n 0-0-3129-8235-33049-490507-670531-80620-015-03832.\n\n0:03:45\tSPEAKER_04\n 0-0-3309-490507-670531-80620-015-03832.\n\n0:03:52\tSPEAKER_04\n 0-0-3309-015-030830620-015-03832.\n\n0:04:22\tSPEAKER_02\n 0-0-3309-99900.\n\n0:04:52\tSPEAKER_01\n 0-0-3351-357-0. 0-9-0802-0. 0-1-0-0-7-4-3401-4218-5478-8682-7.\n\n0:04:59\tSPEAKER_01\n 0-0-7-4401-4218-5478-8682-7.\n\n0:05:09\tSPEAKER_01\n 0-0-66450-901-152-6-0-3.\n\n0:05:17\tSPEAKER_01\n 0-0-7-6-3100-5-6-827-92959-5904-2-3.\n\n0:05:23\tSPEAKER_01\n 0-0-7-6-6-6-6-6-6-6-6-5708-5902-0.\n\n0:05:30\tSPEAKER_05\n 0-0-7-6-6-6-7-6-7.\n\n0:05:43\tSPEAKER_05\n 0-0-7-6-6-6-6-6-6.\n\n0:05:48\tSPEAKER_05\n only once the speaker form and the consent form.\n\n0:05:52\tSPEAKER_05\n And the short form, I mean you should read the consent form, but the thing to notice is that we will give you an opportunity to edit all the transcripts.\n\n0:06:00\tSPEAKER_05\n So if you say things and you don't want them to be released to the general public, which these will be available at some point to anyone who wants them, you'll be given an opportunity by email to bleep out any portions you don't like.\n\n0:06:16\tSPEAKER_05\n On the speaker form, just to tell you as much of the information as you can, if you're not exactly sure about the region, we're not exactly sure either.\n\n0:06:22\tSPEAKER_05\n So don't worry too much about it.\n\n0:06:24\tSPEAKER_05\n It's just self-rating.\n\n0:06:28\tSPEAKER_05\n I think that's about it.\n\n0:06:30\tSPEAKER_05\n And should I do want me to talk it all about why we're doing this and what this project is?\n\n0:06:34\tSPEAKER_04\n Yeah, no, let's see.\n\n0:06:36\tSPEAKER_02\n Does that see no, that we're meeting in here?\n\n0:06:38\tSPEAKER_04\n She's got an even, she was notified whether she knows.\n\n0:06:43\tSPEAKER_04\n It's another question.\n\n0:06:47\tSPEAKER_04\n So are the people going to be identified by name?\n\n0:06:50\tSPEAKER_05\n Well, we'll anonymize it in the transcript, but not in the audio.\n\n0:06:55\tSPEAKER_04\n OK, so in terms of people worrying about excising things from the transcript that's unlikely since it isn't attributed.\n\n0:07:05\tSPEAKER_04\n Oh, I see, but the other thing.\n\n0:07:07\tSPEAKER_05\n Right, so if I said, oh, hi, Jerry, how are you?\n\n0:07:09\tSPEAKER_05\n We're not going to go through and cancel out the jerry.\n\n0:07:12\tSPEAKER_05\n Sure.\n\n0:07:13\tSPEAKER_05\n So we will go through.\n\n0:07:14\tSPEAKER_05\n And in the speaker ID tags, there'll be M1 or 7M108.\n\n0:07:19\tSPEAKER_05\n But I don't know a good way of doing it on the audio and still have people who are doing discourse research to be able to use the data.\n\n0:07:29\tSPEAKER_04\n No, I was in the complain here.\n\n0:07:30\tSPEAKER_04\n I just wanted to understand.\n\n0:07:31\tSPEAKER_03\n Right, OK.\n\n0:07:32\tSPEAKER_03\n We can make up LES for each of us.\n\n0:07:35\tSPEAKER_05\n OK, I mean, whatever you want to do is fine.\n\n0:07:36\tSPEAKER_05\n But we find that we want the meeting to be as natural as possible.\n\n0:07:40\tSPEAKER_05\n We're trying to do real meetings.\n\n0:07:42\tSPEAKER_05\n And so we don't want to have to do aliases and we don't want people to be editing what they say.\n\n0:07:46\tSPEAKER_05\n So I think it's better just as a post-process to edit out every time you bash Microsoft.\n\n0:07:54\tSPEAKER_04\n OK, so why don't you tell us briefly?\n\n0:07:56\tSPEAKER_04\n Give your normal spiel.\n\n0:07:59\tSPEAKER_05\n So this is a project that's called Meeting Recorder.\n\n0:08:01\tSPEAKER_05\n And there are lots of different aspects of the project.\n\n0:08:03\tSPEAKER_05\n So my particular interest is in the pda of the future.\n\n0:08:07\tSPEAKER_05\n This is a mock-up of one.\n\n0:08:08\tSPEAKER_05\n Yes, we do believe the pda of the future will be made of wood.\n\n0:08:12\tSPEAKER_05\n The idea is that you'd be able to put a pda at the table at an impromptu meeting and record it and then be able to do querying and retrieval later on on the meeting.\n\n0:08:20\tSPEAKER_05\n So that's my particular interest is a portable device to do information retrieval on meetings.\n\n0:08:25\tSPEAKER_05\n Other people are interested in other aspects of meetings.\n\n0:08:29\tSPEAKER_05\n So the first step on that in any of these is to collect some data.\n\n0:08:32\tSPEAKER_05\n And so what we wanted is a room that's instrumented with both the tabletop microphones.\n\n0:08:37\tSPEAKER_05\n And these are very high quality pressure zone mics, as well as the close talking mics.\n\n0:08:42\tSPEAKER_05\n What the close talking mics gives us is some ground truth gives us high quality audio, especially for people who aren't interested in the acoustic parts of this corpus.\n\n0:08:52\tSPEAKER_05\n So for people who are more interested in language, we didn't want to penalize them by having only the far-field mics available.\n\n0:08:58\tSPEAKER_05\n And then also, it's a very, very hard task in terms of speech recognition.\n\n0:09:04\tSPEAKER_05\n And so on the far-field mics, we can expect very low recognition results.\n\n0:09:08\tSPEAKER_05\n So we wanted the near-field mics to at least isolate the difference between the two.\n\n0:09:12\tSPEAKER_05\n So that's why we're recording in parallel with the close talking and the far-field at the same time.\n\n0:09:17\tSPEAKER_05\n And then all these channels are recorded simultaneously and framed synchronously so that you can also do things like beam forming on all the microphones and degree search like that.\n\n0:09:27\tSPEAKER_05\n Our intention is to release this data to the public, probably through a body like the LDC, and just make it as a generally available corpus.\n\n0:09:38\tSPEAKER_05\n There's other work going on in meeting recordings.\n\n0:09:40\tSPEAKER_05\n So we're working with SRI with UW, NIST has started an effort, which will include video.\n\n0:09:46\tSPEAKER_05\n We're not including video, obviously.\n\n0:09:50\tSPEAKER_05\n And then also a small amount of assistance from IBM.\n\n0:09:53\tSPEAKER_05\n So I'm so involved.\n\n0:09:56\tSPEAKER_05\n Oh, and the digit strings.\n\n0:09:57\tSPEAKER_05\n This is just a more constrained task.\n\n0:10:01\tSPEAKER_05\n So because the general environment is so challenging, we decided to do at least one set of digit strings to give ourselves something easier.\n\n0:10:10\tSPEAKER_05\n And it's exactly the same digit strings as in TI digits, which is a common connected digits corpus.\n\n0:10:16\tSPEAKER_05\n So we'll have some comparison to be able to be made.\n\n0:10:20\tSPEAKER_05\n OK.\n\n0:10:21\tSPEAKER_05\n Anything else?\n\n0:10:22\tSPEAKER_05\n Nope.\n\n0:10:23\tSPEAKER_05\n OK, so when the last person comes in, just have him wear a wireless.\n\n0:10:26\tSPEAKER_05\n It should be on already, either one of those.\n\n0:10:29\tSPEAKER_05\n And read the digit strings and fill out the forms.\n\n0:10:32\tSPEAKER_05\n So the most important form is the consent form.\n\n0:10:34\tSPEAKER_05\n So be sure everyone signs that if they consent.\n\n0:10:37\tSPEAKER_05\n It should be usual for meeting certain people come late.\n\n0:10:40\tSPEAKER_05\n So you don't have to use these.\n\n0:10:42\tSPEAKER_05\n And just give me a call, which my team is up there when your meeting is over.\n\n0:10:46\tSPEAKER_05\n And I'm going to leave the mic here, but I'm not going to be on.\n\n0:10:50\tSPEAKER_05\n So don't have them use this one.\n\n0:10:52\tNone\n Thank you.\n\n0:10:54\tNone\n Thank you.\n\n0:10:55\tSPEAKER_03\n Any further?\n\n0:10:57\tSPEAKER_03\n Yeah, there we go.\n\n0:10:59\tSPEAKER_04\n Anyway, Adam, we will be using the screen as well.\n\n0:11:04\tSPEAKER_04\n So yeah.\n\n0:11:08\tSPEAKER_04\n Wow.\n\n0:11:11\tSPEAKER_04\n Organization.\n\n0:11:12\tSPEAKER_04\n So you guys got an email about this Friday or something about what we're up to.\n\n0:11:17\tSPEAKER_02\n No.\n\n0:11:21\tSPEAKER_03\n I got it.\n\n0:11:24\tSPEAKER_02\n What was the nature of the email?\n\n0:11:25\tSPEAKER_04\n Oh, this was about inferring intentions from pictures and context and the word.\n\n0:11:33\tSPEAKER_04\n It's like go to sea or visit.\n\n0:11:36\tSPEAKER_02\n Well, I didn't get it.\n\n0:11:38\tSPEAKER_02\n I don't think I did it.\n\n0:11:40\tSPEAKER_04\n I guess these have got better filters.\n\n0:11:41\tSPEAKER_04\n Because I said it to everybody.\n\n0:11:43\tSPEAKER_04\n You just blew it off.\n\n0:11:44\tSPEAKER_04\n OK.\n\n0:11:46\tSPEAKER_03\n It's only simple.\n\n0:11:46\tSPEAKER_03\n So this is the idea.\n\n0:11:47\tSPEAKER_03\n We could pursue, if we thought it's worth it, but I think we will agree on that, to come up with a very, very first crew prototype and do some implementation work and do some research and some modeling.\n\n0:12:08\tSPEAKER_03\n So the idea is if you want to go somewhere and focus on that object down, oh, I can actually walk around this.\n\n0:12:19\tSPEAKER_03\n Down here.\n\n0:12:20\tSPEAKER_03\n That's the powder tower.\n\n0:12:22\tSPEAKER_03\n Now, we found in our data experiments that there are three things you can do.\n\n0:12:34\tSPEAKER_03\n You can walk this way and come really, really close to it and touch it.\n\n0:12:41\tSPEAKER_03\n But you cannot enter or do anything else unless you're interested in rock climbing.\n\n0:12:45\tSPEAKER_03\n It would do you no good standing there.\n\n0:12:47\tSPEAKER_03\n It's just a dark alley.\n\n0:12:48\tSPEAKER_03\n You can touch it.\n\n0:12:50\tSPEAKER_03\n If you want to actually go up or into the tower, you have to go this way and then through some buildings and upstairs and so forth.\n\n0:12:57\tSPEAKER_03\n If you actually want to see the tower, and that's what actually most people want to do, is just have a good look of it, take a picture for the family.\n\n0:13:07\tSPEAKER_03\n You have to go this way and go up here.\n\n0:13:10\tSPEAKER_03\n There you have a really view.\n\n0:13:11\tSPEAKER_03\n It exploded during the 30 years war, really interesting side.\n\n0:13:17\tSPEAKER_03\n And these lines are paths, that's the street network of our geographic information system.\n\n0:13:27\tSPEAKER_03\n You can tell that we deliberately cut out this part, because otherwise we couldn't get our GIS system to lead people this way.\n\n0:13:35\tSPEAKER_03\n It would always use the closest point to the object and then the tourists would be faced in front of a wall, but would do them absolutely no good.\n\n0:13:44\tSPEAKER_03\n So what we found interesting is, first of all, intentions differ.\n\n0:13:51\tSPEAKER_03\n Maybe you want to enter a building.\n\n0:13:54\tSPEAKER_03\n Maybe you want to see it, take a picture of it.\n\n0:13:58\tSPEAKER_03\n Or maybe you actually want to come as close as possible to the building for whatever reason.\n\n0:14:03\tSPEAKER_02\n That's it.\n\n0:14:03\tSPEAKER_02\n What's it made out of?\n\n0:14:06\tSPEAKER_02\n Red limestone.\n\n0:14:10\tSPEAKER_02\n So maybe you would want to touch it.\n\n0:14:11\tSPEAKER_03\n Yeah, maybe you would want to.\n\n0:14:15\tSPEAKER_03\n OK, these intentions we could, if you want to call it the Vista mode, really just want to get the overview or look at it.\n\n0:14:26\tSPEAKER_03\n The Enter mode and the Tango mode, I always call it, with silly names.\n\n0:14:32\tSPEAKER_03\n So this Tango means literally translated to touch.\n\n0:14:36\tSPEAKER_03\n But sometimes the Tango mode is really relevant in the sense that if you don't have the intention of entering a building, but you know that something is really close to it, you just want to approach it or get to that building.\n\n0:14:56\tSPEAKER_03\n Consider, for example, the post office in Chicago building, so large that it has its own zip code.\n\n0:15:01\tSPEAKER_03\n So the entrance could be miles away from the closest point.\n\n0:15:06\tSPEAKER_03\n So sometimes it makes sense maybe to distinguish there.\n\n0:15:10\tSPEAKER_03\n So I've looked through 20 some.\n\n0:15:16\tSPEAKER_03\n I didn't look through all the data.\n\n0:15:19\tSPEAKER_03\n And there is a lot more different ways in people, the ways people phrase how to get, if they want to get to a certain place.\n\n0:15:29\tSPEAKER_03\n And sometimes here it's a little bit more obvious.\n\n0:15:36\tSPEAKER_03\n Maybe I should go back, come on steps.\n\n0:15:38\tSPEAKER_04\n OK, come in, sit down.\n\n0:15:39\tSPEAKER_04\n Grab yourself a microphone.\n\n0:15:43\tSPEAKER_03\n You need to sign some stuff.\n\n0:15:44\tSPEAKER_04\n Well, you can sign afterwards.\n\n0:15:46\tSPEAKER_02\n Yeah, absolutely.\n\n0:15:47\tSPEAKER_02\n I'll have to read some digits afterwards.\n\n0:15:49\tSPEAKER_03\n There are two.\n\n0:16:03\tSPEAKER_00\n Maybe small?\n\n0:16:04\tSPEAKER_00\n OK, I see.\n\n0:16:05\tSPEAKER_00\n OK.\n\n0:16:08\tSPEAKER_04\n Thank you.\n\n0:16:10\tSPEAKER_04\n OK, that was our idea.\n\n0:16:11\tSPEAKER_04\n It also has to be switched on.\n\n0:16:13\tSPEAKER_04\n No, it's already on it.\n\n0:16:14\tSPEAKER_04\n That's all.\n\n0:16:14\tSPEAKER_03\n OK, good.\n\n0:16:15\tSPEAKER_03\n Thank you.\n\n0:16:16\tSPEAKER_03\n OK.\n\n0:16:17\tSPEAKER_03\n There was a year people, when they want to go to a building, sometimes they just want to look at it.\n\n0:16:23\tSPEAKER_03\n Sometimes they want to enter it.\n\n0:16:25\tSPEAKER_03\n Sometimes they want to get really close to it.\n\n0:16:28\tSPEAKER_03\n That's something we found.\n\n0:16:29\tSPEAKER_03\n It's just a truth.\n\n0:16:31\tSPEAKER_03\n And the place is where you will lead them for these intentions is sometimes incredibly different.\n\n0:16:37\tSPEAKER_03\n I gave an example where at the point where you end up, if you want to look at it, it's completely different.\n\n0:16:41\tSPEAKER_03\n You want to enter it.\n\n0:16:43\tSPEAKER_03\n So this is sort of how people may phrase those requests to a mock-up system at least, the way they did it.\n\n0:16:51\tSPEAKER_03\n And we get tons of these.\n\n0:16:54\tSPEAKER_03\n How do I get to?\n\n0:16:55\tSPEAKER_03\n I want to go to.\n\n0:16:56\tSPEAKER_03\n But also, give me directions to, and I would like to see.\n\n0:17:00\tSPEAKER_03\n And what we can sort of do if we look closer at the data that was the wrong one, we can look at some factors that may make a difference.\n\n0:17:12\tSPEAKER_03\n First of all, very important.\n\n0:17:14\tSPEAKER_03\n And that I've completely forgot that when we talked.\n\n0:17:17\tSPEAKER_03\n This is of course a crucial factor.\n\n0:17:19\tSPEAKER_03\n What type of object is it?\n\n0:17:21\tSPEAKER_03\n So some buildings you just don't want to take pictures of or very rarely, but you usually want to enter them.\n\n0:17:30\tSPEAKER_03\n Some objects are more picturesque, more highly photographed.\n\n0:17:35\tSPEAKER_03\n Then of course, the actual phrases may give us some idea of what the person wants.\n\n0:17:43\tSPEAKER_03\n Sometimes I found in looking at the data in a superficial way I found some sort of modifiers that may also give us hint.\n\n0:17:52\tSPEAKER_03\n I'm trying to get to a need to get to sort of instead of the fact that you're not really sightseeing and just for their full pleasure and so forth.\n\n0:18:02\tSPEAKER_03\n And this is straight to the context, which ought to should be considered.\n\n0:18:07\tSPEAKER_03\n That whatever it is you're doing at the moment may also influence the interpretation of a phrase.\n\n0:18:13\tSPEAKER_03\n So this is really my suggestion.\n\n0:18:17\tSPEAKER_03\n It's really simple.\n\n0:18:19\tSPEAKER_03\n We start with now, let me say one more thing.\n\n0:18:25\tSPEAKER_03\n What we do know is that the parser reuse in the smart group system will never differentiate between any of these.\n\n0:18:31\tSPEAKER_03\n So basically all of these things will result in the same XMLM3L structure, sort of action go and then an object and a source.\n\n0:18:43\tSPEAKER_03\n So it's way to capture those differences in intentions.\n\n0:18:49\tSPEAKER_03\n So I thought maybe for a deep understanding task, that's a nice sort of playground or first little thing we can start it and sort of look, OK, we need, we're going to get those M3L structures, the crude, undifferentiated parsed, interpreted input.\n\n0:19:06\tSPEAKER_03\n We may need additional part of speech or maybe just some information on the verb and water fires, auxiliaries, we'll see.\n\n0:19:17\tSPEAKER_03\n And I will try to sort of come up with a list of factors that we need to get out of there.\n\n0:19:21\tSPEAKER_03\n And maybe we want to get a switch for the context.\n\n0:19:25\tSPEAKER_03\n So this is not something we can actually monitor now, but just something we can set.\n\n0:19:33\tSPEAKER_03\n And then you can all imagine sort of a constraint, satisfaction program, depending on what comes out.\n\n0:19:41\tSPEAKER_03\n We want to have a structure resulting if we repeat it through belief net or something along those lines.\n\n0:19:49\tSPEAKER_03\n We get an inferred intention.\n\n0:19:50\tSPEAKER_03\n We produce a structure that differentiates between the vista of the enter and the tango mode, which I think we may want to ignore, but that's my idea.\n\n0:20:03\tSPEAKER_03\n It's up for discussion.\n\n0:20:04\tSPEAKER_03\n We can change all of it any bit of it.\n\n0:20:08\tSPEAKER_03\n Go it all the way.\n\n0:20:11\tSPEAKER_01\n Now, this email you sent actually.\n\n0:20:13\tSPEAKER_01\n What?\n\n0:20:13\tSPEAKER_01\n Now I remember the email.\n\n0:20:14\tSPEAKER_02\n OK.\n\n0:20:16\tSPEAKER_02\n Huh.\n\n0:20:16\tSPEAKER_02\n Still, I have no recollection whatsoever of the email.\n\n0:20:20\tSPEAKER_02\n I'll have to go back and check.\n\n0:20:21\tSPEAKER_04\n Not important.\n\n0:20:23\tSPEAKER_04\n So what is important is that we understand what the proposed task is.\n\n0:20:32\tSPEAKER_04\n And Robert and I talked about this some on Friday.\n\n0:20:36\tSPEAKER_04\n And we think it's well-formed.\n\n0:20:40\tSPEAKER_04\n So we think it's a well-formed starter task for this deep understanding in the tourist domain.\n\n0:20:52\tSPEAKER_01\n So where exactly is the deep understanding being done?\n\n0:20:55\tSPEAKER_01\n Like, I mean, is it before the base net?\n\n0:20:57\tSPEAKER_01\n Is it?\n\n0:20:57\tSPEAKER_04\n Well, it's always all of it.\n\n0:21:00\tSPEAKER_04\n So in general, it's always going to be the answer is everywhere.\n\n0:21:05\tSPEAKER_04\n So the notion is that this isn't real deep, but it's deep enough that you can distinguish between these three quite different kinds of going to see some tourist thing.\n\n0:21:17\tSPEAKER_04\n And so that's the quote deep that we're trying to get at.\n\n0:21:22\tSPEAKER_04\n And Robert's point is that the current front end doesn't give you any way to not only doesn't it do it, but it also doesn't give you enough information to do it.\n\n0:21:34\tSPEAKER_04\n It isn't like if you just took what the front end gives you and use some clever inference algorithm on it, you would be able to figure out which of these is going on.\n\n0:21:44\tSPEAKER_04\n So in general, it's going to be true of any kind of deep understanding.\n\n0:21:49\tSPEAKER_04\n There's going to be contextual things.\n\n0:21:50\tSPEAKER_04\n There's going to be linguistic things.\n\n0:21:51\tSPEAKER_04\n There's going to be discourse things.\n\n0:21:54\tSPEAKER_04\n And they've got to be combined.\n\n0:21:56\tSPEAKER_04\n And my idea on how to combine them is with a belief net.\n\n0:22:00\tSPEAKER_04\n Well, it may turn out that some totally different thing is going to work better.\n\n0:22:05\tSPEAKER_04\n The idea would be that you take your editing your slide.\n\n0:22:15\tSPEAKER_03\n Yeah.\n\n0:22:16\tSPEAKER_03\n So as I get ideas, so discourse, I thought about the course that needs to be.\n\n0:22:25\tSPEAKER_03\n I'm sorry.\n\n0:22:25\tSPEAKER_04\n OK.\n\n0:22:26\tSPEAKER_04\n So this is taking minutes as we go in his own way.\n\n0:22:31\tSPEAKER_04\n But anyway, so the thing is, naively speaking, you've got for this little task a belief net, which is going to have as output the conditional probability of one of three things that the person wants to view it, to enter it, or to tango with it.\n\n0:22:54\tSPEAKER_04\n So the output of the belief net is pretty well formed.\n\n0:22:58\tSPEAKER_04\n And then the inputs are going to be these kinds of things.\n\n0:23:05\tSPEAKER_04\n And then the question is, the two questions is, one, where do you get this information from?\n\n0:23:10\tSPEAKER_04\n And two, what's the structure of the belief net?\n\n0:23:12\tSPEAKER_04\n So what are the conditional probabilities of this, that, and the other, given these things?\n\n0:23:17\tSPEAKER_04\n And you probably need intermediate notes.\n\n0:23:19\tSPEAKER_04\n I don't know what they are yet.\n\n0:23:21\tSPEAKER_04\n So it may well be that, for example, that knowing whether, oh, another thing you want is some information about the time of day.\n\n0:23:37\tSPEAKER_04\n Now, let me want to call that part of context.\n\n0:23:39\tSPEAKER_04\n But the time of day matters a lot.\n\n0:23:42\tSPEAKER_04\n And if things are obviously closed, then people don't want to enter them.\n\n0:23:50\tSPEAKER_04\n And if it's not obvious, you may want to actually point out the people that it's closed, what they're going to is closed, and they don't have the option of entering it.\n\n0:24:02\tSPEAKER_04\n So another thing that can come up, and will come up as soon as you get serious about this, is that another option, of course, is to have it more of a dialogue.\n\n0:24:12\tSPEAKER_04\n So if someone says something, you could ask them.\n\n0:24:16\tSPEAKER_04\n And now, one thing you could do is always ask them.\n\n0:24:18\tSPEAKER_04\n That's boring.\n\n0:24:20\tSPEAKER_04\n And it will also be a pain for the person using it.\n\n0:24:23\tSPEAKER_04\n So one thing you could do is build a little system that say, whenever you've got a question like that, I've got one of three answers, ask them which one you want.\n\n0:24:31\tSPEAKER_04\n But that's not what we're going to do.\n\n0:24:34\tSPEAKER_03\n But maybe that's a false state of the system that it's too close to call.\n\n0:24:38\tSPEAKER_04\n Oh, yeah.\n\n0:24:39\tSPEAKER_04\n You want the ability to ask.\n\n0:24:40\tSPEAKER_04\n You want the ability to ask.\n\n0:24:42\tSPEAKER_04\n But what you don't want to do is build a system that always asks every time.\n\n0:24:46\tSPEAKER_04\n And that's not getting out the scientific problem.\n\n0:24:49\tSPEAKER_04\n And in general, it's going to be much more complex than that.\n\n0:24:55\tSPEAKER_04\n This is purposely a really simple case.\n\n0:24:58\tSPEAKER_03\n So I have one more point to the bus question.\n\n0:25:03\tSPEAKER_03\n I think also the deep understanding part of it is going to be in the extent that we wanted in terms of modeling.\n\n0:25:12\tSPEAKER_03\n We can start, you know, basic from human beings, model that, it's motions, going, walking, seeing.\n\n0:25:18\tSPEAKER_03\n We can model all of that and then compose whatever inferences we make out of these really conceptual primitives.\n\n0:25:25\tSPEAKER_03\n That will be extremely deep in my understanding.\n\n0:25:28\tSPEAKER_04\n So the way that might come up, if you want to do that, you might say, as an intermediate step in your belief net, is there a source path goal schema involved?\n\n0:25:41\tSPEAKER_04\n And if so, is there a focus on the goal, or is there a focus on the path or something?\n\n0:25:48\tSPEAKER_04\n And that could be one of the condition, you know, in some piece of the belief net, that could be the appropriate thing to enter.\n\n0:26:00\tSPEAKER_01\n So when would we extract that information from the M3L?\n\n0:26:03\tSPEAKER_04\n No.\n\n0:26:04\tSPEAKER_04\n No.\n\n0:26:04\tSPEAKER_04\n So the M3L is not going to give you, what he was saying is, the M3L does not have any of that.\n\n0:26:09\tSPEAKER_04\n All it has is some really crude stuff saying, person wants to go to a place.\n\n0:26:13\tSPEAKER_02\n The M3L is the old smart com.\n\n0:26:16\tSPEAKER_04\n Right.\n\n0:26:17\tSPEAKER_04\n Well, M3L itself refers to modeling and markup language.\n\n0:26:20\tSPEAKER_04\n So we have to have a better way of referring to the parts that we're going to put.\n\n0:26:27\tSPEAKER_03\n Yeah, that implies speech.\n\n0:26:29\tSPEAKER_03\n I think you can actually actually intentionally let us, is what we're going to do.\n\n0:26:35\tSPEAKER_04\n But they call it intention lattice.\n\n0:26:36\tSPEAKER_03\n But in the intention lattice, they call it intention hypotheses.\n\n0:26:42\tSPEAKER_04\n So they're going to give us some, or we can assume that you get this crude information about intention.\n\n0:26:51\tSPEAKER_04\n And that's all they're going to provide.\n\n0:26:54\tSPEAKER_04\n And they don't give you the kind of object.\n\n0:26:56\tSPEAKER_04\n They don't give you any discourse history.\n\n0:26:59\tSPEAKER_04\n If you want to keep that, you have to keep it somewhere else.\n\n0:27:04\tSPEAKER_03\n Well, they keep it where they have to request it.\n\n0:27:06\tSPEAKER_03\n Right.\n\n0:27:07\tSPEAKER_04\n Well, they keep it by their lights.\n\n0:27:10\tSPEAKER_04\n It may or may not be what we want.\n\n0:27:14\tSPEAKER_02\n So if someone says I want to touch the side of the powder tower, that would basically we need to pop up Tango mode in the direction.\n\n0:27:25\tSPEAKER_02\n If you've got a simple as that, yeah.\n\n0:27:28\tSPEAKER_04\n But it wouldn't.\n\n0:27:31\tSPEAKER_02\n But that isn't necessarily, we have to infer a source path goal to something really good, touching the side, right?\n\n0:27:40\tSPEAKER_03\n There is a point there if I understand you correct me.\n\n0:27:46\tSPEAKER_03\n Because sometimes people just say, you find very often, where is the city hall?\n\n0:27:56\tSPEAKER_03\n And they don't want to see it on a map, or they don't want to know it's 500 yards away from you, or it's to the north.\n\n0:28:03\tSPEAKER_03\n They want to go there.\n\n0:28:05\tSPEAKER_03\n That's what they say is, where is it?\n\n0:28:07\tSPEAKER_02\n Where is it, that thing?\n\n0:28:08\tSPEAKER_02\n And the parser would output.\n\n0:28:11\tSPEAKER_03\n Well, that's a question mark.\n\n0:28:13\tSPEAKER_03\n A lot of parses just way beyond their scope of interpreting that.\n\n0:28:22\tSPEAKER_03\n But still outcome, the outcome will be some form of structure with a town hall, and maybe it's a WH focus on the town hall.\n\n0:28:33\tSPEAKER_03\n But to interpret it, somebody else has to do that later.\n\n0:28:38\tSPEAKER_02\n I'm trying to figure out what this mark is just without, but depending on these things.\n\n0:28:43\tSPEAKER_03\n It will probably tell you how far away it is.\n\n0:28:46\tSPEAKER_03\n That's even what you've mapped as it has to how far away it is and shows it to your own map.\n\n0:28:53\tSPEAKER_03\n Because we cannot differentiate at the moment between the intention of wanting to go there, or the intention of just knowing or wanting to know where it is.\n\n0:29:02\tSPEAKER_00\n People might not be able to infer that either.\n\n0:29:06\tSPEAKER_00\n I could imagine if someone came up to me and asked where is the city hall, are you trying to get there?\n\n0:29:11\tSPEAKER_00\n Because how I describe this location, probably kind of whether I should give them directions now or say whatever, a half a mile away or something like that.\n\n0:29:21\tSPEAKER_03\n granularity factor because we're people ask you.\n\n0:29:24\tSPEAKER_03\n Where is New York?\n\n0:29:25\tSPEAKER_03\n We'll tell them it's on the East Coast.\n\n0:29:27\tSPEAKER_03\n You won't tell them how to get there.\n\n0:29:29\tSPEAKER_03\n Check that bus to the airport in blah, blah, blah.\n\n0:29:31\tSPEAKER_03\n But if it's the post office, we will tell them how to get there.\n\n0:29:35\tSPEAKER_03\n So they have done some interesting experiments of that in Hamburg.\n\n0:29:40\tSPEAKER_04\n But go back to the, yeah,\n\n0:29:45\tSPEAKER_03\n this is on tour is knowledge about buildings, they're opening times, and then coupled with time of day.\n\n0:29:55\tSPEAKER_00\n So that context was like their presumed purpose context, like business or travel, as well as the utterance context.\n\n0:30:03\tSPEAKER_00\n Like I'm now standing at this point.\n\n0:30:05\tSPEAKER_04\n Yeah, I think we have all along.\n\n0:30:08\tSPEAKER_04\n We've been distinguishing between situational context, which is what you have is context in discourse context, which you have is D H, I don't know what the H means.\n\n0:30:17\tSPEAKER_04\n History, discourse.\n\n0:30:19\tSPEAKER_04\n OK, whatever.\n\n0:30:20\tSPEAKER_04\n So we can work out terminology later.\n\n0:30:22\tSPEAKER_04\n So they're quite distinct.\n\n0:30:25\tSPEAKER_04\n I mean, you need them both, but they're quite distinct.\n\n0:30:27\tSPEAKER_04\n And so what we're talking about doing as a first shot is not doing any of the linguistics except to find out what seems to be useful.\n\n0:30:42\tSPEAKER_04\n So the reason the belief net is in blue is the notion would be, this may be a bad idea, but the idea is to take us a first goal, see if we could actually build a belief net that would make this three-way distinction in a plausible way, given all these transcripts, and we're able to by hand extract the features to put into belief net, saying, here are the things which if you could get them out of the language and discourse and put them into the belief net, it would tell you which of these three intentions is most likely.\n\n0:31:23\tSPEAKER_04\n And to actually do that, build it, run it on the data where you hand transcribe the parameters and see how that goes.\n\n0:31:34\tSPEAKER_04\n If that goes well, then we can start worrying about how we would extract them.\n\n0:31:41\tSPEAKER_04\n So where would you get this information and expand it to other things like this?\n\n0:31:47\tSPEAKER_04\n But if we can't do that, then we're in trouble.\n\n0:31:52\tSPEAKER_03\n And if you can't do this task, we need a different engine.\n\n0:31:59\tSPEAKER_04\n Or something.\n\n0:32:00\tSPEAKER_04\n Well, if it's the belief net, we'll switch to logic or some terrible thing.\n\n0:32:05\tSPEAKER_04\n But I don't think that's going to be the case.\n\n0:32:07\tSPEAKER_04\n I think that if we can get the information, belief net is a perfectly good way of doing the inferential combination of it.\n\n0:32:19\tSPEAKER_04\n The real issue is what are the factors involved in determining this?\n\n0:32:32\tSPEAKER_04\n How many seconds?\n\n0:32:33\tSPEAKER_04\n So I know.\n\n0:32:34\tSPEAKER_04\n Is it clear what's going on here?\n\n0:32:45\tSPEAKER_00\n I missed the beginning.\n\n0:32:46\tSPEAKER_00\n But I guess could you go back to the slide, the previous one?\n\n0:32:50\tSPEAKER_00\n So is it that it's these are all factors that, these are the ones that you said that we are going to ignore now or that we want to take into account.\n\n0:33:02\tSPEAKER_00\n You're saying you're taking them into account.\n\n0:33:04\tSPEAKER_04\n But you don't worry about how to extract them.\n\n0:33:07\tSPEAKER_04\n So let's find out which ones we need first.\n\n0:33:11\tSPEAKER_00\n OK.\n\n0:33:11\tSPEAKER_00\n And it's clear from the data, the correct answer in each case.\n\n0:33:16\tSPEAKER_04\n But let's go back to the slide of data.\n\n0:33:21\tSPEAKER_00\n Like do we know from the data?\n\n0:33:23\tSPEAKER_00\n Which?\n\n0:33:24\tSPEAKER_00\n OK.\n\n0:33:25\tSPEAKER_00\n Not from that data.\n\n0:33:26\tSPEAKER_03\n But since we are designing, compared to this bigger data collection effort, we will definitely take care to put it in the form of the other.\n\n0:33:39\tSPEAKER_03\n To see whether we can get sort of empirically validated data.\n\n0:33:44\tSPEAKER_03\n From this, we can sometimes, you know, and that's not what we need for leave that anyhow.\n\n0:33:49\tSPEAKER_03\n It's sort of sometimes what people want to see.\n\n0:33:52\tSPEAKER_03\n They phrase it more like this.\n\n0:33:54\tSPEAKER_03\n But it doesn't exclude anybody from praising it.\n\n0:33:56\tSPEAKER_03\n Totally different.\n\n0:33:57\tSPEAKER_03\n They still.\n\n0:33:58\tSPEAKER_03\n But then other factors may come into play, the changey outcome of the leave that.\n\n0:34:02\tSPEAKER_03\n So this is exactly what, because you can never be sure.\n\n0:34:06\tSPEAKER_03\n And I'm sure even the most deliberate data collection experiment will never give you data that say, well, if it's phrased like that, be intentional.\n\n0:34:16\tSPEAKER_00\n I mean, the only way you could get those if you were to give the subjects a task, you're a current goal is to.\n\n0:34:24\tSPEAKER_00\n Yeah, that's what we're doing.\n\n0:34:25\tSPEAKER_03\n So that's what you want.\n\n0:34:26\tSPEAKER_03\n We will still get the phrasing all over the place.\n\n0:34:28\tSPEAKER_03\n I'm sure that.\n\n0:34:31\tSPEAKER_00\n No, that's fine.\n\n0:34:31\tSPEAKER_00\n I guess it's just knowing the intention from.\n\n0:34:34\tSPEAKER_00\n From that task, that's the experiment.\n\n0:34:35\tSPEAKER_04\n So I think you all know this.\n\n0:34:37\tSPEAKER_04\n But we are going to actually use this little room and start recording subjects, probably within a month or something.\n\n0:34:44\tSPEAKER_04\n So this is not any, and you guys worry, except that we may want to push that effort to get information we need.\n\n0:34:55\tSPEAKER_04\n So our job is to figure out how to solve these problems if it turns out that we need data of a certain sort, then the sort of data collection branch can be asked to do that.\n\n0:35:08\tSPEAKER_04\n And one of the reasons why we're recording the meeting for these guys is because we want their help when we start doing recording of subjects.\n\n0:35:19\tSPEAKER_04\n So yeah, you're absolutely right, though.\n\n0:35:20\tSPEAKER_04\n No, you will not have.\n\n0:35:22\tSPEAKER_04\n And there it is.\n\n0:35:23\tSPEAKER_04\n And the.\n\n0:35:34\tSPEAKER_00\n And I think the other concern that has come up before too is if it's, I don't know if this was collected.\n\n0:35:41\tSPEAKER_00\n What situation the data was collected?\n\n0:35:42\tSPEAKER_00\n It was the one that you showed in your talk.\n\n0:35:45\tSPEAKER_00\n Like people.\n\n0:35:45\tSPEAKER_00\n No, no.\n\n0:35:46\tSPEAKER_00\n OK.\n\n0:35:46\tSPEAKER_00\n So it was just like someone actually mobile, using a device?\n\n0:35:54\tSPEAKER_03\n No, not.\n\n0:35:55\tSPEAKER_03\n It was mobile, but not really with the system.\n\n0:35:59\tSPEAKER_00\n So there were never answers.\n\n0:36:00\tSPEAKER_00\n OK.\n\n0:36:01\tSPEAKER_00\n OK.\n\n0:36:02\tSPEAKER_00\n But it was it.\n\n0:36:03\tSPEAKER_00\n I guess I don't know the situation of collecting the data.\n\n0:36:06\tSPEAKER_00\n Here, you can imagine them being walking around the city.\n\n0:36:09\tSPEAKER_00\n It's like one situation.\n\n0:36:10\tSPEAKER_00\n And then you have all sorts of other situational context factors that would influence how to interpret, like you said, the scope and things like that.\n\n0:36:18\tSPEAKER_00\n If they're doing it in a, you know, I'm just going to hear with a map and ask questions.\n\n0:36:22\tSPEAKER_00\n I would imagine that the data would be really different.\n\n0:36:25\tSPEAKER_00\n So it's just.\n\n0:36:27\tSPEAKER_03\n Yeah.\n\n0:36:28\tSPEAKER_03\n But it was never the goal of that data connection to surface at a certain purpose.\n\n0:36:34\tSPEAKER_03\n So that's why, for example, the tasks were not differentiated by attentionality.\n\n0:36:38\tSPEAKER_03\n There was no label, you know, attention A, attention B, attention C, or task ABC.\n\n0:36:46\tSPEAKER_03\n I'm sure we can produce some if we needed that will help us in other times.\n\n0:36:51\tSPEAKER_03\n But you've got to leave something for other people to model, so to finding out what the context of the situation really is.\n\n0:37:02\tSPEAKER_03\n It's an interesting thing.\n\n0:37:06\tSPEAKER_03\n So if I'm at the moment curious and I want to approach it from the end where we can sort of start with this toy system that we can play around with.\n\n0:37:15\tSPEAKER_03\n So that we get a clearer notion of what input we need for that, what suffices and what doesn't.\n\n0:37:21\tSPEAKER_03\n And then we can start worrying about where to get this input.\n\n0:37:25\tSPEAKER_03\n What do we need, you know, ultimately, once we are all experts in changing that parser, for example, maybe there's just a couple of really things we need to do.\n\n0:37:33\tSPEAKER_03\n And then we get more whatever part of speech, more construction type, like stuff out of it.\n\n0:37:42\tSPEAKER_03\n Procmedic approach for the moment.\n\n0:37:46\tSPEAKER_02\n How exactly does the data collection, do they have a map?\n\n0:37:49\tSPEAKER_02\n And then you give them a scenario of some sort?\n\n0:37:52\tSPEAKER_03\n OK.\n\n0:37:54\tSPEAKER_03\n You mentioned you're the subject you're going to be in here.\n\n0:37:56\tSPEAKER_03\n And you see either the 3D model or a quick time animation of standing in a square in Heidelberg.\n\n0:38:07\tSPEAKER_03\n So you actually see that.\n\n0:38:10\tSPEAKER_03\n The first thing is you have to read a text about Heidelberg.\n\n0:38:15\tSPEAKER_03\n So just offer textbook, the tourist guide, to familiarize your sample, the sort of all-sound in German street names like Lerkerkasse.\n\n0:38:24\tSPEAKER_03\n So that's part one.\n\n0:38:25\tSPEAKER_03\n Part two is you're told that this new, wonderful computer system exists.\n\n0:38:31\tSPEAKER_03\n You can tell you everything you want to know.\n\n0:38:33\tSPEAKER_03\n And it understands your completely.\n\n0:38:35\tSPEAKER_03\n And so you're going to pick up that phone dialer number, and you get a certain amount of tasks that you have to solve.\n\n0:38:40\tSPEAKER_03\n First you have to know, find out how to get to that place.\n\n0:38:43\tSPEAKER_03\n Maybe with the intention of buying stamps in there.\n\n0:38:46\tSPEAKER_03\n Maybe some next task is to get to a certain place, take a picture for your grandchild.\n\n0:38:51\tSPEAKER_03\n The third one is to get inspiration\n\n0:38:54\tNone\n of the history of the object. The fourth one, and then the system breaks down of crashes.\n\n0:39:01\tSPEAKER_03\n At the third, right then?\n\n0:39:03\tNone\n After the third task.\n\n0:39:05\tSPEAKER_03\n And then, or after the fourth, sometimes you give that file.\n\n0:39:08\tSPEAKER_03\n And then a human operator comes on and apologizes that the system has crashed.\n\n0:39:14\tSPEAKER_03\n Or just you to continue.\n\n0:39:16\tSPEAKER_03\n Now we're the human operator.\n\n0:39:18\tSPEAKER_03\n And so you have basically the same tasks again, just with different objects.\n\n0:39:23\tSPEAKER_03\n And you go through it again, and that was it.\n\n0:39:26\tNone\n Oh, and one little bit.\n\n0:39:29\tNone\n The computer, you've been told the computer system knows exactly where you are via GPS.\n\n0:39:34\tSPEAKER_03\n When the human operator comes on, that person does not know.\n\n0:39:39\tSPEAKER_03\n So the GPS is crashed as well.\n\n0:39:41\tSPEAKER_03\n So the person first has to ask you, where are you?\n\n0:39:44\tSPEAKER_03\n And so you have to do some tell the person sort of where you are, depending on what you see there.\n\n0:39:50\tSPEAKER_03\n This is a bit that I don't think we did.\n\n0:39:53\tSPEAKER_03\n We discussed that bit.\n\n0:39:55\tNone\n Just sort of squeezed that in now.\n\n0:39:57\tNone\n But it's something that we would provide some very interesting data for some people, I know.\n\n0:40:02\tSPEAKER_03\n So.\n\n0:40:03\tSPEAKER_00\n So in the display, you said you might have a display that shows.\n\n0:40:09\tSPEAKER_03\n Yeah.\n\n0:40:10\tSPEAKER_03\n Additionally, you have a sort of a map type.\n\n0:40:12\tSPEAKER_00\n Your perspective.\n\n0:40:13\tSPEAKER_00\n And so as you, OK, so as you move through it, that's they just track it on the for themselves.\n\n0:40:20\tSPEAKER_03\n You don't, I don't know.\n\n0:40:23\tSPEAKER_03\n I don't think you really move.\n\n0:40:25\tSPEAKER_03\n OK.\n\n0:40:26\tSPEAKER_03\n Yeah.\n\n0:40:26\tSPEAKER_03\n I mean, that would be an enormous technical effort.\n\n0:40:30\tSPEAKER_03\n Unless we can show it, walks through, we can have movies of walking, you walking through Hydeburg and ultimately arriving there, maybe we want to do that.\n\n0:40:42\tSPEAKER_00\n I was just trying to figure out how.\n\n0:40:44\tSPEAKER_03\n The map was sort of a bit.\n\n0:40:45\tSPEAKER_03\n You want to go to that place.\n\n0:40:48\tSPEAKER_03\n And then sort of there, you see the label of the name.\n\n0:40:52\tSPEAKER_03\n So we get those labels, pronunciation stuff.\n\n0:40:54\tSPEAKER_03\n And so we can change that.\n\n0:40:57\tSPEAKER_00\n So your tasks don't require you to, I mean, you're told.\n\n0:41:02\tSPEAKER_00\n So when your task is, I don't know, a goodbye stamp or something like that.\n\n0:41:05\tSPEAKER_00\n So do you have to respond or do you, what are you supposed to be telling the system?\n\n0:41:12\tSPEAKER_00\n What you're doing now or?\n\n0:41:15\tSPEAKER_00\n Well, we'll see what people do.\n\n0:41:16\tSPEAKER_00\n OK.\n\n0:41:16\tSPEAKER_00\n So it's just like this figure out what they would say.\n\n0:41:19\tSPEAKER_00\n Yeah.\n\n0:41:20\tSPEAKER_03\n And we will record both sides.\n\n0:41:21\tSPEAKER_03\n I mean, we'll record the wizard.\n\n0:41:23\tSPEAKER_03\n In both cases, it's going to be human and the future and the operating limit.\n\n0:41:28\tSPEAKER_03\n And there will be some dialogue.\n\n0:41:31\tSPEAKER_03\n So you first have to listen to that and see what they say.\n\n0:41:36\tSPEAKER_03\n We can instruct the wizard and how expressive and talkative should be.\n\n0:41:41\tSPEAKER_03\n But maybe what you're suggesting is what you're suggesting is it might be to pour the data if you sort of limit it to this ping pong one.\n\n0:41:51\tSPEAKER_03\n Your task results in a question.\n\n0:41:53\tSPEAKER_03\n And then there's an answer.\n\n0:41:54\tSPEAKER_03\n And that's the end of the task.\n\n0:41:55\tSPEAKER_03\n Do you want to have it more steps sort of?\n\n0:41:58\tSPEAKER_00\n I don't know how much direction is given to the subject about what their interaction.\n\n0:42:03\tSPEAKER_00\n I mean, they're unfamiliar with interacting with the system.\n\n0:42:06\tSPEAKER_00\n All they know is it's this great system that can do stuff.\n\n0:42:09\tSPEAKER_04\n So some extent, this is a different discussion.\n\n0:42:12\tSPEAKER_04\n OK.\n\n0:42:13\tSPEAKER_04\n So we have to have this discussion of the experiment, the data collection, and all that sort of stuff.\n\n0:42:18\tSPEAKER_04\n And we do have a student who is a candidate for wizard.\n\n0:42:27\tSPEAKER_04\n She's going to get in touch with me.\n\n0:42:28\tSPEAKER_04\n It's a student of Eves, F-E-Y-F-A spelled F-E-Y-D.\n\n0:42:33\tSPEAKER_04\n Oh, PayPal.\n\n0:42:34\tSPEAKER_04\n You know her?\n\n0:42:36\tSPEAKER_04\n OK.\n\n0:42:37\tSPEAKER_00\n She started taking the class last year and then didn't continue.\n\n0:42:42\tSPEAKER_00\n She's a graduate.\n\n0:42:43\tSPEAKER_00\n She's a graduate.\n\n0:42:44\tSPEAKER_00\n Yeah, she is a good.\n\n0:42:45\tSPEAKER_00\n OK.\n\n0:42:46\tSPEAKER_00\n Yeah.\n\n0:42:46\tSPEAKER_00\n I know her very, very briefly.\n\n0:42:48\tSPEAKER_00\n I know she's interested in.\n\n0:42:51\tSPEAKER_00\n OK.\n\n0:42:51\tSPEAKER_04\n So anyways, she's looking for some more part-time work while she's waiting, actually, for graduate school.\n\n0:42:57\tSPEAKER_04\n And she'll be in touch.\n\n0:42:58\tSPEAKER_04\n So we may have someone to do this.\n\n0:43:02\tSPEAKER_04\n And she's got some background in all this stuff, and there's a linguist.\n\n0:43:07\tSPEAKER_04\n So Nancy, at some point, we'll have another discussion on exactly how that's going to go.\n\n0:43:18\tSPEAKER_04\n And Jane, but also Liz, have offered to help us do this data collection and design and stuff.\n\n0:43:29\tSPEAKER_04\n So when we get to that, we'll have some people doing it to know what to do.\n\n0:43:34\tSPEAKER_00\n I guess the reason I was asking the details of this kind of thing is that it's one thing to collect data for.\n\n0:43:41\tSPEAKER_00\n And speech recognition are various other tests that have pretty clear correct answers.\n\n0:43:46\tSPEAKER_00\n But with intention, obviously, as you point out, there's a lot of other factors.\n\n0:43:50\tSPEAKER_00\n I'm not really sure how the question of how to make it a appropriate toy version of that.\n\n0:43:58\tSPEAKER_00\n It's just hard.\n\n0:43:59\tSPEAKER_00\n So I mean, I guess that was my question.\n\n0:44:01\tSPEAKER_02\n Is the intention implicit in the scenario that's given?\n\n0:44:07\tSPEAKER_00\n It is if they have these tasks that they're supposed to.\n\n0:44:09\tSPEAKER_02\n Yeah, it just wasn't sure what level of detail it has.\n\n0:44:12\tSPEAKER_03\n Yeah.\n\n0:44:13\tSPEAKER_03\n Right.\n\n0:44:14\tSPEAKER_03\n No one is a moment.\n\n0:44:16\tSPEAKER_04\n So that's part of what we'll have to figure out.\n\n0:44:19\tSPEAKER_04\n But the problem that I was going to try to focus on today was let's suppose by magic you could collect dialogues in which one way or the other, you were able to figure out both the intention and set the context and know what language was used.\n\n0:44:40\tSPEAKER_04\n So let's suppose that we can get that kind of data.\n\n0:44:45\tSPEAKER_04\n The issue is can we find a way to basically featureize it so that we get some discrete number of features so that when we know the values to all those features or as many as possible, we can come up with the best estimate of which of the, in this case, three little intentions are most likely.\n\n0:45:09\tSPEAKER_00\n But where did the three intentions go there to see it?\n\n0:45:12\tSPEAKER_00\n And to come this close to?\n\n0:45:14\tSPEAKER_04\n The terminology we're using is to go back.\n\n0:45:18\tSPEAKER_04\n To view it, okay, to enter it.\n\n0:45:23\tSPEAKER_04\n Now, it seems to me, you have no trouble with those being distinct.\n\n0:45:26\tSPEAKER_04\n Take a picture of it.\n\n0:45:28\tSPEAKER_04\n You might well want to be really rather different place than entering it.\n\n0:45:32\tSPEAKER_04\n And for objects, it's at all big, sort of getting to the nearest part of it could be quite different than either of those.\n\n0:45:41\tSPEAKER_00\n Just sort of.\n\n0:45:42\tSPEAKER_00\n Okay, so now I understand the referendum.\n\n0:45:44\tSPEAKER_00\n Tango mode.\n\n0:45:45\tSPEAKER_02\n So I would have thought it was more of a wall.\n\n0:45:48\tSPEAKER_02\n To wall suits?\n\n0:45:50\tSPEAKER_00\n Yeah, like how close are you?\n\n0:45:52\tSPEAKER_00\n Yeah, because it's really close.\n\n0:45:54\tSPEAKER_00\n Yeah, well anyway.\n\n0:45:56\tSPEAKER_01\n So, so like what features do you want to try to extract from either parts or whatever?\n\n0:46:02\tSPEAKER_01\n Like the presence of a word or the presence of a certain stem?\n\n0:46:05\tSPEAKER_04\n Right.\n\n0:46:07\tSPEAKER_04\n Is there a construction or the kind of object or anything else that's in the, it's either in the discourse itself or in the context.\n\n0:46:18\tSPEAKER_04\n So if it turns out that whatever it is, you want to know whether the person's a tourist or not.\n\n0:46:24\tSPEAKER_04\n Okay, that becomes a feature.\n\n0:46:25\tSPEAKER_04\n And then how you determine that is another issue.\n\n0:46:28\tSPEAKER_04\n But for the current problem, it would just be, okay, if you can be sure that it's a tourist versus a businessman versus a native or something, that would give you a lot of discriminatory power and then you just have a little section in your belief net that said, though it's in the short run, you'd set them and see how it worked.\n\n0:46:48\tSPEAKER_04\n And then in the longer run, you would figure out how you could derive them from previous discourse or anything else you knew.\n\n0:47:09\tSPEAKER_01\n So what's the, like how should we go about it?\n\n0:47:12\tSPEAKER_04\n Okay, so first of all, do we, either of you guys, you got a favorite belief net that you've played with Java Bayes or something?\n\n0:47:22\tSPEAKER_01\n No, I'm getting it.\n\n0:47:25\tSPEAKER_04\n Okay, anyway, get one.\n\n0:47:27\tSPEAKER_04\n Okay, so one of the things we want to do is actually pick a package, it doesn't matter which one.\n\n0:47:34\tSPEAKER_04\n Presumably one that's got good interactive abilities because a lot of what we're gonna be, we don't need the one that'll solve massive belief nets quickly, these are not gonna get big in the foreseeable future.\n\n0:47:47\tSPEAKER_04\n And we do want one in which it's easy to interact with and modify it because a lot of what it's gonna be is playing with this.\n\n0:48:01\tSPEAKER_04\n And probably one in which it's easy to have what amounts to transcript files so that if we have all these cases, okay, so we make up cases that have these features, okay, and then you'd like to be able to say, okay, here's a bunch of cases.\n\n0:48:16\tSPEAKER_04\n There are even ones that you can do learning.\n\n0:48:20\tSPEAKER_04\n Okay, so you have all their cases and their results and you have algorithms go through and run around trying to accept the probabilities for you.\n\n0:48:29\tSPEAKER_04\n Probably that's not worth it.\n\n0:48:32\tSPEAKER_04\n I mean, my guess is we aren't gonna have enough data that's good enough to make these data fitting ones worth it that I don't know.\n\n0:48:40\tSPEAKER_04\n So I would say the first task for you too, guys, is to pick a package.\n\n0:48:47\tSPEAKER_04\n You know, the standard thing, you want it stable, you want it.\n\n0:48:51\tSPEAKER_04\n And as soon as we have one, we can start trying to make a first cut at what's going on.\n\n0:49:02\tSPEAKER_04\n But what I like about it is it's very concrete.\n\n0:49:05\tSPEAKER_04\n We know what the outcomes are gonna be and we have some data that's loose.\n\n0:49:09\tSPEAKER_04\n We can use our own intuition and see how hard it is.\n\n0:49:14\tSPEAKER_04\n And importantly, what intermediate notes we think we need.\n\n0:49:18\tSPEAKER_04\n So it turns out that just thinking about the problem you call things that you really need to, this is the kind of thing that is an intermediate little piece in your belief net.\n\n0:49:28\tSPEAKER_04\n That'd be really interesting.\n\n0:49:33\tSPEAKER_03\n It may self as a platform for a person, maybe me or whoever who is interested in doing some linguistic analysis, we have a whole frame that grew up here.\n\n0:49:45\tSPEAKER_03\n We can see what they have found out about those concepts already that are contained in the data.\n\n0:49:52\tSPEAKER_03\n You know, to come up with a nice set of features and maybe even means abstracting them.\n\n0:49:59\tSPEAKER_03\n And that altogether could also be a common nice paper that's going to be published somewhere between the two of them, right?\n\n0:50:06\tSPEAKER_03\n And when you said Java-based belief net, you were talking about when said Rano Coffee or that are in programming.\n\n0:50:13\tSPEAKER_04\n No, it turns out that there is a, the no end of Java libraries.\n\n0:50:19\tSPEAKER_04\n Okay, and it turns out one called Java-based, which is one that people around here use a fair amount.\n\n0:50:25\tSPEAKER_04\n I have no idea whether that's, the obvious advantage of that is that you can then relatively easily get all the other Java packages for GUIs or whatever else you might want to do.\n\n0:50:37\tSPEAKER_04\n So that's, I think, why a lot of people are doing research use that.\n\n0:50:42\tSPEAKER_04\n But that may not be, I have no idea whether that's the best choice.\n\n0:50:45\tSPEAKER_04\n And there are plenty of people around students in the department who live and breathe based nets.\n\n0:50:53\tSPEAKER_04\n So.\n\n0:50:54\tSPEAKER_00\n There's the toolkit that Kevin Murphy has developed which might be useful to you.\n\n0:50:59\tSPEAKER_04\n So yeah, Kevin would be a good person to start with.\n\n0:51:03\tSPEAKER_04\n Nancy knows them well.\n\n0:51:04\tSPEAKER_04\n I don't know whether you guys have met Kevin yet or not, but yeah, this is really probably a pretty sure\n\n0:51:09\tNone\n that the, for example, this, the dialogue history is producing XML documents everywhere.\n\n0:51:18\tSPEAKER_03\n Of course, XML and the ontology that a student is constructing for me back in, in email is in oil.\n\n0:51:28\tSPEAKER_03\n It's also an XML.\n\n0:51:29\tSPEAKER_03\n So that's where a lot of knowledge about bakery is about what else about the process and stuff that is going to come from.\n\n0:51:36\tSPEAKER_03\n So it has, I owe capability and it's a job of engineering and then it'd be able to.\n\n0:51:41\tSPEAKER_04\n So yeah, we're sort of committed to XML as the kind of interchange, but that's not a big deal.\n\n0:51:48\tSPEAKER_04\n So in terms of interchange in and out of any module we build, it'll be XML.\n\n0:51:55\tSPEAKER_04\n And if you're going off the queries to the ontology, for example, you'll have to deal with its interface, but that's fine.\n\n0:52:03\tSPEAKER_04\n And all of these things have been built with much bigger projects than this in mind.\n\n0:52:12\tSPEAKER_04\n So they've worked very hard.\n\n0:52:14\tSPEAKER_04\n It's kind of blackboards and multi-way blackboards and ways of interchanging and registering.\n\n0:52:19\tSPEAKER_04\n And so that, I don't think, is even worth us worrying about just yet.\n\n0:52:26\tSPEAKER_04\n I mean, if we can get the core of the thing to work in a way that we're comparable with, and we can even out of it XML little descriptors.\n\n0:52:40\tSPEAKER_03\n I believe.\n\n0:52:41\tSPEAKER_03\n I don't see that.\n\n0:52:42\tSPEAKER_03\n I like for some of the, what you said about the getting input from just Pius about where you have the data is specified for features and so forth.\n\n0:52:51\tSPEAKER_03\n That's, of course, easy also to do with.\n\n0:52:53\tSPEAKER_04\n You can make an XML format for that, sure.\n\n0:53:01\tSPEAKER_04\n You know, feature value XML format.\n\n0:53:03\tSPEAKER_04\n Probably as good a way as any.\n\n0:53:06\tSPEAKER_04\n So it's all, yet, I guess it's also worth while you're proking around for XML packages, the things you'd like.\n\n0:53:16\tSPEAKER_03\n Does my comps just have set it back?\n\n0:53:18\tSPEAKER_03\n Yeah, sure.\n\n0:53:19\tSPEAKER_03\n The library does that.\n\n0:53:21\tSPEAKER_04\n And the question is, you have to look.\n\n0:53:24\tSPEAKER_04\n That should be, we should be able to look at that.\n\n0:53:27\tSPEAKER_04\n No, the, what I sort of came to my mind\n\n0:53:32\tSPEAKER_03\n was the notion of an idea that if there are nets that can actually try to set their own probability factors based on input, just in file format, if we get really wild on this, we may actually want to use some copper that other people made.\n\n0:53:53\tSPEAKER_03\n And for example, if they are in mate, then we get XML documents with discourse annotations.\n\n0:54:00\tSPEAKER_03\n From the discourse act down to the phonetic level, Michael has a project about recognizing discourse acts and he does it all in mate.\n\n0:54:07\tSPEAKER_03\n And so they're actually annotating data and data and data.\n\n0:54:10\tSPEAKER_03\n So if we think it's worth it one of these days, not with this first prototype, maybe with a second.\n\n0:54:16\tSPEAKER_03\n And we have the possibility of taking input that's generated elsewhere and learning from that.\n\n0:54:22\tSPEAKER_03\n That'd be nice.\n\n0:54:23\tSPEAKER_04\n It'd be nice, but I don't want to come.\n\n0:54:26\tSPEAKER_04\n I mean, you can't run your project based on the speculation that the data will come and you don't have to actually design the nets.\n\n0:54:35\tSPEAKER_04\n Just a backdoor.\n\n0:54:37\tSPEAKER_04\n It could happen.\n\n0:54:38\tSPEAKER_04\n So in terms of what the smart com gives us for M3L packages, it could be that they're fine.\n\n0:54:45\tSPEAKER_04\n It could be, you know, you don't really like it.\n\n0:54:48\tSPEAKER_04\n So we're not required to use their packages.\n\n0:54:52\tSPEAKER_04\n We are required at the end to give them stuff in their format, but hey, it doesn't control what you do in internal language.\n\n0:55:15\tNone\n What was the time frame for this?\n\n0:55:17\tNone\n Today?\n\n0:55:19\tNone\n I have to do this.\n\n0:55:20\tSPEAKER_04\n Yeah, but this week, to have you guys pick a package and tell us what it is and give us a point where you can play with it or something.\n\n0:55:34\tSPEAKER_04\n And then as soon as we have it, I think we should start trying to populate it for this problem, make a first cut, you know, what's going on.\n\n0:55:46\tSPEAKER_04\n And probably the easiest way to do that is some online way.\n\n0:55:54\tSPEAKER_04\n I mean, you can figure out what you want.\n\n0:55:55\tSPEAKER_04\n I'm going to website her.\n\n0:55:56\tNone\n You know, I was actually more joking with the 2.3.\n\n0:56:01\tNone\n It was a usual joke.\n\n0:56:05\tNone\n It was a big as long as you guys need for that.\n\n0:56:10\tSPEAKER_03\n Maybe it might be interesting if the two of you can agree on who's going to be the speaker next, but to tell us something about it, you should know what it does\n\n0:56:19\tNone\n and how it does that. Well, or about the speaker.\n\n0:56:21\tSPEAKER_03\n Yeah, or you split it up.\n\n0:56:23\tSPEAKER_03\n So there will be sort of the assignment for that.\n\n0:56:28\tSPEAKER_03\n Force lines or whatever.\n\n0:56:30\tNone\n I think that what it can do and how far you can.\n\n0:56:33\tSPEAKER_04\n Well, I like the also that I have a first cut at what the bleaching at looks like.\n\n0:56:40\tSPEAKER_04\n It's really crude.\n\n0:56:43\tSPEAKER_02\n So, you know, here are your features and whatnot.\n\n0:56:46\tNone\n Right.\n\n0:56:47\tNone\n Yeah.\n\n0:56:50\tSPEAKER_04\n And as I said, what I like to do is, I mean, what would be really great is bring it in if we could in the meeting, say, you know, here's the package here is the current one we have, what other ideas you have.\n\n0:57:06\tSPEAKER_04\n And then we can think about this idea of making up the data file of get a tentative format for it.\n\n0:57:15\tSPEAKER_04\n Let's say XML that says, you know, these are the various scenarios we can just add that.\n\n0:57:23\tSPEAKER_04\n And then we just this file of them.\n\n0:57:24\tSPEAKER_04\n And when you think you've got a better belief in it, use run it against this data file.\n\n0:57:34\tSPEAKER_01\n So we'd be like, yeah.\n\n0:57:35\tSPEAKER_01\n Oh, yeah.\n\n0:57:40\tNone\n Until we know more.\n\n0:57:42\tSPEAKER_02\n And what's the relation to this with the table so that the system works out with English?\n\n0:57:49\tSPEAKER_03\n So this is why what you are doing this, I received two lovely emails, the full and T in the whole Linux version.\n\n0:57:56\tSPEAKER_03\n There, I uploaded the most.\n\n0:58:01\tSPEAKER_03\n And I started to unpack the Linux one, the int1 work kind of has a package.\n\n0:58:06\tSPEAKER_03\n Linux, when it told me that you can't really unpack it because it's a future date.\n\n0:58:11\tNone\n So this is the tentative.\n\n0:58:12\tNone\n It's between Germany.\n\n0:58:12\tNone\n I had to wait until one of the talk was actually a new two package.\n\n0:58:17\tSPEAKER_03\n Now, then it will be my job to get this whole thing running both on suite and on this machine.\n\n0:58:24\tSPEAKER_03\n And so that we have it.\n\n0:58:27\tSPEAKER_03\n And then hopefully that hoping that my urgent message will now come through to Ralph and Tillman that it will send some more documentation along.\n\n0:58:38\tSPEAKER_03\n We, I can show, maybe that's what I will do next Monday.\n\n0:58:41\tSPEAKER_03\n I'll show the state of the system and show that.\n\n0:58:45\tSPEAKER_03\n Yeah.\n\n0:58:45\tSPEAKER_04\n So the answer to John O. is that these are at the moment separate.\n\n0:58:52\tSPEAKER_04\n One hopes is that when we understand how the analyzer works, we can both worry about converting it to English and worry about how it could extract the parameters we need for the belief net.\n\n0:59:06\tSPEAKER_02\n I guess my question is more about time frame.\n\n0:59:08\tSPEAKER_02\n So we're going to do belief nets this week.\n\n0:59:10\tSPEAKER_02\n And then, oh, yeah, I don't know.\n\n0:59:11\tSPEAKER_04\n None of this is neither of these projects has got a real tight timeline in the sense that over the next month, there's a deliverable.\n\n0:59:22\tSPEAKER_04\n So it's opportunity.\n\n0:59:23\tSPEAKER_04\n In that sense, it's opportunistic.\n\n0:59:27\tSPEAKER_04\n If we don't get any information for these guys for several weeks, then we aren't going to sit around wasting time trying to do the problem.\n\n0:59:34\tSPEAKER_04\n Or guess what?\n\n0:59:35\tSPEAKER_04\n I'm just going to do other things.\n\n0:59:40\tSPEAKER_03\n Yeah, but this point is really, I think, very valid.\n\n0:59:45\tSPEAKER_03\n But ultimately, we hope that both will merge into harmonious the wonderful state where we cannot only do very necessities, i.e. changing the tables.\n\n1:00:00\tSPEAKER_03\n That does exactly in English where it doesn't German.\n\n1:00:02\tSPEAKER_03\n But also that we can sort of have the system where we can say, OK, this is what it usually does.\n\n1:00:07\tSPEAKER_03\n And now we add this little thing to it, whatever John knows of musk or us, great belief net.\n\n1:00:14\tSPEAKER_03\n We plug it in.\n\n1:00:15\tSPEAKER_03\n And then for these certain tasks, and we know that navigational tasks are going to be core domain of the new system, it all of a sudden it does much better.\n\n1:00:24\tSPEAKER_03\n And because it produced better answers, tell the person, as I showed you on this map, produce either a red line that goes to the vista point or a red line that goes to the tango point or a red line that goes to the door, which would be great.\n\n1:00:40\tSPEAKER_03\n So you don't only can you show that you know something sensible, but ultimately, if you produce a system like this, it takes the person world points to go, rather than taking them always to the geometric center of a building, which is what they do now.\n\n1:00:56\tSPEAKER_03\n And we even had to take out, the men's you missed that part.\n\n1:00:59\tSPEAKER_03\n We had to take out a bit of the road work so that it doesn't take you to the wall.\n\n1:01:05\tSPEAKER_03\n Every time.\n\n1:01:06\tSPEAKER_03\n Yeah, really?\n\n1:01:07\tSPEAKER_03\n So this was actually an actual problem that we encountered, which is your way up.\n\n1:01:15\tSPEAKER_03\n Because current navigation systems don't really care.\n\n1:01:18\tSPEAKER_03\n They get you to the beginning of the street, somehow do the house number.\n\n1:01:22\tSPEAKER_03\n But even that is problematic.\n\n1:01:24\tSPEAKER_03\n If you go, if you want to drive to the SAP in Balthorff, I'm sure the same is true with Microsoft.\n\n1:01:29\tSPEAKER_03\n It takes you to the address, whatever street number blah, blah, blah.\n\n1:01:34\tSPEAKER_03\n I'm miles away from the entrance.\n\n1:01:37\tSPEAKER_03\n Because this postal address is maybe a mailbox somewhere.\n\n1:01:42\tSPEAKER_03\n But the entrance where you actually want to go is something completely different.\n\n1:01:46\tSPEAKER_03\n So unless you're a mail person, you really don't want to go.\n\n1:01:51\tSPEAKER_04\n Probably not then, because you probably can't drop the mail there anyway.\n\n1:01:54\tSPEAKER_04\n I don't even know if you're better.\n\n1:02:03\tNone\n Clear?\n\n1:02:04\tNone\n OK.\n\n1:02:04\tNone\n That's it.\n\n1:02:12\tSPEAKER_02\n Outer towers made of red limestone.\n\n1:02:16\tSPEAKER_00\n I was wondering.\n\n1:02:17\tSPEAKER_00\n Do you want to see a picture?\n\n1:02:18\tSPEAKER_03\n Sure.\n\n1:02:20\tSPEAKER_03\n After we boot for that, though.\n\n1:02:22\tSPEAKER_00\n So you two who will be working on this, I mean, are you supposed to just do it by thinking about the station?\n\n1:02:30\tSPEAKER_00\n Can you use the sample data?\n\n1:02:31\tSPEAKER_00\n Of course, I do just sample data.\n\n1:02:33\tSPEAKER_00\n Is there more than there are lots of sample data that\n\n1:02:36\tSPEAKER_03\n is beyond what you have there? I think this is an in part my job to look at that and to see whether there are features in there that can be extracted.\n\n1:02:48\tSPEAKER_03\n And to come up with some features that are not purely based on a real experiment or on reality, but sort of on pure intuition of real, this is maybe a sign for that.\n\n1:03:02\tSPEAKER_03\n And this is maybe a sign for this.\n\n1:03:11\tSPEAKER_01\n So later this being should get together.\n\n1:03:14\tSPEAKER_01\n Talked down.\n\n1:03:15\tSPEAKER_01\n Let's see what we look at.\n\n1:03:19\tSPEAKER_04\n OK, we can end the meeting and call Adam.\n\n1:03:25\tSPEAKER_04\n And then we want to look at some filthy pictures of high version.\n\n1:03:29\tSPEAKER_04\n We can do that as well.\n\n1:03:32\tSPEAKER_03\n Is that OK?\n\n1:03:32\tSPEAKER_03\n They used the ammunition.\n\n1:03:34\tSPEAKER_03\n They start the ammunition in that tower.\n\n1:03:36\tSPEAKER_03\n And that's why when it was hit by a cannon ball.\n\n1:03:45\tSPEAKER_02\n That's what they call it, the powder tower.\n\n1:03:49\tSPEAKER_02\n At this time, I had so many material.\n\n1:03:50\tSPEAKER_02\n That's why I asked.\n\n1:03:53\tSPEAKER_00\n That's right.\n\n1:03:53\tSPEAKER_00\n OK.\n\n1:03:55\tNone\n The little synthrodotter, powder metal.\n\n1:03:58\tNone\n Hi there, we're done.\n\n1:04:09\tSPEAKER_00\n Is there a lot of ontological information available about the very time marks then?\n\n1:04:14\tSPEAKER_00\n And it's presumably what title we're doing.\n\n1:04:18\tSPEAKER_00\n I mean, we have this example of post office behind that.\n\n1:04:22\tSPEAKER_00\n Well, I think whether post office is going to be\n\n1:04:25\tNone\n building a contextual feature, which we determined would have wanted to look at it.\n\n1:04:29\tSPEAKER_00\n Yep.\n\n1:04:30\tSPEAKER_00\n So going to those facts about that.\n\n1:04:33\tSPEAKER_00\n No.\n\n1:04:34\tSPEAKER_04\n Well, Robert knows exactly what they're going to be.\n\n1:04:36\tSPEAKER_04\n But the point is that, again, for our purposes, the probability to be six or seven buildings that will drop in the dialogue and we will put as much in as we need.\n\n1:04:44\tSPEAKER_04\n And if that turns out to be critical, that's part of what we learned is, OK, we're going to mean that kind of information in order to do this.\n\n1:04:56\tSPEAKER_04\n Oh, Nancy didn't read her numbers yet.\n\n1:04:58\tSPEAKER_04\n Sorry.\n\n1:05:01\tSPEAKER_04\n You love me, Andy?\n\n1:05:02\tSPEAKER_05\n Yeah.\n\n1:05:03\tSPEAKER_05\n So it's been a little bit more since then.\n\n1:05:05\tSPEAKER_05\n I think every time.\n\n1:05:07\tSPEAKER_04\n Let's see.\n\n1:05:08\tSPEAKER_04\n So you turned off your rhythm for two seconds.\n\n1:05:10\tSPEAKER_05\n I turned it off for two seconds.\n\n1:05:11\tSPEAKER_05\n So I have to turn off the strings like this.\n\n1:05:13\tSPEAKER_04\n Right.\n\n1:05:13\tSPEAKER_04\n OK.\n\n", "summary": [{"summary_text": "The participants are supposed to read digits at the same time. Speaker is going to read the strings of digits strings first and then they can go on from there. The transcript is 3, 6, 5, 1, 3, 3.6, 6. 6, 7, 7. 7, 9, 9. 7. 9, 2, 4, 4. 5, 6 and 7."}]}