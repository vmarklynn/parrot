{"filename": "./data/Bmr029.txt", "transcript": "0:00:00\tSPEAKER_01\n Okay, so the one, one thing I knew I wanted to talk about was about sort of last minute stuff to try to get some recognition results.\n\n0:00:13\tSPEAKER_05\n Recognition results?\n\n0:00:14\tSPEAKER_01\n Yeah, so on needing data.\n\n0:00:19\tSPEAKER_01\n And so I'm not sure exactly what you're doing already and the stuff I talked today.\n\n0:00:24\tSPEAKER_05\n But recognition on the antelis segments, which was about using the closed-talk microphone.\n\n0:00:34\tSPEAKER_05\n Okay, so I know you wanted the five field.\n\n0:00:37\tSPEAKER_01\n Right, so we have some stuff with no overlap for which there would be near-near field results.\n\n0:00:46\tSPEAKER_01\n We wanted to get the far field results for that.\n\n0:00:48\tSPEAKER_01\n And then this real long shot thing would be that we'd apply Dave's processing to potentially training and test data and do the look at the same thing.\n\n0:01:02\tSPEAKER_01\n And in talking this morning with Chuck and Dave, one thought was to use, we couldn't remember how different the numbers were, but if you just worked with males only and use the short training, there, I think Chuck's recollection was that when he was doing the future stuff, it took maybe a day and a half to do the training.\n\n0:01:21\tSPEAKER_01\n Yeah.\n\n0:01:22\tSPEAKER_05\n Yeah.\n\n0:01:23\tSPEAKER_05\n That's about right.\n\n0:01:26\tSPEAKER_05\n Actually, it should probably be, depending on who else is using machines, but we have a machine.\n\n0:01:39\tSPEAKER_05\n Probably.\n\n0:01:40\tSPEAKER_01\n How much worse is the short training set than the large one in terms of ultimate performance?\n\n0:01:47\tSPEAKER_05\n Like, 3%, 3%, 4%, 4% absolute.\n\n0:01:56\tSPEAKER_01\n So that should be fine for this, I think.\n\n0:01:59\tSPEAKER_01\n So you have short training results for the close case?\n\n0:02:07\tSPEAKER_05\n Not for meetings, because we didn't train.\n\n0:02:10\tSPEAKER_05\n We didn't ever recognize with the small models on meeting data.\n\n0:02:19\tSPEAKER_05\n But I have the model, so I could.\n\n0:02:22\tSPEAKER_01\n So how do you know it's 3% on Hub 5?\n\n0:02:26\tSPEAKER_01\n I see.\n\n0:02:27\tSPEAKER_01\n Yeah, but we have the model, so we could get that number.\n\n0:02:31\tSPEAKER_01\n And so the question is, what?\n\n0:02:33\tSPEAKER_05\n I mean, the recognition also takes a lot of time, so we might want to restrict it to maybe a few meetings if you want to do a full comparison.\n\n0:02:49\tSPEAKER_01\n It has to be enough so that, I mean, it's not overlap only.\n\n0:02:56\tSPEAKER_01\n And it has to be enough to be sort of comparable to what you folks were seeing and what you reported already.\n\n0:03:04\tNone\n I mean.\n\n0:03:05\tSPEAKER_05\n Well, do we have the process data that's also?\n\n0:03:13\tSPEAKER_01\n No, he has to create that.\n\n0:03:16\tSPEAKER_01\n So we have a whole parallel set of things over here, which are all with digits.\n\n0:03:23\tSPEAKER_01\n And Dave has been working with that, and there's all those issues.\n\n0:03:28\tSPEAKER_01\n But I know that if I go in with something that's not just digits, it would be good.\n\n0:03:34\tSPEAKER_01\n And so we already have these results that you, I mean, on a particular test set that you reported at HLT.\n\n0:03:46\tSPEAKER_01\n Right.\n\n0:03:47\tSPEAKER_01\n It'd be nice to have something more than that.\n\n0:03:49\tSPEAKER_01\n And we had talked about was having distant.\n\n0:03:53\tSPEAKER_01\n And then if we could on top of that, I mean, so this is going to be a lot worse.\n\n0:03:58\tSPEAKER_01\n Right.\n\n0:03:59\tSPEAKER_01\n Whatever comparison we want one would assume.\n\n0:04:01\tSPEAKER_01\n But we don't know how much worse, which is certainly one interesting thing.\n\n0:04:06\tSPEAKER_01\n And then Dave, I think we figured that it would probably take a day or two to compute the, well, how many hours to train?\n\n0:04:20\tSPEAKER_05\n I did retrain.\n\n0:04:21\tSPEAKER_05\n I recently retrained for another reason on the full training set.\n\n0:04:28\tSPEAKER_05\n And that took only two days.\n\n0:04:33\tSPEAKER_05\n Yeah.\n\n0:04:34\tSPEAKER_05\n So it's actually conceivable to do use the full training set.\n\n0:04:39\tSPEAKER_01\n Yeah, but we also have to do this other processing.\n\n0:04:41\tSPEAKER_01\n So having a smaller training set, if it's only a few percent difference, it might be.\n\n0:04:45\tSPEAKER_01\n Oh, you were just doing it.\n\n0:04:46\tSPEAKER_01\n But how big is the small training set?\n\n0:04:53\tSPEAKER_05\n Do you remember?\n\n0:04:57\tSPEAKER_05\n I know something like something between 30 and 50 hours.\n\n0:05:12\tSPEAKER_05\n Maybe I forget the.\n\n0:05:13\tSPEAKER_01\n It's around there.\n\n0:05:15\tSPEAKER_01\n And Mayo is roughly half of that or.\n\n0:05:17\tSPEAKER_01\n Or is that only me?\n\n0:05:19\tSPEAKER_05\n Actually, I don't know.\n\n0:05:22\tSPEAKER_05\n I can look it up.\n\n0:05:24\tSPEAKER_05\n It's just, I don't know, just remember the number.\n\n0:05:28\tSPEAKER_02\n We could only just do the Mayo only, right?\n\n0:05:31\tSPEAKER_05\n Or well, the Mayo account for most of this meeting that I need help.\n\n0:05:35\tSPEAKER_05\n Yeah.\n\n0:05:36\tSPEAKER_05\n Yeah, I would say we do only males.\n\n0:05:39\tSPEAKER_00\n Yeah.\n\n0:05:40\tSPEAKER_01\n So, yeah, so that's certainly part of the issue is that right now he hasn't written his stuff for efficiency.\n\n0:05:48\tSPEAKER_01\n It's in Matt Lab and so on.\n\n0:05:51\tSPEAKER_01\n And it's not an impossible amount of time.\n\n0:05:54\tSPEAKER_01\n We were guesstimating.\n\n0:05:55\tSPEAKER_01\n It was like one and a half times faster than real time or something.\n\n0:06:00\tSPEAKER_01\n So if there's 30 hours of data, you can calculate, you can do with the enhancement in a day, or something.\n\n0:06:10\tSPEAKER_01\n But if we were dealing with 200 hours or something, I think it would be prohibitive.\n\n0:06:14\tSPEAKER_05\n No, it's definitely less than 100 hours, for sure.\n\n0:06:18\tSPEAKER_05\n Yeah.\n\n0:06:19\tSPEAKER_05\n It's probably actually, I think it's around 30 hours.\n\n0:06:25\tSPEAKER_05\n Yeah.\n\n0:06:26\tSPEAKER_01\n It's a much gender.\n\n0:06:27\tSPEAKER_01\n Yeah.\n\n0:06:28\tSPEAKER_01\n Yeah.\n\n0:06:29\tSPEAKER_01\n So I mean, it's a bit of a push, but it seems like, okay, we've got some models, we've got some training data, we have software that works.\n\n0:06:34\tSPEAKER_01\n He's got a method that helps with other tasks.\n\n0:06:40\tSPEAKER_01\n It appears to be debugged.\n\n0:06:44\tSPEAKER_02\n So one thing I was wondering is, did you already do that middle one, or should we redo that one too?\n\n0:06:50\tSPEAKER_05\n No, I didn't do that.\n\n0:06:52\tSPEAKER_05\n We haven't even cut the waveforms.\n\n0:06:55\tSPEAKER_05\n Yeah, that's what I was going to say next.\n\n0:06:57\tSPEAKER_02\n We have to cut the, so Morgan is the plan to just pick one of the Carfield mics.\n\n0:07:04\tSPEAKER_05\n And there's a bit of a course, whether you want to use what segmentations you want to use.\n\n0:07:11\tSPEAKER_05\n David just, sorry, Dom just created a new version of the first meetings that we have previously recognized, but with different segmentations.\n\n0:07:24\tSPEAKER_05\n So it would be nice, I mean, if the results are comparable to what we had before, to use those segmentations, because then we could claim that everything's automatic.\n\n0:07:39\tSPEAKER_02\n Do you know when he'll have the comparisons?\n\n0:07:44\tSPEAKER_05\n Well, as I said, I just started the recognizer.\n\n0:07:55\tSPEAKER_05\n It will probably be a couple of hours before we'll start that.\n\n0:07:58\tSPEAKER_01\n Oh, okay.\n\n0:07:59\tSPEAKER_02\n Because I don't think the new data will be ready for a couple of days.\n\n0:08:07\tSPEAKER_01\n Me and the training.\n\n0:08:08\tSPEAKER_05\n The training.\n\n0:08:09\tSPEAKER_05\n But the segmentations matter for the filtering, right, because for the test.\n\n0:08:17\tSPEAKER_05\n But first, of course, you wouldn't want to process the training data because you want to get that started.\n\n0:08:22\tSPEAKER_05\n Yeah.\n\n0:08:23\tSPEAKER_01\n I mean, it'd be really great if it was all automatic, but I think that given the pressure of time, since you're going to find out in the short amount of time, that's great.\n\n0:08:33\tSPEAKER_01\n But if it doesn't work out, I think it would rather charge ahead with the older segmentations.\n\n0:08:38\tSPEAKER_01\n And we were going to use one of the PCMs.\n\n0:08:42\tSPEAKER_01\n I don't know.\n\n0:08:43\tSPEAKER_01\n Probably whatever one you've been using for the digits, is it this one?\n\n0:08:47\tSPEAKER_01\n F.\n\n0:08:48\tSPEAKER_01\n Which, that's F?\n\n0:08:49\tSPEAKER_01\n That's F.\n\n0:08:50\tSPEAKER_02\n How do you know?\n\n0:08:51\tSPEAKER_02\n It's the second nearest machine.\n\n0:08:54\tSPEAKER_01\n Oh.\n\n0:08:55\tSPEAKER_01\n The answer.\n\n0:08:57\tSPEAKER_01\n Okay.\n\n0:08:58\tSPEAKER_01\n All right.\n\n0:08:59\tSPEAKER_01\n So.\n\n0:09:00\tSPEAKER_01\n Let's go with the whole family.\n\n0:09:03\tSPEAKER_01\n Okay.\n\n0:09:04\tSPEAKER_02\n So, I just want to make sure I understand what we need to run.\n\n0:09:11\tSPEAKER_02\n Let's see.\n\n0:09:15\tSPEAKER_02\n So it's, okay.\n\n0:09:16\tSPEAKER_02\n So if we're talking about, let's assume that we're going to use the new segmentations.\n\n0:09:21\tSPEAKER_02\n We need to run recognition, just looking at the no overlap column.\n\n0:09:28\tSPEAKER_02\n Basically, we have to do recognitions for all three of those cases, right?\n\n0:09:34\tSPEAKER_02\n Because we're going to be using the just the male model, short training set for the male.\n\n0:09:39\tSPEAKER_02\n So we need to have results for all three of those.\n\n0:09:41\tSPEAKER_05\n Even though we have...\n\n0:09:42\tSPEAKER_05\n Maybe we should limit ourselves to the meeting recorder.\n\n0:09:45\tSPEAKER_05\n Meetings.\n\n0:09:46\tSPEAKER_02\n Okay.\n\n0:09:47\tSPEAKER_05\n If you were going to cut out the notes, I would take a switch.\n\n0:09:51\tSPEAKER_01\n Well, maybe.\n\n0:09:52\tSPEAKER_01\n But I mean, how long does it take?\n\n0:09:54\tSPEAKER_05\n Actually, for the test.\n\n0:09:55\tSPEAKER_05\n For the test, the meetings take longer because this is one speaker who talks a lot.\n\n0:09:59\tSPEAKER_05\n And so, the...\n\n0:10:01\tSPEAKER_05\n No, it's because for all the adaptation and normalization steps, you cannot...\n\n0:10:06\tSPEAKER_05\n You have to...\n\n0:10:07\tSPEAKER_05\n You cannot chop it up in small pieces.\n\n0:10:12\tSPEAKER_05\n So you're sort of limited by how long the longer speaker is speaking.\n\n0:10:18\tSPEAKER_05\n So how much data there is from the speaker who talks the most.\n\n0:10:23\tSPEAKER_05\n So you parallelize across different speakers, but if you have a bunch of speakers who speak very little and then one speaks a lot, then effectively everybody waits for the longest one to process.\n\n0:10:37\tSPEAKER_01\n But what was your result for that we had at the HLT?\n\n0:10:41\tSPEAKER_01\n Was that a combination?\n\n0:10:42\tSPEAKER_05\n That was both types of meetings, but there were only two robustus meetings and four or fives.\n\n0:10:47\tSPEAKER_02\n And we're redoing the baseline.\n\n0:10:48\tSPEAKER_02\n We need workers.\n\n0:10:49\tSPEAKER_02\n It would be okay, right?\n\n0:10:51\tSPEAKER_02\n Right.\n\n0:10:52\tSPEAKER_02\n So we're going to just limit ourselves to this.\n\n0:10:54\tSPEAKER_02\n How long would it take to run recognition if we did that?\n\n0:10:59\tNone\n I don't have...\n\n0:11:03\tSPEAKER_05\n I mean, is it like a day or is it a few hours of rough?\n\n0:11:10\tSPEAKER_05\n For everything?\n\n0:11:11\tSPEAKER_05\n For all the meetings?\n\n0:11:12\tSPEAKER_02\n Yeah, let's say we just did the meeting recorder meetings for our test set.\n\n0:11:17\tSPEAKER_05\n It's probably more than a day, but probably less than two.\n\n0:11:25\tSPEAKER_02\n Oh, really?\n\n0:11:26\tSPEAKER_02\n I didn't realize each test at that long.\n\n0:11:27\tSPEAKER_05\n Well, no, I mean for all the meetings.\n\n0:11:30\tSPEAKER_05\n Because it's...\n\n0:11:32\tSPEAKER_05\n So you're doing each meeting takes something like...\n\n0:11:41\tSPEAKER_05\n Again, when we ran these, we were sort of short of machines.\n\n0:11:46\tSPEAKER_05\n I don't know.\n\n0:11:49\tSPEAKER_05\n I would estimate maybe four hours per meeting.\n\n0:11:53\tSPEAKER_05\n Something like that.\n\n0:11:54\tSPEAKER_03\n Four hours per meeting.\n\n0:11:56\tSPEAKER_03\n Right.\n\n0:11:57\tSPEAKER_01\n Yeah, but if you...\n\n0:11:59\tSPEAKER_01\n So if you do have dozen meetings, that's about a day.\n\n0:12:03\tSPEAKER_01\n We also have more machines now.\n\n0:12:05\tSPEAKER_05\n Right.\n\n0:12:06\tSPEAKER_05\n So that's why I'm saying I'm not sure how that would scale with more machines.\n\n0:12:09\tSPEAKER_01\n Yeah.\n\n0:12:10\tSPEAKER_01\n I mean, if we had about six hour test sets, not bad, right?\n\n0:12:15\tSPEAKER_00\n Six minutes.\n\n0:12:16\tSPEAKER_02\n Six meetings.\n\n0:12:17\tSPEAKER_02\n Okay.\n\n0:12:18\tSPEAKER_02\n Right.\n\n0:12:19\tSPEAKER_01\n I mean, a lot of the evaluations have been...\n\n0:12:22\tSPEAKER_01\n We have MR2C4.\n\n0:12:23\tSPEAKER_05\n I think there are four meeting recorder meetings that we worked with.\n\n0:12:31\tSPEAKER_01\n Four that you worked with?\n\n0:12:32\tSPEAKER_04\n Because at the same set as the alignment, I think it's five-five.\n\n0:12:35\tSPEAKER_04\n You're going to meetings?\n\n0:12:36\tSPEAKER_01\n Okay.\n\n0:12:37\tSPEAKER_01\n That would be okay, too.\n\n0:12:38\tSPEAKER_01\n I mean, so if they have a set that they worked with, then you got...\n\n0:12:41\tSPEAKER_01\n Did you do similarly in performance between them and the other meetings?\n\n0:12:44\tSPEAKER_01\n Or was it...\n\n0:12:45\tSPEAKER_01\n With a robustness?\n\n0:12:46\tSPEAKER_05\n Yeah.\n\n0:12:47\tSPEAKER_05\n The big variation is by whether it's a native speaker or not.\n\n0:12:54\tSPEAKER_05\n Yeah.\n\n0:12:55\tSPEAKER_05\n And whether it's...\n\n0:12:58\tSPEAKER_05\n I think that's the...\n\n0:13:00\tSPEAKER_05\n And then of course, what...\n\n0:13:02\tSPEAKER_05\n You know, whether it's lapel or...\n\n0:13:05\tSPEAKER_05\n Headset microphone.\n\n0:13:07\tSPEAKER_05\n And overlap or not.\n\n0:13:09\tSPEAKER_01\n Yeah.\n\n0:13:10\tSPEAKER_01\n So, maybe just the meeting recorder set of the...\n\n0:13:14\tSPEAKER_01\n And we can exclude...\n\n0:13:15\tSPEAKER_05\n We don't need to record.\n\n0:13:16\tSPEAKER_05\n It has the...\n\n0:13:17\tSPEAKER_05\n Not natives, because we know that...\n\n0:13:19\tSPEAKER_05\n I mean, in fact, we excluded them previously.\n\n0:13:21\tSPEAKER_01\n Yeah, so we want to do the same thing.\n\n0:13:23\tSPEAKER_02\n Okay, so...\n\n0:13:24\tSPEAKER_02\n Alright, so if we got a list of the segmentations for these five meeting recorder meetings, we could start the first two experiments going right away using the short male models.\n\n0:13:39\tSPEAKER_02\n So we can get those going while Dave is creating waveforms for the retraining the short male models.\n\n0:13:49\tSPEAKER_01\n Once we know which segmentation is reducing.\n\n0:13:51\tSPEAKER_02\n Right.\n\n0:13:52\tSPEAKER_02\n Okay.\n\n0:13:53\tSPEAKER_02\n Okay.\n\n0:13:54\tNone\n And then...\n\n0:13:55\tSPEAKER_02\n Okay.\n\n0:13:56\tSPEAKER_02\n So do we also want to run that bottom experiment without retraining the short male models on this thing?\n\n0:14:04\tSPEAKER_02\n Did you want that?\n\n0:14:06\tSPEAKER_01\n Yeah.\n\n0:14:09\tSPEAKER_01\n I agree that that would be an interesting thing to do, but I sort of regard it as secondary.\n\n0:14:13\tSPEAKER_01\n So if there's sort of machines sitting around and people sitting around and they're waiting for other things to finish, then sure.\n\n0:14:20\tSPEAKER_01\n But Chuck had been asking about that earlier.\n\n0:14:23\tSPEAKER_01\n It's kind of a control to know.\n\n0:14:26\tSPEAKER_01\n Because, I mean, you could imagine a fantasy in which you said that Dave's processing made the...\n\n0:14:32\tSPEAKER_01\n a far microphone like the near microphone in which case you shouldn't actually have to retrain.\n\n0:14:38\tSPEAKER_01\n But it's not really true.\n\n0:14:40\tSPEAKER_01\n It's sort of fantasy.\n\n0:14:41\tSPEAKER_01\n It does block up the data in some funny ways.\n\n0:14:44\tSPEAKER_01\n And so I'm kind of questioning that.\n\n0:14:47\tSPEAKER_01\n But...\n\n0:14:48\tSPEAKER_02\n Well, in a more basic level also it means that that third experiment, there are actually two differences between the other experiments, not one.\n\n0:14:54\tSPEAKER_02\n Right.\n\n0:14:55\tSPEAKER_02\n So it's hard to know.\n\n0:14:56\tSPEAKER_01\n It involves retraining and it involves a...\n\n0:14:58\tSPEAKER_01\n Right.\n\n0:14:59\tSPEAKER_01\n That's right.\n\n0:15:01\tSPEAKER_01\n I mean, the other thing which you might come into is if there was some problem in the retraining, maybe you just have some mechanical thing we do wrong.\n\n0:15:13\tSPEAKER_01\n Right.\n\n0:15:15\tSPEAKER_01\n That since Dave's experience was that it didn't help as much if you didn't retrain, that it does help some.\n\n0:15:24\tSPEAKER_01\n That we would hopefully see that.\n\n0:15:27\tSPEAKER_01\n So that's true.\n\n0:15:29\tSPEAKER_05\n So when you use original models and you just process the test set in this way, do you get any...\n\n0:15:40\tSPEAKER_05\n do you get decent performance or not?\n\n0:15:42\tSPEAKER_03\n I think for the far mic HDK system I was using, it did help somewhat.\n\n0:15:49\tSPEAKER_03\n I could recheck that but it was such a bad baseline that I don't know what that means.\n\n0:15:54\tSPEAKER_05\n Right.\n\n0:15:55\tSPEAKER_03\n Okay. Because the baseline word error rate was around 40% on digits.\n\n0:16:00\tSPEAKER_02\n On the far field.\n\n0:16:02\tSPEAKER_03\n Right.\n\n0:16:07\tSPEAKER_01\n Right.\n\n0:16:08\tSPEAKER_01\n So...\n\n0:16:09\tSPEAKER_05\n Okay, well I can get started on the...\n\n0:16:12\tSPEAKER_05\n well the first...\n\n0:16:14\tSPEAKER_05\n the one that already has a...\n\n0:16:16\tSPEAKER_05\n Oh, there.\n\n0:16:17\tSPEAKER_05\n We need to redo that with small models.\n\n0:16:19\tSPEAKER_05\n Right.\n\n0:16:20\tSPEAKER_05\n And then I have to ask, I guess, Dawn to cut the...\n\n0:16:27\tSPEAKER_05\n cut the segments for the two distant like...\n\n0:16:33\tSPEAKER_05\n So we would be using the same channel for each... for everything?\n\n0:16:38\tSPEAKER_05\n Yeah.\n\n0:16:39\tSPEAKER_05\n Okay.\n\n0:16:40\tSPEAKER_01\n I mean, do you have to rely on your segmentations at all to do the...\n\n0:16:43\tSPEAKER_05\n No, no, we would use the same segmentations but we need to extract...\n\n0:16:47\tSPEAKER_05\n Oh, okay.\n\n0:16:50\tSPEAKER_02\n So when you said you were going to start that top one, were you going to use the new segmentations?\n\n0:16:59\tSPEAKER_05\n Yeah.\n\n0:17:01\tSPEAKER_05\n If assuming that the performance turns out to be comparable with...\n\n0:17:06\tSPEAKER_05\n with the old experiments.\n\n0:17:09\tSPEAKER_05\n Right.\n\n0:17:10\tSPEAKER_05\n And the old segmentations.\n\n0:17:12\tSPEAKER_05\n Now there's the issue of...\n\n0:17:15\tSPEAKER_05\n Oh, okay. So there's the issue of speaker normalization.\n\n0:17:18\tSPEAKER_05\n So with the distant microphone, you wouldn't know which speaker is talking.\n\n0:17:25\tSPEAKER_05\n Right.\n\n0:17:28\tSPEAKER_01\n We talked about this before.\n\n0:17:30\tSPEAKER_01\n I think what we were saying was that...\n\n0:17:34\tSPEAKER_01\n the very fact that in both cases we're ignoring the overlap section means that...\n\n0:17:42\tSPEAKER_01\n we're to some extent finessing that.\n\n0:17:45\tSPEAKER_01\n So I think for the purposes of just determining whether a fire field microphone...\n\n0:17:51\tSPEAKER_01\n what the effect of the fire field microphone is we should do the same to both.\n\n0:17:56\tSPEAKER_05\n That's it.\n\n0:17:57\tSPEAKER_05\n So you want to cheat?\n\n0:18:03\tSPEAKER_01\n We want to incorporate certain data that would not be available during final tests...\n\n0:18:10\tSPEAKER_01\n under a full fair test of it, much as we are in the...\n\n0:18:15\tSPEAKER_05\n All the numbers we have so far.\n\n0:18:17\tSPEAKER_05\n We have speakers as...\n\n0:18:20\tSPEAKER_05\n in a way that's compatible with the closed-talking.\n\n0:18:24\tSPEAKER_01\n Yeah. We'd simply wanted to determine what's the difference in performance due to it being distant versus those.\n\n0:18:29\tSPEAKER_02\n So does that mean you turn off speaker normalization if you let in?\n\n0:18:33\tSPEAKER_03\n No, it means...\n\n0:18:35\tSPEAKER_05\n you group together the segments that by magic you know belong to one speaker.\n\n0:18:44\tSPEAKER_01\n I mean to a lesser extent you had that same magic the other way too, because you have leakage into other microphones, right?\n\n0:18:50\tSPEAKER_01\n But it's just you're using the fact that this is where this person is, right?\n\n0:18:55\tSPEAKER_05\n But it's just easier to do.\n\n0:18:59\tSPEAKER_05\n Well in the new test actually, that's not true.\n\n0:19:02\tSPEAKER_05\n Again, if these new segmentations work okay, then it's a fair...\n\n0:19:10\tSPEAKER_05\n it's a completely fair...\n\n0:19:12\tSPEAKER_01\n So how do you determine what you use to group together to be a...\n\n0:19:16\tSPEAKER_05\n You group together all the data coming in through one channel and where Pilos speech detector has determined that there is speech and that speech is deemed to come from that speaker, whether that's true or not.\n\n0:19:31\tSPEAKER_05\n So if you get some crosstalk from another microphone then you just processes it as if it were from that speaker.\n\n0:19:38\tSPEAKER_01\n The only other alternative would be to turn off speaker adaptation in both.\n\n0:19:42\tSPEAKER_05\n Well that's more of a problem.\n\n0:19:44\tSPEAKER_05\n I mean because it's...\n\n0:19:45\tSPEAKER_05\n you can just pretend it's some kind of...\n\n0:19:49\tSPEAKER_05\n I mean you can pretend it's all from one speaker and do all this processing the same but then you're going to get results that are worse on a kind of not doing proper speaker normalization and you're going to have...\n\n0:20:01\tSPEAKER_05\n So you could certainly do better than that by doing, for instance, cluster the segments which is what we do, say in a broadcast news system where you don't have speaker labels.\n\n0:20:11\tSPEAKER_05\n But that would be another processing step that I would have to debug first and so forth and so we want to avoid that.\n\n0:20:19\tSPEAKER_05\n So I agree with you.\n\n0:20:20\tSPEAKER_05\n We should do the...\n\n0:20:22\tSPEAKER_05\n you know, this sort of cheating experiment.\n\n0:20:26\tSPEAKER_01\n Yeah, and so that will tell us what the difference is between the mics and then in order to...\n\n0:20:33\tSPEAKER_01\n the other difference that we'd have to take care of is that, yeah, we don't have a mic that is particular to a person and so we'll have to do some clustering and that'll be another issue too.\n\n0:20:51\tSPEAKER_01\n But it... I could be wrong but it seems to me that the speaker, the level of degradation that you get from having the distant mic in a normal acoustic is much greater than what you get from, say, not applying speaker adaptation or applying speaker adaptation.\n\n0:21:12\tSPEAKER_01\n I think that the... I mean we'll see but I think that the kind of gains that we've seen from speaker adaptation and how five sort of things are like a few percent, right?\n\n0:21:22\tSPEAKER_05\n And it's not just speaker adaptation, it's the whole no feature normalization process.\n\n0:21:27\tSPEAKER_05\n It's all that is speaker-based.\n\n0:21:30\tSPEAKER_05\n You know, so we...\n\n0:21:33\tSPEAKER_05\n So in that, I'm...\n\n0:21:37\tSPEAKER_05\n you know, the most important, of course, is the capital means attraction.\n\n0:21:42\tSPEAKER_05\n Yeah, and that... I don't know if we... we never really... I don't remember because it's so far...\n\n0:21:49\tSPEAKER_05\n it's so long ago that we didn't do that on that per speaker basis.\n\n0:21:52\tSPEAKER_01\n It doesn't make that much difference, I think.\n\n0:21:54\tSPEAKER_01\n I would doubt that it would be a huge amount of difference for that.\n\n0:21:57\tSPEAKER_01\n So, I mean, I think that that difference would definitely be marginal.\n\n0:22:01\tSPEAKER_01\n I think the main thing is to do something, to do some extra means attraction at some level.\n\n0:22:05\tSPEAKER_01\n And so it's different about this processing, just that we're doing it a much longer time scale, right?\n\n0:22:11\tSPEAKER_01\n But...\n\n0:22:16\tSPEAKER_05\n And by the way, it's... actually, we're already...\n\n0:22:20\tSPEAKER_05\n if we use the same segmentations that we use for the first talking microphone, then the segmentations assume that we have access to all channels and cross...\n\n0:22:28\tSPEAKER_05\n That's right.\n\n0:22:29\tSPEAKER_05\n...parallel them.\n\n0:22:30\tSPEAKER_05\n So there's no point in not using that knowledge for speaking about that.\n\n0:22:35\tSPEAKER_03\n I think also for the log-special means attraction, we want to know which speakers talking when, because we want to chain together the audio from one particular speaker to calculate the mean and subtract it, and we don't...\n\n0:22:48\tSPEAKER_01\n Right. Right.\n\n0:22:50\tSPEAKER_01\n Okay.\n\n0:22:53\tSPEAKER_01\n Yeah, I guess.\n\n0:22:55\tSPEAKER_01\n But I also think that, again, once we got into it, that using some kind of clustering, we probably work reasonably well there, too.\n\n0:23:06\tSPEAKER_01\n Certainly for the two microphone case, which we're not going to mess with, because it's another whole deal with the locality microphones, we ought to be able to at least tell that it appears that things are coming from a particular direction.\n\n0:23:21\tSPEAKER_01\n So we ought to be able to use that information as well.\n\n0:23:25\tSPEAKER_01\n So I think we might be able to do not too bad a job of separating out segments that appear to come from a single speaker, both in terms of acoustic similarity and in terms of direction.\n\n0:23:40\tSPEAKER_01\n So I mean, but that's another research thing to do, and probably won't get done the next week.\n\n0:23:46\tSPEAKER_05\n Right, so what is this schedule here?\n\n0:23:49\tSPEAKER_01\n Well, I mean, I'm leaving for the New Orleans meeting next Saturday, and we kind of nice to have some results, at least a day or two before that, so that I could figure out what to say.\n\n0:24:03\tSPEAKER_01\n I'll call you when you get there.\n\n0:24:05\tSPEAKER_02\n You'll have email, right?\n\n0:24:08\tSPEAKER_01\n Yeah.\n\n0:24:10\tSPEAKER_01\n Not to mention that Mari's putting together this report next week, too.\n\n0:24:15\tSPEAKER_01\n So what we were hoping was that over the weekend, we could do the calculation on the training set, and maybe we could, by the end of the weekend, we could have the top one, and then early next week do these.\n\n0:24:32\tSPEAKER_01\n If we had enough machines, maybe do them in parallel, so that by the middle of the week, we had some kind of result.\n\n0:24:38\tSPEAKER_01\n I mean, it's one of these Hail Mary kinds of things.\n\n0:24:41\tSPEAKER_01\n I mean, it might not work out, but I figured I may as well ask for it.\n\n0:24:50\tSPEAKER_05\n So I'll ask the other thing is, and I'll ask Don, which is easier to process in terms of creating the test data for the far microphone.\n\n0:25:03\tSPEAKER_05\n If it turns out that for some reason, that's easier for him to use the old segmentations, then we'll just use that, I figure.\n\n0:25:19\tSPEAKER_01\n Right.\n\n0:25:20\tSPEAKER_02\n So I don't want you to have to be burdened with doing a lot of stuff.\n\n0:25:25\tSPEAKER_02\n What can I do to...\n\n0:25:29\tSPEAKER_02\n You said it would be easy for you to do that top one there, and I guess Don can do the segmentations of the channel app.\n\n0:25:40\tSPEAKER_02\n I can certainly help with retraining the short-male models once we have the new data.\n\n0:25:47\tSPEAKER_01\n You have models or short-mails?\n\n0:25:52\tSPEAKER_05\n Right.\n\n0:25:56\tSPEAKER_05\n Let's see.\n\n0:26:00\tSPEAKER_05\n You could run the...\n\n0:26:07\tSPEAKER_05\n Basically, once the top one is done, you could easily rerun the whole set of experiments, manage the jobs and so forth.\n\n0:26:21\tSPEAKER_02\n The bottom of it would just be a matter of pointing at it and set a file and kicking it off.\n\n0:26:26\tSPEAKER_02\n So that would be...\n\n0:26:27\tSPEAKER_02\n Not the bottom of it, but the middle one would be really easy once you've got the top one going.\n\n0:26:31\tSPEAKER_02\n I could do that.\n\n0:26:33\tSPEAKER_02\n Right.\n\n0:26:34\tSPEAKER_02\n I guess I just need to get Don to...\n\n0:26:38\tSPEAKER_05\n So somehow, assuming he uses the new naming scheme, then he should call the waveforms...\n\n0:26:48\tSPEAKER_05\n So the waveforms have the meeting ID and the microphone.\n\n0:26:55\tSPEAKER_05\n I guess the channel and the microphone and the speaker...\n\n0:27:03\tSPEAKER_05\n some something that identifies the speaker.\n\n0:27:07\tSPEAKER_05\n So...\n\n0:27:08\tSPEAKER_02\n Keep it the same, but just change it all to the channel app.\n\n0:27:11\tSPEAKER_05\n Exactly.\n\n0:27:12\tSPEAKER_05\n So you still need to be able to distinguish the different speakers.\n\n0:27:15\tSPEAKER_05\n I'm going to go back to the point because if you want to do what we just discussed, the easiest way to do that would be to just make the channel app, but then keep the speaker names the same as they would be in the old, in the close talking version.\n\n0:27:32\tSPEAKER_02\n Okay.\n\n0:27:33\tSPEAKER_02\n And so that's something that Don would do when he creates the speaker.\n\n0:27:36\tSPEAKER_02\n Right.\n\n0:27:37\tSPEAKER_02\n Exactly.\n\n0:27:38\tNone\n Okay.\n\n0:27:39\tSPEAKER_02\n So will you talk to him about that?\n\n0:27:42\tSPEAKER_05\n I'll get dark too.\n\n0:27:46\tSPEAKER_02\n And then the bottom one in terms of the test will be...\n\n0:27:53\tSPEAKER_02\n that will just be a copy of the one above it except for different models.\n\n0:27:59\tSPEAKER_03\n We also have to means attract the test data.\n\n0:28:03\tSPEAKER_02\n Okay.\n\n0:28:05\tSPEAKER_02\n So we need to run...\n\n0:28:07\tSPEAKER_02\n Okay.\n\n0:28:08\tSPEAKER_02\n Well, once we have the new...\n\n0:28:12\tSPEAKER_02\n Once I do that, second experiment will have the files.\n\n0:28:16\tSPEAKER_02\n And I can give you those to process.\n\n0:28:18\tSPEAKER_03\n Okay.\n\n0:28:19\tSPEAKER_03\n And so the way this means subtraction expects to work is it expects to have...\n\n0:28:25\tSPEAKER_03\n this continuous stream of audio data from a particular speaker to operate on.\n\n0:28:29\tSPEAKER_03\n And it goes along with the sliding window, calculating the mean, using the data in the window, and then subtracting that.\n\n0:28:35\tSPEAKER_02\n So I mean...\n\n0:28:38\tSPEAKER_02\n I'll create this continuous stream from the individual utterance file.\n\n0:28:40\tSPEAKER_03\n That's how I've been doing it just by concatenating files together.\n\n0:28:43\tSPEAKER_03\n And if these files...\n\n0:28:45\tSPEAKER_03\n And since their individual utterance files, long silence periods are removed, which is a good thing, because this method might estimate the mean badly if you had to face long silence periods.\n\n0:28:55\tSPEAKER_03\n But that does mean that I need as much...\n\n0:28:58\tSPEAKER_03\n I need twice as much disk space as the original set.\n\n0:29:01\tSPEAKER_03\n Because I need... Well, I'm running it because I need to create this intermediate set of these big files.\n\n0:29:07\tSPEAKER_03\n And then, finally, the mean subtracted little files.\n\n0:29:12\tSPEAKER_03\n And then I can get rid of the big files.\n\n0:29:15\tSPEAKER_03\n But while I'm doing the processing, I need twice as much disk space.\n\n0:29:20\tSPEAKER_02\n Okay. I'll check with Markam and see what happened with the disks.\n\n0:29:25\tSPEAKER_02\n He went to persona a couple of weeks ago and something.\n\n0:29:28\tSPEAKER_01\n On vacation. I'll check the tape.\n\n0:29:31\tSPEAKER_02\n You haven't seen new disks pop up, have you?\n\n0:29:34\tSPEAKER_04\n No, I was wondering if they're in the big roller-launchies then.\n\n0:29:38\tSPEAKER_01\n But they were like mushrooms.\n\n0:29:41\tSPEAKER_01\n Shake them, popping out.\n\n0:29:43\tSPEAKER_02\n He went to put them on and then something happened.\n\n0:29:45\tSPEAKER_02\n He sent a note around saying, oh, something...\n\n0:29:48\tSPEAKER_02\n He didn't work again. We'll have to schedule another time.\n\n0:29:51\tSPEAKER_02\n Nothing happens.\n\n0:29:53\tSPEAKER_02\n I'll check with David about that.\n\n0:29:55\tSPEAKER_01\n Okay.\n\n0:29:57\tSPEAKER_01\n Because we still have that other one going, which is the macro phone.\n\n0:30:02\tSPEAKER_03\n Right.\n\n0:30:03\tSPEAKER_03\n So, Andreas, in you, Dr. Speech Data, SRI-FogFive, there's this Hub5 training set.\n\n0:30:10\tSPEAKER_03\n Is that the long training set there?\n\n0:30:12\tSPEAKER_05\n That's everything.\n\n0:30:14\tSPEAKER_05\n So I can give you a list of the short version.\n\n0:30:18\tSPEAKER_03\n Okay. I think you already did, actually.\n\n0:30:20\tSPEAKER_03\n Okay.\n\n0:30:21\tSPEAKER_03\n And so say the microphone files that are included in this short training are just a subset of the microphone files, right?\n\n0:30:28\tSPEAKER_05\n That's right.\n\n0:30:29\tSPEAKER_03\n Okay. So when you did some TI Digits experience training on microphone.\n\n0:30:36\tSPEAKER_03\n But that's not necessarily any less data than the SRI Hub5 set.\n\n0:30:41\tSPEAKER_03\n It's not a subset of the short SRI Hub5 set, right?\n\n0:30:47\tSPEAKER_05\n No, it is.\n\n0:30:50\tSPEAKER_05\n Sorry. Can you repeat the question?\n\n0:30:53\tSPEAKER_03\n When you trained on microphone to do those Digits experiments, did you use the entire microphone purpose?\n\n0:30:59\tSPEAKER_05\n Only the portion that was in the Hub5 training set.\n\n0:31:02\tSPEAKER_01\n Oh. That was in Hub5 small training set.\n\n0:31:05\tSPEAKER_05\n Well, the Hub5 small training set contains as much microphone as the large training set for historical reasons.\n\n0:31:11\tSPEAKER_05\n Yeah.\n\n0:31:13\tSPEAKER_03\n Okay. So.\n\n0:31:17\tSPEAKER_05\n Sorry, you have that process, right?\n\n0:31:19\tSPEAKER_05\n Because you already did that.\n\n0:31:20\tSPEAKER_05\n Did you already do that experiment?\n\n0:31:22\tSPEAKER_03\n I got confused because I thought you were using the whole microphone set.\n\n0:31:27\tSPEAKER_03\n Okay. Well, if I just need to use that subset, I can get it processed.\n\n0:31:32\tSPEAKER_03\n I actually got, I think I got into it before and then I thought I was doing the wrong thing and I stopped and it shouldn't take that long to do it.\n\n0:31:40\tSPEAKER_05\n Right.\n\n0:31:42\tSPEAKER_05\n Okay. Have you need only the mail?\n\n0:31:46\tSPEAKER_02\n So basically, Dave, so for you to get your processing going, you need the list of the wave, I guess it'll be, you don't need to get the segmentations.\n\n0:31:55\tSPEAKER_02\n Yeah.\n\n0:31:56\tSPEAKER_02\n You're going to whether we're using anywhere the old from dawn.\n\n0:31:59\tSPEAKER_02\n And then from that, you need the, from the segmentations, you'll have the list of wave files that the short set is trained on.\n\n0:32:08\tSPEAKER_02\n And then you'll need this space.\n\n0:32:10\tSPEAKER_02\n And once you've got those things, then you can start your processing.\n\n0:32:14\tSPEAKER_02\n Yeah.\n\n0:32:15\tSPEAKER_02\n Okay.\n\n0:32:17\tSPEAKER_05\n There's this, this, it's sort of, it's not very nice to use the small train set for another reason, which is that the, you also add losing on, again, because you don't use all the data you have for one speaker.\n\n0:32:33\tSPEAKER_05\n So the normalizations you compute for your train speakers will be, uh, crumbier than the word in the large train set.\n\n0:32:42\tSPEAKER_05\n So, um, I have to solve, to make it really matching experiment, I have to find, uh, I have to use short models that were trained on normalizations that were also only estimated on the short set, which is, I think so, I, I have to check.\n\n0:33:06\tSPEAKER_05\n In any case, I could retrain short models within a few hours, actually, if I use, I wonder about that though.\n\n0:33:13\tSPEAKER_01\n I mean, because all we're doing, the only reason we're using a short training set is, is for speed.\n\n0:33:20\tSPEAKER_01\n And they're, we're not really making any claims about using a smaller training set.\n\n0:33:25\tSPEAKER_01\n So as long as we're not using any testing data from...\n\n0:33:28\tSPEAKER_05\n But the thing is, if, if we use, if we use the whole training set for normalizations, then David would have to process much more data, which, that's, that's one bottleneck for us, right?\n\n0:33:41\tSPEAKER_05\n Oh, you mean for, for his normalizations?\n\n0:33:43\tSPEAKER_01\n Yeah.\n\n0:33:44\tSPEAKER_01\n Oh, oh, oh, I'm trying.\n\n0:33:45\tSPEAKER_05\n Right. So you want to do the exact same thing.\n\n0:33:47\tSPEAKER_05\n Right. Well, you have apples and oranges.\n\n0:33:50\tSPEAKER_00\n Yeah.\n\n0:33:57\tSPEAKER_05\n It doesn't make, I don't think it makes that much of a difference.\n\n0:33:59\tSPEAKER_05\n It's just this little detail that, if you can take care of that, then you should.\n\n0:34:05\tSPEAKER_05\n I think I have, I have the model site.\n\n0:34:08\tSPEAKER_05\n I have, um...\n\n0:34:11\tSPEAKER_05\n Yeah, and if not, I can retrain those models very...\n\n0:34:17\tSPEAKER_02\n Oh, there's, there's one other issue.\n\n0:34:20\tSPEAKER_02\n And that is that David throws out speakers that have less than 12 seconds of training data.\n\n0:34:28\tSPEAKER_02\n And he said there were a few in the Macaphone set like that.\n\n0:34:33\tSPEAKER_02\n So do we need to wait to find out who he's going to throw out so that we create a new set of short models that don't include those speakers?\n\n0:34:45\tSPEAKER_02\n Uh...\n\n0:34:46\tSPEAKER_02\n Again, sorry, I messed up.\n\n0:34:47\tSPEAKER_02\n So, and the problem is that if we proceed like we just described, um, when he goes to create the new training data with his processing, he throws out some speakers, so the two training sets won't be identical.\n\n0:35:04\tSPEAKER_01\n Yeah, he throws out some speakers that are very small.\n\n0:35:07\tSPEAKER_05\n Yeah, I don't think it'll make a better.\n\n0:35:10\tSPEAKER_01\n Yeah, I think they've sewn a few.\n\n0:35:12\tSPEAKER_05\n In fact, I thought about throwing those out too, because when I heard how they'd speech there was for some of them, I thought they could only heard your models, because again, their normalizations will be all over the map.\n\n0:35:23\tSPEAKER_05\n And you won't get very, very clean models from the anyhow, so...\n\n0:35:30\tSPEAKER_02\n Do you think it's okay then?\n\n0:35:31\tSPEAKER_05\n Yeah, in fact, if you want to do this, just speed things up.\n\n0:35:39\tSPEAKER_05\n We can leave out the Macaphone data altogether.\n\n0:35:43\tSPEAKER_05\n That hurt...\n\n0:35:44\tSPEAKER_05\n Actually, oh no, sorry, not in the short, then you have to do little data.\n\n0:35:48\tSPEAKER_05\n Okay, sorry, forget that.\n\n0:35:51\tSPEAKER_05\n When you go to the large training set, then leaving on Macaphone actually sometimes helps you, because it's just not relevant to the meaning or to conversational speech anyway.\n\n0:36:06\tSPEAKER_05\n Okay, yeah, leave it out.\n\n0:36:08\tSPEAKER_05\n And in the event that I retrain, the short models, why don't you give me a list of the files that you throw out, and I'll throw them out too.\n\n0:36:17\tSPEAKER_05\n And then we have to complete the identical training conditions.\n\n0:36:22\tSPEAKER_02\n Actually, you should be able to figure out, Dave, right, once you know the segmentations, who you're going to, which speakers will get left out even before you run your process.\n\n0:36:33\tSPEAKER_02\n The segmentations?\n\n0:36:34\tSPEAKER_02\n Yeah, the segmentations from Don.\n\n0:36:36\tSPEAKER_05\n The segmentations are only... they only affect the test set.\n\n0:36:39\tSPEAKER_05\n We're talking about the training speakers.\n\n0:36:41\tSPEAKER_01\n No, the training's going through right now, see how long now.\n\n0:36:45\tSPEAKER_02\n Right, I'm just wondering how long it will take to get that information.\n\n0:36:50\tSPEAKER_05\n You already have it.\n\n0:36:52\tSPEAKER_05\n You already have it.\n\n0:36:53\tSPEAKER_03\n I have it for Macrophone already, I think.\n\n0:36:58\tSPEAKER_03\n And I think by tomorrow I'll have it for the rest.\n\n0:37:04\tSPEAKER_00\n All right.\n\n0:37:13\tSPEAKER_01\n That's that one, maybe.\n\n0:37:19\tSPEAKER_01\n We're looking at synthesizers.\n\n0:37:22\tSPEAKER_04\n You were like, yeah, I was doing something for the smart computer collection as Robo was taking his laptop back to Germany, so we needed a new synthesis machine.\n\n0:37:32\tSPEAKER_04\n And we have now a sun workstation in the library, which does the synthesis and the festivals.\n\n0:37:36\tSPEAKER_04\n So Roboch is...\n\n0:37:38\tSPEAKER_04\n His laptop, which we used for the smart computer collection for the synthesis.\n\n0:37:44\tSPEAKER_04\n And so he took it to Germany.\n\n0:37:47\tSPEAKER_04\n And so we couldn't do any data collection.\n\n0:37:49\tSPEAKER_04\n Is he gone now?\n\n0:37:50\tSPEAKER_04\n No, he's just gone to a smart computer workshop.\n\n0:37:53\tSPEAKER_04\n Oh, oh.\n\n0:37:54\tSPEAKER_04\n And so we have now the sun in the library, which can do that.\n\n0:37:58\tSPEAKER_04\n And I looked into the F-0 thing and talked to Liz.\n\n0:38:03\tSPEAKER_04\n And it seems that it's quite what she wants, but we'll have to think about the energy thing.\n\n0:38:11\tSPEAKER_01\n This was a business about coming up with something that was purely prasadic.\n\n0:38:17\tSPEAKER_01\n And so I'm just going to use pitch detector, drive a synthesizer.\n\n0:38:22\tSPEAKER_01\n And since it doesn't have a hook in it for modifying energy, you'll have a little box at the output that will modify the energy.\n\n0:38:29\tSPEAKER_01\n So...\n\n0:38:33\tSPEAKER_01\n I think, okay.\n\n0:38:36\tSPEAKER_02\n Are you interfacing to that thing with the C++ routines?\n\n0:38:40\tSPEAKER_02\n Is there another interface that you use?\n\n0:38:42\tSPEAKER_04\n For festival?\n\n0:38:43\tSPEAKER_04\n Yeah.\n\n0:38:44\tSPEAKER_04\n You can just use it from the...\n\n0:38:46\tSPEAKER_04\n Yeah, basically from the command line and defining the phones, whatever you want to have synthesized.\n\n0:38:52\tSPEAKER_04\n And if the F-0 targets and then it keeps on away from, and I want to manipulate the away from them.\n\n0:39:00\tSPEAKER_02\n Oh, great.\n\n0:39:06\tSPEAKER_01\n Okay.\n\n0:39:07\tSPEAKER_01\n Did it?\n\n0:39:17\tSPEAKER_02\n Okay.\n\n0:39:23\tSPEAKER_02\n Transcript L-288.\n\n0:39:26\tSPEAKER_02\n 523-817-719.\n\n0:39:29\tSPEAKER_02\n 423-158823.\n\n0:39:33\tSPEAKER_02\n 766-608-2212.\n\n0:39:37\tSPEAKER_02\n 6926-9249.\n\n0:39:41\tSPEAKER_02\n 5736-010645.\n\n0:39:45\tSPEAKER_02\n 666-9118.\n\n0:39:48\tSPEAKER_02\n 022-676651.\n\n0:39:52\tSPEAKER_03\n Transcript L-290.\n\n0:39:55\tSPEAKER_03\n 428-0231313823526059881.\n\n0:40:05\tSPEAKER_03\n 1585994156.\n\n0:40:10\tSPEAKER_03\n 887-792-722.\n\n0:40:15\tSPEAKER_03\n 397-333226.\n\n0:40:19\tSPEAKER_03\n 006-380331.\n\n0:40:24\tSPEAKER_03\n 836-3646-7473.\n\n0:40:29\tSPEAKER_03\n 592-008279.\n\n0:40:35\tSPEAKER_00\n Transcript L-280.\n\n0:40:38\tSPEAKER_00\n 530-464438.\n\n0:40:43\tSPEAKER_00\n 6384-9850-9939709093.\n\n0:40:51\tSPEAKER_00\n 597-5.\n\n0:40:53\tSPEAKER_00\n 021-5.\n\n0:40:54\tSPEAKER_00\n 313-3.\n\n0:40:56\tSPEAKER_00\n 816-6.\n\n0:40:58\tSPEAKER_00\n 974-847-973.\n\n0:41:03\tSPEAKER_00\n 274-159-1269.\n\n0:41:08\tSPEAKER_00\n 2425-5719-00.\n\n0:41:13\tSPEAKER_00\n 517-397-0618.\n\n0:41:19\tSPEAKER_05\n Transcript L-281.\n\n0:41:22\tSPEAKER_05\n 043-180781459.\n\n0:41:26\tSPEAKER_05\n 297-1466204.\n\n0:41:31\tSPEAKER_05\n 036-597-6247.\n\n0:41:35\tSPEAKER_05\n 5215-841166.\n\n0:41:39\tSPEAKER_05\n 4621-4664.\n\n0:41:43\tSPEAKER_05\n 9402-12332073.\n\n0:41:48\tSPEAKER_05\n 9778-047295.\n\n0:41:53\tSPEAKER_05\n 378-797-3466.\n\n0:41:57\tSPEAKER_04\n Transcript L-208.\n\n0:42:00\tSPEAKER_04\n 292-269634.\n\n0:42:05\tSPEAKER_04\n 6917-29778-2.\n\n0:42:10\tSPEAKER_04\n 283-491-277.\n\n0:42:14\tSPEAKER_04\n 490-8696432.\n\n0:42:18\tSPEAKER_04\n 966-704940.\n\n0:42:23\tSPEAKER_04\n 6387-2826-9059.\n\n0:42:28\tSPEAKER_04\n 177-586-814.\n\n0:42:32\tSPEAKER_04\n 566-656809.\n\n0:42:38\tSPEAKER_01\n Transcript L-287-18388-8155.\n\n0:42:45\tSPEAKER_01\n 2309-336109.\n\n0:42:49\tSPEAKER_01\n 368510-3452.\n\n0:42:53\tSPEAKER_01\n 1143936702.\n\n0:42:57\tSPEAKER_01\n 1571-00728896.\n\n0:43:01\tSPEAKER_01\n 2246-5913.\n\n0:43:05\tSPEAKER_01\n 0721-4598.\n\n0:43:08\tSPEAKER_01\n 6454-116336.\n\n0:43:14\tSPEAKER_01\n It's all folks.\n\n", "summary": [{"summary_text": "SPEAKER_01 and Dave are trying to get recognition results for the antelis segments, which was about using the closed-talk microphone. They have some stuff with no overlap for which there would be near-near field results. They wanted to get far field results for that and apply Dave's processing to potentially training and test data and do the same thing. The short training set is worse than the large one in terms of ultimate performance."}]}