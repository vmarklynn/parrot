Speaker A: I'm not sure that that thing should move.
Speaker F: Yes, positive.
Speaker F: Ah, positive.
Speaker F: I'm positive.
Speaker F: It's the POS.
Speaker F: Yes.
Speaker F: You've soaked up all the positivity.
Speaker E: Yes, right.
Speaker E: It's sort of happened.
Speaker E: That would be good day, man.
Speaker E: That's nice.
Speaker A: It really is.
Speaker A: Oh, it's really bad.
None: It's so few.
Speaker G: Yeah, they're getting bigger.
Speaker G: Oh, so it's recording.
Speaker G: Thank you, man.
Speaker C: So we got this.
Speaker C: That's weird.
Speaker C: I remember when she was gathering her data for a dissertation.
Speaker C: Were you around?
Speaker C: I was doing the I-Ever.
Speaker C: All right.
Speaker G: Well, I don't know why the POS isn't moving.
Speaker E: Yeah.
Speaker E: That's something the trick cut.
Speaker E: Check out.
Speaker F: So, so, this is the first we are.
Speaker F: We have, our and have been recording.
Speaker H: Yeah, it is working.
Speaker H: I don't know why the numbers aren't going up, but the files are getting bigger.
Speaker H: And what is it?
None: Nothing.
Speaker H: Well, I didn't change anything.
Speaker H: It may have been working before.
Speaker H: It probably was working before.
Speaker H: And the only way I just checked is that the numbers are getting bigger and the files.
Speaker H: The files are getting bigger.
Speaker H: The file size is.
Speaker H: You want to put your mic on while we're talking?
Speaker D: Probably, stop.
Speaker D: So, in the future, if the POS is not moving.
Speaker H: Ignore it.
Speaker H: It's probably working.
Speaker H: I mean, we'll find out for sure in a moment.
Speaker H: So, that means we got the joke on that.
Speaker F: That's what we asked.
Speaker E: We have the joke.
Speaker E: Yeah.
Speaker H: So, I want to discuss digits briefly, but that won't take too long.
Speaker E: Okay, agenda items.
Speaker E: We have digits.
Speaker E: What else we got?
Speaker E: You were the presentation.
Speaker E: You were the presentation.
Speaker E: I'm going to ask you a question.
Speaker C: Do we want to say something?
Speaker C: Yeah, why don't you summarize an update of the transcript?
Speaker E: Update on transcripts.
Speaker D: And I guess that includes the filtering for the ASRFs.
Speaker F: And filtering for what?
Speaker D: For the references that we need to go from the fancy transcripts to the sort of...
Speaker C: Basically, it'll be recap on the meeting that we had in jointly this morning.
Speaker C: I think it's done as well.
None: Got it.
Speaker E: Anything else more pressing than those things?
Speaker E: So, finally, just do those.
Speaker H: As you can see from the numbers on the digits we're almost done.
Speaker H: The digits goes up to about 4,000.
Speaker H: And so, we probably will be done with the TI digits in another couple of weeks, depending on how many we read each time.
Speaker H: So, there were a bunch that we skipped.
Speaker H: Someone fills out the form and then they're not at the meeting.
Speaker H: So, it's blank.
Speaker H: But those are almost all filled in as well.
Speaker H: And so, once it's done, it would be very nice to train up a recognizer and actually start working with this data.
Speaker G: So, what's a corpus that's the size of the TI digits?
Speaker H: One particular test set of TI digits.
Speaker H: So, I extracted...
Speaker H: There was a file sitting around which people have used here as a test set.
Speaker H: It had been randomized.
Speaker H: That's just what I used to generate the order of these particular ones.
Speaker E: So, first of all, what we could do is take the standard training set for TI digits, train up with whatever great features we think we have.
Speaker E: For instance, and then test on this test set.
Speaker E: Presumably, you should do reasonably well on that.
Speaker E: And then, presumably, we should go to the distant mic and do poorly.
Speaker E: And then, you should get really smart over the next year or two.
Speaker H: And then, you should get a good reason by one or two percent.
Speaker H: But, in order to do that, we need to extract out the actual digits.
Speaker H: So, the reason it's not just a transcript is that their false starts and misreads and miscues and things like that.
Speaker H: And so, I have a set of scripts and next waves where you just select the portion, hit R, it tells you what the next one should be, and you just look for that.
Speaker H: So, it'll put on the screen.
Speaker H: The next set is 69922. And you find that and hit the key and it records it in a file in a particular format.
Speaker H: So, the question is, should we have the transcribers do that or should we just do it?
Speaker H: Or some of us? I've been doing, I've done eight meetings, something like that.
Speaker H: Just by hand. Just myself, rather.
Speaker H: So, it will not take long.
Speaker G: What do you think?
Speaker C: What's not going to be discussed is, we discussed this for coffee and I think it's a fine idea, partly because it's not unrelated to the present skill set.
Speaker C: But it will add, for them, an extra dimension might be an interesting break for them.
Speaker C: And also, it is contributing to the composition of the transcript, because we can incorporate this number strictly and it will be more complete transcripts.
Speaker C: So, I think it's fine.
Speaker E: So, you think it's fine to have the transcribers do it?
Speaker H: There's one other small bit which is just entering the information which is at the top of this form onto the computer to go along with where the digits are recorded automatically.
Speaker H: And so, it's just typing in name, time date and so on, which again, either they can do, but it is firing up an editor or again, I can do or someone else can do.
Speaker C: And that, you know, that one, I'm not so sure fits into the things that I want to use.
Speaker C: I want to use the hours for because in the time that they'd be spending doing that, they wouldn't be able to be putting more words on it.
Speaker C: But that's really your choice.
Speaker G: So, are these two separate tasks that can happen?
Speaker G: Or do they have to happen at the same time?
Speaker H: No, they don't have this.
Speaker H: You have to enter the data before you do the second task, but they don't have to happen at the same time.
Speaker H: So, it's just I have a file which has this information on it.
Speaker H: And then when you start using my scripts for extracting the times, it adds the times at the bottom of the file.
Speaker H: And so, I mean, it's easy to create the files and leave them blank.
Speaker H: And so, actually, we could do it in either order.
Speaker H: Okay.
Speaker H: It's sort of nice to have the same person do it just as a double check to make sure you're entering for the right person.
Speaker H: But either way.
Speaker E: Yeah. Yeah, just by way of a order of magnitude, we've been working with this Aurora data set.
Speaker E: And the best score on the nicest part of the data, that is where you've got training and test set that are basically the same kinds of noise and so forth, is about, I think, the best score was something like 5% error per digit.
Speaker E: So, that digit.
Speaker E: Right. So, if you were doing 10 digit recognition, it would really be in trouble.
Speaker E: So, the point there, and this is car noise, things but real situation, well, real.
Speaker E: There's one microphone that's close that they have.
Speaker E: It's this sort of thing, the close first is distant, but in a car instead of having a projector noise, it's car noise.
Speaker E: But it wasn't artificially added to get some artificial signal that was racially when it was just people driving around a car.
Speaker E: So, that's an indication.
Speaker E: That was with many sites competing, and this was the very best score and so forth.
Speaker G: Although, more typical models weren't that good, right?
Speaker G: I mean, the models are pretty crappy.
Speaker E: You're right. I think that we could have done better on the models.
Speaker E: But the thing is that this is a kind of typical number for all of the things in this task, all of the languages.
Speaker E: And so, I think we probably, the models, would be better in some than in others.
Speaker E: So, anyway, just an indication.
Speaker E: Once you get into this kind of realm, even if you're looking at connected digits, it can be pretty hard.
Speaker C: It's going to be fun to see how we compare it.
Speaker C: How do we do with the prosaudage?
Speaker H: There's so much difference.
Speaker H: It's going to be strange.
Speaker H: I mean, the prosaudics are not the same as TI digits, for example.
Speaker H: So, I'm not sure how much of effect that we'll have.
Speaker H: How do we connect the prosaudage?
Speaker H: Just what we were talking about with grouping.
Speaker H: That with these, the grouping, there's no grouping at all.
Speaker H: And so, it's just the only sort of discontinuity you have is at the beginning and the end.
None: So, what are they doing in Aurora?
Speaker H: Are they reading all number?
Speaker H: Aurora, I don't know.
Speaker D: I don't know what they're doing in Aurora.
Speaker E: I'm not sure.
Speaker E: No, no, I mean, it's connected.
Speaker E: It's connected to digits.
Speaker D: But it's also not just the prosaudage that cross the crossword model.
Speaker H: Right.
Speaker H: But in TI digits, they're reading things like zip codes and phone numbers and things like that.
Speaker H: How do we do on TI digits?
Speaker H: I don't remember.
Speaker H: I mean, very good, right?
Speaker H: Yeah, I mean, we were in one and a half percent, two percent.
Speaker E: Oh, I think we got under a percent.
Speaker E: Oh, really?
Speaker E: But I mean, the very best system that I saw in literature was 0.25 percent or something, and somebody had a Bell Labs.
Speaker E: Right.
Speaker E: But sort of pulling out all the stuff.
Speaker E: But I think a lot of systems sort of get half a percent.
Speaker E: Right.
Speaker E: We were in there somewhere.
Speaker H: But that means it's really, it's close talking mics, no noise, clean signal, just digits.
Speaker H: I mean, everything is good.
Speaker D: Yeah, exactly.
Speaker D: Yeah.
Speaker H: And we only recently got it to anywhere near a human.
Speaker H: Prehistory.
Speaker D: Beginning of life, yeah.
Speaker H: And it's still like an order of magnitude worse than humans, too.
Speaker E: Right.
Speaker E: When they're right away, yeah.
Speaker E: Yeah.
Speaker E: After coffee.
Speaker E: After coffee.
Speaker E: Not after lunch.
Speaker H: Okay, so what I'll do then is I'll go ahead and enter this data and then hand off to Jane and the transcribers to do the actual extraction of the digits.
Speaker E: Yeah.
Speaker E: One question I have that, I mean, we wouldn't know the answer to now, but do some guessing about it.
Speaker E: I was talking before about doing some modeling, marking of our territory features, with overlap, and so on.
Speaker E: And on some subset, one thought might be to do this on the digits or some of these digits.
Speaker E: It would be easier.
Speaker E: So if we're the only thing is I'm a little concerned that maybe the kind of phenomena.
Speaker E: And the reason for doing it is because the argument is that certainly with conversational speech, the stuff that we've looked at here before, just doing the simple mapping from the phone to the corresponding features that you could look up in a book, isn't right.
Speaker E: It isn't actually right.
Speaker E: In fact, there's these overlapping processes where some voicing comes up and then some, you know, some miziality comes in here and so forth.
Speaker E: And you do this gross thing and saying, well, I guess it's this phone starting there.
Speaker E: So that's the reasoning.
Speaker E: But it could be that when we're reading digits because it's for such a limited set that maybe that phenomenon doesn't encourage much.
Speaker E: Do you have any opinion about that?
Speaker C: It strikes me that there are more, each of them is more informative because it's so random and that people might articulate more and they might end up with more across the correspondence.
Speaker C: Yeah, I agree.
Speaker G: It's sort of less predictability.
Speaker D: Yeah.
Speaker D: Well, it's definitely true that when people are reading, even if they're re-reading what they had said spontaneously, that they have very different patterns, mid-show that and the desertations have shown that.
Speaker D: So the fact that they're reading, first of all, whether they're reading in a room of people or, you know, the fact that they're reading will make a difference.
Speaker D: Yeah.
Speaker D: Well, it depends what you're interested in.
Speaker H: Would this corpus really be the right one to even try that on?
Speaker E: See, I don't know.
Speaker E: So maybe the thing will be to do takes a very small subset.
Speaker E: I mean, I'd have a big program and take a small subset of the conversational speech and a small subset of the digits and look and just get a feeling for it.
Speaker E: Just take a look, really.
Speaker E: Because I don't think anybody is, at least I don't know of anybody.
Speaker E: Well, I don't know.
Speaker E: I can be interesting in design too.
Speaker C: Because then you have the comparison of the predictable speech versus the last predictability speech and maybe you'd find it at work too.
Speaker C: And the case of the beginning.
Speaker G: I have to think about the particular acoustic features to mark too because, I mean, the things they wouldn't be able to mark, like, you know, tens lacks.
Speaker G: And some things are really difficult, you know.
Speaker H: I think we can get a hollow end to give us some advice on that.
Speaker C: Also, I thought you were thinking of a much more restricted set of features.
Speaker E: Yeah, but I was like, he said, I was going to bring Todd in and then, but I mean, you want to be restrictive, but you also want to have coverage.
Speaker E: You know, you should, it should be such that if you, if you, if you had all of the features determined that you, that you have chosen, that that would tell you in the study's day case, the phone, so.
Speaker H: Even, I guess, with vowels, that would be pretty hard, wouldn't it?
Speaker H: Which identify, actually, you know, which one it is?
Speaker C: It seemed to me that the points of articulation would be more, I mean, that's, I think, about articulation, which means, rather than bells.
Speaker G: Points of articulation, what do you mean?
Speaker C: So is it bilabial or dental or is it, you know, paladine?
Speaker C: Which are all like, where are you talking about?
Speaker C: Place of articulation.
Speaker C: Yeah.
Speaker C: Just thank you, whatever I said.
Speaker E: Okay.
Speaker E: Yeah.
Speaker E: Okay, we got our jargon in.
Speaker E: Yeah.
Speaker D: Well, it's also, there's really a difference between the pronunciation, vowels, and the dictionary, and the pronunciations that people produce.
Speaker D: And so, you get some of that information from Steve's work on that labeling, and it really, I actually think that data should be used more, that maybe, although I think the meeting context is great, that he has transcriptions that give you the actual phone sequence, and you can go from, not from that to the articulatory features, but that would be a better starting point for marking the gestural features than data where you don't have that, because we, we want to know both about the way that they're producing a certain sound and what kinds of, what kinds of phonemic differences you get between the transcribed sequences and the dictionary ones.
Speaker E: Well, you might be right, that might be the way at getting it, what I was talking about, but the particular reason why I was interested in doing that was because I remember when that happened, and John O'Hall was over here, and he was looking at the spectrograms of the more difficult ones.
Speaker E: He didn't know what to say about what is the sequence of phones there.
Speaker E: They came up with some compromise, because that really wasn't what it looked like.
Speaker E: It didn't look like a sequence of phones.
Speaker E: It looked like this blending thing happening here.
Speaker G: Yeah, so you have this feature here.
Speaker D: There's no name for that.
Speaker D: But it still is, there's a, there are two steps.
Speaker D: One is going from a dictionary pronunciation of something, like, you're going to see you tomorrow.
Speaker D: It could be going to or gun to, you know, or gun to.
Speaker D: Yeah, you're going to see you tomorrow, you'll see you tomorrow.
Speaker D: And that would be nice to have these intermediate or these, some, these reduced pronunciations that those transcribed marked, or to have people mark those as well, because it's not, it's not that easy to go from the dictionary word pronunciation, the dictionary phone pronunciation to the gestural one without this intermediate sort of civil level kind of representations.
Speaker H: Well, I don't think more consistent thing that we do that, though.
Speaker E: Yeah, I mean, I'm just, the moment, of course, we're just talking about what to provide is a tool for people to do research.
Speaker E: You have different ideas about how to do it.
Speaker E: So for instance, you might have someone who just has a word, has words with states and has, comes from our particular gestures to that.
Speaker E: And someone else might actually want some phonetic, you know, intermediate thing.
Speaker E: So I think it would be best to have all of it if we could.
Speaker E: But, uh,
Speaker H: What I'm imagining is a score like notation, where each line is a particular feature.
Speaker H: Right? So you would say, you know, it's a voice through here, and you have label here, you have nasal here, and they could be overlapping in all sorts of bizarre ways that don't correspond to the timing on phones.
Speaker E: I mean, this is the kind of reason I remember when one of the switchboard workshops that when we talked about doing the transcription project, Dave Tolkien said can't be done.
Speaker E: Right.
Speaker E: And he was, what he meant was that this isn't, you know, a sequence of phones, and we actually look at switchboard, that's not what you see.
Speaker H: And in fact, the interannotary agreement was not that good, right?
Speaker H: On the harder ones?
Speaker H: Yeah, I mean...
Speaker D: I don't know how you look at it, and I understand what you're saying about this kind of transcription, exactly, because I've seen, you know, where does the voicing of our start and so forth.
Speaker D: All I'm saying is that it is useful to have that the transcription of what was really said and which syllables were reduced if you're going to add the features.
Speaker D: It's also useful to have some level of representation, which is a reduced...
Speaker D: It's a pronunciation variant that currently the dictionaries don't give you because if you add them to the dictionary and you run recognition, you add confusion.
Speaker D: So people purposely don't add them.
Speaker D: So it's useful to know which variant was produced at least at the phone.
Speaker G: So it would be great if we had either these kind of lablings on the same portion of switchboard that Steve marked or Steve's type markings on this data.
Speaker D: Exactly.
Speaker D: Exactly.
Speaker D: Yeah, no, I don't disagree with that.
Speaker D: It's not that slow, I don't know what that means.
Speaker E: Yeah, I don't disagree with it.
Speaker E: The only thing is that what you actually would end up with is something...
Speaker E: It's all compromise, right?
Speaker E: So the string that you end up with isn't actually what happened, but it's the best compromise that a group of people scratching their heads could come up with to describe what happened.
Speaker G: And it's more accurate than...
Speaker E: And it's more accurate than the dictionary, or if you've got a pronunciation lexicon that has three or four, this might have been the fifth one.
Speaker D: So it's like a catcher or whatever, go and start the way down.
Speaker D: That's what I meant.
Speaker D: And in some places it would fill in the kinds of gestural features are not everywhere.
Speaker D: So there are some things that you don't have access to either from your ear or the spectrogram, but you know what phone it was, and that's about all you can say.
Speaker D: And then there are other cases where...
Speaker G: It's basically to have multiple levels of information marking.
Speaker H: Well, the other difference is that the features are not synchronous, right? They overlap with each other in weird ways.
Speaker H: So it's not strictly one-dimensional signal.
Speaker H: So I think that's sort of qualitatively different.
Speaker D: You can add those features in, but it'll be under specified.
Speaker D: There'll be no way for you to actually mark what was said completely by features.
Speaker H: Well, not with our current system, but you could imagine designing a system that the states were features rather than phones.
Speaker D: Well, we probably have a separate discussion of that.
Speaker D: It's not that it was that.
Speaker C: But that was not the kind of direction I thought.
Speaker E: Yeah, so I mean, where this is...
Speaker E: I mean, I wanted...
Speaker E: I would like to have something that's useful to people other than those who are doing the specific kind of research.
Speaker E: I have in mind, so it should be something broader.
Speaker E: But what I'm coming from is we're coming off of stuff that Larry Saul did with...
Speaker E: with John Dallin and Muzzie Rahim, in which they have a multi-band system that is trained through combination and gradient learning and NDM to estimate the value for particular feature.
Speaker E: And this is part of a larger image that John Dallin has, but how he brain does it, which he's sort of imagining that individual frequency channels are coming up with their own estimate of these kinds of...
Speaker E: something like this might not be exact features that Jacobs and Thott over something.
Speaker E: But I mean, something like that's a kind of low-level features which are not fully phone classification.
Speaker E: And this particular image of how it's done is that then given all of these estimates at that level, there's a level above it, which is making some kind of sound unit classification such as phone.
Speaker E: And you could argue what those sound units should be.
Speaker E: That's sort of what it was imagining doing.
Speaker E: But it's still open within that, whether you would have an intermediate level in which it was actually phones or not, you wouldn't necessarily have to.
Speaker E: But again, I wouldn't want to...
Speaker E: I wouldn't want what we produced to be so local and perspective that it was matched what we were thinking of doing one week.
Speaker E: And what you're saying is absolutely right that if we can, we should put in another level of description there if we're going to get into some of this low-level stuff.
Speaker G: Well, you know, I mean, if we're talking about having the annotators annotate these kinds of features, it seems like, you know, the question is, do they do that on meeting data, or do they do that on switchboard?
Speaker H: That's what I was saying. Maybe meeting data isn't the right corpus.
Speaker C: Well, it seems like you could do both. I mean, I was thinking that it would be interesting to do it with respect to parts of switchboard anyway, in terms of, partly to see if you could generate first guesses at what the articulatory feature would be based on the phone representation of that lower level, that might be a time game, but also in terms of compatibility of...
Speaker G: Well, because then, yeah, and then also, if you did it on switchboard, you would have the full continuum of transcriptions.
Speaker G: You'd have it from the lowest level, the acoustic features, then you'd have the, you know, the fanatic level that Steve did.
Speaker G: It would be a complete set of things.
Speaker E: It could be an altered game.
Speaker E: It's so it's a little different.
Speaker E: So, I mean, we'll see how much we can get the people to do and how much more I have in all this stuff.
Speaker E: It might be good to do with your boat.
Speaker G: You know, seed it with guesses about what we think the features are based on, you know, the phone or Steve's transcriptions or something.
Speaker G: It's a basic...
Speaker H: Quicker.
Speaker H:...of the phone transcripts, so they would all be synchronous, but then you could imagine adjusting them here and there.
Speaker G: Exactly.
Speaker G: Scoot the voicing over a little bit.
Speaker E: Right. Well, I think what, I mean, I'm a little behind in what they're doing now and the stuff they're doing on switchboard now, but I think that Steve and getting are doing something with an automatic system first and then doing some adjustment as a recall.
Speaker E: So, I mean, that's probably the right way to go anyways.
Speaker E: It's to start off with an automatic system with a pretty rich pronunciation dictionary that, you know, tries to label it all.
Speaker E: And then people go through and fix it.
Speaker C: So, in our case, you'd think about us starting with regular dictionary.
Speaker E: Well, regular dictionary, I mean, it's a pretty rich dictionary. It's got a number of pronunciation.
Speaker G: You could start from the, if we were going to do the same set of sentences that Steve had done, we could start with those transcriptions.
Speaker G: That's actually what I was thinking about.
Speaker D: The problem is when you run, if you run a regular dictionary, even if you have variants in there, which most people don't, you don't always get out the actual pronunciation, so that's why the human transcribers giving you that pronunciation.
Speaker D: Actually, maybe they're using phone recognizers.
Speaker D: They, they, they, they were, I think they were, I think they would be good.
Speaker E: Yeah, so I think that we also don't have, I mean, we've got a good start on it, but we don't have a really good meeting recorder recognizer or transcribe or anything yet.
Speaker E: So, I mean, the other way to look at this is to, is to do some stuff on switchboard, which has all this other stuff to it.
Speaker E: And then, as we get further down the road, we can do more things ahead of time.
Speaker E: We can do some of the same things to the meeting data.
Speaker E: Yeah.
Speaker C: And these people might, they, they are, most of them are trained with IPA. They'd be able to do, than any level coaching.
Speaker G: Are they busy for the next couple of years?
Speaker C: You know, I mean, they, they're interested in continuing working with us.
Speaker C: So, I mean, I, and this would be up their alley, so we could, when, when you meet with, with John O'Hall and find what, just text on him, he want to apply them.
Speaker C: Maybe, you're good to train them too.
Speaker C: Yeah.
Speaker E: Anyway, this is not an urgent thing at all. It's just a team-mob team that have that data.
Speaker H: How would you do a forced alignment?
Speaker H: Interesting idea. You want to iterate somehow.
Speaker H: Interesting thing they think about.
Speaker H: I mean, you'd want models for spreading.
Speaker G: Of the acoustic teachers.
Speaker F: Yeah.
Speaker D: Well, it might be, you need to do some phonetic features.
Speaker D: These non-word words, or these kinds of words that people never, the, the, the, hums or the, these, no, I'm serious.
Speaker D: They're all these kinds of functional elements.
Speaker D: I don't know what you call them.
Speaker D: It's not just field-positive, but all kinds of ways of interrupting and some of them are, yeah, a hus and, hmm.
Speaker D: Okay. Grants. Now, that might be interesting.
Speaker C: He's got the Vibes eating.
Speaker F: We should move on.
Speaker F: New version of pre-signitation.
Speaker B: Oh, yeah. I worked a little bit on a pre-signitation to get the non-version, which does channel-specific speech-transpeak detection.
Speaker B: And what I did is I used some normalized features, which look into the, which is normalized energy, energy normalized by the mean over the channels and by the minimum over the, over within each channel.
Speaker B: And to, to, yet, to normalize also loudness and modified loudness and things, and those special features, which are in my feature actor.
Speaker B: And, and therefore, to be able to somewhat distinguish between foreground and background speech in, in the different, in each channel.
Speaker B: And, I tested it on, on three or four meetings, and it seems to work really well, I would say.
Speaker B: There are some problems with the lapel mic, of course.
Speaker H: That's great.
Speaker H: Yeah. So, I understand that's what you were saying about your problem with minimum.
Speaker B: Yeah, and I had, I had specific problems with the...
Speaker H: 90th quartile rather than minimum.
Speaker B: Wow.
Speaker B: Yeah, yeah. I did some, some, something like that.
Speaker B: And, I mean, there are, there are some, some problems in, when, in the channel there, there, there, there's a speaker dozen, doesn't work match or doesn't talk at all.
Speaker B: Then, the, yeah, there are, there are some problems with, with the normalization and, then, the system doesn't work at all.
Speaker B: So, I'm glad that there is the, the digit part where everybody's supposed to say something.
Speaker B: So, that's, that's great for, for my purpose.
Speaker B: And, the thing is, I, I, the evaluation of, of the system is a little bit hard as I don't have any references.
Speaker H: Well, we did the hand, the one by hand.
Speaker B: Yeah, that's the one, where, where I do the training on, so I come to the evaluation on that.
Speaker B: So, the thing is, can the transpray was perhaps do some, some, some meetings in, in terms of speech, non speech in, in the specific transpray?
Speaker B: Well, won't you have that from their transcriptions?
Speaker G: Well, okay.
Speaker C: So, I think I might have done what you're requesting, though I did it in a service of a different thing.
Speaker C: I have 30 minutes that I, more tightly transcribe with reference to individual channels.
Speaker B: Okay, that's great, that's great for me, okay.
Speaker H: So, hopefully that's not the same meeting that we did.
Speaker H: No, actually it's different meeting.
Speaker C: So, um, so, you know, we have the, they transcribe as if it's one channel with these, with the slashes to separate the overlapping parts.
Speaker C: And then we run it through, then I'm going to edit it, then I'm going to run it through channelize, which takes it into Dave Gilbert's format.
Speaker C: And then you have all these things split across according to channel.
Speaker C: And then that means that if a person contributed more than one synagogum overlap during that time bin that, that two parts of the utterance end up together, it's the same channel.
Speaker C: Okay.
Speaker C: And then I took his tool, and last night for the first 30 minutes of one of these transcripts, I tightened up the boundaries on individual speakers channels.
Speaker C: Because his, his interface allows me to have total flexibility in the time tags across the channels.
Speaker B: And, um, so, yeah, yeah, that's great, but would be nice to have some more meetings, not just one meeting to be sure that that we could get a couple meetings done with that level of precision.
Speaker H: I think that would be a good idea.
Speaker C: Oh, okay.
Speaker C: How much time, so the meeting is very in length.
Speaker C: What are we talking about in terms of the number of minutes you'd like to have as your, as your dream is at?
Speaker B: It seems to me I would be good to have a few minutes from, from different meetings.
Speaker B: So, but I'm not sure about how much.
Speaker C: Okay. Now, you're saying different meetings because of different speakers or because of different audio quality or both different, different, different number of speakers, different speakers, different.
Speaker E: Yeah, we don't have that much variety of meetings yet. I mean, we have this meeting and future meeting. We have a couple others that we have a couple examples of.
Speaker A: Even probably with the games differently will affect it.
Speaker B: Not really as, because of the normalization.
Speaker D: We can try running, we haven't done this yet because Andreas is going to move over the SRI recognizer.
Speaker D: Basically, I ran out of machines at SRI because we're running the e-vails and I just don't have machine time there.
Speaker D: But once that's moved over, hopefully in a couple days, then we can take what Jane just told us about as the pre-segmented, the segmentations that you did at level eight or some threshold that same, right.
Speaker D: And try doing forced alignment on the word strings. And if it's good, then that will, that may give you a good boundary. Of course, if it's good, we don't, then we're fine.
Speaker D: But I don't know yet whether these segments that contain a lot of pauses around the words will work or not.
Speaker B: I would quite like to have some manually transcribed references for a system as I'm not sure if it's really good to compare with some other automatic boundaries.
Speaker C: Well, now if we were to start with this and tweak it manually, they might be okay. It really depends on a lot of things.
Speaker D: But I would have maybe a transcriber look at the old forced alignment and then adjust those that might save some time.
Speaker D: If they're horrible, it won't help at all. But they might not be horrible.
Speaker D: So I'll let you know on the...
Speaker D: Okay, great.
Speaker C: How many minutes would you want from... I mean, we could easily get a section, you know, like say a minute or so, from every meeting that we have. So from the newer ones that we're working on.
Speaker C: And then...
Speaker B: If it's not the first minute of the meeting, that's okay with me, but in the first minute, there are often there are some strange things going on, which are really well for which are really good.
Speaker B: So what I'd quite like perhaps is to have some five minutes of different meetings.
Speaker C: Somewhere not in the very beginning, five minutes, okay.
Speaker C: And then I wanted to ask you just for my information then, would you be training...
Speaker C: So would you be training then the segment or so that it could on the basis of that segment the rest of the meeting?
Speaker C: So if I give you like five minutes, is the idea that this would then be applied to providing a higher...
Speaker B: I could do a retraining with that, yeah.
Speaker B: But I hope that I don't need to do it.
Speaker B: So it can be doing an unsupervised way.
Speaker C: Excellent.
Speaker C: Okay.
Speaker B: I'm not sure, but for those three meetings, which I did, it seems to be quite well.
Speaker B: But there are some, as I said, some of some problems with the lapel mic, but perhaps we can do something with cross correlations to get rid of those.
Speaker B: That's what I, that's my future work.
Speaker B: What I want to do is to look into cross correlations for removing those false overlaps.
Speaker D: Wonderful.
Speaker D: Are the wireless different than the wired mics at all?
Speaker B: I'm not sure. If there are any wired mics in those meetings, or I have to look at them.
Speaker B: But I think there's no difference between...
Speaker D: This is just the lapel versus everything else.
Speaker C: Okay, so then if that's five minutes per meeting, we got like 12 minutes, 12 meetings.
Speaker C: We're happy that I've been working with.
Speaker E: Of the meetings that you're working with, how many of them are different?
Speaker E: Are there any of them that are different than these two meetings?
Speaker C: Oh, in terms of the speakers or the...
Speaker C: Yes, speakers. Sorry.
Speaker C: We have different combinations of speakers.
Speaker C: I mean, just from what I've seen, there are some where your present are not present.
Speaker C: And then you have the difference between the network's group and this group.
Speaker C: Yeah, so I didn't know any of the group you had.
Speaker C: So you have the networks meeting?
Speaker E: Do you have any of Jerry's meetings in your time?
Speaker C: No.
Speaker C: No.
Speaker C: We could. I mean, you recorded one last week or so.
Speaker C: Yes, we.
Speaker D: We're going to be recording them every Monday.
Speaker E: Because I think he really needs a variety and having as much variety for speaker, we certainly would be a big part of that.
Speaker C: Okay, so if I, okay, include.
Speaker C: Okay, then if I were to include all together samples from 12 meetings, that would only take an hour.
Speaker C: And I could get the transcripts to do that, right?
Speaker C: I mean, what I mean is that would be an hour of sample, and then they transcribe those that hour, right?
Speaker C: That's what you should do.
Speaker E: Yeah.
Speaker E: And, right, like the yours.
Speaker C: I mean, I mean, adjust.
Speaker C: So they get it into the multi-channel format and then adjust the time bands.
Speaker E: So that should be faster than the 10 times.
Speaker C: Absolutely.
Speaker C: I did, I did.
Speaker C: So last night I did.
Speaker C: Well, last night I did about half an hour in three hours, which is not terrific, but anyway, it's an hour and a half per.
Speaker C: So I can't calculate on my.
Speaker B: The transcripts actually stop with transcribing new meetings or are they?
Speaker C: Well, they're still working. They still have enough to finish that I haven't assigned a new meeting.
Speaker C: But the next, I was about to need to assign a new meeting and I was going to take it from one of the new ones, and I can easily give them Jerry Feldman's meeting no problem.
Speaker C: Okay.
Speaker D: So they're really running out of data.
Speaker D: I mean, that's good.
Speaker D: At that first set, okay.
Speaker E: They're running out data unless we make the decision that we should go over and start transcribing the other set.
Speaker E: So the first set, the first set.
Speaker C: And so I was in the process of like adding this wonderful news.
Speaker C: We funded experiment, but also we were thinking maybe applying that to getting the final date very useful to getting the overlaps to be more precise all the time.
Speaker E: This blends nicely into the update on transcripts.
Speaker C: Yes, it does.
Speaker C: Well, Liz and Don and I met this morning in the Barco room and this afternoon.
Speaker C: It's afternoon.
Speaker C: It's afternoon.
Speaker C: It's afternoon to the afternoon.
Speaker C: Concerning this issue of the, well, there's basically the issue of the interplay between the transcript format and the processing that they need to do for the SRI recognizer.
Speaker C: And well, so I mentioned the process that I'm going through with the data.
Speaker C: So I get the data back from the transcript.
Speaker C: Well, I met it for like, get the data back from the transcriber and then I check for a simple things like spelling errors and things like that.
Speaker C: And I'm going to be doing a more thorough editing with respect to consistency of the conventions.
Speaker C: But they're generally very good.
Speaker C: And then I run it through the channelized program to get into the multi-channel format.
Speaker C: Okay.
Speaker C: And what we discussed this morning, I would summarize, is saying that these units that result in a particular channel and a particular time band at that level, very in length.
Speaker C: And their recognizer would prefer that the unit's not the overly long, but it's really an empirical question.
Speaker C: Whether the units we get at this point through just that process I described might be sufficient for them.
Speaker C: So as a first pass through, the first chance without having to do a lot of hand editing, what we're going to do is I'll run it through channelized, give them those data after I've done the editing process.
Speaker C: I'm sure it's great.
Speaker C: I can do that pretty quickly with just that minimal editing without having to hand break things.
Speaker C: And then we'll see if the units that we're getting at that level are sufficient and maybe don't need to be further broken down.
Speaker C: And if they do need to be further broken down, then maybe it just be piecewise.
Speaker C: Maybe it won't be the whole thing.
Speaker C: So that's what we were discussing.
Speaker C: Right?
Speaker C: Also, we discussed a lot of educational things.
Speaker C: So it's like, I hadn't incorporated a convention explicitly to handle acronyms, for example.
Speaker C: But if someone says PCM, it would be nice to have that be directly interpretable from the transcript, what they said, or tickle TCL.
Speaker C: And so I've incorporated also convention with that, but that's easy to handle with the post-editing phase.
Speaker C: And I'll mention it to transgarbers for the next phase.
Speaker C: And that's okay.
Speaker C: And then a similar convention for numbers.
Speaker C: So they say 183 versus 183.
Speaker C: And also I'll be encoding, as I do my post-editing, the things that are in curly brackets, which are clarificational material, to incorporate keyword at the beginning.
Speaker C: So it's going to be either a gloss, or it's going to be a vocal sound like a laugh or a cough, or a non-vocal sound like a dorsal, dorsal, and that can be easily done with a, you know, just a lemon-like additional thing in a general format.
Speaker D: Yeah, we just needed a way to strip, you know, all the comments, all the things that the linguist wants, but the recognizer can't do anything with.
Speaker D: But to keep things that we mapped to like reject models, or mouth noise, or cough.
Speaker D: And then there was this interesting issue, Jane, brought up, which I hadn't thought about before, but I was realizing as I went through the transcripts that there are some noises like, well, the good example was an in-breath where a transcriber working from the mixed signal doesn't know whose breath it is.
Speaker D: And they've been assigning it to someone that may or may not be correct. And what we do is, if it's a breath sound, you know, a sound from the speaker, we map it to a noise model, like a mouth noise model in the recognizer.
Speaker D: It probably doesn't hurt that much once in a while to have these, but if they're in the wrong channel, that's not a good idea.
Speaker D: And then there's also things like door slams that's really in no one's channel. They're like, it's in the room. And Jane had this nice idea of having like an extra...
Speaker D: An extra channel.
Speaker D: Yeah, I've been adding that.
Speaker D: And we were thinking that is useful also when there's uncertainty. So if they hear a breath and they don't know whose breath it is, better to put it in that channel than to put it in the speaker's channel because maybe it was someone else's breath.
Speaker D: So I think that's a good... you can always clean that up post-processing. There's a lot of little details, but I think we're coming to some kind of closure.
Speaker D: So the idea is that Don can take Jane's post-process channelized version and with some scripts convert that to a reference for the recognizer and we can run these.
Speaker D: So when that's ready, as soon as that's ready and as soon as the recognizer is here, we can get 12 hours of force aligned and recognize data and start working on it.
Speaker D: So I don't know, a couple of weeks or two away, I would say. If that process is automatic once we get your post-processed transcript.
Speaker C: And that doesn't mean that a better thing that it would require is not very much. They're just hoping that the units that are provided in that way will be sufficient because that would save a lot of time dividing things.
Speaker D: Yeah, some of them are quite long. I don't know how long you did one.
Speaker A: I saw a couple around 20 seconds and that was just without looking too hard for it. So I would imagine that there might be something that are longer.
Speaker A: One question, would that be a single speaker? Is that multiple speakers overlap? No, but if we're going to segment it, like if there's one speaker in there that says okay or something, right in the middle is going to happen around it.
Speaker D: It's not the fact that we can't process a 20 second segment. It's the fact that there's 20 seconds in which to place one word in the wrong point.
Speaker D: If someone has a very short utterance there and that's where we might want to have this individual, you know, have your pre-processed input.
Speaker B: And I just don't know how to do that. It's not that perhaps the transcripts could stop then from those multiple speakers detections.
Speaker D: Right. And enjoying the hand margin. Yeah, that's what I was thinking too. That's probably what will happen, but we'll try it this way and see.
Speaker D: I mean, it's probably good enough for a force alignment. If it's not then we're really, and we definitely, but for free recognition, it'll probably not be good enough.
Speaker D: Like it lots of errors because of the crosstalk and noise.
Speaker E: Good. I think that's probably agenda. Oh, I want to ask one thing. Yeah, then microphones and microphones. When do we get to?
Speaker D: They said to take about a week. You already. So what happens to our old microphones?
Speaker H: Well, the only thing we're going to have extra. Right. We don't have extra now is just the lapel, not the body pack, just lapel.
Speaker H: And then one of the one of those since what I decided to do on Morgan's suggestion was just get two new microphones and try them out.
Speaker H: And then if we like them, we'll get more. Since they're there like 200 bucks a piece. We want to at least try them out.
Speaker H: So it's a replacement for this headset and mic. Yeah. And they're going to do the wiring for us. What's the style of the headset?
Speaker H: It's it's by crown and it's one of these sort of mount around the ear thingies. And when I when I mentioned that we thought it was uncomfortable, he said it was a common problem with the Sony. And this is how apparently a lot of people are getting around it. And I checked on the web and every side I went to raved about this particular mic. It's apparently comfortable and stays on the head well. So we'll see if it's any good. But I think it's promising.
Speaker H: Yeah. Yeah. So it was.
Speaker H: It was accurate.
Speaker F: Yeah. It was a great employee. It was for the record Adam is not a patient. That's our self-employee. It's a listed out. Well we're using the crown. These are crown aren't they? The PCMs are crown aren't they?
Speaker H: Yeah. Yeah. They were you bet. And they worked very well. So if we go to a workshop about all this, it's going to be a meeting about meetings about meetings.
Speaker H: And then we have to go to the planning session for that workshop. Oh, that would be a meeting about the meeting.
Speaker H: Start saying M4. Yeah.
Speaker E: And we did it.
Speaker H: Yep. Go for it.
Speaker F: Okay.
Speaker E: Transcript.
Speaker E: 261-126-3-0. 3-20-5653-4-450-57566-662-789-0215263-2512-37-4706800-5681-93 04012
Speaker G: Transcript 2591-2610-2497980-3406-4656-7087621-93-0205-0413100-1754 280-2814-3405-620-73913-841-9720100-0106-7931
Speaker H: Transcript 3731-3750-7200-499-888-9800-2116-337-47-59304-69922-78-0104-19557-2990-34 509-714-1200
Speaker D: Transcript 3711-3730-607-307-890454-12439-263456095-81966-3810-057012-30708-308-1080 7 0 8 5 3 1 4 8 0 3 6 4 2
Speaker H: Paws between lines remember.
Speaker C: 2 1 7 1 dash 2 1 9 0 6 2 9 7 7 2 7 0 7 9 7 8 6 9 9 0 0 8 9 0 1 0 7 3 2 3 1 4 2 2 4 5 7 7 8 4 6 8 0 7 0 7 8 0 3 4 7 9 0 2 0 1 7 7 0 7 2 3 0 8 3 9 4 9 5
Speaker B: Transcript 2 1 5 1 dash 2 1 7 0 5 2 4 8 6 3 7 6 8 7 5 6 9 8 9 0 0 7 8 1 1 2 3 6 3 8 4 8 1 3 0 2 5 6 7 9 2 8 3 5 2 8 0 3 0 0 9 3 1 8 0 8 2 3 2 2 3
Speaker A: 0 4 8 6 0 9 6 5 Transcript 2 3 5 1 dash 2 3 7 0 2 3 5 4 0 4 2 6 4 1 7 7 4 8 7 2 1 8 9 0 0 2 3 3 0 4 8 2 5 8 9 6 0 9 0 4 8 2 7 8 0 9 8 0 4 0 0 0 1 7 0 4 3 0 2 8 1 1 7 8 0 5 7 2 9 6 2 0 excuse me 2 9 6 2 1 1 0 ok
