Speaker D: A, A.
Speaker D: It's a string.
Speaker D: Test.
Speaker D: Test.
Speaker E: Test.
Speaker E: Test.
Speaker E: Test.
Speaker E: So, let's see.
Speaker E: Yeah, Barry's not here and Dave's not here.
Speaker E: I can say about this quickly to get through it.
Speaker E: David, I submitted this.
Speaker E: This is for you.
Speaker G: Yes, I do.
Speaker E: Yeah, it's interesting.
Speaker E: I mean, basically, we're dealing with reverberation.
Speaker E: When we deal with pure reverberation technique, it's using works really, really well.
Speaker E: When they have reverberation in here, we'll measure the signalized ratio.
Speaker E: It's about 9 dB.
Speaker E: So, fair amount of...
Speaker F: I mean, for that actual recording.
Speaker E: Yeah.
Speaker E: And actually, it brought up a question, which may be relevant to the Aurora stuff, too.
Speaker E: I know that when you figured out the filters that we're using for the MellScale, there were some experimentation that went on at OGI.
Speaker E: But one of the differences that we found between the two systems that we were using, the Aurora HTK system, baseline system, and the system that we were...
Speaker E: The other system we were using, the SRI system, was that the SRI system had maybe 100 hertz high pass.
Speaker E: And the Aurora HTK was like 24.
Speaker D: 64.
Speaker E: Yeah. If you're using the baseline...
Speaker E: Is that the band center?
Speaker D: No, the edge.
Speaker E: The edge is really 64.
Speaker E: Some reason...
Speaker A: So, the center would be somewhere around like 100 and...
Speaker A: 100 and 100 and maybe it's like 100 hertz.
Speaker E: But do you know, for instance, how far down there would be at 20 hertz?
Speaker E: What, how much rejection would there be at 20 hertz?
Speaker E: At 20 hertz.
Speaker E: Yeah, any idea what the curve looks like?
Speaker A: 20 hertz frequency...
Speaker A: Oh, it's zero at 20 hertz, right?
Speaker A: The filter?
Speaker B: Actually, the left edge of the first filter is 64.
Speaker A: So anything less than 64 is zero.
Speaker E: It's actually set to zero.
Speaker E: Yeah. You can filter that.
Speaker E: Oh, from the...
Speaker B: This is the filter bank.
Speaker B: Oh, so you're going to be doing this?
Speaker A: Yeah, it's zero.
Speaker A: Yeah, so it's a weight on the power spectrum.
Speaker A: Triangular weighting.
Speaker E: Right.
Speaker E: Okay.
Speaker E: Okay.
Speaker E: So that's still different than Dave thought, I think.
Speaker E: But still, it's possible that we're getting in some more noise.
Speaker E: So I wonder, was there...
Speaker E: There was experimentation with, say, throwing away that filter or something?
Speaker E: And...
Speaker A: Throwing away the first?
Speaker A: Yeah.
Speaker A: Okay, we've tried including the full band, right, from zero to four k.
Speaker A: And that's always worse than using 64 hertz.
Speaker E: Right.
Speaker E: But the question is whether 64 hertz is too low.
Speaker A: Yeah, I mean, make it 100 or something?
Speaker A: Yeah.
Speaker A: I think I've tried 100 and it was...
Speaker A: More or less the same as likely was.
Speaker A: On the same speech that car.
None: Aurora.
Speaker E: It was on the speech that car.
Speaker A: Yeah.
Speaker A: So I tried 100 to 4 k.
Speaker A: Yeah.
Speaker E: So it was...
Speaker E: And on the...
Speaker E: The address also.
Speaker D: No, no, no.
Speaker A: I think I just tried it on speech that car.
Speaker E: Maybe something to look at sometime because what he was looking at was performance in this room.
Speaker E: Would that be more like...
Speaker E: Well, you think that'd be more like speech that car, I guess, in terms of the noise.
Speaker E: The speech that car is more sort of roughly stationary, a lot of it.
Speaker E: Yeah.
Speaker E: And TI digits maybe is not so much.
Speaker E: Yeah.
Speaker D: Yeah.
Speaker D: Okay, well maybe it's not a big deal.
Speaker E: But anyway, that was just something we wondered about.
Speaker E: But certainly a lot of the noise is below 100 hertz.
Speaker E: The signalized ratio looks fair and mouth better if you have passed filter it from this room.
Speaker E: But it's still pretty noisy.
Speaker E: Even 100 hertz up, it's still fairly noisy.
Speaker E: The signalized ratio is actually still pretty bad.
Speaker E: So, I mean, the main...
Speaker E: So that's something.
Speaker F: The far field.
Speaker F: Yeah, that's the far field.
Speaker E: Yeah, the near field would be pretty good.
Speaker F: So what is...
Speaker F: What's causing that?
Speaker E: Well, we got a video projector in here.
Speaker E: And which we keep on during every session we record.
Speaker E: We were aware of, but we thought it wasn't a bad thing.
Speaker E: That's a nice noise source.
Speaker E: And there's also the air conditioning, which is pretty low frequency kind of thing.
Speaker E: So those are major components, I think, for the stationary kind of stuff.
Speaker E: But I guess I've maybe said this last week too, but it really became apparent to us that we need to take a count of noise.
Speaker E: So I think when he gets done with his pre-limps study, I think one of the next things we want to do is to take this noise processing stuff and synthesize some speech from it.
Speaker E: What are his pre-limps?
Speaker E: I think in about a little less than two weeks.
Speaker C: Oh, wow.
Speaker E: Yeah.
Speaker E: Yeah.
Speaker E: So.
Speaker D: It might even be sooner.
Speaker D: I see this is 16th, 17th.
Speaker D: Yeah, I don't know if it's before.
Speaker F: It might even be a week.
Speaker F: So a week we can have.
Speaker E: I guess they were going to do it sometime during the semester.
Speaker E: They seem to be...
Speaker E: Well, the semester actually is starting out.
Speaker E: Is it already?
Speaker E: Yeah, the semester is late August.
Speaker E: They start here.
Speaker E: So they do it right at the beginning of the semester.
Speaker E: Yeah.
Speaker E: So, yeah, I mean, that was sort of one... I mean, the overall results seemed to be first place in the case of either artificial reverberation or a modest sized training set either way.
Speaker E: It helped a lot.
Speaker E: But if you had a really big training set, a recognizer system that was capable of taking advantage of a really large training set.
Speaker E: So one thing with the HTK is that it has...
Speaker E: As we're using... the configuration we're using is being gone by the terms of Aurora.
Speaker E: We have all those parameters just set as they are.
Speaker E: So even if we had 100 times as much data, we wouldn't go out to, you know, 10 or 100 times as many Gaussian or anything.
Speaker E: So it's kind of hard to take advantage of big chunks of data.
Speaker E: Whereas the other one does sort of expand as you have more training data.
Speaker E: So that's what's about the matric rate actually.
Speaker E: And so that one really benefited from the larger set.
Speaker E: And it was also a diverse set with different noises and so forth.
Speaker E: So that seemed to be...
Speaker E: So if you have that better recognizer, that can build up more parameters.
Speaker E: And if you have the natural room, which in this case has a pretty bad signal noise ratio, then in that case...
Speaker E: The right thing to do is just to use speaker adaptation.
Speaker E: And not bother with the psychostica processing.
Speaker E: But I think that that would not be true if we did some explicit noise processing as well as the kind of additional kind of things we were doing.
Speaker E: So that's sort of what we found.
Speaker F: I started working on the city state recognizer.
Speaker F: Oh, OK.
Speaker F: So I got a touch with Joe and from your email and things like that.
Speaker F: They added me to the list.
Speaker F: Oh, good question.
Speaker F: The mailing list.
Speaker F: He gave me all the pointers and everything that I needed.
Speaker F: So I downloaded the... There were two things that they had to download.
Speaker F: One was the software.
Speaker F: And another was a sample.
Speaker F: So I downloaded the software.
Speaker F: And piled all of that.
Speaker F: Oh, great.
Speaker F: And I grabbed the sample stuff that I haven't...
Speaker A: That sample was released only yesterday or the day before, right?
Speaker F: Well, I haven't grabbed that one yet.
Speaker F: So there was another short sample.
Speaker F: So I haven't grabbed the latest one that he just...
Speaker F: Oh, OK.
Speaker F: But the software is going to be fine and everything.
Speaker E: Is there any word yet about the issues about adjustments for different future sets?
Speaker F: No.
Speaker F: You asked me to write to him and I think I forgot to ask him about that.
Speaker F: I don't remember yet.
Speaker F: I'll check that.
Speaker F: Yeah.
None: Yeah.
Speaker E: So that turned out to be an important issue for us.
Speaker E: Yeah.
Speaker F: Because they have...
Speaker F: The old send is the best.
Speaker A: Because they have already frozen those insertion penalties and all those stuff is what I feel.
Speaker A: Because they have this document explaining the recognizer and they have this tables with various language model weights insertion penalties.
Speaker F: OK.
Speaker A: I haven't seen that one yet.
Speaker A: It's there on that.
Speaker A: And on that, I mean, they have run some experiments using various insertion penalties and all those things.
Speaker A: Yeah.
Speaker E: I think they picked the values for what test set?
Speaker A: So the one that they have reported is NIST evaluation Wall Street Journal.
Speaker E: But that has nothing to do with testing.
Speaker A: So they are actually trying to fix those values using the clean training part of the Wall Street Journal, which is...
Speaker A: I mean, the Aurora.
Speaker A: Aurora has a clean subset.
Speaker A: I mean, they're going to train it and then they're going to run some evaluations.
Speaker E: So they're setting it based on that.
Speaker E: OK. So now we may come back to the situation where we may be looking for a modification of the features to account for the fact that we can't modify these parameters.
Speaker E: But it's still worth, I think, just chatting with Joe about the issue.
Speaker F: Do you think that something I should just send to him or do you think I should send it to this?
Speaker F: I'm mailing this.
Speaker E: Well, it's not a secret. We're certainly willing to talk about it with everybody.
Speaker E: But I think that it's probably best to start talking with him just to...
Speaker E: Yeah, it's a dialogue between two of you about what, you know, what does he think about this and what could be done about it.
Speaker E: If you get 10 people involved in it, there'll be a lot of perspectives based on you.
Speaker E: But I think it all should come up eventually. But if there's any way to move in a way that would be more open to different kinds of features.
Speaker E: But if there isn't, it's just kind of shut down.
Speaker E: And that's also, it's probably not worthwhile bringing it into a larger form where political issues will come in.
Speaker E: OK.
Speaker A: So this is now... It's compiled on a Solaris.
Speaker A: Yeah. Because there was some mail saying that it's not stable for Linux and all those.
Speaker F: Yeah, that was a particular version.
Speaker F: Soos. Yeah. Soos. Yeah.
Speaker D: Yeah.
Speaker D: OK.
Speaker D: So it should be OK.
Speaker F: It could be compiled by actually no errors.
Speaker E: There's a slightly off topic, but I noticed just glancing at the Hopkins Workshop website that...
Speaker E: I don't know, we'll see how much they accomplished. One of the things that they were trying to do.
Speaker E: And the graphical models thing was to put together a toolkit for doing arbitrary graphical models for speech recognition.
Speaker E: So Jeff, two Jeffs.
Speaker F: Who's the second Jeff?
Speaker E: Oh, do you know Jeff Swag?
None: Yeah.
Speaker E: Oh, he was here for a couple of years and he got his PhD.
Speaker E: And he's been an IBM last couple of years.
Speaker E: OK.
Speaker E: So he did his PhD on dynamic basenets for speech recognition.
Speaker E: And he had some continuity built into the model, presumably to handle some inertia in the production system.
Speaker E: So...
Speaker B: I've been playing with first the VAD.
Speaker B: So it's exactly the same approach, but the features that the VAD neural network use are MFCC after noise compensation.
Speaker D: I think it results. What was it used before?
Speaker B: It was just PLP.
Speaker A: Yeah, it was actually...
Speaker A: No, it was just the noisy features, I guess.
Speaker B: Yeah, you see, not compensated features.
Speaker B: This is what we get after...
Speaker B: So actually, the features are noise compensated and there is also the LDA filter.
Speaker B: And then it's a pretty small neural network which use 9 frames of 6 features from C0 to C5 plus the first derivatives.
Speaker B: And it has 100 hidden units.
Speaker F: Is that 9 frames centered around the current frame?
Speaker E: So I'm sorry, there's how many inputs?
Speaker E: So it's 12 times 9 inputs and 100 hidden items.
Speaker B: Two outputs.
Speaker E: OK, next we have 11,000 parameters which actually shouldn't be a problem.
Speaker F: So what is different between this and...
Speaker B: So the previous system, it's based on the system that has a 53.66% improvement.
Speaker B: It's the same system, the only thing that changes the estimation of the silence probabilities.
Speaker B: Which now is based on cleaned features.
Speaker B: A lot better.
Speaker B: So it's not bad, but the problem is still that the latency is too large.
Speaker B: What's the latency?
Speaker B: The latency of the VAD is 220 milliseconds.
Speaker B: And the VAD is used for online normalization.
Speaker B: And it's used before the delta computation.
Speaker B: So if you add this components, it goes to 170.
Speaker E: I'm confused, you start off with 220 and you're under the output 170.
Speaker B: With 270.
Speaker B: If you add the delta computation, which is done afterwards.
Speaker E: So it's 220.
Speaker E: Is this at least 20 milliseconds frames?
Speaker E: Is that why?
Speaker E: Is it after-down?
Speaker B: The 220 is 100 milliseconds for the...
Speaker B: No, it's 40 milliseconds for the cleaning of the speech.
Speaker B: Then there is the neural network, which uses 9 frames.
Speaker B: So it adds 40 milliseconds.
Speaker B: After that, you have the filtering of the silence probabilities.
Speaker B: Which is a median filter.
Speaker B: It creates 100 milliseconds delay.
Speaker A: Plus there is a delta input.
Speaker B: And there is a delta input, which is...
Speaker E: 1.2 milliseconds for smoothing.
Speaker E: So it's...
Speaker D: 40 plus...
Speaker D: And then 40 is 40 plus...
Speaker D: 40 plus 100.
Speaker A: So it's 200 actually.
Speaker B: Yeah, there are 20 that comes from...
Speaker B: There is 10 that comes from the LDA filters also.
Speaker B: So it's 210, yeah.
Speaker A: If you're using...
Speaker A: But if you're using 3 frames...
Speaker A: If you're using 3 frames, it is 30 here for delta.
Speaker B: I think it's 5 frames.
Speaker A: So 5 frames are just 20.
Speaker A: Okay, so so 200 in time.
Speaker E: 40 for the cleaning of the speech.
Speaker E: 40 for the NN.
Speaker E: 100 for the smoothing.
Speaker B: 24 delta.
Speaker A: I think 24 delta.
Speaker A: I mean that's the input of the net.
Speaker D: And delta inputs in that.
Speaker A: Yeah, so it's like...
Speaker A: 5, 6, 7 plus delta.
Speaker A: 9 frames of.
Speaker E: And then 10 milliseconds for...
Speaker A: This is an LDA filter.
Speaker E: 10 milliseconds for LDA filter.
Speaker E: And another 10 milliseconds you said for the frame.
Speaker B: For the frame, I guess I computed 220 here.
Speaker B: Yes, it's for the...
Speaker E: Okay, and it's delta beside that.
Speaker B: So this is the features that are used by the network.
Speaker B: And then afterwards you have to compute the delta on the main feature stream, which is...
Speaker B: Delta and double delta, which is 15 milliseconds.
Speaker E: Yeah, now I mean after the noise part, the 40, the other 180...
Speaker E: Well, I mean...
Speaker E: Wait a minute, some of this is in parallel, isn't it?
Speaker E: I mean the LDA...
Speaker E: Well, the LDA is part of the VAD, right?
Speaker B: The VAD use LDA filter at features also.
Speaker B: Oh, it does.
Speaker E: Ah.
Speaker E: So in that case there isn't too much in parallel.
Speaker B: No, there is...
Speaker B: Just don't sampling, up sampling.
Speaker E: So the delta at the end is how much?
Speaker D: It's 50.
Speaker D: 50.
Speaker B: All right, so...
Speaker B: Well, we could probably put the delta before online normalization.
Speaker B: Should not make a big difference because...
Speaker F: We use this smaller window for the delta.
Speaker F: I guess there's a lot of things.
Speaker E: Yeah, if you put the delta before the...
Speaker E: Yeah, because then I click on parallel.
Speaker B: Yeah, because the time constant of the online normalization is pretty long compared to the delta window.
Speaker E: Okay, so...
Speaker E: And you ought to be able to pull off 20 milliseconds from somewhere else to get under 200, right?
Speaker E: I mean, it's 200 milliseconds for smoothing, it's sort of an arbitrary amount, it could be 80.
Speaker F: Yeah, probably.
Speaker F: So what's the baseline to be under?
Speaker E: Well, we don't know.
Speaker E: They're still arguing, but...
Speaker E: I mean, if it's 250, then we can keep the delta where it is if we shaved off 20.
Speaker E: If it's 200, if we shaved off 20, we could meet it by moving the delta back.
Speaker F: So, have you know that what you have is too much if they're still the same?
Speaker E: Oh, we don't, but it's just...
Speaker E: I mean, the main thing is that since we got burned last time, but not worrying about it very much, we're just staying conscious of it.
Speaker E: Oh, okay.
Speaker E: And so, I mean, if a week before we have to be done, someone says, well, you have to have 50 milliseconds less than you have now,
Speaker F: it would be pretty magic around here, so. That's still best.
Speaker F: That's a pretty big win, and it doesn't seem like you're in terms of your delay or...
Speaker F: That.
Speaker E: He added a bit on, I guess, because before we were able to have the noise stuff and the LDA being parallel, and now he's requiring it to be done first.
Speaker B: Well, I think the main thing, maybe it's the cleaning of the speech, which takes 40 milliseconds or so.
Speaker B: Right.
Speaker E: Well, so you say, let's say 10 minutes, I'll take seconds for the LDA.
Speaker E: LDA is pretty short, right now.
Speaker E: Well, 10, yeah.
Speaker E: And then four...
Speaker A: Yeah, the LDA, we don't know, is it very crucial for the features, right?
Speaker B: No, I just...
Speaker B: Yeah.
Speaker B: This is the first try, I mean, maybe it's not very useful.
Speaker E: But I think you have, I mean, you have 20 for Delta computation, which you now is already doing twice, right?
Speaker E: Were you doing that before?
None: Hmm.
Speaker B: Well, in the proposal, the input of the VAD network were just three frames.
Speaker A: Yeah, just a static.
Speaker B: No, that's the features.
Speaker E: So what you have now is 40 for the noise, 20 for the Delta and 10 for the LDA.
Speaker E: That's 70 milliseconds of stuff, which was formerly in parallel, right?
Speaker E: So I think, you know, that's the difference, as far as the timing.
Speaker E: Yeah.
Speaker E: Right.
Speaker E: And you can experiment with cutting various pieces of these back a bit.
Speaker E: But, I mean, we're not...
Speaker E: We're not in terrible shape.
Speaker F: Yeah, that's what it seems like.
Speaker E: Yeah.
Speaker E: It's not like it's heading up to 100 milliseconds or something.
Speaker F: Where is this 57.02?
Speaker F: And in comparison to the last evaluation?
Speaker F: Well, it's, I think,
Speaker E: it's better than anything in any bodyguide. I was alright.
Speaker B: The best was 54.
Speaker B: Yeah, 5.
Speaker B: And our system was 49, but with the neural network.
Speaker B: With the neural net.
Speaker E: Yeah.
Speaker A: So this is like the first proposal.
Speaker A: The proposal wanted was 44, actually.
Speaker A: Yeah.
Speaker E: Yeah.
Speaker E: And we still have a neural net in.
Speaker E: So it's, you know, it's, you know, so it's, we're doing better.
Speaker E: I mean, we're getting really good.
Speaker E: Better recognition.
Speaker E: I mean, I'm sure other people work in this, I'm not sitting still either.
Speaker E: But, yeah.
Speaker E: But, I mean, important thing is that we learn how to do this better.
Speaker E: So.
None: Yeah.
Speaker E: So our...
Speaker E: Yeah, you can see the kind of numbers that we're having, say, in speech deck car, which is a hard task.
Speaker E: It's really, you know, it's a sort of...
Speaker E: sort of reasonable numbers.
Speaker E: It's starting to be.
Speaker B: Yeah, even for a well-matched case, it's 60% ever-right reduction.
Speaker C: Yeah.
Speaker C: Yeah.
Speaker C: Yeah.
Speaker C: Yeah.
Speaker D: Okay.
Speaker G: Yeah.
Speaker B: So actually, this is in between what we had with the previous VAD and what Sunil did with an ideal VAD, which gave 62% improvement.
Speaker A: Yeah, it's almost...
Speaker A: It's almost an average.
Speaker A: It's a little around here.
Speaker A: Yeah.
Speaker G: What is that?
Speaker B: So if you use, like, an ideal VAD for dropping the frames...
Speaker B: All of the best we can get.
Speaker B: The best that we can get, that means that we estimate the second probability on the clean version of the utterances.
Speaker B: Then you can go up to 62% ever-right reduction, probably.
Speaker G: Yeah.
Speaker F: So that would be even...
Speaker F: That would change this number down here to 62.
Speaker B: Yeah.
Speaker B: Yeah, so you had a good, very good VAD that works as well as VAD working on clean speech.
Speaker B: Yeah.
Speaker F: Then you would go...
Speaker F: So that's sort of the best you could hope for.
Speaker E: So 53 is what you were getting with the old VAD.
Speaker E: Yeah.
Speaker E: And 62 with the...
Speaker E: You know, quote unquote cheating VAD and 57 is what you got with the real VAD.
Speaker E: That's great.
None: Yeah.
Speaker B: The next thing is I started to play...
Speaker B: But I don't want to worry too much about the delay now.
Speaker B: Maybe it's better to wait for the decision.
Speaker B: Yeah.
Speaker B: The committee.
Speaker B: But I started to play with the...
Speaker B: TAN-DEM, your own network.
Speaker B: I just did the configuration that's very similar to VAD.
Speaker B: I just did the configuration that's very similar to what we did for the February proposal.
Speaker B: And...
Speaker B: So there is a first feature stream that use straight MFCC features.
Speaker B: Well, these features actually.
Speaker B: And the other stream is the output of a neural network using as input also these cleaned MFCC.
Speaker F: I don't know what is going into the 10 and that.
Speaker B: So there is just this video stream, the 15 MFCC plus delta and the bell delta.
Speaker B: So it makes 45 features that are used as input to the HTK.
Speaker B: And then there are more inputs that comes from the TAN-DEM MFCC.
Speaker B: Oh, okay.
Speaker E: Yeah, he likes using both.
Speaker E: So then it has one part that's discriminating with one part that's not.
Speaker B: So yeah, right now it seems that I just tested on speech that current while the experiment are running on the IDGs.
Speaker B: Well, it improves on the well matched and the mismatched conditions, but it gets worse on the highly mismatched.
Speaker B: Compared to these numbers?
Speaker B: Compared to these numbers, yeah.
Speaker B: Like on the well matched and medium mismatch, the gain is around 5% relative, but it goes down a lot more like 15% on the HM case.
Speaker E: You're just using the full 90 features.
Speaker E: You have 90 features?
Speaker B: From the networks, it's 28.
Speaker E: And from the other side, it's 45.
Speaker E: It's 45, yeah.
Speaker E: It's the 73 features.
Speaker E: You're just feeding them like that.
Speaker E: There isn't any KLT or anything like that.
Speaker B: There is a KLT after the neural network.
Speaker F: That's how you get added 28.
Speaker F: Yeah.
Speaker B: I don't know.
Speaker B: It's because it's what we did for the first proposal.
Speaker B: We tested it.
Speaker B: So we're trying to go down.
Speaker B: 27.
Speaker B: Yeah.
Speaker B: So I wanted to do something very similar to the proposal as a first try.
Speaker B: But we have to, for sure, we have to go down because the limit is now 60 features.
Speaker B: We have to find a way to decrease the number of features.
Speaker F: So it seems funny that I quite understand everything, but that adding features, I guess if you're keeping the backend fixed, maybe that's it.
Speaker F: Because it seems like just adding information shouldn't get worse results.
Speaker F: But I guess if you're keeping the number of Gaussian fixed in the recognized system.
Speaker E: Well, yeah.
Speaker E: But I mean, just in general, adding information, suppose the information you added was a really terrible feature and all that brought in was noise.
None: Right?
Speaker E: So, or suppose it wasn't completely terrible, but it was completely equivalent to another one feature that you had, except it was noisier.
Speaker E: In that case, you wouldn't necessarily expect it to be better at all.
Speaker F: Oh, yeah.
Speaker F: I wasn't necessarily saying it should be better.
Speaker F: But it's worse.
Speaker E: On the highly mismatched condition, so highly mismatched condition means that in fact your training is a bad estimate of your test.
Speaker E: So having a greater number of features, if they aren't maybe the right features that you use, certainly can easily make things worse.
Speaker E: I mean, right, if you have lots and lots of data, and you have your training as representing a group of your tests, then getting more source of information should just help.
Speaker E: But it doesn't necessarily work that way.
Speaker E: So I wonder, well, what's your thought about what to do next with it?
Speaker B: I don't know, I'm surprised because I expected the neural net to help more when there is more mismatch as it was the case for the...
Speaker A: So was the training set the same as the February proposal?
Speaker A: Or the DLG?
Speaker B: Yeah, it's the same training set, so it's the same with the DLG.
Speaker B: Now, it's added.
Speaker E: Well, we might have to experiment with better training sets.
Speaker E: But the other thing is, I mean, before you found that was the best configuration, but you might have to retest those things now that we have different, the rest of it is different, right?
Speaker E: So, for instance, what's the effect of just putting the neural net on without the other path?
Speaker E: You know what the straight features do that gives you this.
Speaker E: You know what it does in combination, you know, necessarily, you know what.
Speaker F: What if you did what it makes sense to do the KLT on the full set of combined features?
Speaker F: Instead of just on the...
Speaker B: Yeah, I guess the reason I did it this way is that in February we tested different things like that.
Speaker B: So, I think two KLT, I think just a KLT for a network or having a global KLT.
Speaker B: So, you try the global KLT?
Speaker B: Yeah, and the difference is between these configurations were not huge, but it was marginally better with this configuration.
Speaker E: But, yeah, that's obviously another thing to try since things are different.
Speaker E: And I guess if the...
Speaker E: These are all...
Speaker E: So, all of these 73 features are going into the HMM.
Speaker E: Yeah.
Speaker E: And are there any delta's being computed of them?
Speaker B: Of the straight features, yeah.
Speaker E: So, not of the...
Speaker B: The tandem features are...
Speaker B: I know.
Speaker B:...used that there are...
Speaker B: So, yeah, maybe we can add some context from these features.
Speaker B: So, it's good.
Speaker B: And, dating, this last work.
Speaker E: Yeah, but the other thing I was thinking was...
Speaker E: No, I lost track of what I was thinking.
Speaker F: What is the...
Speaker F: You said there's a limit of 16 features or something.
Speaker F: What's the relation between that and the...
Speaker F: I know, I should say.
Speaker F: 4800 bits per second.
Speaker B: No relation.
Speaker B: Not the relation.
Speaker B: The 4800 bits is for transmission of some features.
Speaker B: And, generally, it allows you to transmit like 15...
Speaker B:...capsetrums.
Speaker E: The issue was that this is supposed to be a standard that's then going to be fed to somebody's recognize or somewhere, which might be...
Speaker E: It might be concerned how many parameters are used and so forth.
Speaker E: And so, they felt they wanted to settle a limit.
Speaker E: So, they chose 60.
Speaker E: Some people wanted to use hundreds of parameters and...
Speaker E:...that bothered some other people.
Speaker E: So, it just chose that.
Speaker E: I think it's kind of arbitrary too.
Speaker E: But that's kind of what's chosen.
Speaker E: I remember what it was going to say.
Speaker E: What I was going to say is that maybe...
Speaker E:...maybe with the noise removal, these things are now more correlated.
Speaker E: So, you have two sets of things that are kind of uncorrelated within themselves.
Speaker E: But they're pretty correlated with one another.
Speaker E: And they're being fed into these variants only, Gaussian and so forth.
Speaker E: And so, maybe it would be better idea now than it was before to have one kLT over everything.
Speaker E: To decore a little.
Speaker B: Yeah, I see.
Speaker E: Maybe.
Speaker A: What are the SNLs in the training set limit?
Speaker B: It's ranging from zero to clean.
Speaker B: Yeah, from zero to clean.
Speaker E: Yeah.
Speaker E: So, we found this macrophone data and so forth that we were using for these other experiments to be pretty good.
Speaker E: So, that's after you expose other alternatives that might be another way to start looking, is just improving the training set.
Speaker E: I mean, we were getting lots better recognition using that.
Speaker E: Of course, you do have the problem that we're not able to increase the number of Gaussian's or anything to match anything.
Speaker E: So, we're only improving the training of our feature set.
Speaker E: But that's still probably something.
Speaker F: So, you're saying at the macrophone data, the training of the neural net?
Speaker E: Yeah, that's the only place that we can train.
Speaker E: We can't train the other stuff with anything other than the standard amount.
Speaker F: What was the train on again?
Speaker B: The one that you?
Speaker B: It's timid with noise.
Speaker B: So, yeah, it's rather small.
Speaker D: How big is the net, by the way?
Speaker B: It's 500 unit units.
Speaker E: And again, you did experiments back then where you made it bigger.
Speaker E: That was sort of the threshold point.
Speaker E: Much less than that, it was worse.
Speaker E: Much more than that.
Speaker E: It wasn't much better.
Speaker A: So, is it the performance degradation in the high mismatch is something to do with the cleaning up that is done on the timid after adding noise?
Speaker A: All the noises are from the TI digits.
Speaker A: Well, it's like the high mismatch of the speech that car after cleaning up, maybe having more noise than the training set of timid after cleaning after you do the noise cleaning.
Speaker A: Earlier, you never had any compensation.
Speaker A: You just trained it straight away.
Speaker A: So, you had like all these different conditions of SNS.
Speaker A: Actually, in the training set of neural net, after cleaning up, you have now a different set of SNS for the training of the neural net.
Speaker A: And is it something to do with the mismatch that's created after the cleaning up, like the high mismatch?
Speaker B: You mean the more noisy utterances on the speech that car might be a lot more noisy than that?
Speaker A: I mean, SNR after the noise compensation of the speech deck.
Speaker E: So, the training that is being trained with noise compensated.
Speaker E: Yeah, it's tough, which makes sense.
Speaker E: But you're saying, yeah, the noisy or ones are still going to be even after our noise compensation.
Speaker E: You're still going to be pretty noisy.
Speaker A: So, now, the after-noise compensation, the neural net is seeing a different set of SNS than that was originally there in the training set of timid.
Speaker A: So, the net saw all the SNR conditions.
Speaker A: Now, after cleaning up, it's a different set of SNR.
Speaker A: That SNR may not be covering the whole set of SNS that you're getting in the speech deck.
Speaker E: Right, but the speech deck car that you're seeing is also reduced in noise.
Speaker A: Yeah, it is, but there could be some issues of...
Speaker B: Well, if the initial range of SNR is different, the problem was already there before.
Speaker E: Yeah, because...
Speaker E: Yeah, I mean, it depends on whether you believe that the noise compensation is equally reducing the noise, the test set and the training set.
Speaker E: I mean, you're saying there's a mismatch in noise that wasn't there before, but if they were both the same before, then if they were both reduced equally, then there would not be a mismatch.
Speaker E: So, I mean, this may be...
Speaker E: Haven't forbid this...
Speaker E: Most compensation process may be imperfect, but...
Speaker E: It's maybe stringing something differently.
Speaker A: No, that could be seen from the TI digits testing condition, because the noises are from the TI digits, right?
Speaker A: Noise.
Speaker A: Yeah, so...
Speaker A: So, cleaning up the TI digits and if the performance goes down in the TI digits mismatch, high mismatch like...
Speaker A: Cleaning training.
Speaker A:...or a clean training or a zero-db test.
Speaker A: Yeah, so we'll see.
Speaker A: Yeah, then it's something to do.
Speaker E: I mean, one of the things about... I mean, the macrophone data, I think, was recorded over many different telephones.
Speaker E: And so, there's lots of different kinds of acoustic conditions.
Speaker E: It's not artificially added, no, I swear, anything.
Speaker E: So, it's not the same. I don't think there's anybody recording a record from a car, but I think it's varied enough that if...
Speaker E: If doing this adjustments, and playing around with it doesn't make it better, it seems like the most obvious thing to do is to improve the training set.
Speaker E: I mean, the condition... it gave us an enormous amount of improvement in what we were doing with meeting recorded digits, even though...
Speaker E: There again, these macrophone digits were very, very different from what we were going on here.
Speaker E: We weren't talking over telephone here, but it was just, I think, just having a nice variation in acoustic conditions was just a good thing.
Speaker G: Yeah.
Speaker B: Yeah, actually, to... when I observed on the HM cases that the number of deletions dramatically increases.
Speaker B: It doubles... when I had the neural network doubles, the number of deletions.
Speaker B: So, you don't know how to interpret that, but... I'm here either.
Speaker F: And... and... and...
Speaker F: I don't understand the same insertion substitution.
Speaker B: Maybe they're a little bit lower.
Speaker B: They are a little bit better for me, but...
Speaker E: Did they increase the number of deletions, even for the cases that got better?
Speaker E: No, it doesn't. So, it's only the highly mismatched.
Speaker E: And remind me again, the highly mismatched means that...
Speaker B: It's clean training, well, close microphone training.
Speaker B: Close microphone training, distant microphone, I speed, I think.
Speaker B: Well, the most noisy cases of the distant microphone from testing.
Speaker E: Right.
Speaker E: So, maybe the noise subtraction is... subtracting half speech.
Speaker B: But... I mean, but we thought the neural network is... well, it's a better one.
Speaker B: It's just when we add the neural networks.
Speaker B: Yeah, right.
Speaker E: The feature of the same insertion.
Speaker F: Well, that says that, you know, the models and the recognizer are really paying attention to the neural net features.
Speaker E: Yeah. But, yeah, actually, the timet noises are sort of a range of noises, and they're not so much the stationary driving kind of noises, right?
Speaker E: It's pretty different, isn't it?
Speaker B: There is a car noise, so there are just four noises.
Speaker B: The car, I think, Babel, Subway, right, and Street, or Airport or something.
Speaker B: Train station.
Speaker B: So, it's mostly while car is stationary, Babel is stationary background, plus some voices, some speech over it, and the other two are rather stationary also.
Speaker E: Well, I think that if you run it, actually, maybe you remember this, when you, in the old experiments, when you ran with the neural net only, and didn't have the side path with the pure features as well, did it make things better to have the neural net?
Speaker E: Was it about the same...
Speaker E: It was a little bit worse.
Speaker B: Then, just the features.
Speaker E: So, until you put the second path in with the pure features, the neural net wasn't helping at all.
Speaker B: What's interesting?
Speaker B: It was helping if the features were bad, just playing the B's around the MCC's.
Speaker B: But, as soon as we added the linearization, they were doing similar enough things.
Speaker E: Well, I still think it would be interesting to see what would happen if you just had the neural net without the side thing.
Speaker E: And the thing I have in mind is, maybe you'll see that the results are not just a little bit worse, maybe that there are a lot worse.
Speaker E: But if, on the other hand, it's somewhere in between what you're seeing now and what you'd have with just the pure features, then maybe there is some problem of a combination of these things, or correlation between them somehow.
Speaker E: If it really is the net, is hurting you at the moment, then I think the issue is to focus on improving the net.
Speaker E: So what's the overall, I mean, you haven't done all the experiments, but you said it was somewhat better, say, 5% better for the first two conditions and 15% worse for the other one.
Speaker E: But of course that one's rated lower, so I wonder what the net effect is.
Speaker B: I think it was 1 or 2%. That's not that bad, but it was like 2% relative worse once we did that curve.
Speaker B: I have to check that to a live.
Speaker A: Well, overall it will be still better, even if it is 15% worse, because the 15% worse is given like 25.
Speaker G: 0.25 weight.
Speaker E: Right, so the worst it could be if the other is exactly as it is 4% and in fact, since the others are somewhat better.
Speaker A: So either you get cancel load or you'll get almost the same?
Speaker B: Yeah, it was slightly worse.
Speaker E: Yeah, it should be pretty close to cancel that.
Speaker F: You know, I've been wondering about something. In a lot of the Hub 5 systems recently have been using LDA.
Speaker F: And they run LDA on the features right before they train models.
Speaker F: So there's the LDAs right there before the HMMs.
Speaker F: So you guys are using LDA, but it seems like it's pretty far back in the process.
Speaker A: This LDA is different from the LDA that you are talking about. The LDA that you are saying is like you take a block of features like 9 frames or something and then do an LDA on it and then reduce the dimensionality to something like 24 or something like that.
Speaker F: And then feed it to HMM.
Speaker A: Yeah, so this is like a 2-dimensional time.
Speaker A: So this is a 2-dimensional time.
Speaker A: And the LDA that we are applying is only in time, not in frequency, across frequency. So it's like more like a filtering in time rather than doing it.
Speaker F: So what about, I mean, I don't know if this is a good idea or not, but what if you put, ran the other kind of LDA on your features right before they go into the HMM?
Speaker B: Actually, I think, well, what we do with the HMM is something like that, except that it's not linear.
Speaker B: But it's like a nonlinear or discriminant.
Speaker F: Yeah, so sort of like the tandem stuff is kind of like it's nonlinear, I'll be.
Speaker B: But the other features that you have, the non-hand ones, while in the proposal they were transformed using PCA, but it might be that LDA.
Speaker E: The argument is kind of, and it's not like we really know, but the argument anyway is that we always have the problem.
Speaker E: So, the other thing is that LDA, they're good. They're good because you learn to distinguish between these categories that you want to be good at distinguishing between.
Speaker E: And PCA doesn't do that. PCA, or PCA, throws away pieces that are maybe not going to be helpful just because they're small.
Speaker E: But the problem is, training sets are perfect and testing sets are different. So you face the potential problem with discriminative stuff, B-L-D-A, or neural nets, that you are training to discriminate between categories in one space, but what you're really going to be getting is something else.
Speaker E: And so, Stefan's idea was, let's feed both this discriminatively trained thing and something that's not.
Speaker E: So, you have good set of features that everybody's worked really hard to make, and then you discriminatively train it, but you also take the path that doesn't have that, and putting those in together.
Speaker E: And that seems, so it's kind of like the combination of what Dan has been calling a feature combination versus posterior combination or something.
Speaker E: You have the posterior combination, but then you get the features from that and use them as feature combination with these other things.
Speaker E: And that seemed, at least on the last one, as he was saying, when he only did discriminative stuff, it actually didn't help at all in this particular case.
Speaker E: There was enough of a difference, I guess, between the testing and training. But by having them both there, the fact is, some of the time, the discriminative stuff is going to help you.
Speaker E: And some of the time, it's going to hurt you by combining two information sources.
Speaker F: So, you wouldn't necessarily then want to do LDA on the non-tandom features because that you're doing something to them.
Speaker E: I think that's counter to that idea. Now, again, we're just trying these different things. We don't really know what's going to work best.
Speaker E: But if that's the hypothesis, at least to be counter to that hypothesis, to do that.
Speaker E: And in principle, you would think that the neural net would do better at the discriminant part than LDA.
Speaker F: Well, maybe not. Yeah, exactly.
Speaker F: I mean, we were getting ready to do the tandem stuff for the Hubbive system. And Andreas and I talked about it.
Speaker F: And the idea, the thought was, well, yeah, the neural net should be better. But we should at least have a number to show that we did try the LDA in place of the neural net.
Speaker F: So we can show a clear path. You have a valid, then you have LDA, then you have the neural net.
Speaker F: You can see theoretically. So, I was just wondering, I think that's a good idea.
Speaker D: Did you do that?
Speaker F: No, that's what we're going to do next, as soon as I finish this.
Speaker F: Yeah. Yeah. No, well, that's a good idea.
Speaker E: You just want to show. I mean, it's not even believed it.
Speaker E: Oh, no, it believes it.
Speaker E: No, no, but it might not even be true. I mean, it's a great idea.
Speaker E: I mean, one of the things that always disturbed me in the resurgence of neural nets that happened in the 80s was that a lot of people, because neural nets were pretty easy to use.
Speaker E: A lot of people were just using them for all sorts of things without looking at all into the linear versions of them.
Speaker E: And people were doing their neural nets, but not looking at our filters.
Speaker E: So I think, yeah, it's definitely a good idea to try it.
Speaker F: Yeah, and everybody's putting that on their systems now.
Speaker F: That's what made me wonder about.
Speaker E: Well, even putting them in their systems off in the out of ten years.
Speaker F: Yeah, what I mean is it's like in the Hub5 evaluations, you know, and you read the system descriptions.
Speaker F: And now they all have that.
Speaker F: Everybody's got LDA on their features.
Speaker B: It's the transformation that are estimating, but they are trained on the same data as the final nature.
Speaker B: Yeah, so it's different.
Speaker F: Yeah, exactly, because they don't have these mismatches that you guys have.
Speaker F: So that's why I was wondering if maybe it's not even a good idea.
Speaker F: I don't know.
Speaker F: That's about it.
Speaker E: I mean, part of why I think part of why you were getting into the KLT, you were describing to me at one point that you wanted to see if, you know, getting good orthogonal features was in combining the different temporal ranges was the key thing that was happening, or whether it was the scrumid thing, right?
Speaker E: So you were just trying.
Speaker E: I think you, I mean, this is, it doesn't have the LDA aspect, but as far as the orthogonalizing transformation, you were trying that, at one point, right?
Speaker E: Thank you.
Speaker G: Yeah.
Speaker E: That's something.
Speaker E: That's what is well.
Speaker E: Yeah.
Speaker A: So, I've been exploring a parallel VAD without neural network with, like, less latency using SNR and energy after the cleaning up.
Speaker A: So what I've been trying was, after the, after the noise compensation, I was trying to find a feature based on the ratio of the energy that is after clean and before clean.
Speaker A: So that if, if they are like pretty close to one, which means it's speech, and it is, if it is close to zero, which is, so it's like it's scaled nicely to a probability value.
Speaker A: So, just trying with full band and multiple bands, separating them to different frequency bands and deriving separate decisions on each band and trying to combine them.
Speaker A: The advantage being like it doesn't have the latency of the neural network if it, if it can, and it gave me like one point, one more than one percent relative improvements.
Speaker A: So from 53.6, it went to 54.8. So it's like, from the slightly more than a person improvements, it's like, which means that it's doing a slightly better job than the previous VAD.
Speaker A: I'd lower delay.
Speaker A: So, sorry, this is still of the median.
Speaker A: It's still as the median field.
Speaker E: So it was most of the, yeah.
Speaker A: The delay that's gone is the input, which is the 60 millisecond, the 40 plus 20.
Speaker A: At the input of the neural network, you have this nine frames of context plus the delta.
Speaker E: Oh, plus the author, right?
Speaker A: Yeah.
Speaker A: So that delay plus the LDA.
Speaker A: So the delay is only the 40 millisecond of the noise cleaning plus the 100 milliseconds, smoothing a output.
Speaker A: So, yeah, so the, because the problem for me was to find a consistent threshold that works well across a different databases, because I, I try to make it work on speech.car, and it fails on TI digits, or try to make it work on that's just the Italian or something it doesn't work on the finish.
Speaker A: So, so there was, there was like some problem in balancing the deletions and insertions when I try different thresholds.
Speaker A: So the, I'm still trying to make it better by using some other features from the, after the cleanup, maybe some correlation or the correlation or some additional features.
Speaker A: So mainly the improvement of the word.
Speaker A: I've been trying.
Speaker E: Now this, this, this, before and after clean, it sounds like you think that's a good feature that, that you think that the, it appears to be a good feature, right?
Speaker E: Yeah, what about using the neural net?
Speaker A: Yeah, so, yeah, so that's, yeah, so we've been thinking about putting it into the neural net also.
Speaker A: Yeah, because we did that itself.
Speaker A: You don't have to worry about the threshold.
Speaker A: Yeah, yeah, so that's the, yeah.
Speaker E: Yeah, so if we, if we can live with latency or cut the latency elsewhere, then that would be a good thing.
Speaker E: Anybody has anybody, you guys or, or Naren, somebody tried the second, second stream thing?
Speaker A: Oh, I just, I just put the second stream in place and, and one experiment, just like, just to know whether everything is fine.
Speaker A: So it was like, 45 kept stream plus 23 mile log mail.
Speaker A: Yeah.
Speaker A: And it was like, it gave me the baseline performance of the Aurora, which is like zero improvement.
Speaker A: Yeah.
Speaker A: So I just tried it on the talent just to know whether everything is, but I didn't expect anything out of it because it was like a weird feature set.
Speaker E: Yeah.
Speaker E: Yeah, well, what I think, you know, what, the more what you want to do is, is, is put into another neural net.
Speaker D: Yeah.
Speaker E: But yeah, we're not quite there yet.
Speaker E: So we have to figure out the neural net.
Speaker E: I guess.
Speaker E: Yeah.
Speaker A: The other thing I was wondering was, if the neural net has any, because of the different noise, unseen noise conditions for the neural net, like you train it on those four noise conditions, but you're feeding it with like additional some four plus some few more conditions, which it doesn't seem actually from the wild testing.
Speaker A: Yeah.
Speaker A: Instead of just having those clean up kept stream should be feed some additional information like the, we have the VAD flag and should be feed the VAD flag also at the input.
Speaker A: So that it has some additional discriminating information at the input.
Speaker A: We have the VAD information also available at the back end.
Speaker A: So if it is something, the neural net is not able to discriminate the classes.
Speaker A: Yeah.
Speaker A: I mean, because most of it is, when we have dropped some silence, we have dropped silence, we haven't dropped silence for him still.
Speaker B: Still not yet.
Speaker A: Yeah. So the biggest classification will be the speech and silence.
Speaker A: So by having an additional feature which says this is speech and this is non speech.
Speaker A: I mean, it certainly helps in some unseen noise conditions for the neural net.
Speaker F: What do you have that feature available for the test data?
Speaker A: Well, I mean, we have, we are transmitting the VAD to the back end, feature to the back end because we are dropping it at the back end after everything.
Speaker A: All the features are computer.
Speaker A: Oh, oh, I see.
Speaker A: So that is coming from a separate neural net or some VAD, which is certainly giving you that also.
Speaker A: Yeah.
Speaker A: So it's an additional discriminating information.
Speaker E: You could feed it into the neural net.
Speaker E: And the other thing you could do is just modify the output probabilities of the neural net based on the fact that you have a silence probability.
Speaker E: So you have an independent estimator of what the silence probability is and you could multiply the two things and re-normalize.
Speaker E: Yeah, I mean, you have to do the non-linearity part.
Speaker E: Do that.
Speaker E: I mean, go backwards once the non-linearity would be.
Speaker F: But, uh, maybe I went, but in principle, wouldn't it be better to feed it in unless the net do that?
Speaker E: Well, not sure.
Speaker E: I mean, let's put it this way.
Speaker E: I mean, you have this complicated system with thousands of thousand parameters and you can tell it learn this thing or you can say, it's silence, go away.
Speaker E: I mean, I think the second one sounds a lot more direct.
Speaker F: What if you, right?
Speaker F: So what if you then, uh, since you know this, what if you only use the neural net on the speech portions?
Speaker F: Well, well, it's, well, it's that's the same.
Speaker F: Yeah, I mean, you have to actually run it continuously, but it's just.
Speaker F: I mean, train the net only on.
Speaker E: Well, no, you want to train it on the non-speech also because that's part of what you're learning in it to generate that it's, you have to distinguish between.
Speaker F: You need to multiply the output and it by this other decision.
Speaker F: Uh, then you don't care about whether that makes that distinction.
Speaker E: Wait, but this other thing isn't perfect.
Speaker E: Uh, so that you bring in some information in the net itself.
Speaker F: That's a good point.
Speaker E: Yeah.
Speaker E: Now, the only thing that bothers me about all this is that I, I, the fact it's sort of bothersome that you're getting more deletions.
Speaker B: Yeah.
Speaker B: So I might maybe look at, is it due to the fact that, uh, the probability of the silence, the output of the network is, uh, is too high to I or.
Speaker B: Yeah.
Speaker B: So maybe it's okay.
Speaker A: Yeah, it's not really doing any distinction between speech and non speech or I mean different MN classes.
Speaker F: I'd be interested to look at the, yeah, for the, but if you look at the, um, high mismatch, the output of the net on the high mismatch case and just look at, you know, the distribution versus the, the other ones, you, you see more peaks or something.
Speaker B: Yeah, yeah, entropy of the output.
Speaker B: Yeah.
Speaker E: For instance.
Speaker B: But it seems that the VAD network doesn't, well, it doesn't drop, uh, too many frames because the, the number of deletion is reasonable.
Speaker B: But it's just when we had to tend them, the final MLB at them.
None: Yeah.
Speaker E: Now, the only problem is you don't want to take, I guess, wait for the output of the VAD before you can put something into the other system because I'll shoot up the latency a lot, right?
Speaker E: Am I missing something here?
Speaker B: Yeah, right.
Speaker E: Yeah. So that's maybe a problem with what I was just saying.
Speaker E: But, but I guess.
Speaker F: But if you were going to put it in as a feature, it means you already have it at the time you get to the tandem net, right?
Speaker A: Um, well, we, we don't have it actually because it's, it has a higher rate in the VAD as a.
Speaker E: Yeah.
Speaker E: Okay. It's kind of done, I mean, some of the things are not in parallel, but certainly it would be in parallel with a, with a tandem net in time.
Speaker E: So maybe if that doesn't work, um, but it'd be interesting to see if that was the problem anyway.
Speaker E: And, and, and I guess another alternative would be to take the feature that you're feeding into the VAD and feeding it into the other one as well.
Speaker E: And then maybe we just learn, learn it better.
Speaker E: Um, but that's, yeah, that's an interesting thing to try to see if what's going on is that in the highly mismatched condition, it's, um, causing deletions by having this silence probability up, up too high.
Speaker E: At some point where the VAD is saying it's actually speech. Yeah. So it's probably true.
Speaker E: Because well, the VAD said since the VAD is, is, is right a lot. Anyway, might be. Yeah, well, we just started working with it. These are some good ideas, I think.
Speaker B: Yeah, and the other thing, well, there are other issues, maybe for the tandem, like, what do we want to, we want to work on the targets.
Speaker B: Like, instead of using phonemes, using work on text dependent units.
Speaker B: But the time, yeah, I'm thinking also about dance work where we trained the network, not on phonemed targets, but on the HMM state targets.
Speaker B: And you know, it's giving slightly better results.
Speaker E: The problem is if you were going to run this on different tests, including large vocabulary. Yeah.
Speaker B: I was just thinking maybe about, like, generalized iPhones. Come up with a reasonable, not too large set of context dependent units.
Speaker B: Yeah. And then anyway, we would have to reduce this. Yeah.
Speaker E: Yeah. So, but. Yeah. Maybe. But I, it's all worth looking at, but it sounds to me like looking at the relationship between this and the speech and voice stuff is, is probably a key thing.
Speaker F: That and the correlation between stuff. So if, if the high mismatch case had been more like the other two cases, in terms of giving you just better performance, how would this number have changed?
Speaker B: It would be, yeah, around 5% better, I guess, if like, six if. Well, we don't know what's going to be a TI that it's, yeah, it's back.
Speaker B: If you extrapolate the speech that car will match in medium mismatch, it's around maybe five. So this would be 62.
Speaker B: So, around 60 must be. Right. Yeah. Well, it's around 5% because if everything is 5%.
Speaker B: All the other ones were 5%. I just have just speech that car. So, yeah.
Speaker B: So, I'm just running each, should have the result today during the afternoon.
Speaker E: Well. So, I won't be here for.
Speaker E: I'm leaving next Wednesday. May or may not be in the morning or the afternoon.
Speaker E: Are you, you're not going to be around the afternoon? Yeah. Oh, well, I'm talking about next week. I'm leaving, leaving next Wednesday.
Speaker E: This afternoon, oh, right for the meeting meeting. Yeah, that's just because of something on campus.
Speaker E: Ah, okay. Okay. But, yeah. So next week I won't. And the week after I won't. So, I'll be in Finland.
Speaker E: And the week after that I won't. By that time you'll be, you both become from here.
Speaker E: So, there'll be no, definitely no meeting on September 6th. What September 6th?
Speaker E: Ah, that's during your speech. Oh, so, Sonil will be in Oregon.
Speaker E: Stephanie and I will be in Denmark. Right. So, it'll be a few weeks really.
Speaker E: Before we have a meeting of the same cast of characters. But, I guess, just, I mean, you guys should probably meet and maybe bury.
Speaker E: I'll be around. And then, we'll start up again with Dave and, Dave and Barry and Stefan.
Speaker E: And us on the 20th. No. 13th.
Speaker F: So, you're going to be gone for the next three weeks or something. I've gone for two and a half weeks starting next Wednesday.
Speaker F: So, you won't get the next three of these meetings. Right.
Speaker E: I won't. I was probably four because of the three. See, 23rd, 36th. That's right. Next three.
Speaker E: And the third one probably won't be a meeting because, because, uh, Sonil, Stefan and I will not be here.
Speaker E: So, it's just the next two where there will be, there may as well be meetings, but I just won't be at them.
Speaker E: And then starting at the 13th. Uh, we'll have meetings again that we'll have to do without Sonil here somehow.
Speaker E: 31st. Yeah. Yeah. So.
Speaker F: When is the evaluation of November or something? Yeah, supposed to be November 15th. Does anybody heard anything different?
Speaker B: I don't know. The meeting is the five and six of December. So, yeah. Stentatively.
Speaker B: That's a proposed deal, I guess. Yeah. So, the evaluation should be a week before. Yeah.
Speaker E: But, no, this is good progress. So. Okay. Did it. Yeah.
Speaker F: That's good. L-352. 576456704693. 685091394648. 34427182. 187499845897. 1839.
Speaker F: 1839 01453629. 543 626673. 7151 60725942.
Speaker B: 888819818. Transcript L-353. 791126 542. 873 984 9646. 3574225961.
Speaker B: 599 97 985182. 75456653012. 9907 3926. 019 398 0350. 286 202 181.
Speaker A: Transcript L-354 296 8637605. 7156 1370 4256. 9537 0218 1863 987 111029.
Speaker A: 35334930315. 4086489503. 80289 791. 8991351804.
Speaker E: Transcript L-3508442322617. 1283199113. 4524596233. 3846552025.
Speaker E: 4693133646. 2846414464. 2994 3287 8742. 428207 480.
Speaker E: It's a wrap.
