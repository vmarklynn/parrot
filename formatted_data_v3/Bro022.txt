Speaker E: I think for two years we were two months away from being done.
Speaker G: And what was that Morgan?
Speaker G: What project?
Speaker E: The Taurant chip.
Speaker E: Oh yeah.
Speaker E: We went through, Jim and I went through all the emails at one point and for two years there was this thing saying yeah we were two months away from being done.
Speaker E: It was very believable schedules.
Speaker E: We went through and schedule some years.
Speaker E: Oh yeah, it was very true.
Speaker G: So should we just do the same kind of deal where we go around and do status report kind of things?
Speaker G: Okay, and I guess when Sunil gets here he can do his last or something.
Speaker E: So we probably should wait for him to come before we do his.
Speaker G: Okay, good idea.
Speaker G: Any objection?
Speaker G: All in favor.
Speaker G: Do you want to start Morgan?
Speaker G: Do you have anything?
Speaker E: I don't do anything.
Speaker E: No, I mean I have involved in discussions with people about what they're doing but I think they're since they're here they can talk about it themselves.
Speaker D: Okay, so should I go so that you go ahead and talk about where we're at stuff for sake.
Speaker D: Okay.
Speaker D: Well this past week I've just been getting down and dirty into writing my proposal.
Speaker D: I just finished a section on talking about these to really categories that classify as a little step.
Speaker D: And I hope to get this full rough draft by Monday and you're tomorrow.
Speaker G: When is your meeting?
Speaker D: My meeting with oh you mean the cool ones.
Speaker D: Are the calls are happening in July 25th?
Speaker G: Oh, soon.
Speaker G: Yeah, D-Day.
Speaker G: Yeah.
Speaker G: So the idea you're going to do this paper and then you pass it out to everybody ahead of time.
Speaker D: Right, right.
Speaker D: So you write up a proposal and give it to people ahead of time and you have a short presentation.
Speaker D: And then everybody asks you questions.
Speaker G: Yeah, I remember now.
Speaker G: Yeah, so I was just going to ask you want to say any a little bit about it or a little bit about what you're going to you said you're talking about the features that you're looking
Speaker D: at. Right.
Speaker D: Well, I was I think one of the perplexing problems is for a while I was thinking that I had to come up with a complete set of intermediate features and intermediate categories to classify right away.
Speaker D: But what I'm thinking now is I would start with a reasonable set something like a regular phonetic features just to just to start off that way.
Speaker D: And do some phone recognition, build a system that classifies these these features of these intermediate categories using multi band techniques, combine them and do phone phoneme recognition.
Speaker D: Look at then I would look at the errors produced in the phoneme recognition and say, okay, well, I could probably reduce the errors if I included this extra feature or this extra intermediate category that would that would reduce certain confusions over other confusions.
Speaker D: And then and then reiterate build the intermediate classifiers and do phoning recognition, look at the errors and then postulate new or remove intermediate categories and then do it again.
Speaker G: So you're going to use Timit?
Speaker D: Or that for that part of the process here, I used Timit and then after doing Timit, right, that's just the phone recognition task.
Speaker D: I want to take a look at things that I can model within Word.
Speaker D: So I would then shift the focus to something like switchboard where I would be able to model intermediate categories that span across phonemes and just within the phonemes themselves.
Speaker D: And then do the same things as there on a large regular task like switchboard.
Speaker D: And for that part, I would use the SRI recognizer.
Speaker D: So it's already set up for a switchboard and I run some sort of tandem style processing with intermediate classifiers.
Speaker G: So that's why you were interested in getting your own features into the SRI files?
Speaker D: That's why I was asking about that.
Speaker D: I guess that's it.
Speaker D: Any questions?
None: Sounds good.
Speaker G: You just have a few more weeks.
Speaker D: It's about a month from now.
Speaker D: It's a month and a week.
Speaker G: So you want to go next, David?
Speaker G: Oh, OK, sure.
Speaker F: So last week I finally got results from the SRI system about this means subtraction approach.
Speaker F: And we got an improvement in Word, error rate, training on the TI digits, data set, and testing on meeting recorded digits of 6% to 4.5% on the far mic data using PCMF.
Speaker F: But the near mic performance worsened from 1.2% to 2.4%.
Speaker F: And why would that be considering that we actually got an improvement in near mic performance using HDK?
Speaker F: So with some input from Andreas, I have a theory in two parts.
Speaker F: First of all, the SRI system is doing channel adaptation.
Speaker F: And so, HDK wasn't.
Speaker F: So this means subtraction approach will do a channel normalization.
Speaker F: And so that might have given the HDK use of it a boost that wouldn't have applied in the SRI case.
Speaker F: And also, the Andreas pointed out the SRI system is using more parameters.
Speaker F: It's got finer grain acoustic models.
Speaker F: So those finer grain acoustic models could be more sensitive to the artifacts in the recenthasized audio.
Speaker F: And me and Barry were listening to the recenthasized audio.
Speaker F: And sometimes it seems like you get a bit of an echo of speech in the background.
Speaker F: And so it seems like it could be difficult for training because you could have different phones lined up with a different foreground phone, depending on the timing of the echoes.
Speaker F: So I'm going to try training on a larger data set.
Speaker F: And then the system will have seen more examples of these artifacts.
Speaker F: And hopefully, it will be more robust to them.
Speaker F: So I'm trying to use the microphone set of red speech.
Speaker E: And I have another thought just now, which is, remember we were talking before about, we were talking in our meeting about some of the other stuff that Evernano did where they were getting rid of low energy sections.
Speaker E: If you did a high pass filtering as Hirsch did late 80s to reduce some of the effects of reverberation, Evernano and Hermansky were arguing that perhaps one of the reasons for that working was that it may not have even been filtering so much, but the fact that when you filter an all positive power spectrum, you get some negative values and you've got to figure out what to do with them.
Speaker E: If you continue treating this as a power spectrum, so what Hirsch did was set them to zero, set the negative values to zero.
Speaker E: So if you imagine a waveform that's all positive, which is the time trajectory of energy, and shifting it downwards and then getting rid of the negative parts, that's essentially throwing away the low energy things.
Speaker E: That's the low energy parts of the speech where the reverberation is most audible.
Speaker E: You have reverberation from higher energy things showing up in.
Speaker E: So in this case, you have some artificially imposed reverberation like that.
Speaker E: I mean, you're getting rid of some of the other effects of reverberation, but because you have these non-calls or windows, you're getting these funny things coming in.
Speaker E: What if you did, I mean, there's nothing to say that the processing for this recynthesis has to be restricted to trying to get it back to the original according to some equation.
Speaker E: I mean, you also could just try to make it nicer.
Speaker E: And one of the things you could do is you could do some sort of VAD-like thing.
Speaker E: You actually could take very low energy sections and set them to some very low or near zero value.
Speaker E: I mean, I'm just saying if in fact it turns out that these echoes that you're hearing are pre-echos, whichever they are, are part of what's causing the problem.
Speaker E: You actually can get rid of them.
Speaker H: Uh-huh.
Speaker E: Be pretty simple.
None: Okay.
Speaker E: You can do it in a pretty conservative way so that if you made a mistake, you were more likely to keep in an echo than to throw out speech.
Speaker B: What is the reverberation time like in this room?
Speaker B: The one in the speech that you were using?
Speaker F: Yeah.
Speaker F: I don't know.
Speaker F: So is this room?
Speaker E: It's this room.
Speaker E: So is this just a microphone?
Speaker E: This microphone, a close microphone and a distant microphone.
Speaker E: He's doing these different tests on.
Speaker E: We should do measurement here.
Speaker E: I think we never have.
Speaker E: I think it's, I would guess, 0.7.8 seconds, RT60, something like that.
Speaker E: But it's, you know, it's this room.
Speaker E: But the other thing is he's putting in, I was using word reverberation in two ways.
Speaker E: He's also putting in a, he's taking out some reverberation, but he's putting in something because he has averages over multiple windows stretching out the 12 seconds, which are then being subtracted from the speech.
Speaker E: And since, you know, what you subtract, sometimes you'll be subtracting from some larger number and sometimes you won't.
Speaker E: So you can end up with some components in it that are affected by things that are seconds away.
Speaker E: And if it's a low energy comp, portion, you might actually hear some funny things.
Speaker F: One thing I noticed is that the mean subtraction seems to make the PZM signals louder after they've been recenticized.
Speaker F: So I was wondering, is it possible that one reason it helped with the Aurora baseline system is just as a kind of gain control because some of the PZM signals sound pretty quiet if you don't amplify them?
Speaker C: I don't see why you're signally louder after processing because you...
Speaker F: Yeah, I don't know why either.
Speaker E: I don't think just multiplying the signal by two would have any effect.
Speaker F: Oh, okay.
Speaker E: Yeah.
Speaker E: I mean, I think if you really have louder signals, what you mean is that you have better signalized ratio.
Speaker E: So if what you're doing is improving the signalized ratio, then it would be better, but just it being bigger with the same signalized ratio.
Speaker E: It would have effect.
Speaker E: Okay.
Speaker C: Well, the system is...
Speaker C: You use the absolute energy, so it's a little bit dependent on the signal level, but not so much.
Speaker E: Well, yeah, but it's trained and tested on the same thing.
Speaker E: So if you change in both training and test, the absolute level by factor too, it will matter no effect.
Speaker G: So if you add this data to the training site for the Aurora, or you just test it on this, did I...
Speaker F: What, sorry?
Speaker G: When we're going to just saying that as long as you do it in both training and testing, you can't have any effect.
Speaker G: But I was sort of under the impression that you just tested with this data.
Speaker G: You didn't train at all.
Speaker F: Right, I trained on clean TI digits.
Speaker F: I did the mean subtraction on clean TI digits, but I didn't...
Speaker F: I see.
Speaker F: But it made the clean TI digits any louder.
Speaker F: I only remember noticing it made the PCM signal louder.
Speaker E: Okay, well, I don't understand then.
Speaker F: I don't know.
Speaker F: If it's trying to find a reverberation filter, it could be that this reverberation filter is making things quieter, and then if you take it out, that...
Speaker F: Taking it out makes things louder.
Speaker E: Are you...
Speaker E: No.
Speaker E: I mean, there's nothing inherent about removing...
Speaker E: If you're really removing...
Speaker E: I mean, then I don't see how it makes it louder.
Speaker E: So I should maybe let this stuff get...
Speaker E: Yeah, it might just be some artifact of the processing that...
Speaker E: If you're...
Speaker E: Yeah.
Speaker E: I don't know.
None: Okay.
Speaker G: I wonder if there could be something like...
Speaker G: For the PCM data, if occasionally somebody hits the table or something, you could get a spike.
Speaker G: I'm just wondering if there's something about the...
Speaker G: You know, doing the mean normalization where it could cause you to have better signal of noise ratio.
Speaker E: Well, you know, there is this, right?
Speaker E: It may be...
Speaker E: If subtracting the mean log spectrum is like dividing by a spectrum.
Speaker E: So depending what you divide by, if your estimate is often sometimes you're getting a small number, you could make it bigger.
Speaker E: So it's just a question of...
Speaker E: It could be that there's some normalization that's missing or something to make it...
Speaker E: You think it shouldn't be larger, but maybe in practice it is.
Speaker E: That's something to think about.
Speaker H: I don't know.
Speaker C: I had a question about the system, the SRI system.
Speaker C: So you trained it on TI digits, but except this, it's exactly the same system as the one that was tested before and that was trained on microphone, right?
Speaker C: So on TI digits it gives you 1.2% error rate and on microphone it's still 0.8.
Speaker C: But is it exactly the same system?
Speaker F: I think so.
Speaker F: If you're talking with the microphone results that Andreas had about a week and a half ago, I think it's the same system.
Speaker C: So you use VTA vocal track length normalization and like MLLR transformation also.
Speaker E: I'm sorry, was his 0.8% result on testing on microphone or training?
Speaker C: Training on microphone and testing on meeting digits.
Speaker E: So that was done already.
Speaker E: So it's 0.8.
None: Okay.
Speaker C: Yeah, I've just been testing the new Aurora front end with...
Speaker C: Well, the system actually, so front end and HTK acoustic models on the meeting digits and it's a little bit better than the previous system.
Speaker C: We have 2.7% error rate and before with the system that was proposed, it was 3.9.
Speaker C: Oh, that's a lot better.
Speaker C: Getting better.
Speaker B: So with the HTK back in what we have, the Aurora?
Speaker C: Yeah, 2.7.
Speaker C: On the meeting, we have 2.7.
Speaker D: That's with the new IIR shelters.
Speaker C: Yeah, yeah.
Speaker C: So yeah, we have the new L.E. filters and I think maybe I didn't look, but one thing that makes different is this DC offset compensation.
Speaker C: Did you have a look at the meeting digits if they have DC component or...?
Speaker F: I didn't know.
Speaker B: No, the DC component could benefit from it if you have recording it through a mic coming.
Speaker B: All the mics have the DC remote, some capacity to receive right?
Speaker E: Yeah, but this...
Speaker E: No, because there's a sampling hold in the A to D and these periods these typically do have a DC offset and they can be surprisingly large.
Speaker B: It depends on the electronics.
Speaker E: The microphone isn't going to pass in a DC, but actually there are instrumentation mics that do pass go down to DC, but no, it's the electronics.
Speaker E: And then there's amplification afterwards and you can get...
Speaker E: I think it was in the Wall Street Journal data that I can't remember one of the darker things that was this big DC offset.
Speaker E: We didn't know about it for a while.
Speaker E: We were messing with it.
Speaker E: We were getting these terrible results.
Speaker E: We're talking to somebody and they said, oh yeah, everybody knows that.
Speaker E: There's all this DC offset.
Speaker E: So yes, you can get DC offset in the data.
Speaker G: Was that everything, Dave?
Speaker F: Oh, and I also did some experiments about normalizing the phase.
Speaker F: So I came up with a web page that people can take a look at.
Speaker F: And the interesting thing that I tried was Adam and Morgan had this idea since my original attempts to take the mean of the phase spectra over time and normalize using that by subtracting that off didn't work.
Speaker F: So we thought that might be due to problems with the arithmetic of phases.
Speaker F: They added this module to Pi way and there's reason to believe that that approach of taking the mean of the phase spectrum wasn't really mathematically correct.
Speaker F: So what I did instead is I took the mean of the FFT spectrum without taking the log or anything.
Speaker F: And I took the phase of that and I subtracted that phase off to normalize that didn't work either.
Speaker E: So we have a different interpretation of this.
Speaker E: He says it doesn't work.
Speaker E: I think it works magnificently, but just not for the task we intended.
Speaker E: It gets rid of the speech.
Speaker E: What does it leave?
Speaker E: At least, you know, it leaves the junk.
Speaker E: I mean, I think it's tremendous.
Speaker E: All he has to do is go back and reverse what he did before.
Speaker E: Well, could you take what was left over?
Speaker E: Exactly.
Speaker E: Yeah, you got it.
Speaker E: So it's a general rule.
Speaker E: Just listen very carefully to what I say and do the opposite.
Speaker E: Including what I just said.
Speaker H: And yeah, that's everything.
Speaker G: Do you want to go step on?
Speaker C: Yeah, maybe concerning this still is meeting digits.
Speaker C: I'm interested in trying to figure out what's still the difference between the SRI system and the overall system.
Speaker C: Yeah, so I think I will maybe train gender-dependent models because this is also one big difference between the two systems.
Speaker C: The other difference is where the fact that maybe the acoustic models of the SRI and the SRI system are more complex.
Speaker C: But Chuck, you did some experiment with this.
Speaker C: It didn't seem to happen.
Speaker C: Did you have some improvement with this?
Speaker E: What sounds like they also have, he's saying they have all these tumor kinds of adaptation, their channel adaptation, their speaker adaptation.
Speaker G: But there's also the normalization, like they do, I'm not sure how they do it when they're working with the digits.
Speaker G: But like in the switchboard, there's conversation side normalization for the non-C0 components and then utterance normalization for the C0 components.
Speaker C: Yeah, this is another difference.
Speaker C: The normalization works on utterance levels, but we have to do it, we have a system that does it online, so it might be better, it might be worse if the channel is constant.
Speaker B: And the acoustic models are like a triple model, so the whole world is a SRI.
Speaker C: Yeah, I guess it's tri-fold.
Speaker E: I think it's probably more than that.
Speaker E: So I think they use these genome things, so there's these kind of pooled models and they can go out to all sorts of dependencies.
Speaker B: It's like the tight states.
Speaker E: They have tight states and I think, I'm just guessing here, but I think they don't just have tri-phones, I think they have a range of dependencies.
Speaker C: Yeah, well, the first thing I want to do is just maybe these gender things.
Speaker C: Maybe see with Andreas, I don't know how much it helps.
Speaker G: So the stuff on the numbers you got, the 2.7 is that using the same training data that the SRI system used and got 1.2?
Speaker C: That's right.
Speaker C: So it's a clean DIDGIT training set.
Speaker G: So exact same training data.
Speaker G: Okay.
Speaker C: I guess you used the clean training set.
Speaker F: Right.
Speaker F: With the SRI system, the Aurora baseline is set up with this version of the clean training set that's been filtered with its D712 filter and to train the SRI system under SUs, the original TI digits under you, Dr. Speech Data, TI digits, which don't have this filter, but I don't think there's any other difference.
Speaker E: So are these results comparable?
Speaker E: So you are getting with the Aurora baseline, something like 2.4% on clean TI digits when training the SRI system with clean TRI digits, right?
Speaker E: And so is your 2.7 comparable where you're using the submitted system?
Speaker C: Yeah, I think so.
Speaker E: Okay.
Speaker E: So it's about the same.
Speaker F: It was 1.2 with the SRI system.
Speaker C: I'm sorry.
Speaker C: Complete SRI system is 1.
Speaker E: You were HDK.
Speaker E: Right.
Speaker E: Okay.
Speaker E: That's right.
Speaker E: Okay.
Speaker E: So the comparable number than for what you were talking about, then SRI would be the 2.5.
Speaker C: It was 4.0 something, right?
Speaker C: The HDK system with the SRI system, right?
Speaker F: The baseline Aurora 2 system, trained on TI digits, tested on meeting record in year.
Speaker F: I think we saw it today and it was about 6.6%.
Speaker E: Right.
Speaker E: Right.
Speaker E: So, yeah.
Speaker E: It's different between this and this.
Speaker C: Okay.
Speaker E: Good.
Speaker E: So they are helping.
Speaker E: That's good.
Speaker E: Yeah.
Speaker C: And another thing I maybe like to do is to just test the SRI system that's trained on microphone, tested on the NoECTI digits.
Speaker C: I'm still wondering where this improvement comes from when you train on microphone, it seems better on meeting digits.
Speaker C: But I wonder if it's just because maybe microphone is acoustically closer to the meeting digits than TI digit is, which is, TI digits are very clean recorded digits.
Speaker G: It would also be interesting to see, to do the regular Aurora test, but use the SRI system instead of the same.
Speaker C: Yeah, that's what I wanted just to, yeah.
Speaker C: So just using the SRI system, test, did some, and tested on Aurora TI digits, right?
Speaker G: When at the full Aurora test?
Speaker C: Yeah, there is this problem of multilinguality, so we don't, you have to train the SRI system
Speaker E: with all the different languages.
Speaker G: Yeah, that's what I mean. So you'll have to work.
Speaker G: Yeah.
Speaker G: Well, I mean, I guess the work would be into getting the files in the right formats or something, right?
Speaker G: Yeah, because when you train up the Aurora system, you're also training on all the data.
Speaker G: That's right.
Speaker G: Yeah, I mean, it's...
Speaker C: Yeah, yeah, yeah, yeah, yeah, yeah, yeah.
Speaker C: Right, I see what you mean.
Speaker E: That's true, but I think that also when we've had these meetings, we have to wake off in times people have not done the full range of things because on whatever is they're trying because it's a lot of work, even just with the HTK.
Speaker E: So it's a good idea, but it seems like it makes sense to do some pruning first with a test or two that makes sense for you and then take the likely candidates to go further.
Speaker C: But just testing on TI digit already gave us some information about what's going on.
Speaker C: Oh, yeah, okay.
Speaker C: The next thing is this VAD problem, but...
Speaker C: So I'm just talking about the curve that I sent.
Speaker C: I sent you.
Speaker C: So that shows that when the SNR decreases, the current VAD approach doesn't drop much frames for some particular noises, which might be the noises that are closer to speech.
Speaker E: Just clarify something for me.
Speaker E: They were supposedly in the next evaluation, they're going to be supplying us with boundaries.
Speaker E: So does any of this matter?
Speaker E: I mean, other than our interest in it.
Speaker C: Well, first of all, the boundaries might be like we would have 200 milliseconds before an after speech.
Speaker C: So removing more than that might still make a difference in the results.
Speaker E: Do we...
Speaker E: I mean, there's some reason that we think that's the case.
Speaker C: No, because we don't...
Speaker C: That much at that.
Speaker C: Still, I think it's an interesting problem.
Speaker C: Oh, yeah.
Speaker C: Yeah.
Speaker E: But maybe we'll get some insight on that when the gang gets back from CREAT because there's lots of interesting problems, of course.
Speaker E: And the thing is, if they really are going to have some means of giving us fairly tight boundaries, then that won't be so much the issue.
Speaker B: Yeah, so we were wondering whether that VAD is going to be like a realistic one or is it going to be some manual segmentation?
Speaker B: And they're like, if that VAD is going to be a realistic one, then we can as well use their markers to shift the point around, I mean, the way we want to find the...
Speaker B: Rather than keeping the 20 frames, we can actually go to the markers point, which we find more suitable for us.
Speaker B: But that is going to be something like a manual segmenter.
Speaker B: Then we couldn't use that information anymore because that's not going to be the one that is used in the final evaluation.
Speaker B: Right.
Speaker B: So we don't know what is the type of VAD, if they're going to provide.
Speaker C: And actually, there's...
Speaker C: Yeah, there's...
Speaker C: I think it's still for...
Speaker C: Even for the evaluation, it might still be interesting to work on this because the boundary is a part that they would provide is just starting off speech and end of speech at the utterance level.
Speaker B: With some gap, I mean, with some passes in the center, provided they meet that order at the hangover time, which is...
Speaker C: Yeah, but when you have like five or six frames, both...
Speaker B: Yeah, they'll just fill it up.
Speaker B: It twists, yeah.
Speaker B: Yeah.
Speaker E: So if you could get at some of that, all of that papers.
Speaker C: Yeah, for like, not this animation and not other things that we want to work on.
None: Okay.
None: But...
Speaker C: Yeah, so I did...
Speaker C: I just started to test putting together two VAD, which was not much work, actually.
Speaker C: I implemented VAD that's very close to the energy-based VAD that the other or guys use.
Speaker C: So, which is just putting a threshold on the noise energy, detecting the first group of four frames that have energy that's about this threshold.
Speaker C: From this point, tagging the frames are a speech.
Speaker C: So it removes the first silent portion of each utterance.
Speaker C: And it really removes it.
Speaker C: Still, on the noises where our MADVAD doesn't work a lot.
Speaker E: And so I would have thought that having some kind of spectral information, you know, when you'll days people would use energy in zero crossings, for instance, would give you some better performance, right?
Speaker E: Because you're meant of low energy, fricatives, or stop consonants or something like that.
Speaker C: Yeah.
Speaker C: So your point is way between...
Speaker E: Oh, that if you use purely energy and don't look at anything spectral, then you don't have a good way of distinguishing between low energy speech components and non-speech.
Speaker E: And just as a gross generalization, many non-speech noises have a low-pass kind of characteristic, some sort of slope.
Speaker E: And most low energy speech components that are unvoiced have a high-pass kind of characteristic and upward slope.
Speaker E: So having some kind of a, you know, the beginning of an S sound, for instance, just starting in, it might be pretty low energy, but it will tend to have this high frequency component whereas a lot of rumble and background noises and so forth will be predominantly low-fruicing.
Speaker E: You know, by itself, it's not enough to tell you, but it plus energy is sort of...
Speaker E: It plus energy plus timing information is sort of...
Speaker E: And if you look up in Rebeanor and Schaeff for like 25 years ago, or something, that's sort of what they were using then.
Speaker C: So it might be that what I did is...
Speaker C: So it removes like low energy speech frames because the way it is said just combined the two decisions, so the one from the MLB and the one from the energy based is with the end operator.
Speaker C: So I only keep the frames where to agree that it's speech.
Speaker C: So if the energy based dropped, dropped low energy speech, they are lost.
Speaker C: But still the way it's done right now, it helps on the noises where it seems to help on the noises where RVU was not very good.
Speaker E: Well, I guess I mean, one could imagine combining them in different ways, but I guess what you're saying is that the MLB based one has the spectral information.
Speaker C: Yeah, so...
Speaker C: But the way it's combined is maybe the...
Speaker C: Well, you can imagine...
Speaker C: The way you use the end operator is...
Speaker C: The frames that are dropped by the energy based system are dropped even if the MLB decides to give them.
Speaker E: Right, and that might not be optimal, but I mean, I guess principle what you'd want to do is have a probability estimated by each one and put them together.
Speaker G: Something that I've used in the past is when just looking at the energy is to look at the derivative and you make your decision when the derivative is increasing for so many frames, then you say that's beginning of speech.
Speaker G: But I'm trying to remember if that requires that you keep some amount of speech and a buffer.
Speaker G: I guess it depends on how you do it, but that's been a useful thing.
Speaker B: I mean, you're everywhere as a delay associated with it, you still have to keep the buffer.
Speaker B: There only make a decision because it's still needed to smooth the decision further.
Speaker B: That's always there.
Speaker G: Yeah, okay.
Speaker C: Well, actually, I don't maybe don't want to work too much on it right now.
Speaker C: I just wanted to see if it's what I observed as a ghost by this really problem.
Speaker C: It seems to be the case.
Speaker C: The second thing is the spectral subtraction, which I just started yesterday to launch a bunch of 25 experiments with different values for the parameters that are used.
Speaker C: So it's the Michael type spectral subtraction which use an overestimation factor.
Speaker C: So I subtract more noise than the noise spectra that is estimated on the noise portion of the utterances.
Speaker C: Should I try several overestimation factors?
Speaker C: And after subtraction, I also add a constant noise and I also try different noise values.
Speaker C: And we'll see what happened.
Speaker C: But still, when we look at the value depends on the parameters that you use.
Speaker C: More moderate overestimation factors and moderate noise level that you add, you have a lot of musical noise.
Speaker C: On the other hand, when you subtract more and when you add more noise, you get rid of this musical noise, but maybe you distort a lot of speech.
Speaker C: Well, until now it doesn't seem to have.
Speaker C: So the next thing maybe I will try to do is just to try to smooth the smooth the result of the subtraction to get rid of the musical noise using some kind of filter.
Speaker B: Can smooth the SNR estimate also?
Speaker C: Yeah, right.
Speaker B: You filter is a function of SNR.
Speaker C: Yeah.
Speaker C: So to get something that would be closer to what you try to do with inner filtering.
Speaker C: Yeah.
Speaker C: Actually, maybe you can, I think, let's see it for me.
Speaker B: So I've been playing with this inner filter like, and there were some bugs in the programs I was initially trying to clear them up.
Speaker B: Because one of the bugs was, I was assuming that always the bad initial frames were silence.
Speaker B: It always started in the silence state, but it was in for some utterances.
Speaker B: So it was in estimating the noise initially and then it never estimated because I assumed that it was always silence.
Speaker C: So this is on speech at Karyatalian?
Speaker B: Yeah, speech at Karyatalian.
Speaker B: So in some cases, there are a few cases actually, which I found later.
Speaker B: So that was one of the bugs that was there in estimating the noise.
Speaker B: And so once we were clear, the ran a few experiments with different ways of smoothing the estimated clean speech and how estimated the noise and smoothing of the SNR also.
Speaker B: And so the trend seems to be like smoothing the current estimate of the clean speech for deriving the SNR, which is like deriving the inner filter, seems to be helping then updating it quite fast using a very small time constant.
Speaker B: So I have like few results where the estimating the most smoothing is helping.
Speaker B: But still it's like it's still comparable to the baseline.
Speaker B: I haven't caught anything beyond the baseline, but that is like not using any inner filter.
Speaker B: And so I'm trying a few more experiments with different time constants for smoothing the noise spectrum and smoothing the clean speech and smoothing SNR.
Speaker B: So there are three time constant that I have.
Speaker B: So I'm just playing around.
Speaker B: So one is fixed in the line like smoothing the clean speech is helping.
Speaker B: So I'm not going to change that much.
Speaker B: But the way I'm estimating the noise and the way I'm estimating the SNR, I'm just trying a little bit.
Speaker B: So that and the other thing is like putting a floor on the SNR because that if some in some cases the clean speech is like when it's estimated it goes to very low value.
Speaker B: So the SNR is like pretty low.
Speaker B: So that actually creates a lot of variance in the low energy region of the speech.
Speaker B: So thing of like putting a floor also for the SNR so that it doesn't vary a lot in the low energy regions.
Speaker B: And so the results are like so far I've been testing only with the baseline which is which doesn't have any LDA filtering and online analysis that I just want to separate that the contributions out.
Speaker B: So it's just VAD plus, the VINOR filter plus the baseline system which is just the spectral I mean the male frequency coefficient.
Speaker B: And the other thing that I tried was by I just took one of those Carlos filters which he needed had to see whether it really helps or not.
Speaker B: It just ran to see whether it really degrades all of it helps.
Speaker B: It seems to be like it's not hurting a lot by just blindly picking up one filter which is nothing but a four hertz a band pass filter on the cubic root of the power spectrum.
Speaker B: So that was the filter that Carlos had.
Speaker B: And so just to see whether it really it's worth trying around.
Speaker B: So it doesn't seem to be degrading a lot on that.
Speaker B: So there must be something that I can be done with that type of noise compensation also because I have to ask Carlos about that.
Speaker B: I mean how he derived those filters and where he has any filters which are derived on co-GIS stories added with some type of noise which what we are using currently of something that so maybe we have.
Speaker E: This is cubic root of power spectrum.
Speaker B: Yeah, cubic root of power spectrum.
Speaker E: So if you have this band pass filter you probably get negative values right?
Speaker B: Yeah, and I'm like so it has like the spectrogram has like it actually enhances the onset and the offset of I mean the beginning and the end of the speech.
Speaker B: So it seems to be like deep valleys and the beginning and the end of like high energy regions because the filter has like a sort of mix again high type structure.
Speaker B: So those are the regions where the like when I look at the spectrogram there are those deep valleys on the beginning and the end of the speech but rest of it seems to be like pretty nice.
Speaker B: So that's something I observed using that filter and yeah there are few very not a lot of because the filter doesn't have a really deep negative portion so that it's not really creating a lot of negative values in the cubic root.
Speaker B: So I'll just continue with that for some maybe I'll ask Carlos later more about how to play with those filters and while making this we have that yeah that's it.
Speaker E: Last week you were also talking about building up the subspace.
Speaker B: Yeah I would actually didn't get enough time to work on the subspace last week was mostly about finding those bugs and yeah thanks.
Speaker B: I think about much on that.
Speaker G: How about you coming?
Speaker A: Well I'm still working with BTS and one of the things that last week say here that maybe the problem was with the deep because the signal have different level of energy and maybe talking with Stefan and with Sunil we decide that maybe it was interesting to apply your line normalization before applying BTS but then we decided that it doesn't work absolutely because we modified also the noise.
Speaker A: And well thinking about that with then we decide that maybe it's a good idea we don't know.
Speaker A: I didn't do the experiment yet to apply BTS in the castral domain.
Speaker E: The other thing is so and not and C0 would be a different so you could do different normalization for C0 than for other things anyway.
Speaker E: I mean the other thing I was going to suggest is that you could have two kinds of normalization with different time constants so you could do some normalization before the BTS and then do some other normalization after.
Speaker E: But C0 certainly acts differently than the others do so that's.
Speaker A: Well we decided to obtain the new expression if we work in the castral domain.
Speaker A: Well I am working in that now but I'm not sure if that would be useful.
Speaker A: I don't know it's quite a lot of work.
Speaker A: It's not too much but it's work and I want to know if we have some feeling that the result I would like to know if I don't have any feeling if this will work better than applying BTS in the castral domain will work better than applying in the filter packet of mine.
Speaker A: I'm not sure.
Speaker A: I don't know absolutely nothing.
Speaker E: Yeah well I think you're the first one here to work with BTS so we could call someone else up who has asking their opinion.
Speaker E: I don't have a good feeling for it.
Speaker C: Actually the BTS that you tested before was in the log domain and the code book is kind of dependent on the level of the speech signal.
Speaker C: So I expect it if you have something that's independent of this, I expect it to be a better model of speech.
Speaker E: You wouldn't even need to switch the capture right?
Speaker C: Just normalize the mean.
Speaker C: Remove the mean.
Speaker E: Yeah then you have one number which is very dependent on level because it is the level and the other which is not.
Speaker C: But here also we would have to be careful about removing the mean of speech and not of noise because it's like first doing general normalization and then noise removal which is.
Speaker A: Yeah we were thinking to estimate the noise with the first frames and then apply the VAD before the normalization.
Speaker A: I am thinking about that and working about that but I don't have this way.
Speaker E: I mean one of the things we talked about maybe might be time to start thinking about pretty soon is we look at the pros and cons of these different methods.
Speaker E: How do they fit in with one another because we talked about potentially doing some combination of a couple of them.
Speaker E: Maybe we may be pretty soon we'll have some sense of what their characteristics are so we can see what should be combined.
Speaker E: Is that it?
Speaker E: Okay.
Speaker E: Okay.
Speaker E: We read some digits.
Speaker E: Yeah.
Speaker G: Want to go ahead and mark?
None: Sure.
None: Transcript.
Speaker E: LDASH 212.
Speaker E: 248829109.
Speaker E: 7061782596.
Speaker E: 858779619.
Speaker E: 48441752.
Speaker E: 2462721821.
Speaker E: 35251.
Speaker E: 50381177.
Speaker E: 697496315.
Speaker C: Transcript LDASH 213.
Speaker C: 2454437657.
Speaker C: 318 O2O997.
Speaker C: 317388313.
Speaker C: 071724153985.
Speaker C: 0989062173.
Speaker C: 2995696699.
Speaker C: 859946451.
Speaker C: 2453759935.
Speaker D: Transcript LDASH 2147539591352.
Speaker D: 53510101067.
Speaker D: 170437155.
Speaker D: 924681.
Speaker D: 0710147667915.
Speaker D: 219220956097.
Speaker D: 0250858405.
Speaker D: 34237491.
Speaker G: Transcript LDASH 215.
Speaker G: 357165459919.
Speaker G: 552085225.
Speaker G: 137988958.
Speaker G: 056540589.
Speaker G: 34565268.
Speaker G: 30182868.
Speaker G: 8394407633.
Speaker G: 507932030.
Speaker B: Transcript LDASH 216.
Speaker B: 020132950.
Speaker B: 013809626.
Speaker B: 0274407591.
Speaker B: 2472293233.
Speaker B: 745114817.
Speaker B: 6918953964.
Speaker B: 66763799.
Speaker B: 51700707811.
Speaker F: Transcript LDASH 217.
Speaker F: 723954211.
Speaker F: 9238523639.
Speaker F: 047217119.
Speaker F: 315592660584.
Speaker F: 598848386934.
Speaker F: 851149017.
Speaker F: 03006587.
Speaker F: 532425788.
Speaker A: Transcript LDASH 218.
Speaker A: 8896023667.
Speaker A: 7807172039.
Speaker A: 064322851.
Speaker A: 51559961.
Speaker A: 4378766857.
Speaker A: 036926029.
Speaker A: 381558434.
Speaker A: 415501528.
Speaker A: 8378.
None: 8378.
None: 8378.
None: 8378.
