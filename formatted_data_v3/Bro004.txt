Speaker G: I don't know.
Speaker G: I can't do the crash.
Speaker G: I think I'm going to say.
Speaker G: Wow.
Speaker B: I'm going to say that.
Speaker B: Hello.
Speaker B: Hello.
Speaker B: Hi.
Speaker B: Well, maybe it's a turning off.
Speaker D: It's a turning on.
Speaker D: I'm not turning.
Speaker D: I'm going to turn it over.
Speaker D: So I can't figure out how to tell.
Speaker D: If it's the car or the sentence in illegal,
None: I don't know how to tell it. I think it's you.
None: Yeah, I'll have a good mind.
Speaker B: Oh.
None: Okay.
None: Okay.
Speaker E: Okay.
Speaker E: Okay.
Speaker E: So, I guess we are going to do the digits at the end.
Speaker C: General tree.
Speaker C: Okay.
Speaker A: 75.
Speaker A: Yes, sir.
Speaker E: Yeah, that's the mic over there.
None: It's a written.
Speaker E: Mike, the guy who's here.
Speaker E: The channel.
Speaker B: The channel 4.
Speaker A: Yes.
Speaker A: Oh.
Speaker E: Yes.
Speaker E: And I'm channel 2.
Speaker E: I think we're channel 1.
Speaker E: I think I'm channel 1.
Speaker E: Oh, I'm channel 1.
Speaker E: Channel 1?
Speaker E: Yes.
None: Okay.
Speaker E: Okay.
Speaker E: So, I also copied the results that we all got in the mail.
Speaker E: I think from OGI.
Speaker E: Go through them also.
Speaker E: So, where are we on our runs?
Speaker C: So, we, so as I was already said, we mainly focused on four kind of features.
Speaker C: The PLP, the PLP with Jerasda, the MSG and the MFCC from the business.
Speaker C: Over.
Speaker C: And we focused for the test part on the English and the Italian.
Speaker C: We've trained several neural networks on the DIA digits English.
Speaker C: And on the Italian data and also on the broad English French and Spanish databases.
Speaker C: So, there is a result tables here for the tandem approach.
Speaker C: And actually what we've observed is that if the network is trained on the task data, it works pretty well.
Speaker E: Okay.
Speaker E: I'm sorry.
Speaker E: There's a pausing for a photo from the front of the room.
Speaker E: Yeah.
Speaker E: It's longer.
Speaker E: We're pausing for a photo opportunity here.
Speaker D: So, wait, wait, wait, wait.
Speaker B: Yeah.
Speaker B: Okay.
Speaker E: Let me give you a black screen.
Speaker E: Facing this one.
Speaker E: Okay, this could be a good section for our silencing section.
Speaker E: Musical chairs, everybody.
Speaker E: So, you were saying about the training data.
Speaker C: Yeah.
Speaker C: So, if the network is trained on the task data, tandem works pretty well.
Speaker C: And actually we have results are similar.
Speaker G: Do you mean if it's trained only on the data from just that task, that language?
Speaker C: Yes, but actually we didn't train network on both types of data.
Speaker C: I mean phonetically balanced data and task data.
Speaker C: We only did either task data or wrote data.
Speaker C: Yeah.
Speaker E: So, how, I mean, clearly it's going to be good then, but the question is how much worse is it if you have broad data?
Speaker E: I mean, from what I saw from the early results, I guess last week, was that if you trained on one language and tested on another, say that the results were relatively poor.
Speaker E: But the question is if you train on one language, but you have a broad coverage and then test on another, because that improves things in comparison.
Speaker C: So, you use the same language, you mean?
Speaker E: No, no, definitely.
Speaker E: So, if you train on TI digits and test on Italian digits, you do poorly.
Speaker E: Say, I don't know if the numbers in front of me, so I'm just imagining.
Speaker E: Yeah, but I did not do that.
Speaker E: So, you did not?
Speaker E: We train on 10 minutes and test on Italian digits.
Speaker C: No, we did, for kind of testing, actually, the first testing is with task data.
Speaker C: So, with net strain on task data, so for Italian, on the Italian speech data curve, the second test is trained on a single language, but the broad database, but the same language as the task data.
Speaker C: But for Italian, we choose Spanish, which we assume is close to Italian.
Speaker C: The third test is by using the tree language database.
Speaker C: And the fourth is...
Speaker E: In S3 languages, that's including the...
Speaker E: This includes the one that it's...
Speaker E: Yeah.
Speaker G: But not digits, I mean.
Speaker G: The three languages is not digits, it's the broad data.
Speaker G: Yeah, data, okay.
Speaker C: And the fourth test is excluding from these three languages, the language that is the task language.
Speaker E: Oh, okay, yeah, so that's what I wanted to know.
Speaker E: And just wasn't saying it very well.
Speaker C: Yeah.
Speaker C: So, for the digits, for example, when we go from the digits training to the limit training, we lose around 10%.
Speaker C: The error rate increases...
Speaker C: Right.
Speaker C:...of 10% relative.
Speaker C: So this is not so bad.
Speaker C: And then when we jump to the multilingual data, it's...
Speaker C: It becomes worse and...
Speaker E: Well, how much?
Speaker C: Around, let's say, 20% further.
Speaker C: So...
Speaker C: Yeah.
Speaker C: 12% further.
Speaker C: So 30% further, yeah.
Speaker G: And so remind me that multilingual stuff is just the broad data, right?
Speaker G: Yeah, it's not the digits.
Speaker G: So it's the combination of two things there.
Speaker G: It's removing the task-specific training and it's adding other languages.
Speaker C: Yeah, okay.
Speaker C: But the first step is already a regular task is specific.
Speaker C: So the building.
Speaker C: Yeah, okay.
Speaker C: So basically, when it's trained on the multilingual broad data or number, so the ratio of error rates with baseline error rate is around 1.1.
Speaker E: Yeah.
Speaker E: And it's something like 1.3 of the...
Speaker E: If you compare everything to the first case, it's the baseline.
Speaker E: You get something like 1.1 for the using the same language with a different task.
Speaker E: Something like 1.3 for three languages, a lot of stuff.
Speaker C: Same language we are for English at 0.8.
Speaker C: So it improves compared to the baseline.
Speaker C: But...
Speaker C: That's good.
Speaker E: I meant something different by baseline.
Speaker E: So let me...
Speaker E: Okay, fine.
Speaker E: Let's use the conventional meaning of baseline.
Speaker E: By baseline here, I meant using the task-specific data.
Speaker E: Okay.
Speaker E: But because that's what you were just doing with this 10%.
Speaker E: Yes.
Speaker E: I just trying to understand that.
Speaker E: So if we call a factor just 1, just normalize to 1, the word error rate that you have for using TI digits as training and TI digits as test.
Speaker E: Different words, I'm sure, but the same task and so on.
Speaker E: If you call that one, then what you're saying is that the word error rate for the same language but using different training data and you're testing on TI digit and so forth, it's 1.1.
Speaker E: Yeah, it's around 1.1.
Speaker E: Right. And if you do go to three languages including the English, it's something like 1.3.
Speaker E: That's what you were just saying, I think.
Speaker C: More, actually.
Speaker C: 1.4?
Speaker C: Yeah.
Speaker C: So it's an additional 30%.
Speaker C: What would you say?
Speaker C: Around 1.4.
Speaker E: Okay. And if you exclude English from this combination of that.
Speaker C: If we exclude English, there is not much difference with the data with English.
Speaker E: So, that's interesting.
Speaker E: You see because, so that's important. So what it's saying here is just that yes, there is a reduction in performance when you don't have task data.
Speaker E: Wait a minute.
Speaker E: Wait a minute.
Speaker E: Actually, it's interesting. So when you go to a different task, there's actually not so different.
Speaker E: So what's the difference between 2 and 3? Between the 1.1 case and the 1.4 case, I'm confused.
Speaker D: It's multilingual.
Speaker C: Yeah, the only difference is that it's multilingual.
Speaker E: Because in both of those cases, you don't have the same task.
Speaker E: So is the training data for this 1.4 case? Does it include the training data for the 1.1 case?
Speaker C: Yeah.
Speaker C: Yeah, I'm proud to fit you.
Speaker C: How much bigger is it?
Speaker C: It's 2 times, actually.
Speaker C: The multilingual databases are 2 times the broad English data.
Speaker C: We just wanted to keep it.
Speaker E: So it's 2 times.
Speaker E: So it includes the broad English data.
Speaker E: So that's timet, basically.
Speaker E: So it's band limited timet.
Speaker E: This is all Angular sampling.
Speaker E: So your band limited timet gave you almost as good a result as using TI digits on a TI digits test.
Speaker E: But when you add in more training data, it keeps the neural net the same size.
Speaker E: It performs worse on the TI digits.
Speaker E: Now all of this is noisy TI digits, I assume?
Speaker E: Yeah, both training and test.
Speaker E: Yeah, okay.
Speaker E: Well, we made this need to...
Speaker E: So it's interesting that going to a different task didn't seem to hurt us that much.
Speaker E: Going to a different language...
Speaker E: It doesn't seem to matter.
Speaker E: The difference between 3 and 4 is not particularly great.
Speaker E: So that means that whether you have the language in or not is not such a big deal.
Speaker E: It sounds like we may need to have more of things that are similar to a target language.
Speaker E: I mean, you have the same number of parameters in the neural net.
Speaker E: You haven't increased the size of the neural net.
Speaker E: And maybe there's just not enough complexity to it to represent the very increased variability in the training set.
Speaker E: That could be.
Speaker E: So what about... So these are results that you're describing now that are pretty similar for the different features?
Speaker C: Let me check.
Speaker C: So this was for the PLP.
Speaker C: For the PLP with Geras, that we...
Speaker C: This is quite the same tendency with the slight increase of the error rate.
Speaker C: If we go to the team it, and then it gets worse with the mid-deling.
Speaker C: Yeah, there is a difference actually between PLP and Geras, that Geras seems to perform better with the I-limit-matched condition, but slightly worse for the well-matched condition.
Speaker E: I have a suggestion actually, you know, to lay us slightly.
Speaker E: Would you mind running in the other room and making copies of this?
Speaker E: Because we're all sort of... If we could look at it while we're talking, I think, I'll sing a song or dance or something like that.
Speaker E: So go ahead.
Speaker G: What you're going to ask someone my question.
Speaker E: This way and to slide it to the left, yeah.
Speaker G: What was this number 40?
Speaker G: It was roughly the same as this one, he said.
Speaker G: You had the two language versus the three language.
Speaker G: That's what he was saying.
Speaker E: Or he removed English.
Speaker E: Sometimes actually depends on what features you're using.
Speaker E: Yeah.
Speaker E: But it sounds like... I mean, that's interesting because it seems like what it's saying is not so much that you got hurt because you didn't have so much representation of English because in the other case, you don't get hurt anymore, at least when.
Speaker E: It seems like it might simply be a case that you have something that is just much more diverse but you have the same number of parameters representing it.
Speaker G: I wonder were all three of these nets using the same output, this multi-language...
Speaker G: labeling.
Speaker E: It's using 64 phonemes from sample.
None: Okay.
Speaker G: So this would, from this you would say, well, it doesn't really matter if we could finish into the training of the neural net if there's going to be, you know, finish in the test data, right?
Speaker E: Well, it sounds...
Speaker E: We have to be careful because we haven't done a good result yet, comparing different bad results.
Speaker E: I think it does suggest that it's not so much cross language as cross type of speech.
Speaker E: It's...
Speaker E: But we did, oh yeah, the other thing I was asking though is that I think that in the case...
Speaker E: Yeah, you do have to be careful because of compounded results.
Speaker E: I think they got some earlier results in which you trained on one language and tested on another and you didn't have three, you just had one language, so you trained on one type of digits and tested on another.
Speaker E: Wasn't there something of that where you say trained on Spanish and tested on TI digits or the other way around?
Speaker E: No.
Speaker E: Not there was something like that that he showed me last week.
Speaker E: The way to where you could...
Speaker E: Yeah, that would be interesting.
Speaker E: This may have been what I was asking before stuff, but...
Speaker E: Wasn't there something that you did where you trained on one language and tested on another?
Speaker E: No mixture, but just...
Speaker C: Hello.
Speaker E: We've never just trained on one language.
Speaker C: Training on the single language and testing on one language.
Speaker C: Yeah.
Speaker C: Right, so the only test that's similar to this is training on two languages.
Speaker E: But we've done a bunch of things where we just trained on one language, right?
Speaker E: I mean, you haven't done all your tests on multiple languages.
Speaker C: No, either this is test with the same language, but from the broad data or its test with different languages.
Speaker C: The list of different languages.
Speaker G: Did you do different languages from digits?
Speaker C: No, you mean training digits on one language and using the net to recognize digits on another language?
Speaker G: No.
Speaker E: See, I thought you showed me something like that last week.
Speaker E: You had a little...
Speaker C: No, I didn't think so.
None: What?
Speaker D: He's almost sorry.
Speaker E: So, I mean, what's this table that we're looking at is...
Speaker E: Is all testing for TI digits?
Speaker C: So, you have basically two parts, the upper part is for TI digits.
Speaker C: And it's divided into three rows of four rows each.
Speaker C: Yeah.
Speaker C: The first four rows is well matched, then the second group of four rows is mismatched.
Speaker C: Finally, I'm mismatched.
Speaker C: And then the lower part is for Italian, and it's the same thing.
Speaker G: So, the upper part is training TI digits?
Speaker C: It's the HTK results.
Speaker C: I mean, so it's HTK training testings with different kinds of features and what appears in the left column is the networks that are used for doing this.
Speaker C: So...
Speaker E: What was it that you had done last week when you showed your number?
Speaker E: Why?
Speaker E: When you showed me the table last week?
Speaker C: It was part of these results.
Speaker G: So, where is the baseline for the TI digits located in here?
Speaker C: You mean the HTK or a baseline?
Speaker C: Yeah.
Speaker C: It's the 100 number.
Speaker C: All these numbers are in the ratio with respect to the baseline.
Speaker E: So, this is where it aerates, so a high number is bad.
Speaker C: Yeah, this is a world aerarade ratio.
Speaker G: Okay.
Speaker C: So, 70.2 means that we reduce the aerarade to 30%.
Speaker C: Okay.
Speaker E: Okay.
Speaker E: So, if we take...
Speaker E: Let's see, POP with online normalization and delta-delts.
Speaker E: So, that's the thing you have circled here in the second column.
Speaker E: And multi-english refers to what?
Speaker C: To demit.
Speaker C: Then you have MF, MF and ME, which are from French, Spanish and English.
Speaker C: Actually, I forgot to say that the multilingual net are trained on features without the derivatives.
Speaker C: But with increased frame numbers.
Speaker C: And we can see on the first line of the table that it's less worse when we don't use delta, but it's not that much.
Speaker E: So, I'm sorry, Mr. Watts-MF, does an ME?
Speaker C: Multi-french with this Spanish.
Speaker E: So, it's a broader vocabulary.
Speaker E: Okay.
Speaker E: So, I think what I saw in the smaller chart that I was thinking of was...
Speaker E: There were some numbers I saw, I think, that included these multiple languages.
Speaker E: And I was seeing that it got worse.
Speaker E: I think that was almost.
Speaker E: You had some very limited results at that point, which showed having in these other languages.
Speaker E: In fact, it might have been just this last category having two languages broad that were where English was removed.
Speaker E: So, that was cross-language, and the result was quite poor.
Speaker E: What I hadn't seen yet was that if you had it in the English, it's still poor.
Speaker E: Yeah.
Speaker E: Now, what's the noise condition of the training data?
Speaker E: Well, I think this is what you were explaining. Noise condition is the same. It's the same Aurora noises in all these cases.
Speaker E: You have the training.
Speaker E: So, there's not a statistically strong, statistically different noise characteristic between...
Speaker E: No, these are the training attest.
Speaker E: And yet, we're seeing some kind of data.
Speaker C: At least for the first...
Speaker C: Well matched.
Speaker E: So, there's some kind of an effect from having this broader coverage.
Speaker E: Now, I guess what we should do by doing with this is try testing these on this same sort of thing.
Speaker E: You probably must have this lined up to do to try the same...
Speaker E: With the exact same training, do testing on the other languages.
Speaker E: Oh well, you have it here for the Italian.
Speaker E: That's right.
Speaker C: So, for the Italian, the results are stranger.
Speaker C: So, what appears is that perhaps Spanish is not very close to Italian, because when using the network training on Spanish, your rate is almost twice the baseline error rate.
Speaker E: Well, I mean, let's see.
Speaker E: Is there any difference in...
Speaker E: So, you're saying that when you train on English and test on...
Speaker E: No, you don't have training on English.
Speaker C: There is another difference is that the noises are different.
Speaker C: For the Italian part, I mean the networks are trained with noise from...
Speaker C: or the ATGETs.
Speaker C: And the noise...
Speaker C: Perhaps the noise are quite different from the noises in the speech that Italian.
Speaker E: Do we have any test sets in any other language that have the same noise as in...
Speaker E: You are...
Speaker G: Can I add something real quick?
Speaker G: In the upper part, the English stuff, it looks like the very best number is 60.9.
Speaker G: And that's in the third section in the upper part under PLP, J. Rosta, for the middle column.
Speaker G: Yeah.
Speaker G: Is that a noisy condition?
Speaker F: Yeah.
Speaker G: So, that's matched training. Is that what that is?
Speaker C: It's not a third part, so it's an I-limy smashed.
Speaker C: So, training...
Speaker G: So, why do you get your best number in...
Speaker G: Wouldn't you get your best number in the clean case?
Speaker D: It's relative to the baseline mismatching.
Speaker G: Oh, okay. So, these are not...
Speaker G: Okay. All right. I see.
Speaker G: Yeah.
Speaker G: Okay.
Speaker G: And then, so, in the...
Speaker G: In the non-mismatched clean case, your best one was under MFCC.
Speaker G: That's 61.4.
Speaker C: Yeah. But it's not a clean case.
Speaker C: It's a noisy case, but training and test noises are the same.
Speaker G: Oh, so this upper third?
Speaker G: Yeah.
Speaker G: That's still noisy?
Speaker G: Yeah.
Speaker G: Okay.
Speaker C: So, it's always noisy, basically.
Speaker C: What?
Speaker C: Nice.
Speaker E: Okay.
Speaker E: So, I think this will take some...
Speaker E: Looking at, thinking about it.
Speaker E: What is currently running that's...
Speaker E: That just filling in holes here?
Speaker C: No, we don't plan to fill the holes, but...
Speaker C: Actually, there is something important.
Speaker C: Is that we made a lot of assumption concerning the online normalization.
Speaker C: And we just noticed recently that the approach that we were using was not leading to very good results when we use the straight features to HDK.
Speaker C: So, basically, if you look at the left of the table, the first row with 86, 143 and 75, these are the results we obtained for Italian with straight PLP features using online normalization.
Speaker C: And the...
Speaker C: What's in the table just at the left of the PLP 12 online normalization column.
Speaker C: So, the number 79, 54 and 42 are the results obtained by PDIBA with...
Speaker C:...is online normalization.
Speaker C: Where is that 79?
Speaker E: It's just sort of sitting right on the column line.
Speaker E: Oh, I see.
Speaker C: Yeah. So, these are the results of OGI with online normalization and straight features to HDK.
Speaker C: And the previous result, 86 and so on, yes.
Speaker C: With our features, straight to HDK.
Speaker C: So, what we see there is that the way we were doing this was not correct, but still the networks are very good.
Speaker C: When we use the networks, our numbers are better.
Speaker E: So, do you know what was wrong with the online normalization?
Speaker C: Yeah, there were different things.
Speaker C: Basically, my fourth thing is the alpha values, so the recursion part.
Speaker C: I used 0.5%, which was the default value in the programs here.
Speaker C: And the pretty values are 5%.
Speaker C: So, it adapts more quickly.
Speaker C: But, yeah, I assume that this was not important because previous results from Dan and show that basically both values give the same results.
Speaker C: It was true on TI digits, but not true on Italian.
Speaker C: Second thing is the initialization of the stuff.
Speaker C: Actually, what we were doing is to start the recursion from the beginning of the iterations and using initial values that are the global mean and variance.
Speaker C: Measure that across the world database.
Speaker C: And pretty bad, it's something different is that she initialized the values of the mean and variance by computing this on the 25 first frames of each other.
Speaker C: There were other minor differences, the fact that she used 15 DCT instead of 13 and that she used C0 instead of log energy.
Speaker C: But the main difference is concerns the recursion.
Speaker C: So, I changed the code and now we have a baseline that's similar to the OGI baseline.
Speaker C: It's slightly different because I don't exactly initialize the same way she does.
Speaker C: Actually, I don't wait to have 25 frames before computing the mean and variance to start the recursion.
Speaker C: I use our line scheme and only start the recursion after the 25th frame.
Speaker C: But it's similar.
Speaker C: I retrained the networks with these, well, the networks are retraining with these new features.
Speaker C: So, basically what I expect is that these numbers will a little bit go down but perhaps not so much because I think the neural networks learn perhaps to, even if the feature sound is normalized, it will learn how to normalize.
Speaker E: I think that given the pressure of time, we probably want to draw some conclusions from this, do some reductions in what we're looking at and make some strong decisions for what we're going to do testing on for next week.
Speaker E: Did you have something going on on the side with multi-band?
Speaker C: No, we plan to start this. So, actually, we have discussed what we could do more as a research.
Speaker C: We were thinking perhaps that the way we use the tandem is not, well, there is basically perhaps a flow in the stuff because we train the networks.
Speaker C: What we ask is the network is to put the decision boundary somewhere in the space and ask the network to put one side of the particular phoneme at one side of the boundary decision boundary and one for another phoneme at the other side.
Speaker C: So, there is kind of reduction of the information there that's not correct because if we change task and if the phonemes are not in the same context and the new task, obviously the decision boundaries should not be at the same place.
Speaker C: But the way the network gives the features is that it removes completely information from the features by placing the decision boundaries that optimal places for one kind of data.
Speaker C: But this is not the case for another kind of data. So, what we were thinking about is perhaps one way to solve this problem is to increase the number of outputs of the neural networks.
Speaker C: Doing something like phonemes within contexts, well, basically context dependent phonemes.
Speaker E: Maybe. I mean, I think you could make the same argument, be justice legitimate for hybrid systems as well. But we know that things get better with context dependent versions.
Speaker E: Yeah, but here it's something different. We want to have features. Yeah. But it's still true that what you're doing is you're ignoring, you're coming up with something to represent whether it's distribution, probably distribution or features.
Speaker E: If you're coming up with a set of variables that are representing things that vary over context and you're putting it all together, ignoring the differences in context. That's true for the hybrid system, the street for a tandem system.
Speaker E: So for that reason, when you in the hybrid system, when you incorporate context one way or another, you do get better scores. Yeah. Okay. But it's a big deal to get that.
Speaker E: I'm sort of. And once you the other thing is that once you represent start representing more and more context, it is much more specific to a particular task in language.
Speaker E: So the acoustics essentially particular context, for instance, you may have some kinds of context that will never occur in one language and will occur frequently in the others.
Speaker E: The issue of getting enough training for a particular kind of context becomes harder. We already actually don't have a huge amount of training data.
Speaker C: Yeah, but. I mean, the way we do it now is that we have a neural network and basically the network is trained almost to give binary decisions.
Speaker C: Right. And binary decisions about phonemes. Almost. But I mean, it does give a distribution. Yeah.
Speaker E: And it is true that if there's two phones that are very similar that they may prefer one that it will give a reasonably high value to the other two. Yeah. Sure. But.
Speaker C: Basically, it's almost binary decisions and the idea of using more classes is to get something that's less binary decision. Oh, no, but it would still be even more of a binary decision.
Speaker E: It would be more of one because then you would say that in that this phone in this context is a one, but the same phone in a slightly different context is a zero. That would be even even more distinct for binary decision. I have to think you'd want to go the other way and have fewer classes.
Speaker E: I mean, for instance, the thing I was arguing for before, but again, which I don't think we have time to try is something in which you would modify the code so you could train to have several outputs on and use articulatory features.
Speaker E: Because then that would go that would be much broader and cover many different situations. If you got a very, very fine category. Yeah, but I think.
Speaker C: Yeah, perhaps you're right, but you have more classes. So you have more information in your features. So you have more information in the posterials vector, which means that.
Speaker C: But still information is relevant because it's information that absolutely discriminates if it's posterior to discriminate amongst phonemes in context.
Speaker E: Well, it's an interesting. So I mean, we could disagree about it length, but the real thing is if you're interested in it, you'll probably try it and we'll see.
Speaker E: But, but what I'm more concerned with now and this operational level is, you know, what do we do in four or five days? And so we have to be concerned with, are we going to look at any combinations of things?
Speaker E: You know, once the nets get retrained, so you have this problem out of it. Are we going to look at multi-band, are we going to look at combinations of things?
Speaker E: What questions are we going to ask? Now that we should probably turn shortly to this. So, gee, I know how are we going to combine with what they've been focusing on?
Speaker E: We haven't been doing any of the LDA roster sort of thing. And they, although they don't talk about it in this note, there's the issue of the.
Speaker E: New law business versus the logarithm. So, so what is going on right now? What's right? You've got nets retraining.
Speaker A: Is there any HTK training? I'm trying the HTK with PLP 12 online delta delta MSG feature together.
Speaker E: Combination, I see. But the combination is so. MSG and PLP. And is this with the revised online normalization?
Speaker E: With the old one. So it's using all the nets for that. But again, we have the hope that we have the hope that it maybe is not making too much difference.
Speaker C: So there is this combination, yeah, working on combination, obviously. I will start to work on multi-band. And we plan to work also on the idea of using both features and net outputs.
Speaker C: And we think that with this approach, perhaps we could reduce the number of outputs of neural network. So get simple networks because we still have the features.
Speaker C: So we have come up with different kind of broad phonetic categories. And we have basically we have three types of broad phonetic classes.
Speaker C: Well, something using a base of articulation, which leads to nine, I think, broad classes. Another which is based on manner, which is something also like nine classes.
Speaker C: And then something that combined both. And we have 25, 27, broad classes.
Speaker E: So like back forwards, front forwards. So what should you do? So you have two nets or three nets? How many nets do you have?
Speaker C: For a moment, we don't have nets. I mean, it's just we're just changing the labels to retrain nets with fewer outputs. Right. And then I didn't understand.
Speaker E: The software currently just has allows for I think the one one hot output. So you're having multiple nets and combining them. Or how are you how are you coming up with if you say, if you have a place characteristic and a manner characteristic, how do you?
Speaker C: It's the single net one. Oh, it's just one net. It's one net with 27 outputs if we have 27 classes. So it's basically a standard net with fewer classes.
Speaker E: So you're sort of going the other way of what you were saying a bit ago. Yeah, but I think yeah, including the features. Yeah, I don't think this will work alone.
Speaker C: I think it will get worse because well, I believe the effect that of to reducing too much information is basically what happens.
Speaker C: And I think if you include that plus the other feature. Yeah, because there's perhaps one important thing that the net brings and what I show show that is the distinction between speech and silence because these nets are trained on well controlled condition.
Speaker C: I mean, the labels are obtained on clean speech. We had no is after. So this is one thing. But perhaps something intermediary using also some broad classes could could bring some much more information.
Speaker E: So again, we have these broad classes and well, somewhat, I mean 27 is 64 basically. And you have the original features with your POP or something. Then just to remind me, all of that goes into that all of that is transformed by KL or something.
Speaker C: So we'll probably be one single KL to transform everything.
Speaker E: Well, no, I do something that you know, so there's a question of whether you would write whether you would transform together or just one. Yeah, I want to try it both ways. That's interesting. So that's something that you haven't trained yet, but are preparing to train. Yeah.
Speaker E: So I think you know, we need to choose the choose the experiments carefully.
Speaker E: We get key questions answered before then and leave other ones aside, even if it's in complete tables someplace. It's really time to choose. Let me pass this out, by the way.
Speaker E: Did I interrupt you with or other things that you want?
Speaker E: Something I asked, so they're doing the VAD, I guess they mean voice activity section. So against the silence. So they've just trained up in that, which has two outputs, I believe.
Speaker E: I asked he and I asked he and I, whether they compared that to just taking the nets we already had and summing up the probabilities to get the speech voice activity detection or else just using the silence if there's anyone silence up.
Speaker E: And he didn't think they had, but on the other hand, maybe they can get by with a smaller net and maybe sometimes you don't run the other. Maybe there's a computational advantage to having a separate net anyway.
Speaker E: So the results look pretty good. I mean, not uniformly. I mean, there's an example or two that you can find where it made it slightly worse, but in all but a couple examples.
Speaker A: They have a question of the result. How are trying the LDA filter? How are trying the LDA filter?
Speaker A: I'm sorry, don't understand the question. The LDA filter needs some trying set to obtain the filter. Maybe I don't know exactly how.
Speaker A: Training. Training with the training test of each understanding.
Speaker A: For example, LDA filter needs a set of trying to obtain the filter. Maybe for the Italian, for the EU and for Finnish. These filter are obtained with the own training set.
Speaker E: Yes, I don't know. That's a very good question. Where does the LDA come from? In earlier experiments, they had taken LDA from a completely different database.
Speaker A: Yeah, because maybe it's the same situation that the neural network is running with the own.
Speaker E: So that's a good question. Where does it come from? Yeah, I don't know. But to tell you the truth, I wasn't actually looking at the LDA so much when I was looking at it. I was mostly thinking about the VAD.
Speaker A: What is ASP? Oh, that's the features. I don't understand what you're saying. What is the difference between ASP and baseline over?
Speaker E: Because there's baseline error above it. And this is mostly better than baseline, although in some cases it's a little less.
Speaker E: So it's basically ASP is 23 ml, minus 13. Yeah, it says what it is, but I don't know how that's different from the baseline.
Speaker C: I think this is the same point we were at when we were at the CZ row, using CZ row instead of luck energy. It should be that.
Speaker G: They say in here that the VAD is not used as an additional feature. Does anybody know how they're using it?
Speaker E: So what they're doing here is if you look down at the block diagram, they get an estimate of whether it's speech or silence, then they have a median filter of it.
Speaker E: So basically they're trying to find stretches. The median filter is enforcing and having some continuity. You find stretches where the combination of the frame wise VAD and the median filters say that the stretch of silence. And then it's going through and destroying the data away.
Speaker E: So it's throwing out frames. The median filter is enforcing that it's not going to be single cases of frames.
Speaker E: So it's throwing out frames. And the thing is what I don't understand is how they're doing this with HTK. That's what I was just going to ask. How can this throw out frames?
Speaker E: You can, right? It stretches again. For single frames I think it'd be pretty hard. If you say speech starts here, speech ends there.
Speaker C: Yeah, you can basically remove the frames from the feature files.
Speaker E: Yeah, so I mean in the decoding, you're saying we're going to decode from here to here. I think they're treating it like, well, it's not isolated words, but connected.
Speaker G: In the text they say that this is a tentative block background of a possible configuration we could think of.
Speaker G: So that sort of sounds like they're not doing that yet.
Speaker E: Well, no, they have numbers though, right? So I think they're doing something like that. I think that they're, I think what I mean by that is they're trying to come up with a block diagram that's plausible for the standard.
Speaker E: From the point of view of reducing the number of bits you have to transmit, it's not a bad idea to do that.
Speaker G: I'm just wondering what exactly did they do up in this table if it wasn't this?
Speaker E: But it's the thing is that I certainly would be tricky about it in transmitting voice for listening to is that these kinds of things cut speech off a lot.
Speaker G: Plus it's going to introduce delays.
Speaker E: It does introduce delays, but they're claiming that it's within the boundaries of it. And the LDA introduces delays. And what he's suggesting this here is a parallel path so that it doesn't introduce any more delay.
Speaker E: It introduces 200 milliseconds of delay, but at the same time the LDA down here.
Speaker E: What's the difference between TLDA and SLDA?
Speaker E: It's a temporal spectrum. Oh, thank you. You wouldn't know that.
Speaker E: So the temporal LDA does in fact include the same.
Speaker E: So I think by saying this is a tentative block diagram I think means if you construct it this way this delay would work out in that way.
Speaker E: It clearly did actually remove silent sections because they got these word error results.
Speaker E: So I think that it's nice to do that in this because in fact it's going to give a better word error result.
Speaker E: And therefore what helps with an evaluation was to whether this would actually be in a final standard.
Speaker E: I don't know. As you know part of the problem with evaluation right now is that the word models are pretty bad and nobody wants has approached improving them.
Speaker E: So it's possible that a lot of the problems with so many insertions and so forth would go away if there were better word models.
Speaker E: So this might just be a temporary thing. But on the other hand, maybe it's a decent idea.
Speaker E: The question that we're going to want to go through next week when Henrik shows up I guess is given that we've been, look at what we've been trying. We're looking at by then I guess combinations of features and multi-band.
Speaker E: And we've been looking at cross language, cross task issues.
Speaker E: And they've been not so much looking at the cross task multiple language issues.
Speaker E: But they've been looking at these issues, the online normalization and the voice activity detection.
Speaker E: And I guess when he comes here we're going to have to start deciding about what do we choose from what we've looked at to blend with some group of things, what they've looked at.
Speaker E: And once we choose that, how do we split up the effort? Because we still have, even once we choose, we've still got another month or so.
Speaker E: I mean there's holidays in the way but I think the evaluation data comes January 31st. So there's still a fair amount of time to do things together.
Speaker E: It's just that there probably should be somewhat more coherent between the two sites.
Speaker G: When they remove the silence range, do they insert some kind of a marker so that the recognizer knows when it's time to backtrace it?
Speaker E: Well see there, I think they're, I don't know the specifics of how they're doing it.
Speaker E: They're getting around the way the recognizer works because they're not allowed to change the scripts.
Speaker E: So the recognizer I believe so. Maybe they're just inserting some dummy frames or something?
Speaker E: You know that's what I had thought but I don't think they are.
Speaker E: I mean that's sort of what I had imagined would happen is that on the other side yeah you put some low level noise or something.
Speaker E: Probably don't want all zeros, most of the guys just don't like zeros but you know put some epsilon in or some random variable in or something.
Speaker G: Some constant vector. I mean not a constant.
Speaker E: Or something divide by the variance of that but I mean it's...
Speaker G: But something that what I mean is something that is very distinguishable from speech so that the silence model in HDK will always pick it up.
Speaker E: Yeah so that's what I thought they would do or else. Maybe there is some indicator until it's starting to stop.
Speaker E: I don't know but whatever they did I mean they have to play within the roles of the specific variation.
Speaker E: We can find out.
Speaker G: You got to do something otherwise if it's just a bunch of speech stuck together.
Speaker G: No they're...
Speaker E: Yeah.
Speaker E: They would do badly and they did badly right?
Speaker E: Yeah.
None: Yeah.
Speaker E: So okay so I think this brings me up to date a bit.
Speaker E: I have play things other people have to date a bit.
Speaker E: I think I want to look at these numbers offline a little bit and take a bit and talk to everybody outside of this meeting.
Speaker E: But I mean it sounds like...
Speaker E: I mean they're the usual number of little problems and bugs and so forth but it sounds like they're getting ironed out.
Speaker E: Now we seem to be kind of in a position to actually look at stuff and compare things.
Speaker E: That's pretty good.
Speaker E: I don't know what the...
Speaker E: One of the things I wonder about coming back to the first results you talked about is...
Speaker E: how much things could be helped by more parameters and how many more parameters we can afford to have...
Speaker E: in terms of the computational limits.
Speaker E: Because when we go to twice as much data and have the same number of parameters, particularly when it's twice as much data and it's quite diverse, I wonder if having twice as many parameters would help.
Speaker E: It's kind of a bigger hidden layer.
Speaker E: But I doubt it would help by 40%.
Speaker E: But it's curious.
Speaker E: How are we doing on resources to ask them?
Speaker C: I think we're all right.
Speaker C: There's much problems we can.
Speaker C: Computation?
Speaker C: Well, this table took more than five days to get...
Speaker E: Were you folks using Jin?
Speaker E: That just died, you know.
Speaker C: No, you were using Jin, perhaps.
Speaker E: That's good.
Speaker E: We're going to get a replacement server that will be a faster server.
Speaker E: 750 megahertz.
Speaker E: But it won't be installed for a little while.
Speaker B: Do we have that big UIVM machine?
Speaker E: We have the little tiny IBM machine that might someday grow up to be a big IBM machine.
Speaker E: It's got slots for 8.
Speaker E: IBM was donating 5.
Speaker E: I think we only got 2 so far processors.
Speaker E: We originally hope we were getting 800 megahertz processors.
Speaker E: They end up in 550.
Speaker E: So instead of having 8 processors that were 800 megahertz, we end up with 2 that are 5 or 15 megahertz.
Speaker E: And more are supposed to come soon.
Speaker E: And there's only a moderate amount of memory.
Speaker E: So I don't think anybody has been sufficiently excited by it to spend much time with it.
Speaker E: But hopefully they'll get us some more parts soon.
Speaker E: Yeah, I think that'll be, once we get a populated, that'll be a nice machine.
Speaker E: I mean, we will ultimately get 8 processors in there.
Speaker E: And a nice amount of memory.
Speaker E: So it will be a pretty fast Linux machine.
Speaker B: And if we can do things, not Linux, some of the machines we have going already, like, sweet, it seems pretty fast.
Speaker B: I think 5 is pretty fast, too.
Speaker E: Yeah, I mean, you can check with Dave Johnson.
Speaker E: I think the machine is just sitting there.
Speaker E: And it does have 2 processors.
Speaker E: Somebody could do, you know, check out the multi-threading libraries.
Speaker E: I mean, it's possible that the, I guess, a prudent thing to do would be for somebody to do the work on getting our code running on that machine with 2 processors, and then, you know, the right 5 rate, there's going to be debugging hassles.
Speaker E: Then we'd be set for when we did have 5 or 8.
Speaker E: They have it really be useful.
Speaker E: Notice how I said somebody in my head, you direction.
Speaker E: That's one thing you don't get in these recordings.
Speaker E: You don't get the visuals.
Speaker B: Mostly the, you know, that work training that are, slow down or the HDK runs, that are slow down.
Speaker E: I think, yes.
Speaker E: You're right. I mean, I think you're sort of held up by both, right?
Speaker E: If the neural net trainings were 100 times faster, you still wouldn't be anything running through you as 100 times faster, because you'd be stuck by the HDK training.
Speaker E: But if the HDK, I mean, I think they're both, it sounded like they were roughly equal.
Speaker E: So that's not right?
Speaker B: Because I think that'll be running Linux and sweet, sweet, and fudge already running Linux.
Speaker B: So I could try to get the neural network trainings with the HDK stuff running under Linux, to start with, I'm wondering which one I should pick first.
Speaker E: Probably the neural net, this is probably, it's, it's, well, I don't know, they both, HDK we use for this Aurora stuff.
Speaker E: I think it's not clear yet what we're going to use for trainings.
Speaker E: Well, is the trainings, is it the training that takes the time or the decoding?
Speaker E: Is it about equal?
Speaker E: Training the two for HDK?
Speaker C: For, yeah, for the Aurora.
Speaker C: Training is longer.
Speaker E: Okay.
Speaker E: I don't know how we can, I don't know how to, do we have HDK source?
Speaker E: Yeah.
Speaker E: You would think that would fairly trivially, the training would anyway.
Speaker E: The testing, I don't think would parallelize all that well, but I think that you could certainly do distributed sort of, and though it's the each individual sentence, it's going to be tricky to parallelize, but you could split up the sentences and tell them.
Speaker G: They have a thing for doing that, they have for a while in HDK, and you can parallelize the training in a hundred and several machines, and it just basically keeps counts, and then there's something, a final thing that you run, and it accumulates all the counts together.
Speaker G: I see.
Speaker G: I don't know what their scripts are set up to do for their Aurora stuff,
Speaker E: but something we haven't really settled on yet is other than this Aurora stuff, what do we do, large vocabulary training slash testing for tandem systems, because we haven't really done much for tandem systems for larger stuff.
Speaker E: We had this one collaboration with CMU and we used Sphinx.
Speaker E: We're also going to be collaborating with SRI, and we have theirs.
Speaker E: So, I don't know.
Speaker E: So, I think the advantage of going with the neural net thing is that we're going to use the neural net training no matter what, for a lot of things we're doing, whereas exactly which HMM, the SRI mixture based HMM thing we use is going to depend.
Speaker E: So, with that, maybe we should go to R,
None: digit recitation. Task.
Speaker E: It's about 1150.
None: And...
Speaker E: I can start over here.
Speaker E: 2011-2030.
Speaker E: 0690601423051081.
Speaker E: 4004722617428789759.
Speaker E: 0103010322430101556924063703.
Speaker A: And, transcript number 19912010908276193342055305163274891090737312374743.
Speaker A: 25316616005679196951.
Speaker G: Transcript 2071-209012084641536603789290900581.
Speaker G: 1153564275603654045556779194086031091218100.
Speaker D: Transcript 2051-207000220432213469567808837702861051020219270502.
Speaker D: 263983405725610882849647400.
None: Transcript 1971-199091010101.
Speaker E: 315-074-560338367.
Speaker B: 35074560338163359187.
Speaker C: Transcript 2031-20501012122663273497905606002.
Speaker C: 81294791650834053120607305264881.
Speaker C: 786748619490.
Speaker B: 810101063531.
Speaker B: 810101010106.
Speaker E: 8naden1010107.
Speaker E: Does everyone sign the consent form before in previous meetings?
Speaker E: So you don't have to do it again each time?
Speaker E: The government could only give you two Donna, it's 18, her eyes, and her eyes cannot show it again, I think it's an unrisk campaign
