None: We're going.
None: Okay.
None: Close your door.
Speaker F: Thanks.
Speaker F: Door in the way up.
Speaker F: Yeah, I'm going to get this other door.
Speaker F: Okay.
Speaker F: So, what are we talking about today?
Speaker D: Well, first of all, perhaps these meeting record or digits?
Speaker F: Oh, yeah.
Speaker F: That was kind of interesting.
Speaker F: Both the SRI system and for one thing, that sure shows the difference between having a lot of training data or not.
Speaker F: The best kind of number we have on English, on near microphone only is 3 or 4%.
Speaker F: And it's significantly better than that using fairly simple front ends on the SRI system.
Speaker F: So, I think that that's using pretty huge amount of data.
Speaker F: Mostly not digits, of course, but then again, well, yeah, in fact, mostly not digits.
Speaker F: So, in this case, what is this using digits in the digital image?
Speaker F: Did anybody mention about whether the SRI system is doing the digits?
Speaker F: The word as a word model or as a sub-phone.
Speaker D: I guess it's a lot of fault models.
Speaker D: Yeah, I think so because it's very huge system.
Speaker D: But so, there is one difference.
Speaker D: Well, the SRI system, the result for the SRI system that are presented here are with adaptation.
Speaker D: So, there is a complete system including online, supervised adaptation.
Speaker D: If you don't use adaptation, the error rate is around 50% more.
Speaker F: Okay, is that much?
Speaker D: Yeah, it's quite significant.
Speaker F: Still.
Speaker F: But what I think I'd be interested to do given that is that we should take, I guess, as somebody is going to do this right, is to take some of these tandem things, feed it into the SRI system.
Speaker D: Yeah.
Speaker D: Yeah, because...
Speaker D: But I guess the main point is the data because I'm not sure or back down this is very simple, but still know what the attempts to improve it, that's a thing that what I mean.
Speaker D: Which I tried to do.
Speaker F: Yeah, but he's doing with the same data, right?
Speaker F: So there's two things to be infected.
Speaker F: One is that there's something simple that's wrong with the back end, playing number states.
Speaker F: I don't know if he got to the point of playing with the number of Gaussian's.
Speaker F: But, you know, but yeah, so far he hadn't gotten any improvement, but that's all with the same data, which is pretty small.
Speaker D: So you could retrain some of these tandem?
Speaker F: Well, you could do that, but I'm saying even with that part not retrained.
Speaker F: Just using having the HMMs, much better HMMs, yeah.
Speaker F: But just train those HMMs using different features, features coming from RRAS now.
Speaker D: But what would be interesting to see also is perhaps it's not really the amount of data but the recording conditions, I don't know, because it's probably not the problem of noise, because our features are supposed to be robust today.
Speaker D: It's not the problem of channel because there is normalization with respect to the channel.
Speaker F: So, I'm sorry, what is the problem that you're trying to explain?
Speaker D: The fact that the result with the tandem or a system model.
Speaker D: So much worse?
Speaker D: No, so much worse.
Speaker F: Oh, but I'm almost certain that it has to do with the amount of training data.
Speaker F: It's orders of magnitude on.
Speaker D: Yeah, but retrained on the own digits and it's a digit task.
Speaker F: But having a huge, if you look at what commercial places do, they use a huge amount of data.
Speaker F: This is a modest amount of data.
Speaker F: So, ordinarily you would say, well given that you have enough occurrences of the digits, you can just train the digits rather than with it.
Speaker F: But the thing is if you have a huge, you know, the word models, but if you have a huge amount of data, then you're going to have many occurrences of similar to the alpha one.
Speaker F: And that's just a huge amount of training point.
Speaker F: So, I think it has to be that because as you say, this is near microphone.
Speaker F: It's really pretty clean data.
Speaker F: Now, some of it could be the fact that, let's see, in these multi-trained things, did we include noisy data in the training?
Speaker F: I mean, that could be hurting us actually for the quick.
Speaker D: Yeah, actually we see that the clean train for proposals are better.
Speaker F: It is better.
Speaker F: Yeah, because this is clean data.
Speaker F: So, that's not too surprising.
Speaker D: But, I guess what I meant is that, well, let's say if we add enough data to train on the meeting record digits, I guess we could have better examples of this.
Speaker D: What I meant is that perhaps you can learn something from this, what's wrong?
Speaker D: What is different between the IDGids and these digits?
Speaker F: What kind of numbers are we getting on the IDGids?
Speaker D: It's 0.8 persons.
Speaker F: Oh, I see.
Speaker F: So, on the actual TIDGIDGID database, we're getting 28 percent.
Speaker F: And here we're getting three or three, let's say three for this.
Speaker F: Yeah, sure.
Speaker F: But I mean 0.8 percent is something like double or triple, what people have gotten who have worked very hard at doing that.
Speaker F: And also, as you point out, there's adaptation in these numbers also.
Speaker F: So, if you put the adaptation off, then for the English near, you get something like 2 percent.
Speaker F: And here you had something like 3.4.
Speaker F: And I could easily see that difference coming from this huge amount of data that was trained on.
Speaker F: So, I don't think there's anything magical here.
Speaker F: It's a simple HGK system with a modest amount of data.
Speaker F: And this is a modern system.
Speaker F: It has a lot of nice points to it.
Speaker F: So, the HGK is an older HGK, you know, so.
Speaker F: Yeah, it's not that surprising.
Speaker F: For me, it just meant a practical point that if we want to publish results on digits that people pay attention to, we probably should, because we've had the problem before, we can show some nice improvement on something that seems like too large a number.
Speaker F: And people don't necessarily think it's so serious.
Speaker F: Yeah, so the 3.4 percent for this is, looks, why is it, it's an interesting question of us.
Speaker F: Why is it 3.4 percent for the digits reported in this environment, as opposed to the 0.8 percent for the original TI digits database?
Speaker F: Yeah, given the same, yes, so ignoring the SRA system for a moment, just looking at the NM system, if we're getting 0.8 percent, which, yes, it's high.
Speaker F: It's awfully high, but it's, you know, it's high.
Speaker F: Why is it 4 times it's high?
Speaker F: More.
Speaker F: I guess this much would be harder.
Speaker F: Yeah, I guess.
Speaker F: Right, I mean, even though it's close mic, there still really is background noise.
Speaker F: And I suspect when the TI digits were recorded if somebody fumbled or said something wrong or something, they probably made them take it over.
Speaker F: I mean, there's no attempt to have it be realistic in any sense at all.
Speaker D: And it's quite different.
Speaker D: Yeah, it's very, very clean.
Speaker D: And it's like, it's got a Uricorn, whereas it's a mid-range, it's a mid-range, sometimes you have breath noise.
Speaker F: Right, yeah, so I think they were, it's got a little extra.
Speaker F: Yeah, I think it's, it's, so, yeah, I think it's, it's an indication it's hard.
Speaker F: Yeah, again, that's true either way.
Speaker F: I mean, so take a look at the, yeah, so I result in a much better, but still you're getting something like 1.3% for things that are the same data as in TI digits, the same text.
Speaker F: And I'm sure the same system would get 0.3, 0.4% or something on the actual TI digits.
Speaker F: So this, I think, on both systems, these digits are showing up as harder.
Speaker F: Which I find so interesting, because I think this is closer to, I mean, still red, but I still think it's much closer to what, what people actually face.
Speaker F: And they're dealing with people saying digits over the telephone.
Speaker F: I don't think, I mean, I'm sure they wouldn't erase the numbers, but I don't think that the, the, the company is, the telephone speech get anything like 0.4% on their digits.
Speaker F: I'm sure they get, I mean, for one thing people do, from now prove don't have middle America access.
Speaker F: So you ask it has, as many people.
Speaker F: It's not many different ways.
Speaker F: Okay, that was that topic. What else we got?
Speaker F: Did we end up giving up on, on, in a year speech submissions?
Speaker F: Man, Otilo and Dan Elles are submitting something.
Speaker D: Yeah, I guess the only thing with these, the meeting recorder.
Speaker D: So I think, yeah, basically give it.
Speaker F: Now, for the, for the, we do have stuff for a run, right?
Speaker F: Yeah, yeah, yeah, for sure.
Speaker D: Yeah. Well, that's fine.
Speaker F: So, so we have a couple, couple of little things on meeting recorder.
Speaker F: You don't, we don't have to fly it with papers.
Speaker F: We don't have to prove anything to anybody.
Speaker F: That's fine.
Speaker F: Anything else?
Speaker D: Yeah, well, so perhaps the place that we've been working on.
Speaker D: Yeah, we have put the good VAD in the system.
None: It really makes a huge difference.
Speaker D: So yeah, I think this is perhaps one of the reason why our system was that, not the best because with the new VAD, there is a similar to the, that still come with ourselves.
Speaker D: Perhaps even better sometimes.
Speaker D: So there is this point.
Speaker D: The problem is that it's very big and still have to think how to, where to put it.
Speaker D: Because it, what is VAD, yield some delay.
Speaker D: And we, if we put it on the server side, it doesn't work because of the server side features.
Speaker D: So you already have the LDA applied from the terminal side to your cumulative delay.
Speaker D: So the VAD should be before the LDA, which means perhaps on the terminal side and then smaller.
Speaker F: So where does good VAD come from?
Speaker D: It's from OGI.
Speaker D: So it's the network with the huge amount of, the huge units and nine input frames compared to the VAD that was in the proposal, which has a very small amount of hidden units and fewer inputs.
Speaker F: This is the one they had originally.
Speaker F: Yeah, but they had to be rid of it because of the space.
Speaker D: But the assumption is that we'll be able to make a VAD that's smaller than that.
Speaker F: Well, so that's the problem.
Speaker F: But the other thing is to use a different VAD entirely.
Speaker F: I don't know what the thinking was amongst the LCPOQ, but if everybody agreed to use this VAD.
Speaker D: Or they just one apparently they don't want to fix it really because they think there is some interaction with you.
Speaker D: Feature extraction and video frame dropping.
Speaker D: But they still want to just to give some requirement from this VAD because it will not be part of the standard.
Speaker D: So it must be at least somewhat fixed but not completely.
Speaker D: So there just will be some requirements that are still not yet.
Speaker F: But I was thinking that sure there may be some interaction, but I don't think we need to be stuck on using our OGI's VAD with somebody else's smaller.
Speaker F: Yeah.
Speaker F: That's good.
Speaker D: So there is this thing.
Speaker D: Yeah.
Speaker D: I designed a new filter because when I designed the filters, we showed a delay from the LCPOQ filters.
Speaker D: There was one filter with 60ms delay and the other 10ms.
Speaker D: Right. You'd like to suggest that both could have 65.
Speaker D: I think it's 65.
Speaker D: Yeah.
Speaker D: But should have 65 because I didn't gain anything.
Speaker D: So I did that and it's running.
Speaker D: Let's see.
Speaker D: Let's see if I have one.
Speaker D: But the filter is of course closer to the reference filter.
Speaker E: Yeah.
Speaker F: So that means logically, the principle should be better, so probably it will be worse.
Speaker F: Or the basic perverse nature of reality.
Speaker F: Yeah.
Speaker F: Okay.
Speaker F: Yeah.
Speaker D: Yeah.
Speaker D: And then we've started to work with this.
Speaker D: Vice to the vice.
Speaker D: And we will perhaps try to have a new system with MSG stream.
Speaker D: So something that's similar to the proposal too, but with MSG stream.
Speaker E: Okay.
Speaker B: Now, we're going to play with Matt Lat and to found some parameter robust for voice and voice decision, but only to play.
Speaker B: And we found that maybe when it's a classical parameter, the variance between the FFT of the signal and the small spectrum of time after the Melfilter bank.
Speaker B: And while it's smaller, it's good for clean speech.
Speaker B: It's quite good for noise speech.
Speaker B: But we must have a big statistic with timid.
Speaker B: And it's not right yet to use, well, I don't know.
Speaker D: Yeah.
Speaker D: So basically we want to look at something like excitation signal.
Speaker D: Right.
Speaker D: Which is a variance of 15.
Speaker B: Yeah.
Speaker B: I have here for one thing, for one frame.
Speaker B: The mix of the two noise and noise.
Speaker B: And the signal is this, clean and this noise.
Speaker B: They are the two, the mix, the big.
Speaker B: The signal is for clean.
Speaker F: Well, there's no, these axes are labeled.
Speaker B: So I don't know what this axis is.
Speaker B: This axis is frame.
Speaker F: And what's this?
Speaker B: This is energy, logarithm energy of the spectrum.
Speaker B: No, this is the variance, the difference between the spectrum of the signal and FFT of each frame of the signal and the small spectrum of time after the Melfilter.
Speaker B: For the two.
Speaker B: And the here they are to signal this is for clean and this is for noise.
Speaker F: Oh, there's two things on the same graph.
Speaker B: Yeah, I don't know.
Speaker B: I think that they have another graph.
Speaker E: Which is clean?
Speaker E: Which is noise?
Speaker D: I think the lower one is noise.
Speaker B: The lower is noise and the height is clean.
Speaker F: Okay, so it's harder to distinguish.
Speaker F: It's hard.
Speaker F: But it's worth the answer, of course.
Speaker B: Oh, I must have clean.
Speaker B: I don't have two different.
Speaker F: Yeah, presumably when there is a...
Speaker D: So this should be voiced.
Speaker B: Yeah, this height is voiced portion.
Speaker B: And this is the noise portion.
Speaker B: And this is more or less like this.
Speaker B: But I must have to have two pictures.
Speaker B: This is for example for one frame, the spectrum of the signal.
Speaker B: And this is the smooth version of the spectrum after ML filter bank.
Speaker B: Yeah.
Speaker B: And this is...
Speaker B: This is not the different.
Speaker B: This is trying to obtain with LPC model, the spectrum, but using MATLAB without going factor and...
Speaker B: Not pre-efficient.
Speaker B: Not pre-efficient, nothing.
Speaker B: And I think that this is good.
Speaker B: This is quite similar.
Speaker B: This is another frame, how I obtain the envelope, the same envelope, with the ML filter bank.
Speaker F: Right.
Speaker F: So now I wonder, I mean, do you want to...
Speaker F: I know you want to get it something orthogonal from what you get with the smooth spectrum.
Speaker F: But if you were really trying to get a voicetant voice, do you want to totally ignore that?
Speaker F: I mean, do you...
Speaker F: I mean, clearly a very big, very big cues for a voicetant voice come from a spectrum slope and so on.
None: Right.
Speaker D: Yeah, well, this would be...
Speaker D: This would be perhaps an addition of parameters.
Speaker D: Yeah, I see.
Speaker B: Yeah, because when the noise is clear, in this section is clear, if a high value is indicative that this voice frame and no one...
Speaker F: Yeah, we probably won't.
Speaker F: Certainly, if you want to do a good voicetant voice, it's actually a need of few features.
Speaker F: Each feature is myself.
Speaker F: I don't know, but people look at it slow.
Speaker F: First-dollar correlation coefficient, by by power, or...
Speaker F: There's...
Speaker F: I guess we probably don't have enough computation to do a simple pitched detector or something.
Speaker F: A pitched detector, you could have an estimate of what the...
Speaker F: Or maybe you could just do it going through the FFTs, or you could get out some probable harmonic structure.
Speaker F: Right?
Speaker B: You have read, you have a paper.
Speaker B: The paper just gives me just the...
Speaker B: I don't know, but...
Speaker B: They are some problem.
Speaker B: Yeah, that's a matter of...
Speaker B: It's another problem.
Speaker D: Yeah, there is...
Speaker D: This fact, actually, if you...
Speaker D: Look at this spectrum.
Speaker D: Yeah.
Speaker D: What's this again?
Speaker D: Is it the main field?
Speaker B: Yeah, like this.
Speaker B: Uptime like this.
Speaker D: So, the envelope here is the output of the MFFFF.
Speaker D: And what we clearly see is that in some cases, and clearly appears here, and the harmonics are resolved by the...
Speaker D: Well, there is still a pair after MFFF.
Speaker D: And it happens for a pitched noise, because the width of the low frequency MFFF is sometimes even smaller than pitch.
Speaker D: Yeah, it's a problem.
Speaker D: Right?
Speaker D: 150 hertz.
Speaker D: And so what happens is that this...
Speaker D: Add additional variability to this envelope.
Speaker D: Yeah.
Speaker D: So we were thinking to modify the mass spectrum to have something that's smaller on low frequencies.
Speaker F: That's a separate thing, yeah.
Speaker E: Yeah, it's a separate thing, yeah.
Speaker E: Yeah, maybe so.
Speaker F: Yeah, so, yeah, but that was time I was just starting with the FFT.
Speaker F: You could do a very rough thing to estimate pitch.
Speaker F: And given that, you could come up with some kind of estimate of how much of the...
Speaker E: The energy was explained by...
Speaker E: by those harmonics.
Speaker F: It's very...
Speaker F: The...
Speaker F: The metal does give a smooth thing, but as you say, it's not that smooth here.
Speaker F: So if you just subtract it off, you're the S of the harmonics, then something like this would end up with quite a bit lower energy.
Speaker F: First, 50 hertz or so.
Speaker F: And if it was noisy, the proportion of it would go down to B.
Speaker E: And if it was on lowest, there's nothing.
Speaker E: So you had to be able to pick out voiced segments.
Speaker F: At least it should be another cue.
Speaker E: Okay.
Speaker F: That's what's going on.
Speaker C: So, I went to talk with Mike Jordan this week, and shared with him the ideas about extending the Larry Saul work.
Speaker C: And I asked him some questions about factorial HMM.
Speaker C: So like later down the line, when we come up with these feature detectors, how do we model the time series that happens?
Speaker C: And we talked a little bit about factorial HMMs and how when you're doing inference, or when you're doing recognition, there's like simple, but turbid stuff that you can do for these HMMs.
Speaker C: And the great advantages that a lot of times the factorial HMMs don't over-learn the problem there.
Speaker C: They have a limited number of parameters and they focus directly on the subproblems at hand.
Speaker C: So you can imagine five or so parallel features transitioning independently.
Speaker C: And then at the end, you couple these factorial HMMs with undirected links based on some more data.
Speaker C: So he seemed like really interested in this and said this is something very dual wall and learned a lot.
Speaker C: And I've just been continuing reading about certain things, thinking maybe using modulation spectrum stuff to as features also in the sub-air, because it seems like the modulation spectrum tells you a lot about the intelligibility of certain words and stuff.
Speaker C: So yeah, just about it.
Speaker A: Okay, and so I've been looking at Avandano's work.
Speaker A: I'll try to write up in my next status report, and I'll write down what he's doing, but it's an approach to deal with reverberation, or the aspect of his work that I'm interested in.
Speaker A: The idea is that normally, and analysis frames are too short to encompass reverberation effects in full.
Speaker A: You miss most of the reverberation tail in a 10 millisecond window.
Speaker A: And so you'd like it to be that the reverberation response is simply convolved in, but it's not really with these 10 millisecond frames.
Speaker A: But if you take, say, a two millisecond window, I'm sorry, a two second window, then in a room like this, most of the reverberation response is included in the window.
Speaker A: And then things are more linear.
Speaker A: It is more like the reverberation response is simply convolved.
Speaker A: And you can use channel normalization techniques.
Speaker A: Like in his thesis, he's assuming that the reverberation response is fixed, he just does mean subtraction, which is like removing the DC component of the modulation spectrum.
Speaker A: And that's supposed to deal pretty well with the reverberation.
Speaker A: And the neat thing is you can't take these two second frames and feed them to a speed recognizer.
Speaker A: So he does this method training, the spectral resolution for time resolution.
Speaker A: And synthesizes a new representation, which is with, say, 10 second frames, but a lower frequency resolution.
Speaker A: So I don't really know the theory.
Speaker A: I guess these are called time frequency representations.
Speaker A: And he's making the time finer grain in the frequency resolution.
Speaker A: Let's find grain.
Speaker A: So I guess my first stab actually in continuing his work is to re-implement this thing, which changes the time and frequency resolutions, because he doesn't have code for me, so that it will take some reading about the theory.
Speaker A: I don't really know the theory.
Speaker A: Oh, and another first step is, so the way I want to extend his work is make it able to deal with a time varying reverberation response.
Speaker A: And we don't really know how fast the reverberation response is varying in the meeting recorder data.
Speaker A: So we have this blockly squares, echo counselor implementation.
Speaker A: And I want to try finding the response between a near mic and the table mic for someone using the echo counselor and looking at the echo counselor tabs.
Speaker A: And then see how fast that varies from block to block that should give an idea of how fast the reverberation response is changing.
Speaker E: Okay.
Speaker F: I think we're sort of done.
Speaker F: Yes, we did it.
Speaker F: We did it and go home.
Speaker F: Okay.
Speaker D: I'm reading transcript L-40, 3, 4, 9, 1, 0, 5, 1, 8, 3, 6, 2, 0, 5, 7, 0, 4, 1, 9, 6, 0, 1, 4, 4, 9, 5, 7, 3, 7, 6, 1, 4, 2, 0, 5, 2, 7, 8, 9, 9, 3, 6, 6, 3, 8, 9, 7, 1, 7, 8, 3, 1, 9, 3, 0, 1, 9, 5, 7, 5, 5, 1, 8, 0, 8, 2, 9, 8, 4, 6, 1, 9, 4, 8, 1, 2,
Speaker A: So I think you read some of the zeros as always and some of zeros. Is there a particular way we're supposed to read them?
Speaker D: There are only zeros here.
Speaker F: No, oh, it's 0 and 0, two ways that we say that digit.
Speaker D: So it's perhaps in the sheets, it should be another sign photo.
Speaker D: If we want to, the guy to say photo.
Speaker F: No, I mean, I think people will do what they say.
Speaker F: It's okay.
Speaker F: I mean, digit recognition is done before.
Speaker F: You have two pronunciations for that value.
Speaker D: It's perhaps more difficult for the people to prepare the database.
Speaker D: If, because they already put me in the order.
Speaker D: No, they just write pronounce 0 or 0.
Speaker F: They write down OH or they write down ZRO.
Speaker D: Yeah, but if each other, she'd be prepared with a different sign photo.
Speaker F: But people wouldn't know what that, well, I mean, there is no convention for it.
Speaker F: Yeah.
Speaker F: I mean, you'd have to tell them, okay, when we write this, say it, they just want people to read the digits as you would in early work.
Speaker F: And people say it different ways.
Speaker A: Okay, is this a change from the last batch of, of, um, forms?
Speaker A: Because in the last batch, it was spelled out, which one you should read.
Speaker F: Yes, that's right.
Speaker F: It was spelled out and they decided they wanted to get at more of the way people would really say.
Speaker F: Okay.
Speaker F: That's also why they're, they're bunched together in these different groups.
Speaker F: Okay.
Speaker F: So it's, yeah, so it's, it's, it's, it's, it's, they're being spying.
Speaker F: Transcript L-39.
Speaker F: 2326-1014-2475.
Speaker F: 938-726-2627.
Speaker F: 6734-2224.
Speaker F: 2964040882.
Speaker F: 879-94082.
Speaker F: 780-395123.
Speaker F: 559-8142.
Speaker F: 0209-2926.
Speaker F: Actually, let me just, since it's spread up, I was just, it was hard not to be self-conscious about that one.
Speaker F: The reasons we just discussed it.
Speaker F: But I realized that, that, um, when I'm talking on the phone, certainly, and, and seeing these numbers, I almost always say zero.
Speaker F: And because, because, uh, it's too solvable, so it's, it's more likely to understand what I said.
Speaker F: That, that's the habit I'm in.
Speaker F: But some people say, oh, okay.
Speaker C: Yeah, I normally say, oh, it's easier to say.
Speaker F: Yeah, it's true.
Speaker F: Yeah, so, so, uh, no, don't think about it.
Speaker F: Oh, no.
Speaker C: Okay, I'm reading transcript L-38.
Speaker C: 545-032-858.
Speaker C: 338-904109.
Speaker C: 850-711-140.
Speaker C: 2161-82-5678.
Speaker C: 576-82004.
Speaker C: 710-0587-756.
Speaker C: 561-371913.
Speaker C: 436-009-9220.
Speaker A: I'm reading transcript L-37.
Speaker A: 0519-0327-1669.
Speaker A: 627-026-4510.
Speaker A: 542-9501.
Speaker A: 711-271-8123.
Speaker A: 4084-57-622-9.
Speaker A: 823-6726-764.
Speaker A: 927-3123.
Speaker A: 936-861-9177.
Speaker B: Transcript L-36.
Speaker B: 044-1629-0.
Speaker B: 477-3845654688.
Speaker B: 312-525459.
Speaker B: 970-698-5851.
Speaker B: 0832-973-145.
Speaker B: 6493-844223.
Speaker B: 4553-0245-422.
Speaker B: 682-189-819.
Speaker F: I have no access in postcode.
None: There is some technical problem here.
