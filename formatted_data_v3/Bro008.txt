Speaker D: Okay, so I got these results from CIFON.
Speaker D: So I think that we might hear later today about other results.
Speaker D: And there are some other very good results compared to other results from other places.
Speaker D: I'm sorry.
Speaker D: I got this from you.
Speaker D: And then I sent a note to CINNAL about because he has been running some other systems other than the XEOG.
Speaker D: I want to see what that is.
Speaker D: But, you know, so we'll see what is comparatively later.
Speaker D: But it looks like most of the time even though it's true that the overall number for damage is we didn't improve it.
Speaker D: If you look individually, what I really say is that there's looks like out of the six cases between the different kinds of matching conditions.
Speaker D: Out of the six cases, there's basically a couple of states about the same.
Speaker D: Three work gets better and one work gets worse.
Speaker E: Actually, for the damage, there's still some kind of mystery because when we use the straight features, we are not able to get this nice number with the XEOG I1.
Speaker E: We don't have this 93.78.
Speaker A: 89.44.
Speaker E: So, there's probably something wrong with the feature that we get from Boji and Sunili is working on.
Speaker D: Oh, let me have a little time on that actually.
Speaker D: We have a little bit of time on that actually.
Speaker D: Yeah, there it sounds.
Speaker D: When do you folks leave?
Speaker E: Sunday.
Speaker D: Sunday.
Speaker D: Saturday, midnight or something.
Speaker D: That would be good.
Speaker D: That would be good.
Speaker D: And, you know, whenever anybody figures it out, they should also, for sure, email Heenik because Heenik will be over there telling people we did some of these.
Speaker D: So, we'll hold off on that a little bit. Even with these results as they are, it's really not that bad.
Speaker D: It looks like the overall result, as they are now, even without any bugs being fixed, is that on the other tasks, we had this average of 49% or so improvement.
Speaker D: And here we have somewhat better than that in the Danish and so on.
Speaker D: Worst than that in German.
Speaker D: But, I mean, it sounds like one way or another of the methods that we're doing can reduce the error rate of mail capture down by 4th of them to a half of them.
Speaker D: So, I'm not depending on that.
Speaker D: Yeah, the exact case.
Speaker D: So, that's good.
Speaker D: I mean, I think that one of the things that Heenik was talking about was understanding what was in the other really, the proposals and trying to see if what should ultimately be proposed to some combination of things.
Speaker D: Because there's things that they are doing there that we certainly are not doing.
None: And there's things that we're doing that they're not doing.
Speaker C: Like the things.
Speaker D: How much better was the best system than ours?
Speaker D: Well, we don't know yet.
Speaker D: I mean, first place there's still this thing to work out.
Speaker D: And second place, second thing is that the only results that we have so far before were really development and results.
Speaker D: And this community that's of interest, it's not like everything is being pinned on the evaluation set. But for the development set, our best result was a little bit short of 50%.
Speaker D: And the best result of any system was about 54.
Speaker D: Where these numbers are the relative reduction in word error rate.
Speaker D: Oh, okay.
Speaker D: The other systems were somewhat lower than that.
Speaker D: There was actually, it was much less of a huge range than there was in a RR1.
Speaker D: There were systems that basically didn't approve things.
Speaker D: And here the worst system still reduced their rate by 33%.
Speaker D: So, you know, sort of everybody is doing things roughly 30 years and half a year is being created and bearing on different tests and so forth.
Speaker D: So, I think it's probably a good time to look at what's really going on.
Speaker D: And see if there's a way to combine the best ideas while at the same time not blowing up the amount of resources used.
None: That's critical.
Speaker C: Do we know anything about who's wasn't that had the lowest on the Devset?
Speaker D: There were two systems that were put forth by a combination of French telecom and Alcatel.
Speaker D: And they differ in some respects.
Speaker D: But one was called the French telecom, Alcatel system.
Speaker D: It was called the Alcatel French telecom system.
Speaker D: It was the biggest difference.
Speaker D: And they both did very well.
Speaker D: So, my impression is they also did very well on the evaluation set.
Speaker D: But we haven't seen any minor results from that.
Speaker C: And they used the main thing that they used with spectral subtraction?
Speaker D: There was a couple pieces to it.
Speaker D: There was a spectral subtraction.
Speaker D: And there was some modification of the capture of parameters.
Speaker E: Yeah, actually something that's close to kept strumming subtraction.
Speaker E: But the way the mean is adapted, it's signal dependent.
Speaker E: So basically the mean is adapted during speech and not during silence.
Speaker E: But it's very close to kept strumming subtraction.
Speaker D: And we've done exactly that sort of thing to look in speech only to try to measure these things.
Speaker D: So it looks like they did some reasonable things.
Speaker D: And we did unreasonable things.
Speaker D: Because we like to try strange things.
Speaker D: And our things work too.
Speaker D: It's possible that some combination of these different things that we're done will be the best thing to do.
Speaker D: But the only caveat to that is that everybody is being real conscious of how much memory, how much CPU they're using.
Speaker D: Because these standards are supposed to go on cell phones with moderate resources, both your specs.
Speaker C: Did anybody do anything with the models as an experiment?
Speaker D: They didn't report it.
Speaker D: I think everybody was focused elsewhere.
Speaker D: Now one of the things that's nice about the CPU did is we do have a filter in which leads to a production of the bandwidth and the modulation spectra, which allows us to down sample.
Speaker D: So as we know that we have a reduced transmission rate for the bands.
Speaker D: Now it was reported the first time out. It said the same amount because for convenience sake, in a particular way, this is being tested.
Speaker D: They were repeating the packets.
Speaker D: They had 2400 bits per second, but they were literally creating 4800 bits per second.
Speaker C: So you could have had a repeat count in there or something.
Speaker D: This was just a funny thing to fit into the software that was testing the errors, channel errors, and so on.
Speaker D: So in reality, if you put this system into the field, it would be 2400 bits per second.
Speaker D: So that's a nice feature of what we did.
Speaker D: But we still have to see how it all comes out. And then there's the process, which is a lot together.
Speaker C: When is the development set? I mean the test set results do. Like the day before you leave or something?
Speaker D: Probably the day after they leave, but we'll have to stop it the day before.
Speaker D: I think the meeting is on 13th or something.
Speaker D: Yeah, it's Tuesday.
Speaker D: And the results are due like the day before you leave or something.
Speaker D: Yeah, probably one.
Speaker D: I think they are. Yeah, so since we have a bit farther to travel.
Speaker D: I'll have to get that a little quicker.
Speaker D: I mean, it's just tracing down these bugs. I mean, just exactly this sort of thing.
Speaker D: Why these features seem to be behaving differently in California than in Oregon.
Speaker D: I guess something to do with electricity shortage. We can have enough electrons here.
Speaker D: But I think the main reason for having, you know, it takes to run the two test sets in just in computer time.
Speaker D: It's just a day or so.
Speaker D: And so I think the whole reason for having as long as we have, which is like we can have this because of bugs like that.
Speaker D: So we're going to end up with these same kind of sheets that have the percentages.
Speaker E: Yeah, so there are two more calamity machines. I guess it's the same sheets.
Speaker E: Yeah, it's the same sheets.
Speaker D: Yeah. So I'll just regard these numbers. That's good.
Speaker E: So you can try to push for trying to combine different things.
Speaker D: Well, let's.
Speaker D: Yeah, I mean, I think the question is, is there is there some advantage?
Speaker D: I mean, you could just take the best system and say that's the standard. But the thing is that if different systems are getting at good things, begin with in the constraint of the resources.
Speaker D: If there's something simple that you could do.
Speaker D: For instance, I think very reasonable to have a standard for the terminal side.
Speaker D: And then for the server side, say here's a number of things that could be done.
Speaker D: So everything we did could probably just be added on to what Alcatel did.
Speaker D: We're pretty well with them too.
Speaker D: So that's one aspect of it.
Speaker D: And then on the terminal side, I don't know how much memory and CPU it takes.
Speaker D: But it seems like the filtering.
Speaker D: I mean, the VAD stuff they both had, right?
Speaker D: So, and they both had some kind of online normalization.
Speaker D: So it seems like the main difference there is the filtering.
Speaker D: And the filtering, I think, if you can, it shouldn't take a lot of memory to do that.
Speaker D: And I also wouldn't think the CPU would be much either.
Speaker D: So if you can add those in, then you can cut the data right now.
Speaker D: So it seems like the right thing to do is to, on the terminal side, take what they did if it does seem to generalize well to German Danish.
Speaker D: Take what they did, add in a filter and add in some stuff on the server side.
Speaker D: And that's probably a reasonable standard.
Speaker E: They are working on this already because some filter will be that you are trying already to put some kind of...
Speaker D: Yeah, so that's the thing.
Speaker D: That would be ideal, if they could actually show that in fact the combination of some sort would work even better than any other system chat.
Speaker D: And then it would be something to discuss on the meeting.
Speaker D: But not clear what will go on.
Speaker D: I mean, on the one hand, sometimes people are just anxious to get a standard out there and you can always have another standard after that.
Speaker D: But this process is going on for a while already.
Speaker D: Might just want to pick something and say, okay, this is it.
Speaker D: And then that's a standard.
Speaker D: Standards are always optional, it's just that if you disobey them, then you risk not being able to sell your product.
Speaker D: And people often work on new standards, well-known standards, and so on.
Speaker D: So it's not final, even if they declare a standard.
Speaker D: The other hand, they might just say they just don't know enough yet to declare a standard.
Speaker D: You will become experts on this, more firm or than me, but this particular standard is brought since once you go to this meeting.
Speaker D: So I'd be interested hearing your thoughts now.
Speaker D: I mean, you're almost done.
Speaker D: I mean, you're done in the sense that maybe you'll get some new features from snail and we'll rerun it.
Speaker D: But other than that, you're basically done.
Speaker D: So you're just hearing your thoughts about where you think we should go from this.
Speaker D: I mean, you're trying a lot of things in a hurry.
Speaker D: And if we can back off from this now and sort of take our time with something that is doing things quickly, be quite so much the constraint, what you think would be the best thing to do.
Speaker E: Well, first, to really have a look at the speech from this database is because we tried several things, but we didn't really look.
Speaker E: What's happening?
Speaker E: Where is the noise?
Speaker D: It's a novel idea.
None: Look at the data.
Speaker D: More generally, I guess, what is causing the degradation.
Speaker D: Yeah, yeah.
Speaker E: Actually, there is one thing that generally we think that most of the errors are within phoneme classes.
Speaker E: So I think it could be interesting to see if it, I don't think it's still true when we add noise.
Speaker E: So I guess the confusion, the confusion, matrices are very different when we have noise.
Speaker E: When it's clean speech.
Speaker E: And probably there is much more between classes errors or noisy speech.
Speaker E: So, perhaps we could have a large gain just by looking at improving the recognition, not phoneme, but phoneme classes simply.
Speaker E: Which is a simpler problem perhaps, but which is perhaps important for noisy speech.
Speaker D: So the other thing that strikes me just looking at these numbers is just taking the best cases.
Speaker D: Some of these, of course, even with all of our wonderful processes, still our horrible kinds of numbers.
Speaker D: Just take the best case, the well matched, the German case after, or well matched, Danish, after we, kind of numbers we're getting are about 8% error per digit.
Speaker D: Yeah, this is obviously not usable.
Speaker D: I mean, you have 10 digits.
Speaker D: Oh, no, not very bad.
Speaker D: Now then you get it right.
Speaker D: So, I mean the other thing is that, and also part of what's nice about this is that this is almost realistic database. I mean, it's still not people who are really trying to accomplish something.
Speaker D: But within the artificial setup, it isn't noise artificially simulated.
Speaker D: It's real noise condition.
Speaker D: And the training, I guess, is always done on close talking?
Speaker E: No, actually, actually the well matched condition is still quite difficult.
Speaker E: They have all these data from the close mic and from the decent mic, from different driving conditions, open window, close window, and they take all of this, and they take 70% for training and 30% for testing.
Speaker E: So training is done on different conditions and different microphones and testing also is done on different microphones and conditions.
Speaker E: So probably if we only take the closed microphones, I guess the research should be much, much better than this.
Speaker D: I see.
Speaker D: Okay, that's better.
Speaker E: So there is this, the mismatch is the same kind of thing, but the driving conditions, I mean the speed and the kind of road is different for training and testing.
Speaker E: And the last condition is closed microphone for training and this done for testing.
Speaker D: Okay, so the highly mismatched case is in some sense a good model for what we've been typically talking about when we talk about added noise.
Speaker D: And so it does correspond to a realistic situation in the sense that people might really be trying to call out telephone numbers or something like that in their cars.
Speaker D: They're trying to connect to something.
Speaker E: Actually, yeah, it's very close to clean speech training because the closed microphone and noisy speech testing.
Speaker D: And the well-matched condition is what you might imagine that you might be able to approach if you know that this is the application, you're going to record a bunch of people in cars and so forth, do these training.
Speaker D: And then when you sell it to somebody, it will be a different person with a different car and so on.
Speaker D: So this is somewhat optimistic to view on it.
Speaker D: So the real thing is probably somewhere in between the two. But even the optimistic one is working.
Speaker D: Yeah, right.
Speaker D: That's sort of the dominant thing is even say under development set stuff that we saw the numbers that Alcatel was getting, which was the best single numbers.
Speaker D: It just wasn't good enough for real system.
Speaker D: So still a lot of stuff to do.
Speaker D: And I don't know.
Speaker D: So looking at the data, what's the characteristic of the thing?
Speaker D: What are your thoughts about what else you're thinking about?
Speaker A: A lot of things. Because we're trying a lot of things and we're not working.
Speaker A: We remove this. Maybe we try again with the articulatory feature.
Speaker A: I don't know exactly because we tried with some one experiment and some work and forgot it.
Speaker A: I don't know if it's a trip because maybe to better some step of the general diagram.
Speaker A: I don't know if it's a trip to think what we can improve.
Speaker D: Yeah, because a lot of times it's true. There were a lot of times when we tried something and it didn't work right away.
Speaker D: Even though we had an intuition that there should be something there, so then we would just stop it.
Speaker D: One of the things, I don't remember the details on, but I remember at some point when we were working with a second stream in which I had a fast filtering and a cap stream.
Speaker D: In some case you got, well, it was an MSG-like thing, but it was an MSG.
Speaker D: I think in some case you got some little improvement, but it was sort of a small improvement and it was a big added complication, so you dropped it.
Speaker D: But that was just sort of one try, right? Just took one filter through it there, right?
Speaker D: And it seems to me that if that isn't an important idea, it might be that one could work at it for a while as you're saying.
Speaker D: And you had the multi-band thing, so there's an issue with that.
Speaker D: Barry is going to be continuing working on multi-band things as well.
Speaker D: We were just talking about some work that we were interested in, kind of inspired by the stuff where Larry saw the learning and articulatory feature, I think in the case of his paper, with Sonderance based on multi-band information, where you have a combination of gradient learning and, yeah.
Speaker D: So I think that this is a neat data set, and then, as we mentioned before, we also have the new digit set coming up from recordings in this room.
Speaker D: So there's a lot of things to work with.
Speaker D: Yeah, what I like about it in a way is that the results are still so terrible.
Speaker D: I mean, they're much better than they were. You know, we're talking about 30 to 60% error rate reduction, and it's really great stuff to do that relatively short time.
Speaker D: But even after that, it's still poor that we could use it.
Speaker D: I think that's great, and also because, again, it's not something... sometimes we've got terrible results by taking some data and are officially involving it with some new response or something to take it very one point of binding on downstairs into the basement.
Speaker D: It's a hallway that is very reverberant, and we made some recordings there, and then we made a simulation of the room acoustics there and applied it to other things.
Speaker D: But it was all pretty artificial, and how often would you really try to have your most crucial conversations in this very reverberant hallway?
Speaker D: So this was nice about the Aurora data and the data here is that it's sort of a realistic room situation, acoustics situation, with terms of noise and reflections and so on.
Speaker D: It's something that's still relatively realistic, it's still very hard to do it well.
Speaker E: Yeah, so... well... actually, that's why we... well, it's a different kind of data, we're not used to work with this kind of data.
Speaker E: That's why we should have a little more closer look at what's going on.
Speaker E: So this would be the first thing, and then of course try to... well, kind of debug what was wrong when we do a lot of tests on the MLG, particularly on the multiband.
Speaker E: Yeah.
Speaker D: Yeah. Yeah, I think there's lots of... it's a good thing to do with this.
Speaker D: So... so let's... I guess... you can see as well.
Speaker C: What do you think?
Speaker C: About... anything?
Speaker C: About other experiences.
Speaker C: Now I'm interested in looking at the experiments where you use data from multiple languages to train to neural net.
Speaker C: I don't know how far... or if you guys even had a chance to try that, but that would be something to be interesting to me.
Speaker E: Yeah, but again it's the kind of thing that you were thinking that it would work, but it didn't work.
Speaker E: And... sorry.
Speaker E: Not a bug, but something wrong in that we...
Speaker E: Right.
Speaker E: And... something wrong, perhaps, in the... just in the fact that the labels are... what work best is the end-leabled data?
Speaker E: So, yeah. I don't know if we can get some end-leabled data from other languages.
Speaker E: Yeah.
Speaker E: It's not so easy to find.
Speaker E: Right.
Speaker E: But that would be something interesting to me.
Speaker D: Yeah.
Speaker D: Also, there was just a whole notion of having multiple nets that were trained on different data.
Speaker D: So one form of different data is from different languages, but the other...
Speaker D: Well, in fact, in those experiments there wasn't so much combining multiple nets. It was a single net that had different.
Speaker D: Yeah.
Speaker D: So first thing is, would it be better if they were multiple nets?
Speaker D: So the second thing is never mind the different languages, just having different acoustic conditions, rather than training them all up and one,
None: would it be helpful to have different ones? So that was a question that was kind of raised by Mike Chares, these were seen in that case in terms of through preparation.
Speaker D: Sometimes it might be better to do that.
Speaker D: But I think we know.
Speaker D: So, all right. So next week we won't meet because you'll be here.
None: And when are you two getting back?
Speaker A: I'm...
Speaker A: Sunday because it's less expensive than practice.
Speaker A: I'm not a ticker.
Speaker E: Right. I'll be back Tuesday.
Speaker C: Where is the meeting?
Speaker D: Amsterdam, I think.
Speaker D: So we'll skip next week and we'll meet two weeks from now.
Speaker D: And I guess the main topic will be telling us what happened.
None: Yeah.
Speaker A: Yeah.
Speaker D: So, yeah. Well, we don't have anything else we should turn off the machine and say we don't ask you for.
Speaker C: Should we do digits first?
Speaker D: Oh yes, digits. Yeah, good point.
Speaker C: Okay. Transcript 37913810904 02007 11704 240921 3613509 495 60607 85680 97 0509 0505 0701009 1 2 3 0 7 9 8 8 0 5 7 6 3 9 7 7 1 8 8 9
Speaker D: Transcript 3751-3770 8 557 306 9 6 6 0 0 0 1 2 0 4 2 5 5 6 6 6 7 8 2 8 9 0 0 1 1 2 3 4 3 8 6 4 4 8 3 5 6 7 0 4 3 0 0 7
Speaker B: Transcript 3671-36904 5004 27203 8530510 9718 476 0 9 4 5 103 2 4 115 5 4 9 9 2 6 6 3 7 7 9 8 8 0 0 2 8 5 9 0 7 0 0 0 8 1 2 2 8 6 8 3 5 3 7 10 9 4 5
Speaker E: Transcript 6 9, yeah, sorry. Transcript 3691-3714-5 6 9 4 5 015-032-1720-2624-304-405-4 617-hoho 758-628-3 8 8 4 9 8 0 0 0 19 8 1 0 0 3 2 8 0 9 4 2 5 4 5
Speaker A: Transcript number 3771-3790 9 1 0 8 0 4 6 9 0 0 0 6 4 7 6 5 9 9 0 8 5 7 6 7 2 8 5 6 8 3 6 7 4 1 3 9 8 9 0 0 1 1 I'm sorry 1 1 1 3 1 4 4 7 5 7 7 3 6 9 6 5 7 8
