Speaker G: Are we on?
Speaker C: We're on.
Speaker C: We're down.
Speaker G: Okay, so we can have a cent around the agenda.
Speaker G: Any agenda items that he has to talk about?
Speaker H: What's going on?
Speaker H: Has everyone met at dawn?
Speaker H: Yeah, doing.
Speaker G: Okay, agenda item one, introduce Don.
Speaker D: Well, I had a good question, but I know there was a discussion of it at a previous meeting that I missed, but just about the wish list item of getting good quality close packing mics on every speaker.
Speaker G: Okay, so let's do agenda building right now.
Speaker G: Okay, so let's talk about that a bit.
Speaker G: Close talking mics, better quality.
Speaker E: We can talk about that. You were going to start to say something.
Speaker E: Well, you already know about the meeting that's coming up, and I don't know if this is appropriate.
Speaker G: No, no, it's okay.
Speaker G: So we can talk.
Speaker G: So NIST folks are coming by next week, and so we'll talk about that.
Speaker G: Who's coming?
Speaker G: John Fiskis, and I think George Dunn and we'll be around as well.
Speaker G: Okay, so we can talk about that.
Speaker G: I guess just hear about how things are going with the transcriptions, right?
Speaker G: So, maybe, maybe this is going to discuss.
Speaker G: Anything else?
Speaker G: It's striking, but...
Speaker D: We've started running recognition on one conversation, but it isn't working yet.
Speaker D: But if anyone has... the main thing would be if anyone has knowledge about ways to post-process the waveforms that would give us better recognition, that would be helpful to know about.
Speaker K: Talk conversation.
Speaker I: Yeah, so what about, is there anything new with the speech? Non-speech?
Speaker B: No, I'm working on it, but it's not finished.
Speaker G: All right, that seems like a good collection of things, and we'll end up with other things.
Speaker E: I had thought under my topic that I would mention the four items that I put out for being on the agenda on that meeting, which includes the presugmentation of the developments and multi-trans.
Speaker G: Oh, under the NIST meeting.
Speaker G: All right, when I start off with this... I guess the order we wrote up seems fine.
Speaker G: So, better quality, close talking mics.
Speaker G: So, the one issue was that the lapel mic isn't as good as you would like, and so it'd be better if we had close talking mics for everybody, right?
Speaker G: Is that basically the point?
Speaker D: Yeah, and actually in addition to that, that the close talking mics are worn in such a way as to best capture the signal.
Speaker D: The reason here is just that for the people doing work, not on microphones, but on sort of like dialogue and so forth, or even on prosody, which Donna's going to be working on soon, it adds this extra variable for each speaker to deal with when the microphones aren't similar.
Speaker D: So, and I also talked to Mary this morning, and she also had a strong preference for doing that, and in fact she said that that's useful for them to know and starting to collect their data too.
Speaker D: Right, so one...
Speaker G: Well, one thing I was going to say was that we could get more of the head-mounted microphones, even beyond the number of radio channels we have, because I think whether it's radio or wire is probably second order in the main time.
Speaker G: So, the main thing is having the microphone close to the mic, not too close.
Speaker H: So, actually the way it was A's is correct.
Speaker H: So, it's not correct?
Speaker H: Yes, it's good.
Speaker H: So, it's in the corner of your mouth, so that the sound doesn't get on it, and then just sort of a thumb on the half away from your mic.
Speaker D: Right. But we have more than one type of, I mean, Princeton here.
Speaker H: This one isn't very adjustable, so this is good as I get it, instead of fixed.
Speaker D: But if we could actually standardize, you know, the microphones as much as possible, that would be really helpful.
Speaker G: Well, I mean, it doesn't hurt to have a few extra microphones around, so why don't we just go out and get an order of if this microphone seems okay to people?
Speaker H: I just get a half dozen of them. The only problem with that is right now, some of the gen ones aren't working.
Speaker H: The box is under the table.
Speaker H: And so, I've only been able to find three jacks that are working.
Speaker I: Can we get these?
Speaker D: No, but my point is...
Speaker D: I could just record these signals separately and timeline in what the start of the meeting.
Speaker H: Right.
Speaker G: Right now, we've got two microphones in the room that are not quote-unquote standard.
Speaker G: So why don't we replace those?
Speaker G: Okay, just two.
Speaker G: Well, however many we can plug in, if we can plug in three, let's plug in three.
Speaker G: Also, we've talked before about getting another radio.
Speaker G: And so then that would be three.
Speaker G: So we should go out to our full complement of whatever we can do, but have them all be the same mic.
Speaker G: I think the original reason that it was done the other way was because it was sort of an experimental thing.
Speaker G: And I don't think anybody knew whether people would rather have more variety or more uniformity.
Speaker H: Sounds like you're more into the lens.
Speaker D: Well, for short-term research, there's just so much effort that would have to be done upfront.
Speaker I: You're probably going to be great.
Speaker I: You're saying for dialogue purposes, so that means that the transcribers are having trouble with those mics, is that what you mean?
Speaker D: They would know more about the transcriber.
Speaker E: And that's true.
Speaker E: I mean, we could discuss this.
Speaker E: And a couple times.
Speaker E: So, yeah, the transcribers notice that in fact there's somewhere...
Speaker E: Well, I mean, it's the double thing.
Speaker E: It's the equipment and also how it's worn.
Speaker E: And they always...
Speaker E: They just write about how wonderful Adam's at his class.
Speaker D: So does the recognize there.
None: No, really.
Speaker D: Yeah, that's a problem.
Speaker D: I'm a fah!
Speaker D: Yeah, it's not just that.
Speaker D: It's your talking to someone else's mic.
Speaker E: It's not so loud with no breathing.
None: No, you know, it's like...
None: It's...
None: It's just...
Speaker E: The transcribers point at you and also the one he's just pointing...
Speaker H: The point of doing the close-talk mic is to get a good quality signal.
Speaker H: We're not doing any search-drunken close-talk mics.
Speaker H: So we might as well give it a good view.
Speaker G: Now, this is locked in the barn door after the horse was stolen.
Speaker G: We do have 30 hours of speech, which is on this way.
Speaker G: But, yeah, for future ones, we can get it a bit more uniform.
Speaker G: So I think we just do a field trip at some point.
Speaker G: Yeah, probably.
Speaker G: Yeah, to the store we talked about.
Speaker E: And there was some talk about maybe the headphones that are uncomfortable.
Speaker H: Yeah, so this is...
Speaker H: We'll do a field trip and see if all of the same mic that's more comfortable than these things, which I don't know.
Speaker C: Great.
Speaker C: Thank you very much.
Speaker D: It makes our table a lot easier.
Speaker H: Okay, we're researchers.
Speaker H: And we'll have big hits.
Speaker H: Yeah.
Speaker G: Okay, second item was the NIST visit.
Speaker G: It's going on there.
Speaker G: Okay, so...
Speaker E: John, this is coming on the second of February.
Speaker E: There's a lot of people here, not everyone.
Speaker E: And he expressed an interest in saying the room and in saying a demonstration on the modified multi-trans, which I'll mention in a second.
Speaker E: And also, it was interested in the pre-segmentation.
Speaker E: And then he's also interested in transcription conventions.
Speaker E: And so it seems to me in terms of like...
Speaker E: Okay, so the room is things like the audio, audio, audio, acoustic properties, the room and how the recordings are done and that kind of thing.
Speaker E: Okay, in terms of the multi-trans, well, that's being modified by Dave Gilbert to a hand-roll multi-channel recording.
Speaker H: I see this thing mentioned in my interview to this meeting.
Speaker H: I've got to do it.
Speaker C: Yeah, okay.
Speaker C: Sorry.
Speaker C: Yeah, well that's a game.
Speaker C: Yeah, it looks really great.
Speaker E: He has a prototype.
Speaker E: I didn't see it yesterday, but I didn't see it today.
Speaker E: And that's that will enable us to do nice, tight, time-marking of the beginning and ending of overlapping segments at present.
Speaker E: It's not possible with limitations of the original design and software.
Speaker E: And so in terms of like pre-segmentation, that continues to be a terrific asset to the transcribers.
Speaker E: Do you...
Speaker E: I know that you're also supplementing it for the do you want to mention something about that, too?
Speaker B: Yeah, what I'm doing right now is I'm trying to include some information about which channel there's some speech in, but that's not working at the moment.
Speaker B: I'm just trying to do this by comparing energies, normalizing energies and comparing energies of the different channels.
Speaker B: And so to give the transcribers some information in which channel there's the speech.
Speaker B: In addition to the thing we did now, which is just speech non-speech detection on the mixed file.
Speaker B: So I'm relying on the segmentation of the mixed file, but I'm trying to subdivide the speech portions into different portions if there is some activity in different channels.
Speaker B: Excellent.
Speaker E: So it'd be like providing also a speaker idea.
Speaker G: Something I guess I didn't put in the list, but on that same day later on, and maybe it's...no, actually, this week, they've go over it and I will be visiting with John Canny, who is a CS professor who's interested in array microphones.
Speaker G: Oh, he's doing array.
Speaker G: Thanks.
Speaker G: And so we want to see what commonality there is here.
Speaker G: Maybe they'd want to stick an array of microphones when we're doing things.
Speaker G: Or maybe it's not a specific array microphone they want, but they might want to just...you could imagine them taking the four signals from these table mics and trying to do something with them.
Speaker G: I also had a discussion.
Speaker G: So we'll be over there talking with him after the class I'm fighting, which you know what goes on with that.
Speaker G: I also had a completely unrelated thing I had a discussion today with Berger Kolmeier, who's a German scientist.
Speaker G: He's got a fair-sized group doing a range of things, sort of a lot of Tory related, largely for hearing aids and so on.
Speaker G: But does stuff with auditory models and he's very interested in directionality and location and head models and paper film things.
Speaker G: He and possibly a student, there's a student of his who gave a talk here last year, may come here in the fall for sort of a five month sabbatical.
Speaker G: So it might be around to get him to give some talks and so on.
Speaker I: So anyway, he might be interested.
Speaker I: There reminds me of a thought of an interesting project that somebody could try to do with the data from here.
Speaker I: Either using the mics on the table or using signal energies from the head worn mics.
Speaker I: And that is to try to construct a map of where people were sitting.
Speaker H: Oh, did he?
Speaker H: Oh, that's interesting.
Speaker H: So that's the cross-correlation stuff.
Speaker I: And so you could plot out who was sitting next to who?
Speaker G: I mean, he didn't do a very extreme thing, but it was a sort of given that the block of wood with the two mics on either side.
Speaker G: If I'm speaking, or if you're speaking or someone over there speaking, you're looking across correlation functions.
Speaker G: If someone was on the axis between the two is talking, then you get a big peak there.
Speaker G: And if someone's talking on one side or the other, it was the other way.
Speaker G: It even looks different if the two people are on the other side are talking, then if one in the middle, it actually looks different.
Speaker I: So I was just thinking, you know, as I sit here next to Tilo, when he's talking, my mic probably picks it up better than your guy's's mic.
Speaker I: So if you just looked at the energy on my mic, you could get an idea about who's closest to who.
Speaker I: Or who talks the last.
Speaker G: Yeah, well, you have to, the appropriate normalizations are tricky.
Speaker D: You just search for Adam's voice on each individual mic.
Speaker G: Yeah, we switched positions recently.
Speaker G: So those are just a little couple of news items.
Speaker E: And then one thing, so John Fiskus expressed an interest in microphone arrays is there.
Speaker E: And I also want to say his e can't stay all day, he needs to leave for from here to make a 245 white.
Speaker E: No, it's just morning from all night.
Speaker E: Right.
Speaker E: Makes a sketch on a little bit type.
Speaker E: But do you think that John Canny should be involved in this somehow or not?
Speaker G: Probably not, but I'll know better after I see him this Friday.
Speaker G: What kind of level he wants to get involved.
Speaker G: He might be excited to and it might be very appropriate for him to or he might have noticed once or ever.
Speaker G: I guess really don't know.
Speaker H: Is he involved in, I'm blanking on the name of the project.
Speaker H: NIST has done a big meeting room, instrumented meeting room with video and microphone arrays and very elaborate solvers.
Speaker H: Well, that's what they're starting up.
Speaker H: Okay.
Speaker G: Yeah, no, I mean, that's what all this is about.
Speaker G: They haven't done it yet.
Speaker G: Okay, they wanted to do.
Speaker H: The papers that look like they had already done some work.
Speaker G: Well, I think they've instrumented the room, but I don't think they haven't started recordings yet.
Speaker G: They don't have the transcription standard.
Speaker G: Are they going to do a video as well?
Speaker G: Yeah, I think.
Speaker H: I think so.
Speaker H: What I had read was a fairly large amount of software infrastructure recording, and also live room where you're interacting with the computer and with the video and lots of stuff.
Speaker G: Well, I'm not sure.
Speaker G: All I know is that they've been talking to me about a project that they're going to start out recording people meeting and meetings.
Speaker G: And it is related to ours.
Speaker G: They were interested in ours.
Speaker G: They wanted to get some uniformity with us about the transcriptions and so on.
Speaker G: And one notable difference, actually, I can't remember whether they were going to routinely click video or not.
Speaker G: But one difference from the audio side was that they are interested in using raymines.
Speaker G: So, you know, you just tell you the party line on that.
Speaker G: The reason I didn't go for that here was because the focus, both the buy interest and the madam's interest, was in impromptu situations.
Speaker G: And we're not recording a bunch of impromptu situations, but that's because it's different to get data for research than to actually imply it.
Speaker G: And so, for scientific reasons, we thought it was good to instrument this room as we wanted it.
Speaker G: But the thing we ultimately wanted to aim at was a situation where you were talking with one or more other people in an impromptu way, where you didn't actually know what the situation was going to be.
Speaker G: And therefore, it would not be highly unlikely that room would be outfitted with some very carefully designed array of microphones.
Speaker G: So, it was only for that reason.
Speaker G: You know, yet another piece of research seemed like you had enough to know portable array of mic.
Speaker G: No, so there's a whole range of things. There's a whole array of things that people do on this.
Speaker G: So, the big arrays, places like Rutgers and Brown and other places, they have big arrays with 100 mikes or something.
Speaker G: And so, there's a wall of mics.
Speaker G: You get really, really good beampointing that sort of thing.
Speaker G: And in fact, at one point, we had a proposal in with Rutgers where we were going to do some of the sort of per-channel signal processing, and they were going to do multi-channel stuff.
Speaker I: I've seen demonstrations of the microphone array. It's amazing.
Speaker I: Yeah, how they can cut out noise.
Speaker G: And then they have the little ones.
Speaker G: They don't have a block of wood.
Speaker G: Yeah, our block of wood is unique.
Speaker G: But no, there are these commercial things now. You can buy that have four mics or something.
Speaker G: So, yeah, there's a range of things that people do.
Speaker G: So, if we connect that with somebody who was interested in doing that sort of thing, that's a good thing to do.
Speaker G: I mean, whenever I've described the people who are interested on the acoustic side, that's invariably the question they ask.
Speaker G: Just like someone who's interested in the general dialogue thing, we always ask, are you recording video?
Speaker G: Right.
Speaker G: And you could see people always say, well, are you doing a ray of microphone?
Speaker G: So, it's a good thing to do.
Speaker G: But it doesn't solve the problem of how do you solve things when there's one mic or at best two mics in this imagined VBA that we have.
Speaker G: So, maybe we'll do some more of that.
Speaker E: I mean, I know that having a ray of imagined would be more expensive than a ray of mic and mic and mic and mic.
Speaker E: But couldn't you kind of approximate the natural situation by just shutting off channels when you're later on?
Speaker E: I mean, it seems like if the mic and mic falls down and affect each other, then couldn't you just record them within a ray and then just not use all the data?
Speaker H: It's just a lot of infrastructure for our particular purpose, recalculate the center.
Speaker G: Yeah, if 99% of what you're doing is shutting off most of the mics and going through that.
Speaker G: But if you get someone who has that as a primary interest then that drives the right answer.
Speaker G: That's right.
Speaker H: Someone came in and said, you really want to do it?
Speaker H: I mean, we don't care.
Speaker I: So, to save that data, you have to have one channel recording per mic in the array?
Speaker G: Well, at some level, at some level, but then, you know, there's...
Speaker I: What you save, I mean, if you're going to do research with that.
Speaker G: I don't know what they're going to do and I don't know how big their array is.
Speaker G: Obviously, if you were going to save all of those channels for later research, you'd use up a lot of space.
Speaker H: Well, their software infrastructure had a very elaborate design for plugin helpters and mixers and all sorts of processing so that they can do stuff in real time and not save that each channel.
Speaker H: Yeah, that was...
Speaker G: But I mean, for optimum flexibility later, you want to save each channel, but I think in practical situations, you would have some engine of some sort doing some processing to reduce this to the equivalent of a single microphone that was very directional.
Speaker I: Oh, okay.
Speaker I: Sort of saving the result of the beam plugin.
Speaker D: It seems to me that there's, you know, there are good political reasons for doing this, just getting the data because there's a number of sites like...
Speaker D: Right now, SRI is probably going to invest a lot of internal funding into recording meetings also, which is good, but they'll be recording with video and they'll be...
Speaker D: You know, it'd be nice if we can have at least make use of the data that we're recording as we go.
Speaker D: This is the first site that has really collected these really impromptu meetings and just have this other information available.
Speaker D: So if we can get the investment in just the infrastructure and then, I don't know, save it out or have whoever's interested, save that data out, transfer it there, it would be good to have the recording.
Speaker H: You mean to actually get a microphone or a radio?
Speaker H: Well, even if we're not...
Speaker D: I'm not sure about video that sort of a video has a little different nature since, right now we're all being recorded, but we're not being taped.
Speaker D: But definitely in the case of microphone array since if there's a community interested in this.
Speaker H: Well, I think we need to reset our hears interested in it.
Speaker F: It's a bunch of long.
Speaker G: See, the problem is it took at least six months for Dan to get together the hardware and the software and debug stuff and the microphones and the boxes and it was a really big deal.
Speaker G: And so I think we could get a microphone array in here pretty easily and have it mixed to one channel of some sort.
Speaker G: But I think for...
Speaker G: I mean, how we're going to decide...
Speaker G: For maximum flexibility later, you really don't want to end up with just one channel that's pointed in the direction of the person with the maximum energy or something like that.
Speaker G: I mean, you want actually to have multiple channels being recorded so that you can...
Speaker G: And to do that, we're going to end up greatly increase the this space that we use up.
Speaker G: We also only have boards that will take up to 16 channels and this meeting we've got eight people and six mics in there were already using 14.
Speaker H: And we actually only have 15.
Speaker D: 15, 16. Well, if there's a way to say time to sort of solve each of these those...
Speaker D: So suppose you can get an array in because there's some person that Berkeley who's interested and has some equipment.
Speaker D: And suppose we can...
Speaker D: As we save it, we can transfer it off to some other place that holds this data.
Speaker D: Who's interested, even if it's the itself isn't.
Speaker D: And it seems like as long as we can time align the beginning, do we need to mix it with the rest? I don't know.
Speaker G: Yeah, so I think you need a separate set up and the assumption that you could time align the two.
Speaker D: I mean, it's just... it's worth considering as sort of once you make the upfront investment and can sort of save it out each time and not have to worry about the dis space factor than it might do with having the data.
Speaker G: It's not so much worried about this space. I mentioned that as a practical matter, but the real issue is that there's no way to do a recording extended to what we have now with low skew.
Speaker G: So you would have a completely separate set up, which would mean that the sampling times and so forth would be all over the place compared to this.
Speaker G: So it would depend on the level of processing you're doing later.
Speaker G: The kind of person who's doing array processing you actually care about funny little times.
Speaker G: So you actually would want to have a completely different set up than we have one that would go up to 32 channels or something.
Speaker G: So basically...
Speaker G: So I'm kind of skeptical.
Speaker G: But I don't think we can share the resource in that way.
Speaker G: What we could do is if there's someone else that's interested, they could have a separate set up which they wouldn't be trying to sync with ours, which might be useful for...
Speaker G: And then we can offer up the rule.
Speaker G: Yeah, we can offer the meetings and the physical space and... yeah, the transcripts.
Speaker D: Right, I mean, it'd be nice if we have more information on the same data.
Speaker D: Yeah.
Speaker D: But if it's impossible or if it's a lot of effort then you have to just balance it too.
Speaker G: Yeah, the thing will be... and talking to you, these other people to see what we can do.
Speaker I: Is there an interest in getting video recordings for these meetings?
Speaker I: Right, so we had some...
Speaker H: It's actually the same problem that you have an infrastructure problem.
Speaker H: You have a problem with people not wanting to be video-tabied with your problem.
Speaker H: And no one who's currently involved in the project is really hot to do it.
Speaker I: So there's not enough interest to overcome it.
Speaker D: Not entirely, but I know there is interest from other places that are interested in looking at meeting data and having the video.
Speaker D: So it's just...
Speaker E: Yeah, well, well, I have to mention the human subjects problems that increase with video.
Speaker E: Right.
Speaker G: Yeah, so people getting shy about it, there's this human subjects problem, there's the fact that then...
Speaker G: I heard comments about this.
Speaker G: Well, why don't you just put on a video camera?
Speaker G: You know, it's sort of like saying, well, we're primarily interested in some dialogue things.
Speaker G: But why don't we just throw a microphone out there?
Speaker G: I mean, the thing is, once you actually have serious interest in any of these things, then you actually have to put a lot of effort in.
Speaker G: And you really want to do it right.
Speaker G: So I think NIST or LBC or something like that, I think is much better shape to do all of that.
Speaker G: There will be other meeting recordings.
Speaker G: We won't be the only place to be meeting recordings.
Speaker G: We're doing what we're doing, and hopefully it will be useful.
Speaker E: It's pretty much done in the scientific form.
Speaker H: Probably not.
Speaker H: Did you sign up?
Speaker H: What did you do actually?
Speaker H: Did you read in the industry?
Speaker J: Yeah, I was...
Speaker I: You were here to meet you before.
Speaker J: I was here before once.
Speaker J: You were here to sign up for it.
Speaker J: Did you sign up for it?
Speaker J: I think so.
Speaker J: I'll get it done before we meet.
Speaker E: Thank you.
Speaker E: Yeah.
Speaker E: You don't have to leave me behind.
Speaker E: I just...
Speaker F: I can't really leave you behind.
Speaker F: You're being recorded, isn't it?
Speaker C: We don't perform electroshock during this meeting.
Speaker C: That's fine.
Speaker C: Usually.
Speaker F: Okay.
None: Transcriptions.
Speaker G: Okay.
Speaker E: I thought about the many three aspects of this.
Speaker E: So first of all, I got eight transcribers.
Speaker E: Seven of them were linguists.
Speaker E: One of them was a graduate student in psychology.
Speaker E: I gave each of them their own data set.
Speaker E: Two of them have already finished the data sets.
Speaker E: And the meetings around, you know, it would say an hour.
Speaker E: Sometimes it's not just an hour and a half.
Speaker E: What I mean is one meeting.
Speaker E: Each person got their own meeting.
Speaker E: I don't want to have any conflicts of, you know, when to stop transcribing this one.
Speaker E: So I wanted to keep it clear whose data was.
Speaker E: And so...
Speaker E: And meetings, you know, I think that they go as long as two hours in some cases.
Speaker E: And that means, you know, if we've got two already finished and they're working on right now, all of them have additional data sets.
Speaker E: That means potentially as many as ten might be finished by the end of the month.
Speaker E: Also, the pre-signitation really helps a huge number.
Speaker E: And also, Dan Ellis's innovation of the multi-channel to hear really helped a lot, in terms of clearing up your earrings that evolve over labs.
Speaker E: But just out of curiosity, I asked one of them how long it was taking here.
Speaker E: One of these two is already finished her data set.
Speaker E: She said it takes about 60 minutes transcription for every five minutes of real time.
Speaker E: So it's about 12 to one, which is what we were making.
Speaker E: It's well in the finish.
Speaker E: Okay.
Speaker E: At least still, when they're finished, that means that they're finished with their pastor.
Speaker E: They still need to be edited and all, but it's word-level, the speed for change, the things that we were mentioned.
Speaker E: Okay. Now I wanted to mention the teleconference I had with John Fiskus.
Speaker E: I was sold for an hour and a half, and had not bought a lot of things in common.
Speaker E: He indicated to me that he's been spending a lot of time with, quite sure the connection, but spending a lot of time with the Atlas system.
Speaker E: And I guess that, I mean, I need to read up on that, and there's a website that has lots of papers.
Speaker E: But it looks to me like that's the name that has developed a system that birthed the leave room and developed for the annotated graphs approach.
Speaker E: So what he wants me to do, and what we will do, is to provide them with an RE transcribe meeting for him to be able to experiment with in this Atlas system.
Speaker E: And they do have some sort of self-draight, my impression, related to Alice, and that he wants to experiment with taking our data and putting in that form and see how that works out. I explained to him in tail the conventions that we're using here in this word-level transcript. And I explained the reasons that we were not coding more elaborately, and they focus on reliability.
Speaker E: He expressed a lot of interest in reliability. He's really up on these things.
Speaker E: He's very independently, yes, well, what about reliability?
Speaker E: He's interested in the consistency of the encoding and that sort of thing.
Speaker D: So can you explain what the Atlas and that move?
Speaker E: Well, at this point, I think, well, Adam's read more in more detail than I have on this.
Speaker E: I need to equate myself more with it, but there's a way of viewing, whenever you have coding categories, and you're dealing with taxonomy, then you can have branches that have alternative choices that you could use for each of them.
Speaker E: And it just sends up looking like a graph or a presentation.
Speaker H: Is Atlas the annotated transcription graph stuff?
Speaker H: I don't remember the acronym. The one I think you're referring to, they have this concept of an annotated transcription graph representation.
Speaker H: And that basically, when I based the format that I did, I based it on their work almost directly in combination with the TEI stuff. And so it's very, very similar.
Speaker H: And so it's a data representation and set of tools for manipulating transcription graphs of various types.
Speaker I: Is this the project that's sort of between NIST and a couple of other places?
Speaker I: I think you know the CIFA stuff.
Speaker E: Yeah, yeah.
Speaker E: There's a website that has lots of papers. I'll look through them and they've mainly had to do with this tree structure annotated tree.
Speaker E: And I have a thing.
Speaker E: So, in terms of the conventions that I've adopted, there's no conflict at all.
Speaker E: And he was very interested in, oh, how did you handle this one?
Speaker E: I said, well, you know, this way. And we had a really nice conversation.
Speaker E: Okay, now I also wanted to say a different direction is Brian Kingsbury.
Speaker E: So I correspond briefly with him.
Speaker E: He still has an account here. I told him he could SSH on use multitrans and have a look at the already done transcription.
Speaker E: And he did. And what he said was that what they'll be providing is will not be as fine-grained in terms of the time information.
Speaker E: And that's, you know, I need to get back to him and explore that a little bit more and see what they'll be giving us.
Speaker I: So, the specific piece of the folks that they're subcontracting out the transcription to are they like court reporters?
Speaker E: Apparently, well, I get the sense they're kind of like that. It's like a pool of somewhat secretarial.
Speaker E: I don't think that they're court reporters. I don't think they have a special keyboard and that type of training.
Speaker E: I get the sense they're more secretarial. And that what they're doing is giving them medical transcriptionist types people.
Speaker I: So, it's for their speech recognition products. So, they're hiring them. They're coming in. It's not a service they send the tapes out to.
Speaker H: Well, they do send it out, but my understanding is that that's all that's coming. It's transcription.
Speaker H: So, most of it is via voice people reading. I see. They're trying to real support them.
Speaker E: I see. After that, it's been monologues for us. And what they're doing is, Brian himself downloaded.
Speaker E: So, Adam, something to see, Brian himself downloaded. We wanted to have it so that they were familiar with terms.
Speaker E: What they wanted to do. He downloaded from CD on to audio tapes. Apparently, he did it one channel per audio tape.
Speaker E: So, each of these people is transcribing from one channel. And then what he's going to do is check it.
Speaker E: Before they go beyond the first one, check it.
Speaker G: So, each person gets one of these channels. So, if they hear something off in the distance, they don't...
Speaker G: They just go...
Speaker H: That's okay because you'll do all of them in the end of the mind.
Speaker I: But there could be problems, right? Like that.
Speaker E: I think it would be difficult to do it that way. I really...
Speaker I: Well, if you got that channel right there...
Speaker H: No, no, close talk.
Speaker H: Not the desktop. Are you?
Speaker E: Yes. I sure am. I really foolish to do otherwise.
Speaker E: I think it would be hard to come up with.
Speaker D: I think it's hard just playing the...
Speaker D: Just having played the individual files.
Speaker D: I mean, I know what your voice sounds like. I'm familiar with it.
Speaker D: It's pretty hard to follow, especially...
Speaker D: There are a lot of words that are so reduced phonetically that make sense when you know what the person was saying before.
Speaker D: Yeah, that's it.
Speaker D: Especially to define these where you are in...
Speaker H: We've had this discussion many times. The answers we don't actually know the answer because we haven't tried both ways.
Speaker E: Well, except I can say that my transcribers use the mix signal mostly.
Speaker E: Unless there's a huge disparity in terms of the volume of the mix, in which case, you know, they...
Speaker E: They wouldn't be able to catch anything except from the channel.
Speaker H: That might change in one of really fine time markings.
Speaker H: Well, okay.
Speaker D: But they're not giving really fine markings.
Speaker D: So, are they giving any time markings?
Speaker D: I'm not even asking.
Speaker D: And I stress my email to you and that needs to be with Compa.
Speaker E: But I didn't want to say that it's hard to follow one channel of a conversation even if you know the people.
Speaker E: And if you're dealing furthermore with a highly abstract network concepts you've never heard of.
Speaker E: One of these people was transcribing the network's group talk of the city.
Speaker E: I don't really know what a lot of these abbreviations are.
Speaker E: But I just put them in parentheses.
Speaker E: I just don't know if you're interested in that.
Speaker H: Just that of curiosity.
Speaker H: I mean, a lot of heavy accents.
Speaker I: Given all of the effort that is going on here in transcribing, why do we have IBM doing it?
Speaker I: Why not just do it all ourselves?
Speaker G: It's historical.
Speaker G: I mean, at some point ago we thought that, boy, we'd really have to ramp up to do that.
Speaker G: Like we just did.
Speaker G: And here's a collaborating institution that's volunteering to do it.
Speaker G: So that was a contribution they can make in terms of time, money.
Speaker G: I'm just wondering now.
Speaker I: I'm wondering now.
Speaker D: Yeah, my heart is asking the same question as sort of talking about more e-mail layers.
Speaker D: Yeah.
Speaker F: Yeah.
Speaker G: So, let's see.
Speaker G: I mean, I think they've proceeded long a bit.
Speaker E: Let's see what comes out of it and have some more discussions with them.
Speaker E: It's very, a real benefit having Ryan involved because his knowledge of what the
Speaker H: have a data need to be used and so what is useful to have. Yeah.
Speaker D: So Liz with the SRI recognize or can it make use of some time marks?
Speaker D: I think this is what Don has been, he's already been really helpful in chopping up these.
Speaker D: So first of all, before the SRI front end, we really need to chop things up into pieces that are not too huge.
Speaker D: But second of all, in general, because some of these channels, I'd say like, I don't know, at least half of them, probably on average, have a lot of cross talk.
Speaker D: It's good to get sort of short segments if you're going to do recognition, especially forced alignment.
Speaker D: So, Don has been taking a first stab actually using James first, the first meeting that James transcribed, which we did have some problems with and Tilo, I think told me why this was, but that people were switching microphones around in the very beginning.
Speaker D: So, yes, right.
Speaker B: And they were not switching them, but they were adjusting them.
Speaker B: So, after a minute or so, it's way better.
Speaker D: So, we have to sort of normalize the front end and so forth and have these small segments.
Speaker D: So, we've taken that and chopped it into pieces, based always on your cuts that you made on the mix signal.
Speaker D: And so, every speaker has the same cuts, and if they have speech in it, we run it through.
Speaker D: And if they don't have speech in it, we don't run it through and we face that knowledge on the transcription.
Speaker D: The problem is if we have no time marks, then for forced alignment, we actually don't know where, you know, in the signal, the transcriber heard that word.
Speaker D: And so, if it's a whole conversation and we get a long, you know, paragraph of talk, I don't know how they do this, we actually don't know which piece goes where.
Speaker I: Well, you need to, like, a forced alignment before you do the chopping, right?
Speaker D: No, we use the fact that, so when Jane transcribes in the way she has transcribers doing this, whether it's with a pre-segmentation or not, they have a chunk and then they transcribes the words in the chunk.
Speaker D: And maybe they choose the chunk or now they use a pre-segmentation and then correct it, necessary, but they're supposed to chunk and then a transcription, then a chunk and a transcription.
Speaker D: That's great, because the recognizer can...
Speaker H: It's all to be good sized for the right recognizer roles.
Speaker D: Right, and it helps that it's made based on sort of heuristics and human ear.
Speaker D: But there's going to be a real problem, even if we chop up based on speech silence, these, the transcripts from IBM, we don't actually know where the words were, which segments they belong to.
Speaker I: That's sort of what we're worried about. A forced alignment. That's what she's saying, is that they can't.
Speaker D: If you do a forced alignment on something really...
Speaker D: Well, even if you do it on something really long, you need to know...
Speaker D: You can always chop it up, but you need to have a reference of which words went with which chop.
Speaker D: So...
Speaker G: I think that they are...
Speaker G: Yeah, I'm sure they will, and so we have to have a dialogue with them.
Speaker D: It sounds like we just have concerns.
Speaker D: Maybe actually there is some, even if they're not fine-grained, maybe the transcribers, I don't know, maybe it's saved out in pieces or something, that would help.
Speaker D: But it's just an unknown right now.
Speaker E: I need to know a right to it. I just think it was that I got over 10.
Speaker D: But it is true that the segments... I haven't tried the segments that T-Lo gave you, but the segments that in your first meeting are great.
Speaker D: That's a good length.
Speaker D: Good size, good size.
Speaker E: I was thinking, would you find it to win line?
Speaker E: Give us a pre-signitation.
Speaker E: Maybe you have one already, at the meeting that the first transcrib meeting, the one that I transcried?
Speaker B: Sure, I have some, but that's the one where I'm training on.
Speaker E: Oh, I see.
Speaker B: A little bit of dog soup.
Speaker D: And actually, as you get transcribes for new meetings, we can try...
Speaker D: The more data we have to try the alignments on the better.
Speaker D: So it'd be good for... just to know as transcriptions are coming through the pipeline from the transcribers, we're playing around with parameters on the recognizer.
Speaker D: That would be helpful.
Speaker D: Especially as you get more voices.
Speaker D: The first meeting had, I think, just four people.
Speaker E: It wasn't nice, but it was suddenly on Tuesday.
Speaker E: And I was planning to do just a preliminary look over a two that are finished and then give it to you.
Speaker E: Okay.
Speaker C: That's great.
Speaker G: I guess the other thing, I can't remember if we discussed this in the meeting, but I know you and I talked about this a little bit.
Speaker G: There was an issue of...
Speaker G: Suppose we get in the... I guess it's in a vehicle position, although it is just saying where the weak link is in the chain.
Speaker G: Where we have all the data transcribed and we have these transcribers and we're still a bit slow on feeding.
Speaker G: At that point we've caught up and the weak link is recording meetings.
Speaker G: Okay.
Speaker G: Two questions come is, you know, how do we...
Speaker G: It's not really a problem one because we haven't reached that point, but how do we step out the recorded meetings?
Speaker G: And the other one is, is there some good use that we can make of the transcribers to do other things?
Speaker G: So I can't remember how much we talked about this in this meeting, but...
Speaker E: And there is one use that also we discussed, which was when Dave finishes the...
Speaker E: And maybe it's only been the modification to a multitrans which will allow fine grain encoding of overlaps, then it would be... These people would be very good to shift over to finer grain encoding of overlaps just when providing.
Speaker E: So right now you have two overlapping segments in the same time, and one would be improvement in the database, and in the answering interface, be possible to just do a click and drag thing and get the specific place of each of those at the time of the time.
Speaker E: So say, at the beginning of each segment.
Speaker G: Right, so I think we talked about three things. One was, it's had some discussion that has to have some very high level...
Speaker G:...labelaps, types of overlaps and so forth, that someone could do.
Speaker G: Second was, some lower level of just doing these more precise timings.
Speaker G: And the third one is just completely wild airbrained idea that I have, which is that if we have time and people are able to do it, take some subset of the data and do some very fine grain analysis of the speech, for instance, marking in some overlapping, potentially overlapping fashion, the value of articulatory features.
Speaker G: You know, just sort of say, okay, it's voiced from here to here, there's nasal from here to here, and so forth.
Speaker G: And as opposed to doing phonetic analysis and assuming articulatory feature values for those things, obviously it's extremely time consuming. That would be really valuable.
Speaker G: But we could do it on some small subset.
Speaker E: Also, could you do anyone's consciousness that would be easier than a balance with it?
Speaker E: I don't think that being able to code that there's a fricative extended from here to here would be a lot easier than classifying precisely which a vowel that was.
Speaker E: I think a balanced bowser, I think harder.
Speaker G: Well, yeah, but I think also it's just the issue that when you look at the switchboard, for instance, very close up, where places where whether it's a consonant or a vowel, you still have trouble calling it a particular phone at that point.
Speaker G: Because there's this movement from here to here and it's...
Speaker I: You're saying sort of remove the high level of constraints and go bottom up.
Speaker G: Yeah, describe it now. I'm suggesting articulatory features.
Speaker G: Maybe there's even a better way to do it, but that's sort of a traditional way of describing these things.
Speaker G: And I mean, actually, this might be a neat technique to talk to.
Speaker I: Acoustic features versus psychological categories.
Speaker G: I mean, it's still some sort of categories, but something that allows for overlapping change of these things.
Speaker G: And then this would give some more groundwork for people who are building statistical models that allow for overlapping changes, different timing changes, as opposed to just click.
Speaker G: You're now in a state which, of course, allows to this speech.
Speaker D: So this is like gestural.
Speaker G: And actually, if we get into that, it might be good to haul John O'Hall into this, as his views on it.
Speaker D: But is the goal there to have this on meeting data so that you can do far-field stories of those gestures?
Speaker D: Or is it because you think there's a different kind of actual production in meetings that people use?
Speaker G: No, I think it's for that purpose.
Speaker G: I'm just viewing meetings as being a neat way to get people talking naturally.
Speaker G: And then it's natural in all senses, in the sense that you have microphones that are at a distance that one might have.
Speaker G: And you have the close mics, and you have people talking naturally.
Speaker G: And the overlap is just a digger of the fact that people are talking naturally.
Speaker G: So I think that given that it's that kind of corpus, if it's going to be a very useful corpus, if you say, OK, we've limited the use by some of our sensor choices.
Speaker G: We don't have a video, we don't, so forth.
Speaker G: But there is a lot of use that we could make of it by expanding the annotation choices.
Speaker G: And most of the things we've talked about have been fairly high level, and kind of a bomb up person.
Speaker G: I thought there'd be some of the others.
Speaker E: It's a really nice offer, those things that might range.
Speaker G: Hopefully someone would make use of it. I mean, people made a lot of use of Timit and did its markings.
Speaker G: And then the switchboard transcription thing, I think it's been very useful for a lot of people.
Speaker D: I guess I wanted to make a pitch for trying to collect more meetings.
Speaker D: Yeah.
Speaker D: Actually, I talked to Chuck Filmer, and I think they've what vehemently said no before, but this time he wasn't vehement, and he said, well, Liz, come to the meeting tomorrow and try to convince people.
Speaker D: So I'm going to try, go to their meeting tomorrow and see if we can try.
Speaker D: Because they have something like three or four different meetings.
Speaker D: And we have very interesting meetings from the point of view of a very different type of talk than we have here, and definitely than the front-end meeting, probably.
Speaker I: In terms of the topic.
Speaker D: Well, yes, and in terms of the fact that they're describing abstract things, and it just dialogue-wise, right?
Speaker D: So I'll try.
Speaker D: And then the other thing is, I don't know if this is at all useful, but I asked Lila if I can maybe go around and talk to the different departments in this building to see if there's any groups that, for free lunch, if we can still offer that.
Speaker D: You're not going to see nonexionic, academic, government, I don't know.
Speaker H: The problem is so much that it's done as confidential. It would be very hard for them.
Speaker H: Also, I think it takes its way to the point of that.
Speaker E: I mean, it seems like we had this idea before of having like linguistic students brought down for a few lectures.
Speaker D: Right, and we could also, we might try advertising again, because I think it would be good if we can get a few different sort of non-internal types of meetings, and just also more data.
Speaker I: Does John O'Hill have weekly phonetic reactions?
Speaker D: So I actually wrote to him and he answered, great, that sounds really interesting, but I never heard back, because we didn't actually advertise openly.
Speaker D: I told I'd asked him privately.
Speaker D: And it is a little bit of a trek for campus folks.
Speaker H: It would be nice if we got someone other than me who knew how to set it up to do the recording, so I didn't have to.
Speaker D: Exactly, and I was thinking.
Speaker C: He's supposed to be trained.
Speaker D: Plus, we could also get a student, and I'm willing to try to learn.
Speaker D: I mean, I would do my best.
Speaker D: The other thing is that there's a number of things at the transcription side that transcribers can do, like dialogue, tagging, this fluency tagging, things that are in the speech that are actually something we're working on for language modeling.
Speaker D: And Mari is also interested in Andreas as well.
Speaker D: If you want to process utterance, the first thing they say is, well, and that well is coded as some kind of interrupt tag, and things like that.
Speaker E: Of course, some of that can be done like so clearly.
Speaker E: And I also heard a lot of utility done to tagging.
Speaker D: Great, so a lot of this kind of, I think there is a second pass, and I don't really know what would exist in it, but there's definitely a second pass worth doing to maybe encode some kinds of, you know, is it a question or not?
Speaker D: That maybe these transcribers could do.
Speaker E: Maybe really good.
Speaker E: Well, while we're interested in just briefly to this question of more meeting data, two questions.
Speaker E: One of them is Jerry Falman's group.
Speaker E: They know that they recorded one meeting.
Speaker G: I think they're open to it.
Speaker G: I think all these things, I think there's, we should go beyond XE, but I mean there's a lot of stuff having an XE that we're not getting out of.
Speaker D: Okay, I thought that all these people had sort of said no twice already.
Speaker G: No, no, no, so there was the thing in Film Wars Group, but even there he hadn't what he'd said no to was for the main meeting, but they have several smaller meetings a week.
Speaker G: And the notion was raised before that that could happen, and just, you know,
Speaker I: and the other thing too is when they originally said no, they didn't know about this post-editing capability.
Speaker G: Yeah, so I mean there's possibilities there. I think Jerry's group, yes.
Speaker G: There's the network's group.
Speaker G: Do they still meet regularly?
Speaker H: Well, I don't know if they meet regularly or not.
Speaker H: They're no longer reporting.
Speaker G: But I mean, have they said they don't want to anymore?
Speaker H: What was his name?
Speaker H: Yeah, when with him gone, it sort of tripled off.
Speaker F: Okay, so they're down to three or four people, but the thing is three or four people is okay.
Speaker E: We might be only hearing this.
Speaker H: Well, he was sort of like contact, so I just need to find out who's running in the house.
Speaker E: Okay.
Speaker E: I see that Leva has a much meeting period.
Speaker D: Yeah, I mean, one thing that would be nice, and it sounds bizarre, but I'd really like to look at, to get some meetings where there's a little bit of heated discussion like arguments, or emotions, and things like that.
Speaker D: And so I think if there's any like Berkeley political groups, I mean that would be perfect.
Speaker D: So, yes, we might.
Speaker H: Who's really good at running in the street?
Speaker F: Well, and I saw that in the political party.
Speaker F: Yeah, with potential use from the defense department.
Speaker D: Well, maybe student groups or filmmakers or something, a little bit of color.
Speaker E: If we give them a chance to excite later, we might end up with like five minutes out of the club.
Speaker D: I don't mean that they're angry, but just something with some more variation in prasadic contours and so forth would be neat.
Speaker D: So, if anyone has ideas, I'm willing to do the legwork to go try to talk to people, but I don't really know which group there were.
Speaker E: What is this pursuing idea?
Speaker G: Yeah, there's a problem there in terms of the commercial value of the press.
Speaker G: It turned out to be a bit of a problem.
Speaker E: And I had one other aspect of this, which is John Biscuits expressed a major interest in having meetings which were all English speakers.
Speaker E: Now, he wasn't trying to shape us in terms of what we gathered, but that's what he wanted me to show him.
Speaker E: So, I'm giving him our initial meeting because he asked for all English.
Speaker E: And I think we don't have a lot of all English meetings right now.
Speaker E: Did he mean that non-British?
Speaker E: No, if he meant non-British, I think he said British was okay.
Speaker I: British is okay.
Speaker G: British is okay.
Speaker G: Well, I don't think if he didn't say that.
Speaker E: I bet he meant native speaking for a better.
Speaker F: I bet he did.
Speaker F: Oh, really?
Speaker F: That's why I wouldn't care.
Speaker I: Knowing the population.
Speaker G: I remember studying the BBN where they trained on, this was in Wall Street Journal days, or something they trained on American English, and then they tested on different native speakers from different areas.
Speaker G: And the worst match was people whose native tongue was Mandarin Chinese.
Speaker G: The second worst was British English.
Speaker G: That's fine.
Speaker F: So, it's...
Speaker F: German was much better.
Speaker F: It was Swiss.
Speaker G: So, I think if he's thinking in terms of recognition kind of technology, I think he would probably want to...
Speaker G: Are we on the issue?
Speaker G: Yeah, unless we're going to train with the whole country.
Speaker E: I think that the elements may be more that way, and they sort of feel like they have...
Speaker G: Maybe, so.
Speaker H: Yeah.
Speaker H: And maybe there were a few with us where Dan wasn't there in for us, as he worked on it.
Speaker G: It's pretty tough, this group.
Speaker G: So, what are the people who are involved in some artistic endeavor?
Speaker G: I mean, filmmaking.
Speaker G: Is that great?
Speaker D: I think that they would be...
Speaker D: Something where there is actually discussion, where there's no right or wrong answer, but...
Speaker D: But it's a matter of opinion, kind of thing.
Speaker D: Anyway.
Speaker D: Yeah, I do.
Speaker I: You're also going to get to have a political discussion.
Speaker I: Any department that calls itself science?
Speaker I: Yeah, I'm going to get a computer science.
None: I'm going to say...
Speaker D: I'm actually serious because...
Speaker D: Yeah, yeah.
Speaker D: We have the setup here, and that has...
Speaker D: Chance to give us some very interesting fun data.
Speaker D: So, if anyone has ideas, you know any groups that are...
Speaker D: Well, I guess some of the students...
Speaker D: Student groups like clubs, things like that.
Speaker G: A little ad-up saying, come here and argue.
Speaker D: If you're really angry, someone use our conference room.
Speaker H: The business school might be good.
Speaker H: I actually spoke with some students up there, and they express willingness back when they thought they would be doing research on speech.
Speaker H: But when they lost interest in speech, they also stopped answering my email about other stuff.
Speaker G: Or people who are wrong.
Speaker G: What about tax cuts or something?
Speaker G: I heard that.
Speaker G: How tech they have a special room.
Speaker K: Someone said that it's a special room to get all your frustrations out.
Speaker K: You can go through a little throw up things and break things.
Speaker K: So, we don't know if that is not actually what we...
Speaker D: Yeah, to that extent.
Speaker D: Well, a far-fledged likes can pick up where the recruits are.
Speaker F: But we don't want them to throw the far-fledged mixes.
Speaker H: Please throw everything in that direction.
Speaker H: And itself.
Speaker H: I think I think it looked good.
Speaker H: There was a dorm room at Tech that someone had coded the walls and the ceiling and the floor with mattresses.
Speaker H: The entire room.
Speaker G: I had this my fourth thing here, processing of waveforms.
Speaker G: What did we mean by that?
Speaker H: Liz wanted to talk about menacing, improving accuracy.
Speaker D: Well, I think that was just sort of an IRA-ass deal.
Speaker D: Oh, where did that?
Speaker D: But it would be helpful if I can stay in the loop somehow with people who are doing any kind of post-processing, whether it's to separate speakers or to improve the signal, to noise ratio, or both, that we can sort of try out as we're running recognition.
Speaker D: So, is that...
Speaker D: Who else's work, I guess, Dan Ellis?
Speaker D: Yeah.
Speaker G: And Dave.
Speaker G: And Dave.
Speaker G: Again, he's interested in, in fact, we're looking, starting to look at some macro-canceration kind of things.
Speaker G: Okay.
Speaker H: Which is how much that is.
Speaker H: An issue with a close talking mic.
Speaker G: Well, isn't that what you want?
Speaker D: I don't know.
Speaker G: No, so what you want, when you're saying improving waveform, you want the close talking microphone to be better.
Speaker G: And the question is, to what extent is it getting hurt by any room acoustic source, it's just given that it's close, it's not a problem?
Speaker D: It doesn't seem like big room acoustic problem to my ear, but I'm not an expert.
Speaker D: It seems like a problem with crosstalk.
Speaker H: I bet there's plenty of room acoustic.
Speaker D: That may be true, but I don't know how good it can get either by those methods.
Speaker D: That's true.
Speaker H: I think it's just, yeah, what you said.
Speaker D: All I meant is just that as sort of, as this pipeline of research is going on, we're also experimenting with different ASR techniques, and so it'd be good to know about it.
Speaker I: So the problem is like, on the microphone of somebody who's not talking, they're picking up signals from other people, and that's...
Speaker D: Right, although if they're not talking, using the in-house transcriptions, we're sort of okay because no one transcribed any words there, and we throw it out.
Speaker D: But if they're talking at all, and they're not talking the whole time, so you get some speech, and then some more speech, so that whole thing is one chunk.
Speaker D: And the person in the middle who said only a little bit is picking up the speech around it, that's where it's a big problem.
Speaker E: You know, this does seem like it would relate to someone one of those ASR, who's working on this wall, and coding them.
Speaker E: And he also...
Speaker D: The energy, right?
Speaker E: Exactly.
Speaker E: I was trying to remember you had this interface where you showed us one time when you left off that you had different visual displays.
Speaker A: Yeah.
Speaker A: I only displayed different colors for the different situation, but for me, for my problems, it's enough, because it's possible in a simple view to compare with the equipment, the kind of assessment, what happened with the different parameters, only with different bands of colors for the few situations I consider for acoustic even, is enough to...
Speaker A: I see that you are considering now a very sophisticated set of graphics, as symbols to transcribe, no?
Speaker A: A lot.
Speaker A: Because before you are talking about the...
Speaker A: the possibility to include in the transcribed word program, a set of symbol, or a fixed symbol, to market the different situation during the transition.
Speaker E: So symbols for differences between lab and cell, and...
Speaker E: Yes.
Speaker A: The symbol you talk before, no?
Speaker A: To...
Speaker E: To mal...
Speaker E: The symbols so much.
Speaker E: The main change that I see in the interface is just that we'll be able to...
Speaker E: to more money in time and things.
Speaker E: But I...
Speaker E: There was another aspect of you, I was thinking about this topic, which is that it's not much to me, but it's not to me, so partly inels, is that you're doing involves taking segments, which are of a particular type, and putting them together.
Speaker E: And so if you have like a...
Speaker E: You know, a speech from one speaker, then you cut out the part that's not that speaker, and you combine segments from that same speaker to...
Speaker E: and run them through the recognize as that.
Speaker D: Well, we try to find as close of start and end time as we can to the speech from an individual speaker, because then we're more guaranteed that the recognizer will...
Speaker D: for the forced alignment, which is just to give us the time boundaries, because from those time boundaries then the plan is to compute prosotic features.
Speaker D: And the sort of more space you have that isn't the thing you're trying to align, the more errors we have.
Speaker D: So, you know, that it would help to have either a pre-processing of a signal that creates very good signal noise ratio, which I don't know how possible this is for the lapel, or to have closer time, you know, synced times, basically, around the speech that gets transcribed or both.
Speaker D: And it's just sort of an open world right now of exploring that.
Speaker D: So I just wanted to see, you know, on the transcribing end, from here, things look good.
Speaker D: The IBM one is more... is an open question right now, and then the issue of like global processing of some signal, and then, you know, before we chop it up, is yet another way we can improve things.
Speaker I: What about increasing the flexibility of the alignment?
Speaker I: Do you remember that thing that Michael Fink had did? That experiment? He did a wild bath?
Speaker D: Right. You can...
Speaker D: The problem is just that the acoustic, when the signal to noise ratio is too low, you'll get an alignment with the wrong duration path.
Speaker D: So that's the problem, is the... Yeah, it's not the fact that you have like...
Speaker D: I mean, what he did is allow you to have words that were in another segment, move over to the... at the edges of segmentation.
Speaker D: Or even words inserted that weren't there.
Speaker D: Right. Things here, the boundaries, where if you got your alignment wrong, because what they had done there is a line and then chop.
Speaker D: And this problem is a little bit more global.
Speaker D: It's that there are problems even inside the alignments, because of the fact that there's enough acoustic signal there to recognize or to eat as part of a word, and it tends to do that.
Speaker D: So...
Speaker D: But we'll probably have to do something like that in addition.
Speaker D: Anyway, so yeah, bottom line is just I wanted to make sure I can be aware of whoever's working on these signal processing techniques for detecting energies because that'll really help us.
Speaker G: Okay, T is started out there, I suggest we went through our digits.
Speaker G: So...
Speaker G: Transcript 3031-3050.
Speaker G: 0368.
Speaker G: 04960.
Speaker G: 17050293462.
Speaker G: 7204640.
Speaker G: 8415281.
Speaker G: 9.
Speaker G: 0937762.
Speaker G: 0.
Speaker G: 109376028.
Speaker G: 425495.
Speaker G: 677890.
Speaker H: Transcript 30713090.
Speaker H: 004217337.
Speaker H: 4325.
Speaker H: 5639826.
Speaker H: 670740.
Speaker H: 810.
Speaker H: 051406.
Speaker H: 623964.
Speaker H: 507971.
Speaker H: 7176171.
Speaker H: 8503872.
Speaker H: 9704299.
Speaker H: 099093.
Speaker I: Transcript 3111-3130.
Speaker I: 2878024.
Speaker I: 304080.
Speaker I: 6316884.
Speaker I: 76685989.
Speaker I: 010665.
Speaker I: 33496408.
Speaker I: 590.
Speaker I: 689798.
Speaker I: 804.
Speaker I: 990.
Speaker I: 03.
Speaker I: 160281.
Speaker B: Transcript 3151-3170.
Speaker B: 4499.
Speaker B: 550606780.
Speaker B: 0770394.
Speaker B: 0470616824.
Speaker B: 2939.
Speaker B: 4507608494.
Speaker B: 5107260.
Speaker B: 84084567101.
Speaker B: 0810101.
Speaker B: 20184264019.
Speaker J: Transcript 3333113333.
Speaker J: 0671307474.
Speaker J: 0723864507284.
Speaker J: 96045050081.
Speaker J: 1 2 4 7 397.
Speaker J: 5 4 8 2 6 6 4 5 7.
Speaker J: 7 9 8 8 9 071231.
Speaker D: Transcript 28912910.
Speaker D: 4185277240.
Speaker D: 65347.
Speaker D: 7 9 1 6 8 6 3.
Speaker D: 9015.
Speaker D: 012524.
Speaker D: 3475840.
Speaker D: 457.
Speaker D: 56094.
Speaker D: 7056932.
Speaker D: 9509.
Speaker D: 06031910523.
Speaker A: Transcript 2931.
Speaker A: That's 2950.
Speaker A: 503.
Speaker A: 6 81690008.
Speaker A: 91987.
Speaker A: Old 605.
Speaker A: 048.
Speaker A: 028.
Speaker A: 4123.
Speaker A: 053185216.
Speaker A: 5607400.
Speaker A: 8 9 309 9 9 6 4 4.
Speaker A: 0 0 3752110889.
Speaker A: 3 14 5 0 7 4.
Speaker A: 5 6 6 3.
Speaker E: Transcript 291-3010.
Speaker E: 8 5 9 0 0 1 3 15 0 5 4 4 0 9 4 2.
Speaker E: 5 6 7 4 6 8 3 6 7 7 8 4 8 9 0 1.
Speaker E: 0 1 6 1 3 5 0 1 5 7 2 8 9.
Speaker E: 8 8 9 3 4 5 0 8 7 4 7 1 1 8 4 0 305 1.
Speaker A: Did your feel go back?
