Speaker C: Channel 3.
Speaker I: Yes.
Speaker I: Okay.
None: Channel 3.
Speaker I: Channel 3.
Speaker G: Okay.
Speaker G: Did you sell speech recognition last week?
Speaker G: Alright.
Speaker G: That's the image processing.
Speaker G: Yes, again.
Speaker H: We did it again, Morgan.
Speaker G: Alright.
Speaker A: Okay.
Speaker G: It's April 5th.
Speaker G: Actually, he next to be getting back in town shortly, if he hasn't already.
Speaker G: Is he going to come here?
Speaker G: Well, we'll drag him here.
Speaker H: So when you sit in town, you mean, Oregon?
Speaker G: Hey, man, you know this end of the world.
Speaker G: That's really what I meant.
Speaker G: He's been in Europe.
Speaker G: So.
Speaker H: I have something just fairly brief to report on.
Speaker H: I did some experiments, just a few more experiments before I had to go away for that week.
Speaker H: Was it last week or whatever?
Speaker H: So what I was starting playing with was, again, this is the HTK back end.
Speaker H: I was curious because the way that they train up the models, they go through about four sort of rounds of training.
Speaker H: And in the first round, they do, I think it's three iterations.
Speaker H: And for the last three rounds, they do seven iterations of re-estimation in each of those three.
Speaker H: And so, you know, that's part of what takes so long to train the back end for this.
Speaker H: And so, you know, it's the first one that's been in the first three rounds of training.
Speaker H: I guess you could say iterations.
Speaker H: The first one is three, then seven, seven, and seven.
Speaker H: And what these numbers refer to is the number of times that the HMM re-estimation is run.
Speaker G: It's this program called HE Rest.
Speaker H: So what happens is, at each one of these points, you increase the number of Gaussian's in the model.
Speaker G: Oh, right. This was the mix up.
Speaker H: Yes, the mix up.
Speaker H: And so, in the final one here, you end up with, for all of the digit words, you end up with three mixtures per state.
Speaker H: In the final thing. So I had done some experiments where I was, I want to play with the number of mixtures.
Speaker H: But I wanted to first test to see if we actually need to do this many iterations early on.
Speaker H: And so, I ran a couple of experiments where I reduced that to be three, two, two, five, I think.
Speaker H: And I got almost the exact same results.
Speaker H: But it runs much, much faster.
Speaker H: So I think it only took something like three or four hours to do the full training.
Speaker H: As opposed to...
Speaker H: As opposed to what, 16 hours or something like that?
Speaker H: I mean, you have to do an overnight, basically, the way it is set up now.
Speaker H: So, even if we don't do anything else, doing something like this could allow us to turn experiments around a lot faster.
Speaker G: And then when you have your final thing...
Speaker H: And when you have your final thing, we go back to this.
Speaker H: So, and it's a real simple change to make.
Speaker H: I mean, it's like one little text file you edit and change those numbers.
Speaker H: And you don't do anything else. And then you just run.
Speaker H: So it's a very simple change to make and it doesn't seem to hurt all that much.
Speaker C: So...
Speaker C: You run with three, two, two?
Speaker H: I have to look to see what the exact numbers were.
Speaker H: I thought it was like three, two, two, five. But I'll double check.
Speaker H: It was over a week ago that I did it, so I can't remember exactly.
Speaker H: But it's so much faster.
Speaker H: It makes a big difference.
Speaker H: So we could do a lot more experiments and throw a lot more stuff in there.
Speaker H: That's great.
Speaker H: Oh, the other thing that I did was...
Speaker H: I compiled the HTK stuff for the Linux boxes.
Speaker H: So we have this big thing that we got from IBM, which is a five processor machine, really fast.
Speaker H: But it's running Linux. So you can now run your experiments on that machine.
Speaker H: And you can run five at a time and it runs as fast as five different machines.
Speaker H: So I've forgotten now what the name of that machine is, but I can send an email around about it.
Speaker H: And so we've got it now.
Speaker H: HTK is compiled for both the Linux and for the sparks.
Speaker H: You have to make sure that in your.cshrc it detects whether you're running on a Linux or a spark and points to the right executables.
Speaker H: And you may not have had that in your.cshrc before if you were always just running the spark.
Speaker H: So I can tell you exactly what you need to do to get all of that to work.
Speaker H: But it'll really increases what we can run on.
Speaker H: So together with the fact that we've got these faster Linux boxes and it takes less time to do these, we should be able to crank through a lot more experiments.
Speaker H: So after I did that, then what I wanted to do was try increasing the number of mixers just to see how that affects performance.
Speaker G: Yeah, in fact, you could do something like keep exactly the same procedure and then add a fifth thing onto it.
None: Exactly.
Speaker A: So at the middle where the arrows are showing, that's you're adding one more mixture per state?
Speaker H: Let's see, it goes from this, let's try to go backwards. At this point it's two mixtures per state.
Speaker H: So this just adds one except that actually for the silence model, it's six mixtures per state.
Speaker H: So it goes to two. And I think what happens here is...
Speaker G: Might be between a shared...
Speaker H: Yeah, I think that's what it is or something.
Speaker H: Yeah, it's...
Speaker H: I can't remember now what happens at that first one. I have to look it up and see.
Speaker H: Because they start off with an initial model, which is just this global model, and then they split it to the individuals.
Speaker H: And so it may be that that's what's happening here. I have to look it up and see. I don't exactly remember.
Speaker H: So that's it.
Speaker G: Right. So what else?
Speaker C: Yeah, there was a conference call this Tuesday.
Speaker C: I don't know yet what happened Tuesday, but the points that they were supposed to discuss is still things like the weights.
Speaker G: Oh, this is a conference call for Aurora participants, sort of thing. I see.
Speaker G: Do you know who was... since we weren't in on it, do you know who was in from OGI?
Speaker G: Was he involved or was it Sunil?
Speaker G: Yeah, right.
Speaker D: Oh, you don't know.
Speaker D: Okay.
Speaker C: All right.
Speaker C: Yeah.
Speaker C: So the points where the weights...
Speaker C: Oh, to weight, the different error rates that are obtained from different languages and conditions.
Speaker C: It's not clear that they will keep the same kind of weighting. Right now it's a weighting on improvements.
Speaker C: Some people are giving that it would be better to have weights on...
Speaker C: Well, to combine error rates before computing improvement.
Speaker C: And the fact is that right now for the English, they have weights... they combine error rates.
Speaker C: And the other language is they combine improvements. It's not the very consistent.
Speaker C: Yeah.
Speaker C: And so, well, this is a point.
Speaker C: And right now, actually, there is a thing also that happens with the current weight.
Speaker C: It's that very non-significant improvement on the well-matched case, resulting in huge differences in the final number.
Speaker C: So, perhaps they will change the weights to...
Speaker H: How should that be done?
Speaker H: I mean, it seems like there's a simple way...
Speaker H: This seems like an obvious mistake or something.
Speaker G: Well, I mean, the fact that it's inconsistent is an obvious mistake.
Speaker G: But the other thing... I don't know, I haven't thought of through, but one would think that each...
Speaker G: It's like, if you say, what's the best way to do an average, an arithmetic average or geometric average?
Speaker G: It depends what you want to show.
Speaker G: Each one is going to have a different characteristic.
Speaker H: So...
Speaker H: It seems like they should do like the percentage improvement or something, rather than the absolute improvement.
Speaker H: Well, they are doing that.
Speaker G: No, it is relative.
Speaker G: But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement of that?
Speaker G: And the thing is, it's not just a period average because of these ratings.
Speaker G: It's a weighted average.
Speaker C: Yeah, and so, when you average the relative improvement, it tends to give a lot of importance to the when-match case, because the baseline is already very good.
Speaker H: Why don't they not look at improvements, but just look at your scores.
Speaker H: Figure out how to combine the scores with a weight or whatever, and then give you a score.
Speaker H: Here's your score.
Speaker H: And then they can do the same thing for the baseline system. Here's its score.
Speaker H: And then you can look at...
Speaker G: Well, that's what you're seeing as one of the things they could do.
Speaker G: Just when you get all done, I think that they...
Speaker G: I was in there, but I think they started off this process with an ocean that you should be significantly better than the previous standard.
Speaker G: And so they said, how much is significantly better and so they said, well, you should have half the errors or something that you had before.
Speaker G: So it's...
Speaker G: But it does seem like...
Speaker G: It does seem like it's more logical to combine them first and then do the...
Speaker C: Combining error rates?
Speaker C: Yeah.
Speaker C: Yeah.
Speaker C: Well, but there is still this problem of ways.
Speaker C: When you combine error rates, it tends to give more importance to the difficult cases.
Speaker C: And some people think that they have different opinions about this.
Speaker C: The people think that it's more important to look at 12-10% relative improvement on well-matched cases than 12-50% on the mismatched.
Speaker C: And other people think that it's more important to improve a lot on the mismatch.
Speaker H: It sounds like they don't really have a good idea about what the final application is going to be.
Speaker G: Well, you know, the thing is that if you look at the numbers on the more difficult cases, if you really believe that was going to be the predominant use, none of this would be good enough.
Speaker G: Nothing anybody...
Speaker G: Whereas you sort of, with some reasonable error recovery, could imagine in the better cases these systems working.
Speaker G: So, I think the hope would be that it would work well for the good cases and it would have reasonable soft degradation as you got to worse and worse conditions.
Speaker H: Yeah.
Speaker H: I guess what I'm...
Speaker H: I mean, I was thinking about it in terms of if I were building the final product and I was going to test to see which front end I wanted to use.
Speaker H: Try to wait things depending on the exact environment that I was going to be using the system in if I...
Speaker G: But no, no, no. I mean, this isn't the operating theater.
Speaker G: I mean, they don't really know, I think.
Speaker H: So, if they don't know, doesn't that suggest a way for them to go?
Speaker H: You assume everything's equal. I mean, you...
Speaker G: Well, I think one thing to do is to just not rely on a single number to maybe have three numbers.
Speaker G: And say, here's how much you improve the relatively clean case.
Speaker G: And here's a real match case and here's how much you...
Speaker H: So, not try to combine them.
Speaker G: Yeah, actually it's true. I've forgotten this.
Speaker G: But, well matched, it's not actually clean. What it is is just that...
Speaker G: The training and testing are similar.
Speaker G: So, I guess what you would do in practice is you try to get as many examples of similar sort of stuff as you could.
Speaker G: And then...
Speaker G: So, the argument for that being the more important thing is that you're going to try and do that.
Speaker G: But you want to see how badly it deviates from that when the...
Speaker H: So...
Speaker H: So, you should wait those other conditions very, you know, really small.
Speaker H: But, no, that's more of an information kind of thing.
Speaker G: That's an argument for it. Let me give you the opposite argument.
Speaker G: The opposite argument is you're never really going to have a good sample of all these different things.
Speaker G: Meaning, are you going to have examples with Windows open, half open, full open, going 70, 60, 50, 40 miles an hour on what kind of roads, with what passing you, with...
Speaker G: I think that you could make the opposite argument that the well-matched case is a fantasy.
Speaker G: So, I think the thing is that if you look at the well-matched case versus the medium and the mismatched case, we're seeing really, really big differences in performance, right?
Speaker G: And you wouldn't like that to be the case.
Speaker G: You wouldn't like this. As soon as you step outside, you know, a lot of the cases...
Speaker H: A lot of the cases... A little teach him to roll their window up.
Speaker G: I mean, these cases, if you go from the... I mean, remember the numbers right off, but if you go from the well-matched case to the medium, it's not an enormous difference in the training testing situation.
Speaker G: And it's a really big performance drop.
Speaker G: So, yeah, I mean, the reference one, for instance, this is back old on Italian, was like 6% of the error for the well-matched and 18% for the medium matched and 60% for the...
Speaker G: for how they mismatched.
Speaker G: And, you know, these other systems, we helped it out quite a bit, but still there's something like a factor or two or something between well-matched and medium matched.
Speaker G: And so, I think that if what you're... if the goal of this is to come up with robust features, it does mean...
Speaker G: so you could argue, in fact, that the well-matched is something you shouldn't be looking at at all.
Speaker G: That the goal is to come up with features that will still give you reasonable performance.
Speaker G: You know, it's again, gentle degradation, even though the testing condition is not the same as the training.
Speaker G: So, you know, I could argue strongly that something like the medium mismatch, which is, you know, not pathological, but...
Speaker G: What was the medium mismatch condition again?
Speaker C: Yeah, medium mismatch is everything with the far microphone, but trained on low-noisy condition, low speed and stop-car, and tested on high-speed conditions.
Speaker G: Right, so it's still the same microphone in both cases, but there's a mismatch between the car conditions.
Speaker G: And that's... you could argue that's a pretty realistic situation, and I'd almost argue for waiting that highest, but the way they have it now, I guess it's... they compute the relative improvement first and then average that with a waiting.
Speaker G: And so then that makes the highly matched, the really big thing.
Speaker G: So, since they have these three categories, it seems like the reasonable thing to do is to go across the languages and to come up with an improvement for each of those.
Speaker G: And say, okay, in the highly matched case, this is what happens, in the... the... so the medium, if this happens, in the highly mismatched, that happens.
Speaker G: And you should see a gentle degradation through that.
Speaker G: But I think that... I gather that in these meetings, it's really tricky to make anything... can he policy change?
Speaker G: Because everybody has their own opinion.
Speaker E: Yeah.
Speaker C: So, yeah. But there is probably a big change that we made is that the baseline, they want to have a new baseline, perhaps, which is the MFCC, but with voice activity detector.
Speaker C: And apparently, some people are pushing to still keep this 50% number, so they want to have at least 50% improvement on the baseline, but it should be a much better baseline.
Speaker C: And if we look at the result that's summing the sound, just putting the VAD in the baseline improves like more than 20%, which would mean that 50% on this new baseline is like more than 60% improvement.
Speaker G: So, nobody would be there, probably, right?
Speaker C: Right, nobody would be there.
Speaker G: Good. What to do.
Speaker G: So, who's VAD? Is this...
Speaker C: They didn't decide yet. I guess this was one point of the conference.
Speaker C: But...
Speaker C: Yeah.
Speaker G: I think that would be good. I mean, it's not that the design of the VAD isn't important, but it does seem to be a lot of work to do a good job on that, as well as being a lot of work to do a good job on the feature design.
Speaker G: So, if we can cut down on that, maybe we can make some progress.
Speaker C: But I guess, perhaps...
Speaker C: Yeah. So, one told that perhaps it's not fair to do that because to make a good VAD, you don't have enough to do the business features.
Speaker C: So, you really need to put more in the different things.
Speaker G: Yeah.
Speaker H: Sure. But I'm confused. What do you mean?
Speaker G: Yeah. But, let's say, MFCC, for instance, doesn't have anything related to the pitch. So, just for example. So, suppose you've got what you really want to do is put a good pitch detector on there, and if it gets a non-imbeguous...
Speaker G: Oh, I see.
Speaker G:...if it gets a non-imbeguous result, then you definitely in a region with speech.
Speaker H: So, there's this assumption that the voice activity detector can only use the MFCC?
Speaker C: That's not clear, but...
Speaker G: Well, for the baseline.
Speaker G: Yeah. So, if you use other features, then it's just a question of what is your baseline?
Speaker G: What is it that you're supposed to do better than? And so, having the baseline be the MFCCs means that people could choose to pour their effort into trying to do really good VAD.
Speaker G: But they seem like two separate issues, right?
Speaker G: I mean, there's sort of separate. Unfortunately, there's coupling between them, which is part of what I think Stefan is getting to is that you can choose your features in such a way as to improve the VAD.
Speaker G: And you also can choose your features in such a way as to improve recognition.
Speaker G: But you should do both, right?
Speaker G: You should do both. And I think that this still makes...
Speaker G: I still think this makes sense as a baseline.
Speaker G: It's just saying, as a baseline, we know that we had the MFCCs before, lots of people have done voice activity detectors.
Speaker G: You might as well pick some voice activity detector and make that the baseline, just like you picked some version of HDK and made that the baseline.
Speaker G: And then let's try to make everything better. And if one of the ways you make it better is by having your features be better features for the VAD than that's so be it.
Speaker G: But at least you have a starting point that's...
Speaker G: Because some of the people didn't have a VAD at all, I guess, right? And then they look pretty bad.
Speaker G: And in fact, what they were doing wasn't so bad at all.
Speaker H: Yeah, it seems like you should try to make your baseline as good as possible.
Speaker H: And if it turns out that you can't improve on that, well, I mean, nobody wins and you just use MFCC, right?
Speaker G: Yeah, I mean, it seems like...
Speaker G: It should include sort of the current state of the art that you want to try and improve.
Speaker G: And MFCCs, or PLP or something, it seems like a reasonable baseline for the features.
Speaker G: And anybody doing this task is going to have some sort of voice activity detection at some level.
Speaker G: Some way they might use the whole recognizer to do it, rather than a separate thing.
Speaker G: But they'll have it on some level.
Speaker H: Seems like whatever they choose, they shouldn't purposefully brain damage a part of the system to make a worse baseline.
Speaker G: Well, I think people just... it wasn't that they purposefully brain damage, I think people hadn't really thought through about the VAD issue.
Speaker G: And then when the proposals actually came in, half of them had VADs and half of them didn't.
Speaker G: And half the did did well and half the didn't did poorly so.
Speaker C: Yeah, I'm sorry, we see what happened with this.
Speaker C: Yeah, so what happened since last week is quite from BOTGID's experiments on VAD on the baseline.
Speaker C: And these experiments also are using some kind of noise compensation, so spectra-suppraction.
Speaker C: And putting on nine normalization just after this.
Speaker C: So having spectra-suppraction, LDA filtering and on nine normalization.
Speaker C: So which is similar to the proposal one, but with spectra-suppraction in addition.
Speaker C: And it seems that on nine normalization doesn't help further when you have spectra-suppraction.
Speaker H: Is this related to the issue that you brought up a couple of meetings ago with the musical tones?
Speaker C: I have no idea because the issue I brought up was with a very simple spectra-suppraction approach.
Speaker C: The one that they use at OGI is one from the proposed or a proposal, which might be much better.
Speaker C: So yeah, I asked Sunil for more information about that, but I don't know yet.
Speaker C: And what's happened here is that we have this kind of new reference system which use a nice clean-down sampling of sampling, which uses a new filter that's much shorter, and which also gets the frequency bit of 64 hours, which was not done for the proposal.
Speaker C: When we say we have that, Sunil, have it now too or? No. No. Okay. Because we're still testing. So we have the result for just the features.
Speaker C: We are currently testing with putting the neural network into Kerti.
Speaker C: It seems to improve on the well-matched case, but it's a little bit worse on the mismatched, highly mismatched.
Speaker C: I mean, when we put the neural network, and with the current weight thing, I think it would be better because the one-matched case is better.
Speaker F: But how much worse since the weighting might change? How much worse is it on the other conditions when you say it's a little worse?
Speaker C: It's like... 10% relative.
Speaker G: Okay. But it has the latency is much shorter.
Speaker C: When I say it's worse, it's not... I compare proposal 2 to proposal 1.
Speaker C: Putting neural network compared to not having any neural network. This new system is better because as this 64 hours cutoff, clean, don't sampling, and what else? Yeah, good VAD.
Speaker F: But the latency... but you've got latency shorter now. Yeah. So it's better than the system that we had before.
Speaker C: Yeah, mainly because of the 64 hours and the good VAD.
Speaker C: And then I took this system and we put the old filters also. So we have this good system with good VAD, with the short filter and with the long filter.
Speaker C: And with the short filter it's not worse. So why is it?
Speaker G: Okay, so that's all fine. But what you're saying is that when you do these... let me try to understand. When you do these same improvements to proposal 1, that on the things are somewhat better in proposal 2 for the well-matched case and somewhat worse for the other two cases.
Speaker G: So now that these other things are in there, is it the case maybe that the additions of proposal 2 over proposal 1 are less important?
Speaker C: Yeah, probably.
Speaker C: Okay. So yeah, but it's a good thing anyway to have shorter delay. Then we try to do something like proposal 2, but using also have the G features.
Speaker C: So there is this G L T part which is just standard features and then junior 2 neural networks.
Speaker C: And it doesn't seem to help. However, we just have one result which is the Italian mismatch.
Speaker F: Okay. There was a start of some effort and something related to voicing or something.
Speaker C: Yeah. So basically we try to find good features that will be useful voicing detection.
Speaker C: But it's still on the... basically we're still paying with my laptop.
Speaker H: What sorts of features are you looking at?
Speaker C: So we would be looking at the variance of the spectrum of the excitation, something like this, which is really I have a voiced sound.
Speaker H: What does that mean? The variance of the spectrum of excitation?
Speaker C: Yeah. So basically the spectrum of excitation.
Speaker G: Okay. What you're calling the excitation is that recall is you're subtracting the male filter spectrum from the FFT spectrum.
Speaker C: So we have the male filter bank, we have the FFT.
Speaker G: So it's not really an excitation, but it's something that hopefully tells you something about the excitation.
Speaker C: Yeah, that's right.
Speaker B: Yeah.
Speaker B: There's some histograms.
Speaker B: Yeah, that's...
Speaker C: Yeah, so for a voiced portion we have something that has a mean around 0.3 and for a voiced portion the mean is 0.59.
Speaker C: But the variance seems quite...
Speaker H: How do you know... how did you get your voiced and unvoiced truth data?
Speaker C: We used a timet and we used the cannon guns between the phones and...
Speaker B: But if we look at one send, apparently it's good.
Speaker C: Yeah, but yeah.
Speaker C: So it's noisy timet, that's right.
Speaker C: It seems quite robust to know, so when we take mid-row, this parameter across time for the same values and that's very close.
Speaker C: Yes, so there is this, there will be also the...
Speaker C: Something like the maximum of the other relation functions.
Speaker H: Is this a trained system or is it a system where you just pick some thresholds?
Speaker C: How does it work?
Speaker C: Right now we're just trying to find some features.
Speaker C: Hopefully I think what we want to have is to put these features in kind of...
Speaker C: Well, to obtain a statistical model of these features, just to use an neural network.
Speaker C: Hopefully these features would help.
Speaker H: Because it seems like what you said about the mean of the voiced and the unvoiced, that seemed pretty encouraging.
Speaker G: Well, yes, except the variance was big.
Speaker H: Well, I don't know that I would trust that so much because you're doing these canonical mappings from timet lablings, right?
Speaker H: So really that's sort of a cartoon picture about what's voiced and unvoiced.
Speaker H: So that could be giving you a lot of variance.
Speaker H: It may be that you're finding something good and that the variance is sort of artificial because of how you're getting your truth.
Speaker G: Yeah, but another way of looking at it might be that, I mean, we are coming up with feature sets after all.
Speaker G: So another way of looking at it is that the melkepster, melke spectrum, melkepster, any of these variants, give you this smooth spectrum.
Speaker G: It's a spectral envelope.
Speaker G: By going back to the FFT, you're getting something that is more like the raw data.
Speaker G: So the question is, what characterization, and you're playing around with this, another way of looking at it, is what characterization of the difference between the raw data and the smooth version is something that you're missing that could help.
Speaker G: So I mean, looking at different statistical measures of that difference, coming up with some things and just trying them out and seeing if you add them onto the feature vector, is that make things better or worse in noise.
Speaker G: Where you really just, the way I'm looking at it is not so much you're trying to find the best world's best voiced unvoiced classifier, but it's more that, you know, try some different statistical characteristics of that difference back to the raw data.
Speaker G: Right.
Speaker G: And maybe there's something there that the system can use.
Speaker C: Yeah, but the more of use is that, the more of use is that, well, using the FFT, you just give you just information about if it's voiced or not voiced mainly.
Speaker C: Yeah.
Speaker C: This is why we started to look.
Speaker G: Well, that's the way what I'm arguing is that, yeah, I mean, what I'm arguing is that that that's gives you your intuition.
Speaker G: But in reality, it's, you know, there's all this, this overlap and so forth.
Speaker G: And, but what I'm saying is that maybe okay, because what you're really getting is not actually voiced or sound voiced, both for the fact the reason of the overlap and then, you know, structural reasons like the one the Chuck said, that in fact, well, the data itself is, that you're working with is not perfect.
Speaker G: So, I'm saying is maybe that's not a killer because you're just getting some characterization, one that's driven by your intuition about voiced and voiced certainly, but just some characterization of something back in the, in the, in the almost raw data rather than the smooth version.
Speaker G: Your intuition is driving you towards particular kinds of statistical characterizations of what's missing from the spectra envelope.
Speaker G: Obviously, something about the excitation.
Speaker G: And what is it about the excitation?
Speaker G: And, you know, and you're not getting the excitation anyway, you know, so, so I would almost take especially if these trainings and so forth or faster, but almost just take a scatter shot.
Speaker G: A scatter shot at a few different ways of look of characterizing that difference and you have one of them, but, and see, you know, which of them helps?
Speaker H: So, is the idea that you're going to take whatever features you develop and just add them on to the feature vector? Or what's the use of the voice, unvoiced detector?
Speaker C: I guess we don't know exactly yet, but, yeah. It's not part of a VAD system that you're doing?
Speaker C: Oh, okay.
Speaker C: Yeah, it could be, it could be a neural network that does voice and voice detection, so the big neural networks that does, on the specification.
Speaker G: But each one of the mixture components, I mean, you have variants only, so it's kind of like you're just multiplying together these, probably some individual features within each mixture.
Speaker G: So it's so...
Speaker H: I think it's an 8 thing. It seems like a good idea.
Speaker G: Yeah.
Speaker G: Yeah, I mean, I know that people doing some robustness things are always back, we're just doing, just being gross, just throwing in the FFT and actually it wasn't so bad.
Speaker G: So, and you know that it's got to hurt you a little bit to not have a smooth spectral envelope, so there must be something else that you get in return for that.
Speaker H: So, how does, maybe I'm going into much detail, but how exactly do you make the difference between the FFT and the smooth spectral envelope?
Speaker H: Well, yeah, how is that?
Speaker C: We just...
Speaker B: We have the 23 coefficient of after the FFT and we understand this coefficient between the other frequency range and the interpolation between the point is given for the triangular filter, the value of the triangular filter.
Speaker B: And these weight we obtained is mod.
Speaker G: So, you essentially take the values that you get in the triangular filter and extend them, just sort of like a rectangle.
Speaker G: That's at that value.
Speaker C: So, yeah, we have one point for one energy for a filter bank, which is the energy that's centroid on the surface.
Speaker H: So, you end up with a vector that's the same length as the FFT vector, and then you just compute differences and sum the differences.
Speaker C: And I think the variance is computed only from like 200 hertz to 1500, because...
Speaker C: Right.
Speaker C:...1500, because yeah.
Speaker C:...20000.
Speaker C: Above seems that some voices on can add also like noisy part on the African.
Speaker G: Yeah, but no, it's being sensed to look at.
Speaker G: So, this is...
Speaker H:...basically.
Speaker H: Basically, this is comparing an original version of a signal to a smoothed version of the same signal.
Speaker G: Right. So, this is...
Speaker G: I mean, you could argue about whether it should be linear interpolation or zero-thord or...
Speaker G:...anyway, something like this is what you're feeding your recognizer, typically.
Speaker G: Like which...
Speaker G: No, so the male capstream is the...
Speaker G:...capstream of this spectrum, or the long spectrum, whatever.
Speaker G: You're subtracting in...
Speaker G:...power domain or log domain.
Speaker G: Okay, so it's sort of like division.
Speaker G: You do that, you have this vector.
Speaker G: So, ratio?
Speaker G: Yeah.
Speaker G: But anyway...
Speaker H:...and that's...
Speaker H: So, what's the intuition behind this kind of a thing?
Speaker H: I don't really know the signal processing well enough to understand what...
Speaker H:...what is that doing?
Speaker C: Yeah, like the sub-exam.
Speaker C: What we would like to have is some spectrum of the excitation signal, which is for a very strong, ideally, a full strain.
Speaker C: And for a voiced, it's something that's more flat.
Speaker C: Right.
Speaker C: And the way to do this is that, well, we have the FFT because it's computing the system.
Speaker C: And we have the male filter bands.
Speaker C: And so, if we like remove the male filter band from the FFT, we have something that's close to the excitation signal.
Speaker C: Okay.
Speaker C: Something that's like a train of both strain, a voiced sound, and that's...
Speaker C: I see.
Speaker H: So, do you have a picture that...
Speaker H:...is this for a voiced segment, this picture?
Speaker H: Yeah.
Speaker H: What does it look like for unvoiced?
Speaker B: No, I'm voicing over.
Speaker B: Oh, thanks.
Speaker B: So, you know...
Speaker C: This is the...
Speaker B: This is another voiced accent.
Speaker B: You know, is this part between the frequency that we are considered for the situation, for the difference, and this is the difference.
Speaker H: This is the difference, okay?
Speaker C: Yeah, because we begin in 15.
Speaker H: So...
Speaker H: Does the periodicity of this signal say something about the pitch?
Speaker H: The pitch?
Speaker H: Yeah.
Speaker G: That's like fundamental frequency.
Speaker G: So, I mean...
Speaker G: Day-on-see.
Speaker G: I mean, to first order, what you're doing, you can ignore all the details and all the ways which is these are complete lies, that what you're doing in future extraction for speech recognition is you have, in your head, a simplified production model for speech, in which you have a periodic, a great periodic source of strivings and filters.
Speaker G: First order for speech recognition, you say, I don't care about the source, right?
Speaker G: So, you just want to find out what the filters are.
Speaker G: The filters, roughly act like an overall resonances and so forth, that's prositing the excitation.
Speaker G: So, if you look at the spectral envelope, just the very smooth properties of it, you get something closer to that.
Speaker G: And the notion is, if you have the full spectrum of all the little minigritty details, that that has the effect of both, and it would be a multiplication in frequency domain, so that would be like an addition in log, a spectrum domain.
Speaker G: And so, this is saying, well, if you really do have that, so a vocal tract envelope, and you subtract that off, what you get is the excitation.
Speaker G: I call that lies because you don't really have that.
Speaker G: You just have some kind of signal processing trickery to get something that's kind of smooth.
Speaker G: It's not really what's happening in the vocal tract.
Speaker G: So, you're really getting the vocal excitation.
Speaker G: That's why I was referring to it in a more conservative way when I was saying, well, it's the excitation.
Speaker G: It's not really the excitation.
Speaker G: It's whatever it is that's different between...
Speaker G: So, standing back from that, you sort of say there is this very detailed representation.
Speaker G: You go to a smooth representation.
Speaker G: You go to a smooth representation because it typically generalizes better.
Speaker G: But, whenever you smooth, you lose something.
Speaker G: So, the question is, have you lost something you can use?
Speaker G: Probably you wouldn't want to go to the extreme of just saying, okay, our features that will be the FFT.
Speaker G: Because we really think we do gain something in robustness from going to something smoother.
Speaker G: But maybe there's something that we missed.
Speaker G: So, what is it?
Speaker G: And then you go back to the intuition that, well, you don't really get the excitation, but you get something related to it.
Speaker G: And as you can see from those pictures, you do get something that shows some periodicity in frequency.
Speaker G: And sometimes.
Speaker G: That's really nice.
Speaker H: So, you don't have one for unvoiced picture?
Speaker B: No, I don't think so.
Speaker G: But presumably you'll see something that won't have this kind of regularity and frequency.
Speaker H: I would like to see those pictures.
Speaker H: Yeah.
Speaker G: Yeah.
Speaker H: And so, you said this is pretty doing this kind of thing.
Speaker H: It's pretty robust to noise.
Speaker C: It seems, yeah.
Speaker C: The mean is different.
Speaker B: Because the histogram is different.
Speaker C: And I know that the kind of robustness to noise, so if you take this frame from the noisy utterance, and the same frame from the king utterance,
Speaker H: do you end up with a similar difference over here? Yeah.
Speaker H: Okay.
Speaker B: Cool.
Speaker B: Because here the same frame for the clean speed.
Speaker B: Oh, that's clean.
Speaker B: Okay.
Speaker B: There are differences because here the FFT is only with 256 points.
Speaker B: And this is with 512.
Speaker B: Okay.
Speaker C: This is kind of interesting also because if we use the standard frame length of 25 milliseconds, it happens is that for low pitched voice because of the frame length, you don't really have.
Speaker C: You don't clearly see this period of structure because of the first logo for each each of you.
Speaker H: So this one include is a longer.
Speaker C: It's like 50 milliseconds.
Speaker C: 50 minutes.
Speaker C: Yeah.
Speaker C: But it's the same frame.
Speaker H: What's that time frequency trade off thing, right?
Speaker H: I see.
Speaker H: Sorry.
Speaker H: Oh, so is this the difference here?
Speaker B: No, this is the same frame.
Speaker H: Oh, that's the original.
Speaker C: Yeah, so with the short frame basically, the period is not enough to use these kind of neat things.
Speaker B: But yeah.
Speaker C: Yeah, so probably we'll have to use like long, long frames.
Speaker H: Oh, that's interesting.
Speaker G: Maybe.
Speaker G: Well, I mean, it looks better.
Speaker G: But the thing is if you're actually asking if you actually need to do very long and FFT and maybe pushing things.
Speaker H: Would you want to do this kind of difference thing after you do spectral subtraction?
Speaker G: Maybe.
Speaker G: The spectral subtraction is being done at what level is it being done at the level of FFT bins or at the level of a male spectrum or something.
Speaker C: I guess it depends.
Speaker G: I mean, how are they doing it?
Speaker C: I guess Erickson is still doing that.
Speaker C: So yeah, I'm not really...
Speaker G: So in that case, it might not make much difference at all.
Speaker H: It seems like you'd want to do it on the FFT bins.
Speaker H: Maybe.
Speaker H: I mean, if you were going to edit it for this purpose, that is.
Speaker F: Yeah.
Speaker G: Okay.
Speaker G: What else?
Speaker C: So we'll perhaps try to convince the JIP people to use a new FFT.
Speaker G: Okay.
Speaker G: Has anything happened yet on this business of having some sort of standard...
Speaker C: Sorry, sir.
Speaker C: Not yet, but I will go down.
Speaker C: No, they are.
Speaker C: I think they're more time because they're...
Speaker H: When is the next Aurora deadline?
Speaker F: Early June?
Speaker F: Late June?
Speaker F: Not early June?
Speaker G: Okay.
Speaker G: And he's been doing all the talking, but...
Speaker G: Yeah.
Speaker G: This is by the way a bad thing.
Speaker G: We're trying to get female voices in this record as well.
Speaker G: Make sure Carmen talks as well.
Speaker G: But is he pretty much been talking about what you're doing also?
Speaker B: I am doing this.
Speaker F: Yes.
Speaker B: I don't know.
Speaker B: I think that's for the recognition.
Speaker E: The meeting record that is better than that can be speaking to you.
Speaker G: Well, we'll get to Spanish voices sometime.
Speaker G: We do.
Speaker G: We want to recognize you too.
Speaker B: And the result for the TVG.
Speaker B: The video record that we did for each people.
Speaker G: No, we like...
Speaker G: We're in the...
Speaker F: We're in the Lord Hermansky work in the frame of mind.
Speaker F: We like higher rates.
Speaker F: That way there's lots of work to do.
Speaker F: That's...
Speaker F: Anything to...
Speaker I: Not much is new.
Speaker I: I talked about what I'm trying to do last time.
Speaker I: I said I was going to use Avandano's method of using a transformation to map from long analysis frames, which I used for removing reverberation to short analysis frames for feature calculation.
Speaker I: He has a trick for doing that involving viewing the DFT as a matrix.
Speaker I: But I decided not to do that after all because I realized to use that I'd need to have these short analysis frames get plugged directly into the feature computation somehow.
Speaker I: And right now I think our feature computation is set up to take audio as input in general.
Speaker I: So I decided that I'll do the reverberation removal on the long analysis windows and then just re-sensitize audio and then send that.
Speaker G: This is in order to use the SRI system.
Speaker I: Or even if I'm using R system I was thinking it might be easier to re-sensitize the audio because then I could just use FeeCalc as it is and I wouldn't have to change the code.
Speaker G: Yeah, I mean it's certainly in a short turn.
Speaker F: This sounds easier.
Speaker F: Yeah, I mean longer term if it turns out to be useful when I want to.
Speaker G: Right, that's true.
Speaker G: No, you may be putting other kinds of errors in from the recent analysis.
Speaker I: Okay, I don't know anything about recent analysis.
Speaker F: But I'm not sure if you're likely to think that is.
Speaker F: It's a reasonable way to go for an initial thing.
Speaker F: We can look at exactly what you end up doing and figure out if there's something that could be heard by the end part of the process.
Speaker I: Okay.
Speaker I: That's, that's it, that's it.
Speaker G: Anything to?
Speaker A: Well, I've been continuing reading.
Speaker A: I went off on a little tangent this past week looking at Modulations Spectrum stuff and learning a bit about what it is and the importance of it in speech recognition.
Speaker A: I found some neat papers, historical papers from Kanadera, Hermanski and Array.
Speaker A: And they did a lot of experiments where they take speech and they modify the, they measure the relative importance of having different portions of the Modulation Spectrum intact.
Speaker A: And they find that the spectrum between one and 16 hertz in the Modulation is important for speech recognition.
Speaker G: Sure, I mean this sort of goes back to earlier stuff by Drillman.
Speaker G: And the MSG features were sort of built up this notion.
Speaker G: But I guess I thought you had brought this up in the context of targets somehow.
Speaker G: Right.
Speaker G: But it's not, I mean they're sort of not in the same kind of category as a phonetic target or a syllabic target or a more like a feature or something.
Speaker A: I was thinking more like using them as the inputs to the detectors.
Speaker G: Oh, I see.
Speaker G: Well, that's sort of what MSG does.
Speaker G: Right. So it's, but, but, yeah.
Speaker G: We'll talk more about it later.
Speaker G: We can talk more about it later.
Speaker F: Would you dig it?
Speaker F: Let's do dig it.
Speaker F: You start.
Speaker I: Reading transcript L-5617686691.
Speaker I: 7921.
Speaker I: 20350125.
Speaker I: 40564334.
Speaker I: 9290-3114-8629.
Speaker I: 4136256690.
Speaker I: 4367-615298.
Speaker I: 76633377823.
Speaker I: 842-614627.
Speaker G: Transcript L-55.
Speaker G: Or transcript L-55.
Speaker G: 687-715-075.
Speaker G: 896-03865.
Speaker G: 566-2002-96.
Speaker G: 848-9164.
Speaker G: 1686-24013.
Speaker G: 3126-619960.
Speaker G: 837-08080.
Speaker G: 6236-4006-9743.
Speaker H: Transcript L-49.
Speaker H: 884259-7450.
Speaker H: 787-0106158.
Speaker H: 742-503970.
Speaker H: 858-034714.
Speaker H: 06044-2001.
Speaker H: 2293-3128.
Speaker H: 8558-558-691.
Speaker H: 358-294017.
Speaker A: Transcript L-50.
Speaker A: 9067-3933.
Speaker A: 08308-3481.
Speaker A: 21365-3159.
Speaker A: 4084305211.
Speaker A: 924-584-5504.
Speaker A: 2226168155.
Speaker A: 707-087-8402-803-160507.
Speaker C: Transcript L-53.
Speaker C: 5954-883914.
Speaker C: 860310-9753.
Speaker C: 55529-3365.
Speaker C: 337-074710.
Speaker C: 6418-3166.
Speaker C: 576-89596.
Speaker C: 228-3305595.
Speaker C: 355-059-615-025.
Speaker B: Transcript L-54.
Speaker B: 1431771032.
Speaker B: 9882-488812.
Speaker B: 1310576812.
Speaker B: 628875912.
Speaker B: 5272-8617498-0000709.
Speaker B: 862-458892.
Speaker B: 221-196783.
