Speaker E: Let's see. Test? Test? Yeah? Okay. Channel one. Hello.
Speaker A: Test.
Speaker E: I was saying he can be here next week, once day through Friday, through, I'm sorry, and I won't be here Thursday and Friday, but my suggestion is that, at least for this meeting, people should go ahead. I've seen him go be here.
Speaker E: You know, we don't have any check accent. Yeah, that's...
Speaker E: As far as I know, so. Okay.
Speaker E: There we go.
Speaker E: So other than reading digits, what's our agenda?
Speaker G: I don't really have anything new than working on meeting recorder stuff.
Speaker E: Okay. Do you think that would be the case for next week, also, or is...
Speaker E: what's your projection on?
Speaker E: Because the one thing that seems to me, we really should try, if you hadn't tried it before, because it hadn't occurred to me, it was sort of obvious thing, is adjusting the scaling and insertion.
Speaker E: L.T. sort of stuff.
Speaker G: I did play with that, actually, a little bit.
Speaker G: What happens is, when you get to the noisy stuff, you start getting lots of insertions.
Speaker G: Right.
Speaker G: And so I've tried playing around a little bit with the insertion penalties, things like that.
Speaker G: Yeah.
Speaker G: It didn't make a whole lot of difference.
Speaker G: Like, for the well-matched case, it seemed like it was pretty good.
Speaker G: I could do more playing with that, though.
Speaker E: But you were looking at Mel Capstrom.
Speaker E: Right.
Speaker G: Oh, you're talking about for our features.
Speaker E: Right. So, I mean, it's not the direction that you were working with, that we were saying, what's the best you can do with Mel Capstrom.
Speaker E: But they raised a very valid point, which I guess...
Speaker E: So, to first order, I mean, you have other things you're going to do.
Speaker E: But the first order, I would say, that the conclusion is, that if you do some munking around with the exact HTK training, and with how many states and so forth, that it doesn't particularly improve the performance.
Speaker E: In other words, that even though it sounds pretty dumb, just applying the same number of states to everything, or, no matter what language, isn't so bad.
Speaker E: Right.
Speaker E: And I guess you hadn't gotten to all the experiments you wanted to do with number of Gaussian's.
Speaker E: Right.
Speaker E: But let's just...
Speaker E: If we had to draw a conclusion on the information we have so far, we'd say something like that, right?
Speaker E: So, the next question to ask, which is, I think the one that Andreas was addressing himself to in the lunch meeting, is, we're not supposed to adjust the back end.
Speaker E: But anybody using the system would.
Speaker E: Yeah.
Speaker E: If you were just adjusting the back end, how much better would you do in noise?
Speaker E: Because the language scaling and search and penalty is over, they're probably set to be about right for milk capstrom.
Speaker E: But they're probably not at all set right for these things, particularly these things that look over larger time windows, in one way or another with LDA and KLT and neural nets, all these things.
Speaker E: In the past, we always found that we had to increase the insertion penalty to correspond to such things.
Speaker E: So, I think that's kind of a first order thing that we should try.
Speaker G: So, the experiment is to run our front end like normal with the default insertion penalties and so forth, and then tweak that a little bit and see how much of a difference it makes.
Speaker E: By our front end, I mean, take some version that Stefan has that is our current best version of something.
Speaker E: I mean, don't want to do this over 100 different things that they've tried, but for some version, he said he's a good one.
Speaker E: So, how much does it improve if you actually adjust that?
Speaker E: But it is interesting to say you have for the noise, how about for the mismatched or the medium mismatched conditions?
Speaker E: Have you, when you adjusted those numbers for milk capstrom, did it?
Speaker G: I don't remember off the top of my head.
Speaker G: Yeah, I didn't even write them down.
Speaker G: I don't remember I would hate to.
Speaker G: Well, I did write down.
Speaker G: So, when I was doing, I just wrote down some numbers for the well-matched case.
Speaker G: Yeah.
Speaker G: Looking at, I wrote down with the deletions, the substitutions, insertions, or four different numbers of states per phone.
Speaker G: Yeah.
Speaker G: But that's all I wrote down.
Speaker G: Okay.
Speaker G: I would need to do that.
Speaker E: Okay, so.
Speaker E: I can do that for next week.
Speaker E: Yeah.
Speaker E: And, yeah, also, sometimes if you run behind some of these things, maybe we can get someone else to do it, you can super-rise or something.
Speaker E: But I think it'd be good to know that.
Speaker G: Yeah.
Speaker G: I just need to get a friend and, uh, select for me, or you point me to some files.
Speaker G: Yeah.
Speaker G: You've already come to it.
Speaker G: All right.
Speaker B: Okay.
Speaker G: I probably will have time to do that and time to play a little bit with the silence.
Speaker B: Yeah.
Speaker G: Model.
Speaker G: Maybe I can have that for next week, maybe next week.
Speaker E: Yeah.
Speaker E: Yeah.
Speaker E: Because I mean, the other, that in fact, might have been part of what, uh, the difference was, at least part of it, that we were seeing, you know, we were seeing the SRI system was so much better than the tandem system.
Speaker E: Part of it just be that the SRI system, they always adjust these things to be so optimized.
Speaker G: I wonder if there's anything that we could do to the front end that would affect the insertion?
Speaker G: Yes.
Speaker E: I think you can.
Speaker E: Okay.
Speaker E: Well, um, uh, part of what's going on, um, is the, uh, the range of values.
Speaker E: So if you have something that has a much smaller range or a much larger range, and taking the appropriate root, you know, if something is kind of like the equivalent of a bunch of probabilities multiplied together, you can take a root of some sort of, like, seven probabilities together, you can take the seventh root, or something, or it's in the log domain, divided by seven, but, um, that has a similar effect because it changes the scale of the numbers, if the differences between different candidates from the acoustic model, as opposed to what's coming from the language model.
Speaker G: That's changing the value of your insertion.
Speaker E: Yeah. I mean, it's more directly like the, the language scaling or the, the model scaling or acoustic scaling, but you know that those things have kind of a similar effect to the insertion penalty anyway, a slightly different way of, of handling it.
Speaker G: So, um, so if we know what the insertion penalty is, we can get an idea about what range our numbers should be on.
Speaker G: I think so, yeah.
Speaker E: Yeah, so that's why I think that's another reason, another curiosity, as to why it would, in fact, be kind of neat to find out if we're way off.
Speaker E: I mean, the other things aren't, we're seeing, I'm sure you've already looked at this in these noisy cases.
Speaker E: We are seeing lots of insertions, right? The insertion number is quite high.
Speaker E: I know the VAD takes pretty care part of that.
Speaker C: I've seen that with the male capster. I don't know about the Aurora front end, but I think it's much more balanced with, uh, when the front end is more robust.
Speaker C: Yeah, I could look at it.
Speaker C: At this, yeah.
Speaker E: Yeah, what's the typical number?
Speaker E: I don't, I don't know.
Speaker E: Okay. I'm sure it's more balanced, but it wouldn't surprise me if there's still, I mean, in the, the old system is used to do.
Speaker E: I remember numbers kind of like insertions being half the number of deletions as being, and both numbers being, tend to be on the small side, comparing to, uh, substitutions.
Speaker G: Well, this, the whole problem with insertions was what I think, um, we talked about when the guy from OGI came down at one time, and, and that was when people were saying what we should have a, uh, voice activity detector.
Speaker G: Right. That, because all that stuff that we're getting, the silence that's getting through is causing insertions.
Speaker G: Right.
Speaker G: I've mentioned there's still a lot.
Speaker E: Yeah, and it may be less of a critical thing. I mean, the fact that some get by, maybe less of a critical thing if you, uh, get things in the right range.
Speaker E: So I mean, the insertions is, is a symptom.
Speaker E: It's a symptom that there's something wrong with the range, but there's a, your, your, your substitutions tend to go up as well.
Speaker E: So I, I think that, uh, the most obvious thing is just the insertions.
Speaker E: I don't know if it's, but, uh, um, if you're operating in the wrong range, I mean, that's why just in general, if you change what these, these penalties and scaling factors are, you reach some point that's, uh, that's a minimum.
Speaker E: So, um, we do have to do well over a range of different conditions, some of which are noisier than others.
Speaker E: But, um, I think we may get a better handle on that if we, if we see, um, I mean, it's, if we actually could pick a, a more stable value for the range of these features.
Speaker E: It, um, could, uh, even though it's, it's, it's true that in a real situation, you can, in fact, adjust the, these, these scaling factors and the back end.
Speaker E: And it's artificial here that we're not adjusting those.
Speaker E: You certainly don't want to be adjusting those all the time.
Speaker E: And if you have a nice front end that's roughly the right range, I remember after we got our stuff more or less together in the previous systems, we built that we tended to set those scaling factors at kind of a standard level.
Speaker E: And we would rarely adjust them again, even though you could get a, for an evaluation, you can get an extra point or something if you tweaked it a little bit.
Speaker E: But once we knew what, roughly the right operating range was, it was pretty stable.
Speaker E: And, uh, we might just not even be in the right operating range.
Speaker G: So with the, uh, what a good idea to try to map it into the same range that you get in the well matched case.
Speaker G: So if we computed what the range was and well matched, and then when we get our noisy conditions out, we try to make it have the same range.
Speaker E: No, you don't want to change it for different conditions.
Speaker E: No, no, I, what, what I'm saying.
Speaker G: Oh, I wasn't interested in changing it for different conditions. I was just saying that when we pick a range, we want to pick a range that we map our numbers into.
Speaker G: Yeah.
Speaker G: We should probably pick it based on the range that we get in the well matched case.
Speaker G: Otherwise, I mean, what range are we going to choose to map everything into?
Speaker E: Well, it depends how much we want to do, game ismanship and how much we want to do.
Speaker E: I mean, if it can be actually, even if you want to be played on the game ismanship side, it can be kind of tricky.
Speaker E: So I mean, what you would do is set the, set the scaling factors so that you got the best number for this 0.45 times the, you know, and so on.
Speaker E: But they might change that those weightings.
Speaker E: So I just sort of think we need to explore the space. Let's take a look at a little bit.
Speaker E: We may just find that that we're way off.
Speaker E: Maybe we're not.
Speaker E: You know, that's with these other things that may turn out the, it's kind of reasonable.
Speaker E: But then, I mean, Andreas gave very reasonable response and he's probably not going to be the only one who's going to say this in the future of, you know, people, people within this tight-knit community who are doing this evaluation are accepting.
Speaker E: More or less that these are the rules.
Speaker E: But people outside of it are looking at the broader picture are certainly going to say, well, wait a minute, you're doing all this standing on your head in the front end when all you could do is just adjust this in the back end with one knob.
Speaker E: And so we have to at least, I think, determine that that's not true, which would be okay.
Speaker E: Or determine that it is true, in which case we want to adjust that and then continue with what we're doing. And as you say, as you point out, finding ways to then compensate for that in the front end, also then becomes a priority for this particular test.
Speaker E: So you don't have to do that.
Speaker E: Okay.
Speaker E: So, what's new with you?
Speaker C: So there's nothing new.
Speaker E: What's old with you? It's developed.
Speaker E: I'm sorry.
Speaker E: Okay, what's old with you? That is developed over the last week.
Speaker C: So, if we're mainly working on the report, on the report of the work that was already done,
Speaker G: that's all. Anything new on the thing that you're working on with the... What was that?
Speaker G: The voicing detected.
Speaker A: What's going on now? What are you doing?
Speaker A: We try to use the variance, the difference between the effect spectrum and the male filter band spectrum. Also, the other parameters relates with the autocorrelation function.
Speaker A: Energy and the variance also helps the autocorrelation function.
Speaker E: So, that's what you were describing, I guess a week or two ago.
Speaker A: We don't have result of the aurora jet. We need to try and run network.
Speaker E: So, you're training neural networks now?
Speaker E: No, not yet.
Speaker E: So, what's going on?
Speaker A: Well, I work in the report too, because we have a lot of results, embedded in spares, and what necessary to look at the directory to give some structure.
Speaker E: So, yeah, if I can summarize, basically what's going on is that you're going over a lot of material that you've generated in a furious fashion of generating many results and doing experiments and trying to pull it together into some coherent form to be able to...
Speaker C: Yeah, basically we've stopped experimenting. We're just trying to think of some kind of technical report.
Speaker G: Is this a report that's for aurora?
Speaker G: No.
Speaker G: It's like a tech report for XC.
Speaker A: Yes.
Speaker E: So, my suggestion though is that you not necessarily finish that, but that you put it all together so that you've got a clearer structure to it, you know what things are, you have things documented, you've looked things up that you needed to look up so that such a thing can be written.
Speaker E: And when do you leave again?
Speaker A: July, first of July.
Speaker E: First of July, okay. And that you figure on actually finishing it in June, because you're going to have another much results to fit in there anyway.
Speaker E: And right now it's kind of important that we actually go forward with experiments. So I think it's good to pause and together everything together and make sure it's in good shape so that other people can get access to it so that it can go into a report in June.
Speaker E: But I think to really work on fine tuning the report at this point is probably a bad timing.
Speaker C: Yeah. Well, we didn't, we just planned to work on one week on this report, no more anyway.
Speaker E: I really want to add other things later anyway, because you're, this is more to go.
Speaker G: Yeah, well, so I don't know there are small things that we started to do, but maybe discovering anything that makes you scratch your head as you write this report.
Speaker G: Like, why did we do that? Why did we do this?
Speaker C: Yeah. Yeah.
Speaker C: Actually, there were some tables that were also with partial results. We just noticed that I get a ring of result that for some conditions we didn't have everything.
Speaker C: Yeah, yeah, we have extracted actually the noises from a speech that car.
Speaker C: So we can train neural network with speech and these noises.
Speaker C: It's difficult to say what it will give because when we look at the, or at the IDG experiments, there are these three conditions that have different noises and apparently the system performs as well on the scene noises, on the scene noises.
Speaker C: But I think there's something we have to try anyway. So adding the noises from, from the speech that car.
Speaker E: That's, that's, that's permitted.
Speaker C: Well, OGI did that.
Speaker C: At some point, they did that for the first activity.
Speaker G: Could you say it again? What exactly did they do?
Speaker C: They use some parts of the Italian database to train the voice activity sector, I think.
Speaker E: Yeah, I guess the thing is, yeah, I guess that's a matter of interpretation.
Speaker E: The rules, as I understand it, is that in principle the Italian and Spanish and the English, no Italian and the Finnish, and the English were development data. And what you could adjust things.
Speaker E: And the German and Danish were the evaluation data. And then when they finally actually evaluated things, they used everything.
Speaker E: Yeah, that's right.
Speaker E: And it is true that the performance on the German was, I mean, the improvement wasn't so good. The raw performance was really pretty good.
Speaker E: So, and it doesn't appear that there's strong evidence that even though things were somewhat tuned on those three or four languages, that going to a different language really hurt you. And the noises were not exactly the same, right, because it was taken from a different...
Speaker E: I mean, they were different drives. I mean, it was actual different cars and so on.
Speaker E: So, it's somewhat tuned. It's tuned more than, you know, you really like to have something that needed no particular noise at all, maybe just some white noise or something like that, at most.
Speaker E: But that's not really what this contest is. So, I guess it's okay.
Speaker E: That's something I'd like to understand before we actually use something from it, because it would...
Speaker G: It's probably something that, you know, the experiment designers didn't really think about, because I think most people aren't doing trained systems or, you know, systems that are like ours, but they actually use the data to build models, I mean, just doing things like all the processing.
Speaker E: Well, it's true, except that that's what we used in Aurora 1, and then they designed the things for Aurora 2, knowing that we were doing that.
Speaker E: That's true.
Speaker G: And they didn't forbid us, right, to build models on the data.
Speaker E: No, but I think that it probably would be the case that if, say, we trained on Italian data, and then we tested on Danish data, and it did terribly.
Speaker E: That it would look bad, and I think someone would notice, and would say, well, look, this is not generalizing. I would hope they would.
Speaker E: But it's true, you know, maybe there's parameters that other people have used, you know, that they have tuned in some way for other things.
Speaker E: So it's, we should, maybe that's maybe a topic, especially if you talk with him when I'm not here, that's a topic you should discuss with he may check it's okay.
Speaker G: The speakers or each of the training utterances.
Speaker C: What do you mean?
Speaker E: Social security number.
Speaker C: I think it.
Speaker C: Made a female just me up at least.
Speaker G: What kind of information do you mean?
Speaker G: Well, I was thinking about things like, you know, gender, you know, gender specific nets and both the track link on normalization.
Speaker G: Things like that.
Speaker G: I don't know what information we have about the speakers that we could try to take advantage of.
Speaker E: Right. I mean, again, if you had the whole system you were optimizing, that would be easy to see.
Speaker E: But if you're supposedly just using a fixed back end and you're just coming up with a feature vector, I'm not sure.
Speaker E: I mean, having the two nets, suppose you detected that was male, female, you can look at different both in as separate streams or something.
Speaker E: Maybe.
Speaker G: I don't know. I was just wondering if there was other information we could exploit.
Speaker E: Yeah, it's interesting thought maybe having something along, I mean, you can't really do vocal track normalization.
Speaker E: It's something that had some of that effect. Yeah.
Speaker G: No, I had no idea. I had thought it was too much about it really. It just something that popped into my head just now.
Speaker G: Normalization, you know, you have some sort of a general speech model, maybe just a mixture of galsians that you evaluate every utterance against.
Speaker G: And then you see where each utterance, like the likelihood of each other, and to divide the range of the likelihoods up into discrete bins.
Speaker G: And then each bins got some knob.
Speaker E: Yeah, but just listen to yourself. I mean, that really doesn't sound like a real time thing with less than 200 milliseconds latency that were you're not adjusting the statistical engine at all.
Speaker E: Yeah, well, not just expensive. I don't see how you could possibly do it. You can't look at the whole utterance and do anything.
Speaker E: Each frame comes in and it's got to go out the other end.
Speaker E: So whatever it was, it would have to be sort of on a per frame basis. Yeah. I mean, you can do fairly quickly. You can do male female stuff.
Speaker E: But as far as, I mean, like I thought, maybe I ended a thing with a vocal track normalization, ways back, maybe other people did too, with trying to identify third formant, average third formant, using that as an indicator of.
Speaker E: So, you know, third formant, if you imagine that the first order, what happens with changing vocal track is that the formants get moved out by some proportion.
Speaker E: So if you had a first formant that was 100 hertz before, if the 50, if the vocal track is 50% shorter, then it would be out at 750 hertz and so on.
Speaker E: So that's a move of 250 hertz, whereas the third formant, which might have started off at 2500 hertz, might be out to 3750, you know, so you frequently get less distinct higher formants.
Speaker E: It's still third formant is kind of a reasonable compromise.
Speaker E: So I think, if I recall correctly, they did something like that.
Speaker E: But that doesn't work for just having one frame or something. That's more like looking at third formant over a turn or something like that.
Speaker E: So, but on the other hand, male females is a much simpler categorization than figuring out a factor to squish or expand the spectrum.
Speaker E: You could imagine that, just like we're saying, voiced and voiced is good to know. Male female is good to know also.
Speaker E: But you have to figure out a way to incorporate it on the fly.
Speaker E: I mean, I guess, as you say, one thing you could do is simply have the male and female output vectors, you know, net strain only on males and drain only on females.
Speaker E: But I don't know if that would really help because you already have males and females. It's fitting into one net.
Speaker G: Is it balanced in terms of gender data?
Speaker C: Do you know? Almost.
Speaker E: Okay. You were saying before?
Speaker C: Yes. So this noise.
Speaker C: Yeah, the MSG.
Speaker C: There is something perhaps you could spend some days to look at this thing because it seems that when we train networks on, let's say on timid with MSG features, they look as good as network strain on BLP.
Speaker C: But when they are used on the speech data, it's not the case. The MSG features are much worse. And so maybe they are more sensitive to different recording conditions.
Speaker E: Yeah. But let me ask you this. What's the, do you know, recall of the insertions were higher with MSG?
Speaker C: I don't know. I cannot tell. But it's the error rate is higher.
Speaker E: Yeah, we should always look at insertions, solutions and substitutions. So MSG is very, very different. And BLP is very much like milk hamstring.
Speaker E: MSG is very different from both of them. So if it's very different, then this is the sort of thing. I mean, I'm really glad Andreas brought this point up. I sort of forgotten to discuss it.
Speaker E: We always have to look at how these adjustments affect things. And even though we're not allowed to do that, again, we maybe could reflect that back to our use of the features.
Speaker E: So if it, if in fact, the problem might be that the range of the MSG features is quite different, the range of the BLP or milk hamstring. And you might want to change that.
Speaker C: But yeah, but it's after, well, it's tandem features. So yeah.
Speaker C: Yeah, we have estimation of first, post-serials. Yeah, with BLP and with MSG as input. So why not? Well, that means they're between zero and one.
Speaker E: But it doesn't necessarily, you know, they could be, doesn't tell you what the variance of the things is.
Speaker E: So you're taking log of these things. We could be knowing what the sum of the probabilities are. It doesn't tell you what the sum of the logs are.
Speaker C: So, yeah, so we should look at the likelihood. Or what? Well, the log props. Yeah. Or what, you know, what you're the thing you're actually looking at.
Speaker G: So your, your values that are actually being fed into HTK. What do they look like? So the, for the tandem system, the values that come out of the net don't go through the sigmide, right? They're sort of the cream on linearity values.
Speaker G: Right. So they're kind of like log probabilities. So that's what goes into HTK.
Speaker E: Almost. Then you actually do a KLT. They are normalized after that. Are they?
Speaker E: No. No. Okay. So, right. So the question is, yeah, whatever they are at that point, are they something for which taking a square root or cube root or four-thread or something like that is going to be a good or a bad thing?
Speaker E: So, and that's something that nothing else after that is going to, things are going to scale it. You know, subtract things from it, scale it from it, but nothing will have that same effect.
Speaker G: So, anyway, if the log probs that are coming out, whether the MSG are really big, the standard insertion penalties are going to have very little effect compared to, you know, smaller set of log probs.
Speaker E: Now, again, you don't really look at that. It's something that, and then it's going through this transformation that's probably pretty close to, whatever the KLT is doing, but it's probably pretty close to what a discrete close-any transformation is doing. But still, it's not going to probably radically change the scale of things.
Speaker E: I would think, and yeah, maybe entirely off, and it may be, at least it may be quite different for MSG than it is for milk, up to MPLP. So that would be, so the first thing to look at without adjusting anything would just be to go back to the experiment, look at the substitutions, insertions, and relations.
Speaker E: And if there's a fairly large effect of the ratio between insertions and relations for the two cases, then that would be an indicator that might be in that direction.
Speaker C: Yeah, but my point was more that it works sometimes. Yeah, but sometimes it doesn't work.
Speaker E: And it works on the TID Jits and the speech that Gary doesn't work. Yeah, but some problems are harder than others. And sometimes there's enough evidence for something to work, and then it's harder to break.
Speaker E: But it could be that when you say it works, maybe we could be doing much better even if the TID Jits. Yeah, well, there is also the spectroscopy section, which I think maybe we should try to integrate it.
Speaker C: Yeah. Right. But I think that would involve to use a big bunch of the system of Ericsson.
Speaker C: Because the spectroscopy section then it's followed by other kind of processing that's dependent on the speech on silence.
Speaker C: And there is kind of spectral flattening after if it's silence. And I think it's important to reduce this musical noise and this increase of variance during silence portions.
Speaker C: So, this would involve to take almost everything from the this proposal and then just add some kind of fun like normalization and the neural network.
Speaker E: Okay, well, this would be I think something for discussion with Henik next week. Right. So, how are things going with what you're doing?
Speaker F: Well, it took a lot of time just getting my taxes out of the way. I'm starting to write code now from my work, but I don't have any results yet. It would be good for me to talk to Henik, I think when he's here. Do you know what his schedule will be like?
Speaker E: He'll be around for three days. Okay, so we'll have a lot of time. So, he'll be talking with everybody in this room.
Speaker G: But you said you won't be here next Thursday?
Speaker E: Not Thursday and Friday, so it would be a faculty retreat. So, I'll try to connect with him and people as I can on Wednesday.
Speaker E: How did taxes go? Next go, okay. Yeah.
Speaker E: Yeah, that's one of the big advantages of not making much money. The taxes are easier.
Speaker G: Unless you're getting money into countries. I think you're going to want to cut.
Speaker E: Can't do what? Can't do what's a cut? You have to do two returns. For 2000, I did. Yeah. Oh, yeah. That's right.
Speaker F: I'll still have a bit of Canadian income, but it'll be less complicated because I will not be considered a resident of Canada anymore, so I won't have to declare my American income on my Canadian return.
Speaker D: Very? Oh, right. Continuing looking at phonetic events. And it was Tuesday. I've been meeting with John and Halal and Chuck to talk some more about these neck events.
Speaker D: I came up with a plan of attack. Oh, well, I don't want to say something about what it is.
Speaker D: Okay, well, we're all gathered here together. I hope I can wave my hands. So once I'm thinking of getting a set of acoustic events to be able to distinguish between phones and words and stuff.
Speaker D: Once we figure out a set of these events that can be hand labeled or derived from hand labeled phone targets, we can take these events and do some cheating experiments where we feed these events into an SRI system and evaluate its performance on a switchboard task.
Speaker D: Can you give an example of an event? Sure. I can give you an example of 20 odd events. So in this paper, I'm talking about funding recognition using acoustic events.
Speaker D: So things like vacation or news. Who's paper?
Speaker E: From University of Hamburg and Bielfeld.
Speaker G: I think there's a difference between acoustic features and acoustic events. And I think of acoustic features as being things that linguists talk about.
Speaker G: So stuff that's not based on data. So they talk about features for phones like its height, its tenseness, laxness, things like that, which may or may not be all that easy to measure in the acoustic signal versus an acoustic event, which is just something in the acoustic signal that is fairly easy to measure.
Speaker G: So it's a little different.
Speaker E: When we did the spam work, we had this notion of an auditory event called an event with an A at the front.
Speaker E: And the idea was something that occurred that is important to a bunch of neurons somewhere. So a sudden change or a relatively rapid change in some spectral characteristic will do sort of this.
Speaker E: And there's certainly a bunch of places where you know that neurons are going to fire because something novel has happened. That was the main thing that we were focusing on there. But there's certainly other things beyond what we talked about there that aren't just sort of rapid changes.
Speaker G: It's kind of like the difference between top down and bottom up. I think of the acoustic, you know, phonetic features as being top down. And you look at the phone and you say this phone is supposed to be, you know, have this feature, this feature in this feature.
Speaker G: Whether that those features show up in the acoustic signal is sort of irrelevant. Whereas an acoustic event goes the other way. Here's the signal. Here's some event. And that, you know, that may map to this phone sometimes and sometimes it may not. It just depends on the context and things like that.
Speaker D: So, yeah. Okay. Using these events, we could perform these cheating experiments and how good they are in terms of phoneme recognition or work recognition.
Speaker D: And then from that point on, I would design robust event detectors in a similar spirit that Saul has done with his graphical models and this probabilistic and or model that he uses.
Speaker D: I tried to extend it to account for other phenomena like CMR, co-multulation, release. And maybe also the best of the ways to modify the structure of these models in a data driven way, similar to the way that Jeff, Jeff, builds his work.
Speaker D: And while I'm doing these event detectors, you know, I can measure my progress by comparing the error rates in clean and noisy conditions to something like neural nets.
Speaker D: And so once we have these event detectors, put them together and feed the outputs of the event detectors into the SRI, HMM system and test it on switchboard or maybe even Aurora stuff.
Speaker D: And that's pretty much the big picture of the plan.
Speaker E: By the way, there's a couple people who are going to be here. I forget I already told you this, but a couple people who are going to be here for six months. It's Professor Kolmeier from Germany who's quite big in the hearing aid signal processing area.
Speaker E: And Michael Klanchman who's worked with him who also looks at auditory properties inspired by various brain function things. So I think they'll be interesting to talk to in this sort of issue is these detectors are developing.
Speaker E: He looks at interesting things in different ways of looking at specter in order to get very speech properties out.
Speaker E: Okay, well, short meeting with it. Okay. And that's what we're doing. I encourage you to go ahead and meet next week with HINIK.
Speaker E: All right, I'll start. Okay, I'm doing transcript L76 032 36 5550 7058 592 4657 17 8034 6015 544 445 088 3666 601726 971 235 1588 4821 8042 3770 528 578 8674
Speaker C: Transcript L-77 5 845 474 163 130 287 452 121 161422 3891 4838 1740 651 8676 293 3132 6134 24
Speaker G: 0243 214 1337 001950 7956 transcript L-78 1543589276 449 746 646 787 337 618 2 689 033 313 375 175 536 1141 3007 510 782 461 64382 2503
Speaker D: transcript L-79 885 2526 17 445 191 288 4 13168 4 3004 668 3 9 8 6 4 8 6 6 6 7 5 0 3 1 4 1 7 0 8 5 9 5 0 3 7 6 0 2 6 0 8 2 3 0 7 1 9 8 7 5 7 4 2 8 8 3 7 1 6 8
Speaker F: transcript L-80 9 5 6 6 4 3 9 7 8 3 0 2 6 4 3 6 1 2 8 9 3 3 4 4 0 5 7 9 8 1 3 9 8 8 8 0 1 0 2 0 9 9 9 5 8 9 8 1 8 9 5 4 8 7 9 6 1 8 7 8 8 3 0 9 6 7 6 2 9 0 5 7 5 6 0 7 2 9 7
Speaker A: transcript L-81 7915 908 2 16168 6 4013 4 405 5 6 4 2 1 9 4 2 1 0 5 1 2 7 2 1 2 9 5 8 8 6 3 9 2 5 8 4 3 6 0 5 3 0 2 3 3 2 6 2 4 3 6 5 2 6 2 4 2 8 5 1 4 5
