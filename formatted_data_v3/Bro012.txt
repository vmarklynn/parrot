Speaker H: Hello. Hey, we're on.
Speaker H: Okay, so I had some interesting mail from Dan Ellis. Actually, I think he redirected everybody also. So the PD-A mics have a big bunch of energy at five hertz. Where this came up was that I was showing off these waveforms that we have on the web and I just sort of had noticed this, but the major component in the wave in the second waveform, in the pair of waveforms, is actually the air conditioner.
Speaker H: So I have to be more careful about using that as a good illustration. In fact, it's not of effects of rumor or vibration. It isn't a bad illustration of the effects of room noise on some mics.
Speaker H: And then we had this other discussion about whether this affects the dynamic range, because I know, although you start off with 32 bits, you end up with 16 bits and, you know, are we getting hurt there, but Dan's pretty confident that we're not that the quantization error is still not a significant factor there.
Speaker H: So there was a question of whether we should change things here, whether we should change a capacitor on the put box for that or whether we should. Yeah, he suggested a smaller capacitor, right?
Speaker H: Right. He then I had some other thing discussed with him and the feeling was once we start munching with that, and many other problems could happen. And additionally, we already have a lot of data that's collected with that.
Speaker H: Yeah. The simple thing to do is he has a, I forget if this wasn't that mayor in the following mail, but he has a simple filter, a digital filter that he suggested. We just run over the data, where we deal with it.
Speaker H: The other thing, I don't know the answer to, but when people are using Fickalk here, whether they're using it with the high pass filter option or not. And I don't know if anybody knows.
Speaker H: You can go check. Yeah. So when we're doing all these things using our software, there is, if it's based on the RASTA PLP program, which does both PLP and RASTA PLP, then there is an option there, which then comes up through to Fickalk, which allows you to do a high pass filter.
Speaker H: And in general, we like to do that because of things like this. And it's pretty, it's not a very severe filter, doesn't affect speech frequencies, even pretty low speech frequencies at all.
Speaker E: What's the cutoff frequency that you use?
Speaker H: Oh, I don't know why I wrote this a while ago. Is it like 20? Something like that.
Speaker H: I mean, I think there's some effect above 20, but it's mild.
Speaker H: So, I mean, it's probably, there's probably some effect up to 100 hertz or something, but it's pretty mild.
Speaker H: I don't know, in the strut implementation of this stuff, is there a high pass filter or pre-impicist or something?
Speaker F: I think we use a pre-impic basis.
Speaker H: So, we want to go and check that for anything that we're going to use the PDA mic for.
Speaker H: He says that there's a pretty good roll-off in the PCM mics.
Speaker H: So, we don't need to worry about them, I'm aware of the other, but if we do make use of the cheap mics, we want to be sure to do that that filtering before we process them.
Speaker H: And again, if it's depending on the option that our software is being runway, it's quite possible that's already being taken care of.
Speaker H: But I also have to pick a different picture to show the effects of reverberation.
Speaker E: Did somebody notice it during your talk?
Speaker H: No. Well, they may have, but they were...
Speaker H: They were nice. But, I mean, the thing is, since I was talking about reverberation and showing this thing that was noise, it wasn't a good match, but it certainly was still an indication of the fact that you get noise with distant mics.
Speaker H: It's just not a great example because not only isn't a reverberation, but it's a noise that we definitely know what to do.
Speaker H: It doesn't take deep, new bold new methods to get rid of the fiber-hearts noise.
Speaker H: But so, it was a bad example in that way, but it's still a real thing that we did get out of the microphone in a distance, so it wasn't wrong in the scene appropriate.
Speaker H: But someone noticed it later, right out to me, and they went, oh, man, why didn't I notice that?
Speaker H: So I think we'll change our picture around the web when we go that way.
Speaker H: One of the things I was trying to think about, what's the best way to show the difference?
Speaker H: I had a couple thoughts. One was that spectrogram that we show is okay, but the thing is, the eyes, and the brain behind them, so good at picking out patterns from noise, that in first glance, you look at them, it doesn't seem like it's that bad, because there's many features that are still preserved.
Speaker H: So one thing to do might be to just take a piece of the spectrogram where you can see that something looks different, and blow it up, and have that be the part, just to show as well.
Speaker H: Some things are going to be hurt.
Speaker H: Another I was thinking of was taking some spectral slices, like we look at with the recognizer, look at the spectrum or capstrom that you get out of there, and the reverberation does change that, so maybe that would be more obvious.
Speaker D: Spectral slices?
Speaker H: What do you mean?
Speaker H: I mean, all the recognizers look at frames, so that one instant time.
Speaker H: So it's one point in time, or over 20 milliseconds, or something, you have a spectrum or a capstrom, that's why I meant slice.
Speaker E: You could just throw up the MFCC feature vectors, one from one, one from the other, and then you can look and see how different the numbers are.
Speaker E: Right, what else are I saying either?
Speaker E: See how different these sequences are numbers.
Speaker H: Or I could just add them up and get a different total. It's not the square.
None: What else is going on?
Speaker F: Yeah, at first I had to remark, why, I am wondering why the PDA is so far. We were always meeting at the beginning of the table.
Speaker H: I guess because we had one of the moves. We could move us.
Speaker F: Yeah, so since the last meeting, we've tried to put together the clean, the OPS, the new filter that's replacing the Filters, and also the delay issues, so that we can consider the delay issue for the online organization.
Speaker F: So we've put together all this, and we have results that are not very impressive, but there is no real improvement.
Speaker H: But it's not worse, and it's better latency, right?
Speaker F: Actually, it's better. It seems better when we look at the mismatch case, but I think we are like cheated here by this problem that in some cases when you modify a slight, slightly modified initial condition, you end up completely somewhere error, somewhere else in the space.
Speaker F: Yeah.
Speaker F: Well, the other system, for instance, for Italian, is at 78% recognition rate on the mismatch.
Speaker F: This new system has 89%, but I don't think it indicates something really.
Speaker F: I don't think it means that the system is more robust, it's simply affected.
Speaker H: Well, the test would be if you then tried it on one of the other tests.
Speaker H: So this was Italian, right?
Speaker H: So then if you take your changes, then...
Speaker F: From the 78% recognition rate system, I could change the transition probabilities for the first HMM between the NWC and the MWC.
Speaker F: By using 0.5 instead of 0.6, 0.4, I think the HMMC is great.
Speaker E: Yeah, I looked at the results when Stefan did that, and it really happens.
Speaker E: I mean, the only difference is you change the self-loop transition probability by a tenth of a percent, and it causes 10% difference in the word of a percent.
Speaker E: Yeah, from point...
Speaker E: I'm sorry, you change a point one.
Speaker E: And then not a tenth of a percent, one tenth.
Speaker E: So from 0.6 to 0.5, and you get 10% better.
Speaker E: And I think it's what you basically hypothesized in the last meeting about just being very...
Speaker E: And I think you've mentioned this in your email too. It's just very... you know, you get stuck in some local minimum and this thing throws you out of it, I guess.
Speaker H: Well, what are... according to the rules, what are we supposed to do about the transition probability? Are they supposed to be 0.5 or 0.6?
Speaker E: I think you're not allowed to... yeah, it's supposed to be 0.6.
Speaker H: 0.6. It's supposed to be 0.6.
Speaker E: Yeah, but changing into 0.5, I think, is... which gives you much better results, but that's not allowed.
Speaker F: Yeah, but even if you miss 0.5, I'm not sure if we'll always give you the better results.
Speaker E: Yeah.
Speaker E: That's sad.
Speaker E: Right, we only tested it on the medium mismatch, right?
Speaker E: You said on the other cases you didn't notice.
Speaker F: I think the reason is... yeah, I...
Speaker F: So I see my name, I think.
Speaker F: So it's the fact that the mismatch is trained only on the far microphone.
Speaker F: Well, for the mismatch case, everything is... using the far microphone training testing.
Speaker F: Whereas for the I-N-Mismatch training is done on the close microphone, so it's clean speech-based.
Speaker F: So you don't have this program of looking at the microphone.
Speaker F: And for the well-match, it's a mix of close microphone and distant microphone.
Speaker E: I did notice something...
Speaker E: So it's not just the whole difficult training.
Speaker E: Somebody, I think, was Morgan suggested at the last meeting that I actually count to see how many parameters and how many frames.
Speaker E: And there are almost 1.8 million frames of training data and less than 40,000 parameters in the baseline system.
Speaker E: So it's very, very few parameters compared to how much training data.
Speaker H: Yeah, so that says that we could have lots more parameters.
Speaker E: Yeah, I did one quick experiment just to make sure I had everything worked out.
Speaker E: And I just... for most of the... for all of the digit models, they end up at three mixtures per state.
Speaker E: And so I just did a quick experiment, or I changed it so that it went to four.
Speaker E: And it didn't have any significant effect at the medium mismatch and high mismatch cases.
Speaker E: And it was just barely significant for the well-match better.
Speaker E: So I'm going to run that again, but with many more mixtures per state.
Speaker H: Yeah, because at 40, you could have... yeah, easily four times as many parameters.
Speaker E: And I think also, just seeing what we saw in terms of the expected duration of the silence model when we did this tweaking of the self-loop, the silence model expected duration was really different.
Speaker E: And so in the case where it had a better score, the silence model expected duration was much longer.
Speaker E: So it was like, it was a better match.
Speaker E: I think, you know, if we make a better silence model, I think that will help a lot too for a lot of these cases.
Speaker E: So, but one thing I wanted to check out before I increased the number of mixtures per state was in their default training script, they do an initial set of three re-estimations.
Speaker E: And then they build the silence model.
Speaker E: And then they do seven iterations.
Speaker E: Then they add mixtures. And then they do another seven. Then they add mixtures.
Speaker E: Then they do a final set of seven. And they quit.
Speaker E: Seven seems like a lot to me. And it also makes the experiments go take a really long time.
Speaker E: I mean, to do one turn around of the well-matched case takes like a day.
Speaker E: And so, you know, in trying to run these experiments, I noticed, you know, it's difficult to find machines, you know, compute to run on.
Speaker E: And so one of the things I did was I compiled HTK for the Linux machines.
Speaker E: Because we have this one from IBM that's got like five processors in it.
Speaker E: Right.
Speaker E: And so now you can run stuff on that. And that really helps a lot.
Speaker E: Because now we've got, you know, extra machines that we can use for compute.
Speaker E: And if I'm doing running an experiment right now where I'm changing the number of iterations from seven to three, to see how it affects the baseline system.
Speaker E: And so if we can get away with just doing three, we can do many more experiments more quickly.
Speaker E: And if it's not a huge difference from running with seven iterations, you know, we should be able to get a lot more experiments done.
Speaker E: And so I'll let you know what happens with that.
Speaker E: But if we can, you know, run all of these back ends with many fewer iterations.
Speaker E: And on Linux boxes, we should be able to get a lot more experimenting done.
Speaker E: So I wanted to experiment with cutting down the number of iterations before I increased the number of Gaussian.
Speaker H: So how is it going on the?
Speaker H: So you did some things.
Speaker H: They didn't improve things in a way that convinced you you'd substantially improved anything.
Speaker H: But they're not making things worse and we have reduced latency, right?
Speaker F: Yeah, but actually, actually it seems to do a little bit worse for the one-match case.
Speaker F: And we just noticed that actually the way the final score is computed is quite funny.
Speaker F: It's not a mean of word error rate.
Speaker F: It's not a weighted mean of error rate.
Speaker F: It's a weighted mean of improvements.
Speaker F: So which means that actually the weight on the well-match is...
Speaker F: Well, what happened is that if you have a small improvement or a small bit on the well-match case, it will have a huge influence on the improvement compared to the reference because the reference system is quite good for well-match case also.
Speaker E: So it weights the improvement on the well-match case really heavily compared to the improvement on the other cases?
Speaker F: It's a weighting of the improvement, not of the error rate.
Speaker E: Yeah, and it's hard to improve on the best case because it's already so good, right?
Speaker F: Yeah, but what I mean is that you can have a huge improvement on the HM case, like 5% absolute.
Speaker F: And this will not affect the final score, or most... this will not affect the final score because the improvement relative to the baseline is small.
Speaker H: So they do improvement in terms of accuracy rather than word error rate?
Speaker F: Improvements.
Speaker F: It's compared to the word error rate.
Speaker H: Okay, so if you have 10% error and you get 5% absolute improvement, then that's 50%.
Speaker H: Okay, so what you're saying then is that if it's something that has a small word error rate, then even a relatively small improvement on it, and absolute terms will show up is quite large in this. Is that what you're saying? Yes. Okay, but yeah, that's the notion of relative improvement.
Speaker H: Yeah.
Speaker H: Word error rate.
Speaker F: Sure, but when we think about the weighting, which is 0.5, 0.3, 0.2, it's an absolute... on relative figures.
Speaker F: Yeah.
Speaker F: So we look at this error rate?
Speaker H: That's why I've been saying we should be looking at word error rate and not at accuracy.
Speaker H: I mean, we probably should have standardized the 9 all the way through.
Speaker E: Well, I mean, it's not that different, right? I mean, just subtract the accuracy.
Speaker H: Yeah, but when you look at the numbers, your sense of the relative is quite different.
Speaker H: If you had 90% correct and 5% 5 over 90, it doesn't look like it's a big difference, but 5 over 10 is big.
Speaker H: So just when you're looking at a lot of numbers, getting a sense of what was important.
Speaker E: That makes sense.
Speaker F: Well, anyway, so yeah, so it hurts a little bit on the well match.
Speaker H: What's a little bit like?
Speaker F: Like, it's difficult to say because again, I'm not sure I have to...
Speaker E: Hey Morgan, do you remember that signif program that we used to use for testing signif?
Speaker E: Is that still valid? I've been using that.
Speaker H: Yeah, it was actually updated.
Speaker H: It was updated some years ago and cleaned it up, made some things better.
Speaker E: Okay, I should find that new one. I just used my old one from 92 or whatever.
Speaker H: Yeah, I'm sure it's not that different, but...
Speaker H: He was a little more rigorous as I recall.
Speaker F: So it's around like 0.5...
Speaker F: 0.6% absolute on the Daniel.
Speaker H: Worst?
Speaker H: Worst, yeah.
Speaker H: Out of what?
Speaker F: We start from 94.64, and we go to 94.04.
Speaker H: So that's 6...
Speaker E: 93.64, right?
Speaker E: Is the baseline.
Speaker F: Oh, the baseline.
Speaker F: Yeah.
Speaker F: But I'm not talking about the baseline.
Speaker E: Oh, oh, I'm sorry.
Speaker F: My baseline is the submitted system.
Speaker E: Oh, okay.
Speaker F: For finish, we start 93.84, and we go to 93.74.
Speaker F: And for Spanish, we were at 95.05, and we go to 93.61.
Speaker H: Okay, so we are getting hurt somewhat.
Speaker H: And is that what, you know what piece?
Speaker H: You've done several changes here.
Speaker H: Do you know what piece?
Speaker F: I guess it's the filter.
Speaker F: Because, well, we don't have complete result, but the filter...
Speaker F: So the filter with a shorter delay, earns on the Italian and the light.
Speaker F: And the other things like...
Speaker F: Don't sampling something from simple words.
Speaker F: I'm...
Speaker F: You're like normalization either.
Speaker E: So...
Speaker E: I'm really confused about something.
Speaker E: If we saw that making a small change, like, you know, a tenth to the self-loop had a huge effect, can we really make any conclusions about...
Speaker E: Yeah, that's differences in the stuff.
Speaker E: I mean, especially when they're this small.
Speaker F: I think we can be completely fooled by this thing.
None: But...
Speaker F: Well, yeah...
Speaker F: There is first this thing, and then...
Speaker F: I computed the confidence level on the different test sets.
Speaker F: And for the well-matched, they are around 0.6 percent.
Speaker F: For the mismatched, they are around...
Speaker F: I see 1.5 percent.
Speaker F: For the well-m, HM, they are also around 1.5.
Speaker H: But, okay, so these degradations you were talking about, were on the well-matched case.
Speaker H: Yeah.
Speaker H: Does the new filter make things better or worse for the other cases?
Speaker F: About the same, it doesn't hurt.
Speaker H: Doesn't hurt, but it doesn't get a little better or something.
Speaker H: No.
Speaker H: Okay, so...
Speaker H: I guess the argument one might make is that, yeah, if you look at one of these cases, and you jiggle something and it changes, then, you know, I'm quite sure what to make of it.
Speaker H: You look across a bunch of these, and there's some pattern.
Speaker H: I mean, so, here's all the...
Speaker H: If in all these different cases, it never gets better, and there's significant number of cases where it gets worse, then you're probably...
Speaker H: For any things.
Speaker H: That would say.
Speaker H: So, I mean, at the very least, that would be a reasonable prediction of what would happen with a different test set that you're not jiggling things with.
Speaker H: So, I guess the question is, if you can do better than this.
Speaker H: If we can approximate the old numbers while still keeping the latency down.
Speaker H: So...
Speaker H: So, what I was asking though is, what's the level of communication with the OGI gang now about this?
Speaker F: Yeah. When we are exchanging data, so, as we see, we have significant results.
Speaker F: For the moment, they are working on integrating the spectrosuppraction from Eric's.
Speaker F: Yeah. And so, yeah. We are working northside on other things, also trying to set spectrosuppraction, but...
Speaker F:...the spectrosuppraction.
Speaker F: Yeah, so I think it seems okay.
Speaker H: It's depending further discussion about this idea of having some sort of source code control.
Speaker F: Yeah. Well, for the moment, everybody is quite...
Speaker F: There is this Eurospeg deadline.
Speaker F: I see.
Speaker F: Yeah. But, yeah. As soon as we have something that's significant, that's better than what was submitted, we will fix this.
Speaker E: Okay, so, what is your answer to this?
Speaker H: Do you want me to take this on?
Speaker H: Yeah. Sounds like a good idea, but I think that that keeps saying people are scrambling for your Eurospeg deadline.
Speaker E: But that will be done in a week, so maybe after.
Speaker E: Wow, already a week, man.
Speaker H: You're right. It's amazing.
Speaker H: I think anything for your speech or...
Speaker F: We are trying to do something with the meeting recorded digits.
Speaker F: And the good thing is that there is this first deadline.
Speaker F: Yeah.
Speaker F: Well, some people from OGI are working on paper for this, but there is also the special session about Aurora, which is an extended deadline.
Speaker H: Oh, for your speech?
Speaker F: Yeah. So far.
Speaker H: Oh, special dispensation. That's great.
Speaker E: Where is your Eurospeg this year?
Speaker H: Alborg. Alborg.
Speaker H: So the deadline, once a deadline.
Speaker F: Once a deadline?
Speaker H: That's great.
Speaker H: So we should definitely get something after that.
Speaker H: But on meeting digits, maybe.
Speaker F: Yeah.
Speaker H: So I think that you could certainly start looking at the issue, but I think it's probably on...
Speaker H: It's from what Stefan is saying. It's unlikely to get sort of active participation
Speaker E: from the two sides until after they've... Well, I could at least...
Speaker E: Well, I'm going to be out next week, but I could try to look into like this CVS over the web. That seems to be a very popular way of people distributing changes and over, you know, multiple sites and things.
Speaker E: So maybe if I can figure out how to do that easily and then pass the information on to everybody that's easy to do is possible and people won't interfere with their regular work, then maybe that would be good.
Speaker E: And I think we could use it for other things around here too.
Speaker G: Good. That's cool.
Speaker D: And if you're interested in using CVS, I've set it up here.
Speaker D: Oh, great. Okay.
Speaker E: I used it a long time ago, but it's been a while, so maybe I can ask you some questions.
Speaker D: So I'll be away tomorrow and Monday, but I'll be back on Tuesday or Wednesday.
None: Okay.
Speaker H: You do the other thing, actually, is business about this waveform.
Speaker H: Maybe you and I can talk a little bit at some point about coming up with a better demonstration of the effects of reverberation for our web page.
Speaker H: So the...
Speaker H: Actually, the...
Speaker H: It made a good audio demonstration because when you play that clip, the really obvious difference is that you can hear two voices in the second one.
Speaker E: You can just...
Speaker G: Like, talk into a pet.
Speaker G: Yeah.
Speaker E: It's not good reverb.
Speaker H: No, I mean, it sounds pretty reverber, but I mean, when you play back in a room with a big room, nobody can hear that difference, really.
Speaker H: They hear that it's lower amplitude and they hear this second voice.
Speaker H: But that...
Speaker H: Actually, that makes for a perfectly good demo because that's real obvious thing.
Speaker H: Not a good two voices.
Speaker H: Well, that's okay, but for the visual, just, you know, like to have the spectrogram again because your visual abilities as a human being are so good, you can pick out...
Speaker H: You look at the good one, you look at the screwed up one, and you can see the features in it.
Speaker E: I noticed that in the pictures I thought, my initial thought was, this is not too bad.
Speaker E: Right.
Speaker H: If you look at it closely, you see, well, here's a place where this one has a big format.
Speaker H: The major formats here are moving quite a bit, and then you look in the other one, and they look practically flat.
Speaker H: So, I mean, that's why I was thinking in a section like that, you could take a look at just that part of the spectrogram, and you could say, oh, yeah, this really distorted it quite a bit.
Speaker E: The main thing that struck me in looking at those two spectrograms was the difference in the high frequencies.
Speaker E: It looked like, for the one that was far the way, you know, everything was tenuated.
Speaker E: Right.
Speaker E: I mean, that was the main visual thing that I noticed.
Speaker H: Right.
Speaker H: But it's, so, yeah, so there are clearly spectral effects.
Speaker H: Since you're getting all this indirect energy, then a lot of it does have reduced high frequencies.
Speaker H: But the other thing is the temporal courses, the things really are changed, and we want to show that in some obvious way.
Speaker H: The reason I put the waveforms in there was because they do look quite different.
Speaker H: And so I thought, oh, this is good, but I just, after you put in there, I didn't really look at them anymore.
Speaker H: So I just, they're different.
Speaker H: So, what's something that has a more interesting explanation for why they're different?
Speaker D: So maybe we can just substitute one of these waveforms, and then do some kind of zoom in on this spectrogram on an interesting area.
Speaker H: Something like that.
Speaker H: Yeah. The other thing that we had in there that I didn't like was that the most obvious characteristic of the difference when you listen to it is that there's a second voice.
Speaker H: And the cuts that we have there actually don't correspond to the full waveform.
Speaker H: It's just the first, I think there was something where he was having some trouble getting so much in, or I forget the reason behind it, but it's the first six seconds or something of it, and it's in the seventh or eighth second or something where the second voice comes in.
Speaker H: So we would like to actually see the voice coming in too, I think.
Speaker H: Since that's the most obvious thing when you listen to it.
Speaker G: So.
Speaker F: Yeah.
Speaker F: Yeah.
Speaker F: So.
Speaker F: Figures here. Well, we started to work on spectrograms of traction.
Speaker F: And the preliminary results were very bad.
Speaker F: So the thing that we did is just to add spectrograms of traction before this, the whole process which contains a monomine vision.
Speaker F: And it hurts a lot.
Speaker F: And so we started to look at things like this, which is, well, it's.
Speaker F: So you have the C0 bar meters for one Italian utterance, and I plotted this for two channels.
Speaker F: Channel zero is the closed-wave microphone, which is like just a microphone.
Speaker F: And it's perfectly synchronized.
Speaker F: And the sentence contains only one word, which is doing.
Speaker E: This is, this is a lot of C0, the energy.
Speaker F: There is a lot of C0 when we don't use spectrograms of traction.
Speaker F: And when there is no online normalization.
Speaker F: So there is just some filtering with the LDA.
Speaker F: So that's an example.
Speaker E: C0 is the closed-talking, the closed channel.
Speaker F: And channel one is the.
Speaker F: So C0 is very key.
Speaker F: Yeah.
Speaker F: Then when we apply normalization, it looks like the second figure.
Speaker F: Which is good, well, the noise part is around zero.
Speaker F: The third figure is what happens when we apply mineralization and variance.
Speaker F: So what we can clearly see is that on the speech portion, the two channels become very close.
Speaker F: But also what happens on the noisy portion is that the variance of the analysis.
Speaker E: This is still being a plot of C0.
Speaker E: Can I ask, what does variance normalization do?
Speaker E: What is the effect of that?
Speaker H: Normalization is the variance.
Speaker F: What does that mean?
Speaker F: No, I understand that.
Speaker E: No, I understand what it is.
Speaker E: What is it?
Speaker E: What's the rationale?
Speaker E: Yeah.
Speaker H: Why do it?
Speaker H: Well, I mean, because everything, if you have a system based on Gaussian, everything is based on means and variances.
Speaker H: So if there's an overall reason, you know, it's like if you're doing image processing.
Speaker H: And some of the pictures you were looking at, there was a lot of light.
Speaker H: And some there was low light.
Speaker H: You'd want to adjust for that in order to compare things.
Speaker H: And the variance is just sort of like the next moment.
Speaker H: So what if one set of pictures was taken so that throughout the course, it was one through daylight and night.
Speaker H: Ten times, another time.
Speaker H: I mean, it's, you know, how much, how much very, or, no, I guess a better example would be, how much of the light was coming in from outside rather than artificial light.
Speaker H: So if it was a lot, if more was coming in from outside, then it would be the bigger effect of the change.
Speaker H: So every mean, every, all of the parameters that you have, especially the variances, are going to be affected by the overall variance.
Speaker H: And so it's a bit, okay.
Speaker H: If you remove that source, then, you know, you can...
Speaker E: So the major effect is that you're going to get is by normalizing the means.
Speaker E: But it may first order, but first order, thank you.
Speaker H: But then the second order is the variances.
Speaker H: Because again, if you, if you're trying to distinguish between E and B, if it just so happens that the E's were more, you know, recorded when the energy was large.
Speaker H: It was, was, was larger or something, or the variation in it was larger.
Speaker H: Uh-huh.
Speaker H: Then the B's, then this will give you some, some bias.
Speaker H: So it's removing these sources of variability in the data that have nothing to do with the linguistic component.
Speaker H: Gotcha.
Speaker H: Okay. Sorry, dinner.
Speaker H: But the, let me, I just ask you something.
Speaker H: Is, if, if you have a good voice activity, a checker, isn't, isn't it going to pull that out?
Speaker F: Yes. Sure.
Speaker F: Sure. If you have a good, yeah. Well, what it shows is that, yeah, perhaps a good voice activity detector is, is good before magnumization.
Speaker F: And that's what we already observed.
Speaker F: But, yeah, voice activity detection is not...
Speaker E: But after you do this, after you do the variance normalization, I mean, I don't know, it seems like this would be a lot easier than this signal.
Speaker F: So, to work with... Well, I prefer to look at the second bigger than the third one, because you clearly see where a speech is.
Speaker F: Yeah. Yeah.
Speaker F: But the problem is that on the speech portion, channel zero and channel one are more different than when you use variance on the evaluation, where channel zero and channel one become closer.
Speaker E: But for the purposes of finding this speech, you're more interested in the difference between the speech and the non-speech, right?
Speaker F: So, I think, yeah, for... I think that perhaps it shows that the parameters that the voice activity detector should use, after use should be different than the parameters that have to be used for speech recognition.
Speaker H: Yeah. So, basically, you want to reduce this effect. So, you can do that by doing the voice activity detection. You also could do it by special spectral subtraction before the variance normalization, right?
Speaker F: Yeah, but it's not clear, man.
Speaker F: So... Well, it's just to... Yeah.
Speaker F: The number that are here are recognition experiments on the Italian, HM and M, with these two kinds of parameters.
Speaker F: But it's better with variance normalization.
Speaker H: Yeah. Yeah, so, that's good, better even out of luck, sogley.
Speaker H: But does this have the voice activity detection in it? Yeah.
Speaker F: Okay. So, the fact is that the voice activity detector doesn't work on the channel one, so... Yeah, channel one.
Speaker E: Yeah.
Speaker E: What stage is the voice activity detector applied? Is it applied here or after the variance normalization?
Speaker F: It's applied before variance normalization, so it's a good thing. Because I guess we're setting the detection on this. Yeah. Is it applied all the way back here?
Speaker F: It's applied... Yeah, something like this.
Speaker E: Maybe that's why it doesn't work for channel one.
Speaker F: It could perhaps do just mean normalization before... Nice guy.
Speaker H: Sort of a couple of other questions, which is if most of what the OGI folk are working with is trying to integrate this other spectrum subtraction, where are we working about it?
Speaker F: Speckless subtraction. Yeah. It's just... Well, it's another... They are trying to use the...
Speaker F: Ericsson, and we're trying to use something, nothing else. Yeah, and also to understand what happened, because...
Speaker F: Okay.
Speaker F: Well, when we do spectrum subtraction, actually, I think this is the two last figures.
Speaker F: It seems that after spectrum subtraction, speech is more emerging than...
Speaker G: Mm-hmm. Speech is more what?
Speaker F: Well, the difference between the energy of the speech and the energy of the spectrum subtracted no expression is larger.
Speaker F: Well, if you compare the first figure with this one, I'm trying to just get it's not the same, but if you look at the numbers, you clearly see that the difference between the zero of the speech and the zero of the nice portion is larger.
Speaker F: But when it advances that after spectrum subtraction, you also increase the variance of the zero.
Speaker F: So if you apply variance or my addition of this, it completely is...
Speaker F: Ericsson, everything. Yeah. So yeah.
Speaker F: And what they did at RGI is just...
Speaker F: They don't use online on my addition for the moment of spectrum subtraction, I think.
Speaker F: Yeah. I think as soon as they were trying to do an online addition, it would be a problem.
Speaker F: So yeah, we're working on the same thing, but I think...
Speaker F: With different systems.
Speaker H: Right. I mean, the...
Speaker H: In election, it's interesting to work on things one way or the other, but I'm just wondering if...
Speaker H: The list of things that there are to do, if there are things that we won't do because we got two groups doing the same thing.
Speaker H: That's...
Speaker H: Just asking.
Speaker E: There also could be... I mean, I can maybe see a reason for both working on it too, if...
Speaker E: You know, if you work on something else and you're waiting for them to give you spectral subtraction, I mean, it's hard to know whether the effects that you get from the other experiments you do will carry over once you've then bring in their spectral subtraction module.
Speaker E: So it's almost like everything's held up waiting for this one thing.
Speaker E: I don't know if that's true or not, but I could see how. Maybe that's what you were thinking.
Speaker H: I mean, we still evidently have a latency reduction plan, which isn't quite what you'd like it to be.
Speaker H: That seems like one prominent thing.
Speaker H: And then what there are issues of having a second stream or something, that was...
Speaker H: There was this business that we could use up at 4,800 bits.
Speaker F: Yeah, but I think we want to work on this day, also want to work on this.
Speaker F: Yeah, we will try MSG.
Speaker F: But...
Speaker F: I think they want to work on this second stream or something.
Speaker F: Some kind of...
Speaker F: Not get that one.
Speaker F: And they call it crap.
Speaker G: You know, it's crap.
Speaker H: Okay.
Speaker H: Do you remember when the next meeting is supposed to be?
Speaker H: In June.
Speaker H: Yeah.
Speaker H: Yeah, the other thing is that you saw that mail about the VAD.
Speaker H: VAD is performing quite differently.
Speaker H: So there was this experiment of what if you just take the bass line, a feature, just Melcapster, and you incorporate the different VADs.
Speaker H: And it looks like the French VAD is actually better.
Speaker H: It improves the bass line.
Speaker E: Yeah.
Speaker F: I don't know which VAD they use.
Speaker F: If they use the small VAD, I think it's easy to do better because it doesn't work at all.
Speaker F: So...
Speaker F: Which one is pretty better than the bass as well?
Speaker A: Yeah.
Speaker F: We should ask which VAD.
Speaker A: I think that he said with the good VAD of Prongoji with the Arcade VAD.
Speaker A: And the experiment was sometimes better, sometimes worse.
Speaker F: Yeah, but I think you're talking about the other way that you use VAD on the reference features.
Speaker F: Yes.
Speaker H: And on that one, the French one was better.
Speaker H: I mean, it was enough better that it would account for a fair amount of the difference between a performance, actually.
Speaker H: So if they have a better one, we should use it.
Speaker H: You know, it's...
Speaker H: You can't work on everything.
Speaker F: Yeah, so we should find out if it's really better, if it's...
Speaker F: Yeah.
Speaker F: Compared to the small, on a big band.
Speaker F: Yeah.
Speaker F: And perhaps we can easily prove if we put, like, the normalization before...
Speaker G: Yeah.
Speaker H: He can't go back in town the week after next, back in the country.
Speaker H: So start organizing more visits and connections and so forth.
Speaker H: Working towards students.
Speaker A: Also, Stefan was thinking that maybe it was useful to sing about voice and voice to work here in voice and voice detection.
Speaker A: And we are looking at the singing.
Speaker F: Actually, when we look at all the proposals, everybody is still using some kind of spectrum hand look.
None: Right?
Speaker H: No use of pitch, basically.
Speaker F: Yeah, well, the pitch, but to look at the...
Speaker F: Fine.
Speaker F: Well, that's just everyone to find the pitch and the sound.
Speaker F: It's another feeling that when we look at the...
Speaker F: Yeah, well, there is no way you can tell if it's for it and then for it.
Speaker F: If there is some...
Speaker F: It's easy in clean speech because voice sound, I'm all over it, I'll see.
Speaker F: So there will be more.
Speaker F: Yeah.
Speaker F: There is the first problem, just the larger.
Speaker F: And then voice sound or more eye frequencies because it's vacation.
Speaker F: Right.
Speaker F: But, yeah.
Speaker F: When you have noise, you have a low frequency, you know, you could be even for voice speech.
Speaker H: Yeah, you can make these mistakes, but...
Speaker E: Isn't there some other...
Speaker E: I think it would be good.
Speaker E: I'm not going to say, isn't there... aren't there lots of ideas for doing voice activity or speech non-speech rather by looking at, you know, I guess, harmonics or looking across time?
Speaker H: I think you sound about the voice non-voiced though.
Speaker E: Yeah.
Speaker E: Or that, you know, even with voice non-voiced and voiced, I thought that you were...
Speaker H: I'm really was talking about...
Speaker H: Well, yeah, we should have finished with it.
Speaker F: Okay.
Speaker F: It's good.
Speaker F: Yeah, so yeah, I think if we try to develop a second stream...
Speaker F: Well, there would be one stream that is the envelope and the second, it would be interesting to have something that's more related to the fact structure of the spectrum.
Speaker F: Yeah, so I don't know. We were thinking about like using ideas from Larry Sohn.
Speaker F: Have a good voice detector or...
Speaker F: Have a good voice speech detector.
Speaker F: Let's work in...
Speaker F: Larry Sohn could be an idea.
Speaker F: We were thinking about just kind of taking this background and computing the variance.
Speaker F: I have a resolution spectrum.
Speaker H: Okay, so today's something about that.
Speaker H: We had a guy here some years ago who did some work on making use of voicing information to help in reducing the noise.
Speaker H: So what he was doing is basically you do estimate the pitch.
Speaker H: Or you estimate fine harmonic structure, which are either way, it's more of the same.
Speaker H: The thing is that you then can get rid of things that are not...
Speaker H: If there is strong harmonic structure, you can throw away stuff that's not a harmonic.
Speaker H: And that is another way of getting rid of part of the noise.
Speaker H: So that's something that is sort of finer, brings in a little more information than just spectrum of subtraction.
Speaker H: And he did that sort of in combination with Rostovs, kind of like Rostovs, taking care of convolutional stuff.
Speaker H: And that's some decent results doing that.
Speaker H: But yeah, there's all these cues.
Speaker H: Actually, back when Chuck was here, we did some voice-to-voice classification using a bunch of these.
Speaker H: And works okay, obviously it's not perfect.
Speaker H: But the thing is that you can't, given the constraints of this task, we can't, in a very nice way, feed forward to the recognizer, the information, the probabilistic information that you might get about, whether it's voice-to-own voice, we can't affect the distributions or anything.
Speaker H: But what we, I guess we could.
Speaker E: Didn't the head dude send around that message?
Speaker E: I think you sent us all a copy of the message where he was saying that I'm not sure exactly what he was saying, but something having to do with the voice activity detector and that people shouldn't put their own in or something was going to be...
Speaker H: Okay, so that's voice activity detector as opposed to voicing detector.
Speaker H: So we're talking about something a little different.
Speaker H: I'm sorry.
Speaker H: I'm sorry.
Speaker H: I guess what you could do, maybe this would be.
Speaker H: So if you have, if you view this second stream, before you do KLTs and so forth, if you do view it as probabilities.
Speaker H: And if it's an independent, so if it's not so much envelope based, but fine structure based, looking at harm and acidity or something like that.
Speaker H: If you get a probability from that information and then multiply it by, you know, multiply it by all the voiced outputs and all the unvoiced outputs, you know.
Speaker H: Then use that as the take the log of that or pre-nonlinearity.
Speaker H: And do the KLT on that.
Speaker H: Then that would, I guess, be a reasonable use of independent information.
Speaker H: So maybe that's what you meant.
Speaker H: That would be...
Speaker F: Yeah, well, that would be...
Speaker F: Yeah.
Speaker F: So you'll mean that some kind of probability from the voice thing and that...
Speaker H: Right, so you have a second neural net, it could be pretty small.
Speaker H: Yeah, if you have a tandem system, you have some kind of, it could be pretty small, net.
Speaker H: We did some of this stuff.
Speaker H: I did some use go and you use...
Speaker H: The thing is to use information primarily that's different, as you say, it's more fine structure based than envelope based.
Speaker H: So then you can pretty much guarantee it's stuff that you're not looking at very well with the other one.
Speaker H: And then you only use it for this one distinction.
Speaker H: And so now you've got a probability of the two cases and you've got the probability of the finer categories on the other side, you multiply them more appropriate.
Speaker H: And if they really are from independent information sources, then they should have different kinds of errors and roughly independent errors.
Speaker H: That's a good choice for...
Speaker H: Yeah, that's a good idea.
Speaker F: Because yeah, well, spectrosuffraction is good and we could use the finer structure to let the finer scheme of the null.
Speaker F: But still, the very...
Speaker F: This issue with spectrosuffraction that seems to increase the variance of...
Speaker F: Yeah.
Speaker F: Well, this is a couple of...
Speaker F: Right.
Speaker F: I know in a few of you do some kind of a leg-on-eye addition at the end of the...
Speaker F: Well, spectrosuffraction has a leg-on-eye addition to...
Speaker H: What if you do the spectrosuffraction, do some spectrosuffraction first, then do some online normalization, then do some more spectrosuffraction?
Speaker H: I mean, maybe you can do it in layers or something, so it doesn't hurt too much or something.
Speaker H: But anyway, I think I was sort of arguing against myself there by giving that example.
Speaker H: I mean, because I was already sort of suggesting that we should be careful about not spending too much time on exactly what they're doing.
Speaker H: In fact, if you go into a harmonics-related thing, it's definitely going to be different than what they're doing and should have some interesting properties and noise.
Speaker H: I know that when people have done sort of the obvious thing of taking your feature vector and adding in some variables which are pitch-related or that it hasn't...
Speaker H: My impression is it hasn't particularly helped. It has not.
Speaker H: Yeah, but I think that's a question for this...
Speaker H: Extending the feature vector versus having different streams.
Speaker H: Was it nice and nice and conditioned for the example?
Speaker H: And it may not have been noisy conditions.
Speaker H: I don't remember the example, but it was on some DARPA data some years ago, so it probably wasn't, actually.
Speaker F: But we are thinking we're discussing very...
Speaker F: Perhaps...
Speaker F: I was thinking about some kind of cheating experiment. It's voicing bit.
Speaker H: Why don't you just do it with a rure?
Speaker H: Just in each frame...
Speaker F: We don't have a free-telling brush. We don't have a labeling...
Speaker F: We just have a labeling word model.
Speaker C: You have frame, frame, level.
Speaker H: But you can align so that it's not perfect, but if you know what was said...
Speaker E: The problem is that their models are all word-level models. There's no phone models that you get alignable.
Speaker E: You see, you can find out where the word boundaries are, but that's about it.
Speaker H: Yeah. I see.
Speaker C: But we could use the noisy version of Timit, which is similar to the noises found in the TI digits.
Speaker C: The portion of our aura.
Speaker F: I guess we can see nothing to be out.
Speaker F: If this voicing bit doesn't help...
Speaker F: I think we don't have to...
Speaker H: We want more about this.
Speaker H: Right.
Speaker H: In experiments, we did a long time ago, and it was probably a resource management or something.
Speaker H: I think you were getting something like still 8% or 9% error on the voicing as I recall.
Speaker H: So what that said is that sort of love to its own devices, like without a strong language model and so forth, that you would make significant number of errors just with your probabilistic machinery.
Speaker E: It also... I think there was one problem with that in that we used canonical mapping. So our truth may not have really been true to the acoustics.
Speaker H: Yeah. Well, back 20 years ago when I did this voiced-on-voiced stuff, we were getting more like 97% or 98% correct in voicing, but that was speaker-dependent.
Speaker H: Actually, we were doing training on a particular announcer and getting a very good handle on the features.
Speaker H: We did this complex feature selection thing where we looked at all the different possible features one could have for voicing and exhaustively searched all-size subsets.
Speaker H: For that particular speaker, you'd find the five or six features which really did well.
Speaker H: Doing all of that, we get down to 23% error, but that, again, the speaker-dependent with lots of feature selection and very complex sort of thing.
Speaker H: So I would believe that it was quite likely that looking at envelope-only that would be significantly worse than that.
Speaker F: And the speech-cordes?
Speaker F: Yeah, I do. What do they even have to detect first? The modern ones don't do a simple switch. They work on the code book, excitation.
Speaker H: Yeah, they do analysis by synthesis. They try every possible excitation they have in their code book and find the one that matches best.
Speaker F: Yeah.
Speaker E: Okay. Can I just mention one other interesting thing? One of the ideas that we had come up with last week for things to try to improve the system.
Speaker E: I guess I wrote this in after the meeting, but the thought I had was looking at the language model that's used in the HTK recognizer, which is basically just a big loop.
Speaker E: So it goes digit, and then that can either go to silence or go to another digit, which that model will allow for the production of infinitely long sequences of digits.
Speaker E: So I thought, well, I'm going to just look at the what actual digit strings do occur in the training data. And the interesting thing was it turns out that there are no sequences of two long or three long digit strings in any of their training data. So it's either one, four, five, six, up to 11, and then it skips, and then there's some at 16.
Speaker H: But what about the testing data?
Speaker H: I don't know. I didn't look at the test data yet. So if there's some testing data that has, has two or three.
Speaker E: Yeah, but I just thought that was a little odd that there were no two or three long.
Speaker E: So I just, for the heck of it, I made a little grammar, which had its separate path for each length digit string you could get.
Speaker E: So there was a one long path, and there was a four long and a five long. And I tried that and it got way worse. There were lots of deletions. So it was, you know, I didn't have any weights on these paths, or I didn't have anything like that.
Speaker E: And I played with tweaking the word transition penalties a bunch, but I couldn't go anywhere. But I thought, well, if I only allow, I guess I should have looked at to see how often there was a mistake where a two long or three long path was actually put out as a hypothesis.
Speaker E: So to do that right, you'd probably want to have a lot for them all, but then have weightings and things. So I just thought that was an interesting thing about the data.
Speaker H: Okay, so we're going to read some more just strings, I guess.
Speaker E: Yeah.
Speaker H: I'm going to go ahead and read it.
Speaker H: 5, 5, 4, 0, 1, 8, 8, 7, 2, 2, 4, 6, 3, 8, 5, 3, 9, 5, 6, 1, 6, 1, 4, 0, 2, 9, 4.
Speaker H: 737-339.
Speaker H: 3126-114850.
Speaker H: 685-3-741-3923.
Speaker H: 917-839-7546.
Speaker F: Transcript L-19.
Speaker F: 874-30-928.
Speaker F: 859-43471.
Speaker F: 020-975-009.
Speaker F: 677-601-5254.
Speaker F: 423-575-426.
Speaker F: 903-4-6231.
Speaker F: 380-08007-8351.
Speaker F: 471-929-180.
Speaker E: Transcript L-16.
Speaker E: 5608-425566.
Speaker E: 735-475-477.
Speaker E: 037-715-505.
Speaker E: 3255-8169-34.
Speaker E: 393-057-019.
Speaker E: 588-625-7698.
Speaker E: 850-33434.
Speaker E: 230-46550.
Speaker D: Transcript L-17.
Speaker D: 946-470139.
Speaker D: 671-268-209.
Speaker D: 7500-462280.
Speaker D: 527-1-7133-5202.
Speaker D: 616-1120-8959.
Speaker D: 031-522-71.
Speaker D: 388-420-8457.
Speaker D: 680-4835-00.
Speaker A: Transcript L-18.
Speaker A: 585-771-443.
Speaker A: 9241-91-301-1.
Speaker A: 784-528-3698.
Speaker A: 739-724-523-96.
Speaker A: 572-921-08.
Speaker A: 698-2.
Speaker A: 288-541.
Speaker A: 794-9.
Speaker A: 792-420-563.
Speaker A: 567-978-558-1.
Speaker B: Transcript L-21.
Speaker B: 010-453366.
Speaker B: 125-545-434.
Speaker B: 410-6960-7230.
Speaker B: 793-1-4150.
Speaker B: 417-083-532-4.
Speaker B: 165-687-594.
Speaker B: 377-5030-568.
Speaker B: 747-999-119.
