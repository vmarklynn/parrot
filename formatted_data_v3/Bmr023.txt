None: Hey, we're recording.
None: Oh, wait a minute, wait a minute.
None: Oh boy, I got the harness.
Speaker I: What's the channel?
Speaker A: Make sure to turn your microphone on.
Speaker A: There's a battery.
Speaker F: There we go.
Speaker A: Your channel number is already on this blank sheet.
Speaker D: Channel 5.
Speaker F: Channel 5.
Speaker F: Channel 5.
Speaker F: Channel whatever.
Speaker F: Camera 1.
Speaker F: Camera 2.
Speaker F: Channel 4.
Speaker D: Channel 5.
Speaker D: Channel 4.
Speaker A: The game's up at it what it usually is.
Speaker A: But if you think it's sort of a default.
Speaker A: But I can set it higher if you like.
Speaker E: Yeah.
Speaker D: Yeah.
Speaker D: Test test test test test test.
Speaker D: Okay, that seems better.
Speaker D: Yeah, okay, good.
Speaker D: That's good.
Speaker D: That's good.
Speaker E: Okay.
Speaker D: So I had a question for Adam.
Speaker D: Have we started to read?
Speaker A: Well, we started recording.
Speaker D: Yeah.
Speaker D: It's January.
Speaker D: I saw it earlier.
Speaker G: She can just walk in, I guess.
Speaker A: Yeah, right.
Speaker A: So we're starting late.
Speaker A: I figured we'd better just start.
Speaker D: I was going to ask Adam to say if he thought any more about the demo stuff.
Speaker D: It occurred to me that this is late May and the DARPA meetings in mid-July.
Speaker D: But I remember we...
Speaker D: I know we were going to do something with the transcriber interface.
Speaker D: There's one thing, but I thought there was a second thing.
Speaker D: Anybody remember?
Speaker A: Well, we were going to do a mock-up question answering or something I thought.
Speaker A: That was totally separate from the interface.
Speaker A: Do you remember?
Speaker A: Remember, like, asking questions and retrieving, but in a pre-stored fashion.
Speaker A: That was the thing we talked about, I think, before the transcriber.
Speaker A: Yeah.
Speaker D: All right.
Speaker D: So anyway, we have to sort of that out.
Speaker D: Somebody going on it.
Speaker D: Got a month left, basically.
Speaker A: You like these, right?
Speaker D: Okay, good.
Speaker D: Okay.
Speaker D: Okay, so what do we get else we got?
Speaker D: You just wrote about your stuff.
Speaker A: No, that was all previously here.
Speaker A: I was writing the digits and then I realized I could see Roxas.
Speaker E: Oh, okay.
Speaker A: Because I didn't want people to turn their heads from these microphones.
Speaker A: We have, by the way, have the same digit form for the record.
Speaker D: That's cool.
Speaker D: So the choice is which we want more the comparison.
Speaker D: Everybody is saying it at the same time or the comparison of people saying the same digits at different times.
Speaker A: It's just because I didn't have any more digit sheets.
Speaker A: I know that.
Speaker D: Which opportunity should we exploit?
Speaker A: It might be good to have them separately and have the same exact strings.
Speaker A: I mean, you could use them for normalizing or something.
Speaker A: But it, of course, goes more quickly doing a menu.
Speaker A: I guess we'll see.
Speaker A: I guess we'll see how long we go.
Speaker D: How long we go and how good this snack is out there.
Speaker H: Anyway, there's some pants talked to us.
Speaker H: Somebody is saying zero in some things.
Speaker H: Right.
Speaker H: Right.
Speaker H: It's really not identical.
Speaker D: Yeah, we'd have to train.
Speaker A: We'd be like a chorus.
Speaker D: We'd have to get some experience.
Speaker D: Yeah.
Speaker D: Really boring.
Speaker D: Chorus.
Speaker D: Do we have an agenda?
Speaker D: I had them usually tries to put that together.
Speaker D: I've got a couple of things to talk about.
Speaker D: Yeah.
Speaker C: What might those be?
Speaker C: IBM stuff and just getting meeting information organized.
Speaker D: Okay.
Speaker D: You're applying that it's currently disorganized.
Speaker I: In my mind.
Speaker D: Is there stuff that's happened about the SRI, recognizer, et cetera, those things that were happening before with?
Speaker D: You guys were doing bunch of experiments with different finance.
Speaker D: Is that still sort of where it was the other day we're improving?
Speaker C: Yeah.
Speaker C: Now, you saw the note that the PLP now is getting basically the same as the MOTC.
Speaker C: Yeah.
Speaker I: Actually, it looks like it's getting better.
Speaker I: Just with age.
Speaker I: With age.
Speaker I: Yeah.
Speaker I: But that's not tricky related to me.
Speaker I: So we can talk about it.
Speaker I: It looks like I haven't, but it's the experiment is still not complete.
Speaker I: But it looks like the Polotract-Actualization is working good.
Speaker I: Like, actually, using the warp factors that we computed for the SRI system, just applying them to the PC.
Speaker I: That's pretty funny.
Speaker A: So you just need to copy of it.
Speaker I: Just have to take the supercult to the number of the data.
Speaker I: They have different meanings in the system.
Speaker D: Yeah, all that's always good to do.
Speaker I: One issue, actually, that just came up in discussion with Liz and Darn, was, as far as meeting recognition is concerned, we would really like to move to doing the recognition on automatic segmentations.
Speaker I: Because in all our previous experiments, we were essentially cheating by having the hand segmentation system based on the recognition.
Speaker I: And so now, with Tilo's segment, we're working so well.
Speaker I: I think we should...
Speaker D: We think we should increase the error rate.
Speaker D: Yeah.
Speaker H: That's what I wouldn't do.
Speaker A: Yeah.
Speaker A: And even the good thing is that since you have high recall, even if you have low precision because you're over-generating, that's good, because we could train noise models and the recognizer for these kinds of transients and things that come from the microphones.
Speaker A: But I know that if we run recognition unconstrained on a whole waveform, we do very poorly because we're getting insertions in places that you may well be cutting out.
Speaker A: So we do need some kind of pre-sequentation.
Speaker I: Some extra things like retraining or adapting the models for background noise to the environment.
Speaker A: And using Tilo's, you know, pustyriers or some kind of...
Speaker A: Or right now they're discreet, yes or no, for a speaker to consider those particular speaker background models.
Speaker A: There's lots of interesting things that could be done.
Speaker D: Yeah, if you could do that.
Speaker D: So when we do the IBM stuff?
Speaker C: Yeah, so talked with Brian and gave him the alternatives to the single beep at the end of each utterance that we had generated before.
Speaker C: And so...
Speaker C: Truck, trunks.
Speaker C: Yeah, truck, trunks, right.
Speaker C: And so he talked over the transcriber and the transcriber thought that the easiest thing for them would be if there was a beep and then a number of digit and then a beep at the beginning of each one.
Speaker C: And that would help keep them from getting lost.
Speaker C: And so Adam wrote a little script to generate those style beeps.
Speaker C: And so where...
Speaker C: I think that the digits are...
Speaker C: I came up here and just recorded the numbers one through ten.
Speaker C: So...
Speaker C: That's a great idea.
Speaker C: So, yeah, we just used those.
Speaker I: A few splice to the right for four.
Speaker C: Yeah, I recorded, actually I recorded one through ten three times, three different speeds, and then he picked.
Speaker C: He liked the fastest one.
Speaker C: So he just cut those out and spliced them in between two beeps.
Speaker H: It will be funny.
Speaker H: It will be funny when you're really reading digits and then there are the trunks with...
Speaker H: Yeah, with my digits.
Speaker C: All right.
Speaker C: That'll throw.
Speaker D: Maybe we should have you record a bc for that one.
Speaker D: And she said it wasn't going to...
Speaker C: The transcriber said it wouldn't be a problem because they can actually make a template that has beep, number, beep.
Speaker C: So for them it will be very quick to put those in there and then transcribing.
Speaker C: So we're going to send in one more sample meeting and Tilo is running his segmentation.
Speaker C: Adam is going to generate the chunk file and then we'll give it to Brian and they can try that out.
Speaker C: When we get that back we'll see if that sort of fixes the problem we had with too many beeps in the last transcription.
Speaker D: Okay.
Speaker D: Okay. Do you have any idea to turn around on those steps you just said?
Speaker C: Are on our side or including IBMs?
Speaker C: Including IBMs.
Speaker C: Well I don't know. The last one seemed like it took a couple of weeks.
Speaker C: Maybe even three.
Speaker C: That's just the IBM side.
Speaker C: Our side is quick. I mean, how long does your...
Speaker C: Well I'm at the overall thing.
Speaker D: The reason I'm asking is because Jane and I have just been talking and Jesus has been doing a further hiring of transcribers.
Speaker D: We don't really know exactly what they'll be doing and how long they'll be doing it and so forth.
Speaker D: Because right now she has no choice but to operate in the mode that we already have working.
Speaker D: So it'd be good to sort of get that resolved.
Speaker C: I hope we can get a better estimate from this one that we send them.
Speaker C: So I don't know yet.
Speaker D: Yeah.
Speaker D: In particular, I would really hope that when we do the start meeting in July that we sort of have...
Speaker D: We're into production mode.
Speaker D: We actually have a stream going and we know how well it does and how it operates.
Speaker D: I think that would be a very good thing to be able to do.
Speaker E: Okay.
Speaker D: Maybe before we do the meeting info organized thing, maybe you could say a little bit stuff about where we are in transcriptions.
Speaker B: Okay. So the heat transfer was continued to our past one concept one, which was the second thing.
Speaker B: Talking about it at this point.
Speaker B: They got in five meetings down in that set right now they're in the positive end.
Speaker D: I hired two transcribers today and he hired another one, which was because we had a lot of nutrition.
Speaker C: They died off after they do this for a hell.
Speaker B: Burn out.
Speaker B: Well, that was a nice thing.
Speaker B: One of them had never planned a work past January.
Speaker B: It means that all these various things, because we represented it as possibly month project at 10.
Speaker B: And it's not an extensive nutrition way to be productive to two, but they're really solid.
Speaker B: We're really lucky to really have a lot of me.
Speaker A: I mean basically spending money on coffee to truck drives off
Speaker B: Computer safety sprints Backbie I'm just saying, the key point right now is to keep the staff on the leaner side, rather than hiring like eight to 10 right now, because if the IBM thing comes through really quickly, then we don't want to have to make people all the time so.
Speaker B: That's why I think. I got really a lot of response from my notice and I think I could hire additional people online.
Speaker D: Yeah, and the other thing is, I mean, in the unlikely event, since we're so far from this, it's a little hard to plan this way, in the unlikely event that we actually find that we have transcribers on staff who are twiddling their thumbs because there's all the stuff that was sitting there has been transcribed and they're faster, the pipeline is faster than the generation.
Speaker D: In the event that they actually don't, I bet we could find some other stuff for them to do. So I think that as we were talking, if we hired 12, then we could rent to a problem later. We also just couldn't sustain that for everything, but also it's a reason.
Speaker D: But if we hire, we have five on staff, five or six on staff, then we give time, then it's a small enough number so we can be flexible either way.
Speaker A: It'd be great too if we can, we might need some help again getting the tighter boundaries or some hand to experiment with, you know, to have a ground truth for this segmentation work, which I guess you have some already that was really helpful and could probably use more.
Speaker H: Yeah, that was the thing I planned working on is to use the transcriptions which are done by now and to use them as...
Speaker H: The new ones with the tighter boundaries, yeah.
Speaker H: And to use them for training or for whatever, yeah, to create some speech, non-speech labels out of them. But that's the thing was what I am just looking into.
Speaker H: Okay.
Speaker B: So, there you go, presentation to so much, I was so extremely helpful.
Speaker B: Now there was a couple weeks ago, I needed some new ones and it happened to be during the time that we was on the presentation.
Speaker B: I thought it was just very few days, but I happened to be during that time on humans, so I started them on the non-presegment, and then switched them on in two years and they always appreciate that.
Speaker B: And he's really, really appreciate it.
Speaker B: I was going to say that they do adjust once in a while, you know, once in a while, and they actually talk to them, didn't you?
Speaker H: Yeah, I talked to Helm.
Speaker B: And so I asked her, they're very perceptive.
Speaker B: I really want to have this meeting of trans farms.
Speaker B: I haven't done it yet, but I want to do that, and she's out there for a couple of weeks.
Speaker B: I'm going to do it as she returns.
Speaker B: Because she was saying in a span of very short-fared, it seems like the ones that need to be adjusted are these things.
Speaker B: As she was saying short-fared illnesses, you're here, you're aware of this.
Speaker B: But actually, it's so correct for so much of the time, it's not all this time, it's just this route to a long time.
Speaker A: That's great.
Speaker B: I think it would be interesting to take a moment.
Speaker A: Is there actually a record of where they change?
Speaker A: I mean, you can compare it, do a diff on the, just so that we knew...
Speaker B: It's complicated in that...
Speaker H: I feel when they create new segments or something, it will be not that easy, but...
Speaker A: I mean, if we keep an old copy of the old time marks, just so that if we run it, we know whether we're, which ones we're cheating and...
Speaker B: It would be great, which is a lot of good.
Speaker B: And then, when they start partly through, then when I do as I merge, I'll think down, they've been pre-signed into version.
Speaker B: So it's not a pure, it's not a pure condition.
Speaker B: And then, I think that they started with pre-signed methods and work with signal all the way through.
Speaker B: And I think it wasn't possible for that for many reasons, but it will be possible for future.
Speaker A: That's great.
Speaker A: As long as we have a record, I guess, of the original automatic one, we can always find out how well we would do from the recognition side by using those boundaries, you know, a completely non-shading version.
Speaker A: So if you need someone to record this meeting, I mean, I'd be happy to, for the transcribers, I could do it or chuck her.
Speaker D: So, you were saying something about organizing the meeting intro?
Speaker C: Yeah, so Jane and Adam and I had a meeting where we talked about the reorganization of the directory structure for all of the meeting record.
Speaker C: No.
Speaker C: For all of the meeting record today, we should have.
Speaker C: And so we've got a plan for what we're going to do there.
Speaker C: And then Jane also prepared a, started getting all of the meetings organized so she prepared a spreadsheet, which I spent the last couple of days adding to.
Speaker C: So I went through all of the data that we have collected so far and have been putting it into a spreadsheet with start time, the date, the old meeting name, the new meeting name, the number of speakers, the duration of the meeting, comments, you know, what it's transcription status is, all that kind of stuff.
Speaker C: And so the idea is that we can take this and then export it as HTML and put it on the meeting recorder.
Speaker C: Oh, great.
Speaker C: Keep people updated about what's going on.
Speaker C: I've got to get some more information from Jane because I have some gaps here that I need to get her to fill in.
Speaker C: But so far as of Monday, the 14th, we've had a total number of meeting, 62 hours of meetings that we've collected and some other interesting things.
Speaker C: Average number of speakers per meeting is six.
Speaker C: And I'm going to have on here the total amount that's been transcribed so far, but I've got a bunch of, that's what I have to talk to Jane about, figure out exactly which ones have been completed and so forth.
Speaker C: But this will be a nice thing that we can put up on the website and people can be informed of the status of various different ones.
Speaker C: And it'll also list, like under the status, if it's at IBM or if it's at ICSI or if it's completed or which ones were excluding, and there's a place for comments so we can say why we're excluding things and so forth.
Speaker D: Now with the ones that are already transcribed, we have enough there that, you know, we've already done some studies and so forth.
Speaker D: And shouldn't we go through and do the business of having the participants approve it for the transcriptions for distribution and so forth?
Speaker B: I would say as, although I still am doing some final pass and trying to convert it into a master file is being the channelized version of this.
Speaker B: It seems like I get into that a certain way that something else in the case is going to have to start cleaning up the things like the places where transcribed were in a certain way.
Speaker B: Doing a slouching here and there.
Speaker B: So I guess we may sense the weight of that's done.
Speaker D: Well, let me put another sort of milestone kind of as I get with the pipeline.
Speaker D: We are going to have this DARPA meeting in the whole July and I think it'll be given that we've been, we've given a couple public talks about already spaced by months and months.
Speaker D: I think it'll be pretty bad if we continue to say none of this is available.
Speaker D: We want to be able to say here is a subset that is available right now and that's been through the legal issues and so forth.
Speaker D: And they don't have to.
Speaker I: For July.
Speaker I: You know the netted version they can just give their approval to whatever version.
Speaker D: Well, in principle yes, but I mean if if if if somebody actually did get into some legal issue with it.
Speaker I: Yeah, but I mean the editing will continue presumably if there is a found they will be fixed but they won't change the content of the meetings.
Speaker A: Well, if Jane is clarifying question question, then you know how can they agree to it before they know her final version.
Speaker B: And the thing is, the subtleties where a person uses this word instead of that word which could have been transferred in the other way.
Speaker B: And not and they wouldn't have been signed or submitted in the other word.
Speaker D: You know there is a point at which I agree it becomes ridiculous because you know you could do this final thing and then a year from now somebody could say you know that should be a period not a question.
Speaker D: And you don't you there's no way that we're going to go back and ask everybody do you approve this document now.
Speaker D: So I think what it is is that the the the the thing that they sign I looked at it while but it has to be open enough that it sort of says, okay from now on.
Speaker D: Now that I've read this you can use do anything you want with these data.
Speaker D: And but I think we want to so assuming that it's that kind of wording I don't remember I think we just want to have enough confidence ourselves that it's so close to the final form is going to be in a year from now that they are.
Speaker B: It's just a question of if the person is using the transfer does the way of them judging what they said when it was signed as.
Speaker D: Well I forget how we end I figured how we ended up on this but I remember my taking the position of not making it so so easy for everybody to observe everything and Adam was taking the position of of having it be really straightforward for people to check every aspect of it including the audio.
Speaker D: I don't remember who won Adam.
Speaker D: That's really nice again because I can't remember how we ended up that it was the transcript he wanted to do a web interface that would make it.
Speaker D: That would give you access to the transcript and the audience that's what Adam wanted and I remember how we ended up.
Speaker A: I mean with the web interface it's interesting because you could allow the person who signs to be informed when their transcript changes or something like that.
Speaker A: I would say no like I don't want to know but some people might be really interested and then in other words they would be informed if there was some significant change other than typos and things like that.
Speaker A: I was like you were whispering satanic and continuing under your breath and you were like you know the small heads thing but I'm just saying that like you can sort of say that any things that are deemed...
Speaker A: Anyway I agree that at some point people probably won't care about typos but they would care about significant meaning changes and then they could be asked for their consent I guess if those change.
Speaker A: Because assuming we don't really distribute things that have any significant changes from what they sign anyway.
Speaker I: How about having an approval of the audio and of the transcripts?
Speaker A: Oh my god. But no one will listen to hours and hours of...
Speaker A: That's like...
Speaker I: Unfortunately, this is not a such transcript.
Speaker A: Really?
Speaker A: I think that's a lot to ask for people that have been in a lot of meeting.
Speaker D: We've gone down this path a number of times I know this can lead to extended conversations and not really getting anywhere so let me just suggest that offline that the people involved figured out and take care of it before it's still alive.
Speaker D: So that in July we can tell people yes we have this and you can use it.
Speaker D: So let's see, what else we got?
Speaker D: Don did the report about his project in class and we're all making a version so that was stuff he was doing with you.
Speaker A: I guess one thing we're learning is that we have eight meetings there because we couldn't use the non-native, all non-native meetings.
Speaker A: It's probably below threshold on enough data for us for the things we're looking at because the prosotic features are very noisy and so you need a lot of data in order to model them.
Speaker A: So we're starting to see some patterns and we're hoping that maybe with I don't know, double or triple the data with 20 meetings or so that we would start to get better results.
Speaker A: But we did find that some of the features that Jane would know about that are expressing sort of the distance of boundaries from peaks in the utterance and some local range pitch range effects like how close people are to their floor.
Speaker A: So we're also going to be able to see some of the different features that are showing up in these classifiers which are also being given some word features that are cheating because they're true words.
Speaker A: So these are based on force alignment. Word features like word frequency and whether or not something is a back channel and so forth.
Speaker D: So we're starting to see I think some interesting things including everything where those quasi cheating things.
Speaker F: I think depends what you're looking at actually sometimes positions and sentences obviously or in spurts was helpful.
Speaker A: I don't know if that's cheating too.
Speaker A: Spurts is not cheating except that of course you know the real words but roughly speaking the recognized words are going to give you a similar type of position.
Speaker A: Right.
Speaker F: Either earlier or maybe you gave a number of words.
Speaker A: Not exactly but it should be.
Speaker A: Well we don't know and actually that's one of the things we're interested in doing is I think time position like when the word starts.
Speaker I: I don't know if I can start.
Speaker I: Yeah.
Speaker F: I would have to know how these things to do.
Speaker F: Like there's a lot of different features you could just pull out.
Speaker I: Right.
Speaker I: Right.
Speaker A: And it depends on speaking right.
Speaker A: Yeah.
Speaker A: One of the interesting things was I guess you reported on some punctuation type finding sense boundaries finding disfluency boundaries and then I had done some work on finding from the foreground speech whether or not someone was likely to interrupt.
Speaker A: So where you know if I'm talking now and someone and Andreas is about to interrupt me is he going to choose a certain place in my speech either prosaacically or word based.
Speaker A: And there the prosaac features actually showed up and a neat thing even though the word features were available and a neat thing there too is I tried some putting the speaker so I gave everybody a short version of their name so the real names are in there which we couldn't use.
Speaker A: Should use IDs or something and those don't show up so that means that overall.
Speaker A: It wasn't just modeling Morgan or it wasn't just modeling single person.
Speaker A: But was sort of trying to get a general idea the tree classifier was trying to find general locations that were applicable to different speakers even though there are huge speaker effects so.
Speaker A: But the main limitation now is I because we're only looking at things that happen every 10 words or every 20 words we need more more data and more data per speaker.
Speaker A: So it also be interesting to look at the EDU meetings because we did include meeting type as a feature so whether you were in a meeting recorder meeting or robustness meeting did matter to interrupts because there are just fewer interrupts in the robustness meeting.
Speaker A: And so the classifier learns more about Morgan than it does about sort of the average person which is not bad.
Speaker A: It probably do better than but it wasn't generalizing.
Speaker A: So it's and I think Don what we have a long list of things he's starting to look at now over the summer where we can he'll be able to report on more things in the future but it was great that we could at least go from the you know James transcripts and the recognizer output and get it to this point and I think it's something Mara can probably use in her preliminary report like yeah we're at the point where we're training these classifiers and we just reporting very preliminary but suggestive results that some features both word and prozotic work.
Speaker A: The other thing that was interesting to me is that the pitch features are better than in switchboard and I think that really is from the close talking mics because the pitch processing that was done has much cleaner behavior than the switchboard telephone bandwidth.
Speaker A: First of all the pitch tracks are have less having the doubleings than switchboard and there's a lot less dropout so if you ask how many regions where you would normally expect some balls to be occurring are completely devoid of pitch information.
Speaker A: In other words a pitch tracker just didn't get a high enough probability of voicing for words for for you know five word they're much fewer than in switchboard so the missing we had a big missing data problem in switchboard and so the features weren't as reliable because they were often just not available.
Speaker C: So that's actually good with the lower frequency cutoff on the support.
Speaker A: Maybe I mean the telephone we had telephone bandwidth for switchboard and we had the annoying sort of telephone handset movement problem that I think may also affect it.
Speaker A: So we're just getting better signals in this data which is nice.
Speaker A: Anyway, Dawn's been doing a great job and we hope to continue with Andreas's help and also some of Tilo's help on this to try to get a non cheating version of how all this would work.
Speaker D: I think just talk about this the other day but has anybody had a chance to try changing insertion penalty sort of things with the using the tandem system.
Speaker I: Oh yeah I tried that. It didn't help dramatically.
Speaker C: Were they out of balance? I didn't notice.
Speaker I: We were a little relative number of, I think there were higher number of collisions actually.
Speaker I: Deletions?
Speaker I: So actually it preferred to have a positive, so negative insertion penalty which means that but you know it didn't change by adjusting that the, yeah the arrow changed by probably 1% or so.
Speaker I: I don't know if that's not the problem.
Speaker I: That's not the problem.
Speaker I: But we just, you know, Chuck and I talked and the next thing to do was probably to tune the size of the Gaussian system to this feature vector which we haven't done at all.
Speaker I: We just used the same configuration as we used for the standard system. And for instance, then they understand me a message saying that see me used something like 10 Gaussian per cluster.
Speaker I: Each mixture has 10 Gaussian.
Speaker I: We're using 64.
Speaker I: So that's obviously a big difference and it might be way off and give a very poorly trained, you know, Gaussian set way and poorly trained mixture.
Speaker I: So we have the turnaround time on the training when we train only the male system.
Speaker I: So our small training set is less than 24 hours. So we can run lots of, basically just brute force try a whole bunch of different settings with the new machines that will be better.
Speaker D: Yeah, we get 12 of those.
Speaker I: The PLP features work. You know, continue to improve the, as I said before, using dance, vocal track normalization option works very well.
Speaker I: So I ran one experiment where we just did the vocal track normalization only in the test data. So I did bother to retrain the models and all and the proof I 1% which is about what we get with, you know, just actually doing both training antists normalization with the standard system.
Speaker I: So in a few hours we'll have the numbers for the, for retrain everything with the tracking, so that might be the problem.
Speaker I: So it looks like the PLP features do very well after having triggered out all these little tricks to get it to work.
Speaker A: So you mean you improve 1% over a system that doesn't have any VTL and it already?
Speaker D: Okay, so then we'll have our baseline to compare the currently hideous new thing.
Speaker I: Right, and what that's just also is of course that the current switchboard, MOP, isn't trained on very good features.
Speaker I: Because it was trained on whatever, you know, was used the last time you did up five stuff, which didn't have any of the...
Speaker D: Right, but all of these effects were like a couple percent, right? I mean...
Speaker I: Well, but if you add them all up, you have almost 5% difference now.
Speaker D: And all of them, I thought one was 1.5% and one was 0.8%.
Speaker I: Yeah, and now we have another percent with the VTL.
Speaker D: That's 3.3%.
Speaker I: Actually, and it's...
Speaker I: What's actually interesting is that with...
Speaker I: Well, maybe another half percent if you do the VTL and training.
Speaker I: And then interestingly, if you optimize, you get more of a win out of restoring the...
Speaker I: the investments and optimizing the weights.
Speaker I: Then you do with the standard?
Speaker D: Yeah, but the part that's actually adjustment of the front end per se is opposed to doing putting VTN or something is...
Speaker D: It was a couple percent.
Speaker D: There was one thing that was 1.5% on the VTL.
Speaker D: And let's see if I remember what they were.
Speaker D: One of them was the change to...
Speaker D: because it did it all at once, from barc scale to male scale, which I really feel like saying in quotes, because...
Speaker D: Essentially the same...
Speaker D: Yeah, why did that change?
Speaker D: But any individual particular implementation of those things puts things in a particular place.
Speaker D: So that's why I wanted to look...
Speaker D: I wanted to look at exactly where the filters were in the two.
Speaker D: It's probably something like there's one fewer or one more filter in the sub...
Speaker D: when kill hurts band.
Speaker D: And for whatever reason, with this particular experiment, it was better.
Speaker D: It could be there's something more fundamental, but I don't know yet.
Speaker D: And the other...
Speaker D: That was like 1.5% or something, then there was 0.8% which was...
Speaker C: Well, that was combined with the triangular, right?
Speaker D: Yeah, those two were together.
Speaker D: We were able to separate them out as a system and one thing.
Speaker D: But then there was 0.8%, which was something else.
Speaker C: The low frequency cutoff.
Speaker D: Oh yeah, so that was...
Speaker D: That one I can claim credit for in terms of screwing it up in the first place.
Speaker D: So someone else fixed it, which is that I never put...
Speaker D: We had some problems before with offsets.
Speaker D: This one back to, I think, Wall Street Journal.
Speaker D: So we had...
Speaker D: Everybody else who was doing Wall Street Journal knew that there were big DC offsets in these data and those data and nobody had the mention into us.
Speaker D: And we were getting really terrible results, like two, three times the error everybody else was getting.
Speaker D: And then in casual conversation, someone mentioned, well, I guess, of course, you're taking care of the offsets.
Speaker D: I said, what offsets?
Speaker D: And at that point, we were pretty into the data and we never really looked at it on a screen.
Speaker D: And then we put it on the screen and we...
Speaker D: This big DC offset.
Speaker D: So...
Speaker A: There was that like a hammer?
Speaker A: Or when they recorded?
Speaker D: It's not at all uncommon for record electronics to have different DC offsets.
Speaker D: It's no big deal.
Speaker D: You could have 10, 20, 30 models, whatever.
Speaker D: And if it's consistently in there.
Speaker D: The thing is, most people, front ends, have pre-emphasis with zero-zero frequency so that it's irrelevant.
Speaker D: But with PLP, we didn't actually have that.
Speaker D: We had the equivalent of pre-emphasis in a pletcher-months-in-style waiting that occurs in the middle of PLP.
Speaker D: But it doesn't actually have a zero-zero frequency, like typical simple pre-emphasis does.
Speaker D: We had something more fancy.
Speaker D: It was later on, it didn't have that.
Speaker D: So at that point, I really...
Speaker D: Oh, we better have a high pass filter just, you know, just take care of the problem.
Speaker D: So I put in a high pass filter at, I think, 90 hertz or so for a 16-kiloward sampling rate.
Speaker D: And I never put anything in to adjust it for different sampling rates.
Speaker D: And so the code doesn't know anything about that.
Speaker D: So this is all at 8-kiloward, so it was at 45 hertz instead of 90.
Speaker D: So I don't know if Dan fixed it or...
Speaker I: Well, he made it the prime of her.
Speaker D: He made a parameter, so I guess if he did it right, he did fix it.
Speaker D: And it's taking care of sampling rate, which is great.
Speaker C: What is the parameters, just the lower cutoff that you want?
Speaker I: It's called H-P-F.
Speaker D: Yeah, that's H-P-F on its...
Speaker I: But H-P-F, you know, when you put a number after it, it's the set of the hertz-vide.
Speaker I: Yeah, cutoff.
Speaker D: I mean, frankly, we never did that with the Rasta filter either.
Speaker D: So the Rasta filter is actually doing a different thing in the modulations, where I could remain depending on what sampling rate you're doing, which is another old bugger pipe.
Speaker D: But...
Speaker D: So that was the problem there.
Speaker D: We had always intended to cut off below 100 hertz and we just wasn't doing this or now it is.
Speaker D: So that helped us by like a 10-3 percent, it still wasn't a big deal.
Speaker I: Well, but...
Speaker I: Well, again, after completing the current experiments, we can add up all the...
Speaker I: Oh, yeah.
Speaker D: I guess my point was that the hybrid system thing we did was primitive in many ways.
Speaker D: And I think I agree with you that if we fixed lots of different things and they're all that up, we probably have a competitive system. But I think not that much of it is due to the front end per se.
Speaker D: I think maybe a couple percent of it is as far as you can see from this.
Speaker D: Unless you call... Well, if you call VT, all the front end, that's a little more, but that's sort of more, both.
Speaker C: One experiment, we should...
Speaker C: We'll probably need to do, though, when...
Speaker C: At some point, since we're using that same...
Speaker C: The net that was trained on PLP without all these things in it for the tandem system, we may want to go back and retrain...
Speaker C: Well, that's what I meant.
Speaker C: Yeah, for the tandem, so we can see what effect it has on the tandem process.
Speaker I: So the thing is, do we expect, at this point, I'm wondering, can we expect the tandem system to do better than a properly trained...
Speaker I: A Gaussian system trained directly on the features with the right choice of parameters?
Speaker D: Well, that's what we're seeing in other areas, yes.
Speaker D: So it's...
Speaker C: So we may not...
Speaker C: I mean, if it doesn't perform as well, we may not know why, right?
Speaker C: Because we need to do the exact experiment.
Speaker D: I mean, the reason to think it should is because you're putting in the same information and you're transforming it to be more discriminative.
Speaker D: So... Now, the thing is, in some databases, I wouldn't expect it to necessarily give you much.
Speaker D: And part of what I view as the real power of it is that it gives you a transformation, an okay probability for taking all sorts of different wild things that we do, not just the standard front end, but other things like with multiple streams and so forth.
Speaker D: And it allows you to feed them to the other system through this funnel.
Speaker D: So I think that's the real power, but I wouldn't expect huge improvements.
Speaker D: But it should at least be roughly the same and maybe a little better if it's in way, way worse than...
Speaker C: So we're going to... Another thing that under S and I were talking about was...
Speaker C: In the first experiment that he did, we just took the whole 56 outputs and that's basically compared to a 39 input feature vector from either MFCC or PLP.
Speaker C: But one thing we could do is...
Speaker D: Let me just ask you, say take the 56 outputs, these are the pre-final nonlinearity outputs.
Speaker C: Yeah, through the regular tandem outputs.
Speaker C: Through the KLT.
Speaker C: Through the KLT, all that kind of stuff.
Speaker D: And so then do you use all 56 of the KLT or...?
Speaker C: That's what we did, right?
Speaker C: So one thing we were wondering is if we did principal components and say took out just 13 and then did Delta's and Double Delta's on that.
Speaker C: So we treated the first 13 as though they were standard features.
Speaker C: I mean, did Dandu experiments like that?
Speaker D: Talked with Stefan.
Speaker D: He did some things like that.
Speaker D: He was either him or Carmen, I forget.
Speaker D: I mean, all different databases and different HDK and all that.
Speaker D: So it may not apply, but my recollection of it was that it didn't make it better, but it didn't make it worse.
Speaker D: But again, given all these differences, maybe it's more important in your case that you now take a lot of these low variance components.
Speaker C: Because in a sense, the net's already got quite a bit of context in those features.
Speaker C: So if we did Delta's and Double Delta's on top of those, we're getting sort of even more...
Speaker C: Should be good, right?
Speaker D: Yeah.
Speaker I: But the main point is that, you know, it took us a while, but we have a procedure for coupling the two systems debug now.
Speaker I: I mean, there's still conceivably some bug somewhere in the way we're feeding the tenant features.
Speaker I: We're generating even more feeding them to the SRI system.
Speaker D: But there might be, because that's a pretty big difference.
Speaker I: Yeah.
Speaker I: I'm wondering how we can debug that.
Speaker I: I'm actually quite sure that feeding the features into the system and training it up.
Speaker I: I think that's essentially the same as we use with the POP features.
Speaker I: That's obviously working great.
Speaker C: Yeah, there could be a bug in the somewhere before that.
Speaker I: Another degree of freedom is how do you generate the POP transform?
Speaker E: That's right.
Speaker D: One other one is the normalization of the inputs to the net.
Speaker D: These nets are trained with particular normalization, and that could screwed up.
Speaker C: I'm doing what Eric coached me through them that part of it.
Speaker C: So I'm pretty confident in that.
Speaker C: I mean, the only slight differences that I use normalization values that underest calculated from the original PLP, which is right.
Speaker C: Yeah.
Speaker C: So I do, we actually don't do that normalization for the PLP doing for just the straight PLP features.
Speaker I: No, the SRI system.
Speaker C: SRI system does that, right?
Speaker I: So there's room for bugs that we might not have.
Speaker D: I would actually double check with Stefan at this point.
Speaker D: Because he's probably the one here and he and Dan are the ones who are at this point most experienced with the tandem.
Speaker D: There may be some little bit here and there that is not being handled right.
Speaker C: That's hard with features because you don't know what they should look like.
Speaker C: I mean, you can't just print the values out and ask you and you know, look at them and see if they're there.
Speaker A: And also, they're not, I mean, as I understand, you don't have a way to optimize the features for the final word error, right?
Speaker A: I mean, these are just discriminative, but they're not optimized for the final right.
Speaker A: So there's always this question of whether you might do better with those features if there was a way to train it for the word error metric that you're actually.
Speaker D: Well, you're actually, but in an indirect way.
Speaker D: Well, right. Just in direct so you don't know.
Speaker D: You may not be in this case come to think of it because you're just taking something was trained up elsewhere.
Speaker D: So what you do in the full procedure is you have an embedded training.
Speaker D: So in fact, the net is trained on a Vaterbi alignment of the training did it.
Speaker D: It comes from your full system.
Speaker D: And so that's where the feedback comes around so that it is actually discriminative.
Speaker D: You can prove that it's a, if you believe in the Vaterbi assumption that getting the best path is almost equivalent to getting the best total probability, then you actually do improve that by training up on local frames.
Speaker D: But we aren't actually doing that here because we did, we did that for a hybrid system.
Speaker D: And now we're plugging it into another system. So it isn't, it wouldn't quite apply here.
Speaker C: So another huge experiment we could do would be to take the tandem features, do SRI, forced alignments using those features.
Speaker A: Exactly. It's exactly.
Speaker A: And redo the net so that you can optimize it.
Speaker D: So since you're not using the net for recognition per se, but just for this transformation is probably bigger than it needs to be.
Speaker D: So that would save a lot of time.
Speaker I: And there's a mismatch in the phone sets.
Speaker I: So you're using a large of phone sets.
Speaker D: Yeah, actually all those things could could could could could could have acted as well.
Speaker D: The other thing, just to mention that Stefan, this was an innovation, I Stefan's, which was a pretty neat one. And my particularly apply here given all these things we're mentioning.
Speaker D: Stefan's idea was that discriminant approaches are great.
Speaker D: Even the local ones given, you know, these potential outer loops, which you can convince yourself, turn into the global ones.
Speaker D: However, this time is not good. When something about the test set is different enough from the training set that the discrimination that you're learning is not a good one.
Speaker D: So his idea was to take as the input feature vector to the Gaussian mixture system, a concatenation of the neural net outputs and the regular features.
Speaker A: Yeah, that didn't you do that already or oh, that makes a lot of sense.
Speaker I: When I first started corresponding with Dan about how to go about this, I think that was one of the things that we, I mean, I'm sure that Stefan was the first to think of it, but actually Stefan did it.
Speaker D: And it helped a lot.
Speaker D: So that's our current best system in the, yeah.
Speaker D: Yeah, that makes sense.
Speaker I: You should never do worse.
Speaker I: I'm on the combined feature vector. I miss what you said you do a KLT transform on the combined feature vector.
Speaker D: Yeah, well, actually, you should check with him because he tried several different combinations.
Speaker I: Because you ended up with this huge feature vector. So that might be a problem unless you do something from the first place.
Speaker D: Yeah, I, what I don't remember is which came out best. So he did one where he put a put the whole thing into one KLT.
Speaker D: And another one since the PLP things are already orthogonalized, he left them alone and, and just did a KLT on the, on the net outputs and again a that.
Speaker D: And I don't remember which was better.
Speaker C: Did he, did he try to, so he always ended up with a feature vector that was twice as long as either one of the, no, I don't know, I don't know.
Speaker D: You have to check with him.
Speaker D: I meant a big idea these days.
Speaker A: You need to close up because I need to save the data.
Speaker A: Not to mention that I have a few snacks.
Speaker A: Right. Did people want to do the digits or do them together?
Speaker D: I think given that we're doing for snacks, maybe we should do them together.
Speaker A: Okay. I mean, we're trying to do them in synchrony. That might be fun.
Speaker A: So he's not here to tell me no.
Speaker D: It's not going to work out, but we could, we could just see if we find the rhythm, you know.
Speaker D: Sure.
Speaker D: Oh, there's zero. So we want to agree on that.
Speaker A: Maybe just whatever people would naturally do. I don't know.
Speaker D: Well, but if we were singing group, we would want to decide.
Speaker A: Viet Harman, you know.
Speaker A: Sorry. So I set up and we didn't have enough digit form. So I zerox the same one seven times.
Speaker D: I'm going to have a problem with saying zero.
Speaker D: No.
Speaker D: Okay.
Speaker D: Okay. One and two and three.
Speaker E: Seven eight seven one five two zero three zero two eight one two zero two two six one four six zero three.
Speaker E: Two seven eight two six two two three four nine eight seven zero seven five two nine one zero nine one eight zero six zero five one five six two eight four three six five seven nine five one eight eight.
Speaker E: Eight eight four nine five three two five three eight one two zero zero one seven three three eight six zero zero.
Speaker D: What's more with feeling?
Speaker A: It's transcript L one three eight.
Speaker A: Oh, yeah.
Speaker A: It was.
Speaker F: It sounded like it.
Speaker E: It sounded very.
Speaker F: It was the lack of prasadica.
Speaker A: Exactly.
Speaker A: Everybody's sort of lowers their pitch.
Speaker A: And now we're going to go out and have some access.
Speaker F: Now we know.
Speaker F: No.
Speaker F: All right.
