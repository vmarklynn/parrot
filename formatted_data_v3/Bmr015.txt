Speaker G: We're not crashing anymore.
Speaker G: It really bothers me.
Speaker G: I crashed.
Speaker G: You crashed this morning?
Speaker G: I did not crash this morning.
Speaker F: Oh, well, maybe it's just how many times you crashed in a day.
Speaker F: First time in the day.
Speaker J: Maybe it's one to do enough meeting to a crash.
Speaker J: No matter of experience.
Speaker E: Yeah.
Speaker F: That's great.
Speaker F: Do we have an agenda?
Speaker F: I have an address.
Speaker F: Can't come.
Speaker G: I have a agenda and it's all me because no one sent me anything else.
Speaker J: Did they send the messages to you?
Speaker G: I have no idea, but I just got a few minutes ago, right when you were in my office it arrived.
Speaker G: Does anybody have any agenda items other than me?
Speaker G: I have one more also which is to talk about the digits.
Speaker F: Great.
Speaker F: I'm just going to talk briefly about the NSFITR.
Speaker F: Oh, great.
Speaker F: You won't see much, but you said one time about digits.
Speaker G: I have a short thing about digits and then I want to talk a little bit about naming conventions, although it's unclear whether this is the right place to talk about it.
Speaker G: Maybe just talk about it very briefly and take the details to the people for whom it's relevant.
Speaker C: I could always say something about transcription.
Speaker F: Well, if we, yeah, we shouldn't add things and just add things in.
Speaker F: I actually pre-visit you, so if we're short meeting, would we find?
Speaker G: So the only thing I want to say about digits is we are pretty much done with the first test set.
Speaker G: There are probably forms here and there that are marked as having been read that weren't really read.
Speaker G: So I won't really know until I go through all the transcriber forms and extract out pieces that are an error.
Speaker G: So two things.
Speaker G: The first is what should we do about digits that were misread?
Speaker G: My opinion is we should just throw them out completely and have them read again by someone else.
Speaker G: The grouping is completely random, so it's perfectly fine to put a group together again of errors and have them read just to finish out the test set.
Speaker G: The other thing you could do is change the transcript to match what they really said.
Speaker G: So there's the two options.
Speaker F: But there's often things where people do false starts.
Speaker F: I know I've done it where I say, say, yeah.
Speaker G: What the transcribers did with that is if they did a correction and they eventually did read the right string, you extract the right string.
Speaker G: Wait, were they completely wrong?
Speaker G: Yeah, didn't correct.
Speaker G: And didn't notice, which happens in a few places.
Speaker G: So, so.
Speaker C: So, so.
None: Correct.
Speaker G: And so the two options are change the transcript to match what they really said.
Speaker G: But then the transcript isn't the error test set anymore.
Speaker G: I don't think that really matters because the conditions are so different.
Speaker G: And that would be a little easier.
Speaker J: How many are, how often is that happen?
Speaker G: Five or six times.
Speaker J: Oh, so it's not very much.
Speaker J: No, it's not much at all.
Speaker J: There's a question just change the transcript.
Speaker J: Yeah.
Speaker F: Okay.
Speaker F: Yeah, it's five or six times out of thousands.
Speaker F: Four thousand.
Speaker F: Four thousand.
Speaker F: Yeah, I would do the easy way.
Speaker F: Okay.
Speaker F: Yeah.
Speaker F: It's kind of nice.
Speaker F: I mean, who knows what studies people will be doing on speaker dependent things.
Speaker F: And so I think having having it all the speakers who we had is least interesting.
Speaker J: So, how many digits have been transferred?
Speaker G: And so, I think that's a thousand lines in each line is between one and about ten digits.
Speaker G: I didn't compute the average.
Speaker G: I think the average was around four or five.
Speaker F: So that's a couple hours of speech probably.
Speaker F: Yeah.
Speaker F: Which is a reasonable, reasonable test set.
Speaker G: And Jane, I do have a set of forms which I think you have copies of somewhere.
Speaker G: Oh, you do.
Speaker G: Oh, okay, good.
Speaker G: Good.
Speaker G: I thought I had had all of them back from you.
Speaker G: And then the other thing is that the forms in front of us here that we're going to read later, were suggested by Liz because she wanted to elicit some different prosotics from digits.
Speaker G: And so, I just wanted people to take a quick look at the instructions and the way it worked and see if it makes sense.
Speaker G: And if anyone has any comments on it.
Speaker F: I see.
Speaker F: And the decision here was to continue with the words rather than the numerics.
Speaker G: Yes, although we could switch it back.
Speaker G: The problem was 0 and 0.
Speaker G: Although we could switch it back and tell them always to say 0 or always to say 0.
Speaker F: Or neither, but it's just two things ways that you can say it.
Speaker F: Sure.
Speaker F: That's the only thought I have because if you start talking about these, you know, she's trying to get it natural groupings.
Speaker F: But there's nothing natural about reading numbers this way.
Speaker F: I mean, if you saw telephone number, you would never see it this way.
Speaker G: The problem also is she did want to stick with digits.
Speaker G: I mean, I'm speaking for her.
Speaker G: She's not here.
Speaker G: But the other problem we were thinking about is if you just put the numerals, they might say 43 instead of 43.
Speaker C: Well, the space though between them.
Speaker C: When you space them out, they don't look like 43 anymore.
Speaker G: Well, she and I were talking about it.
Speaker G: And she felt that it's very, very natural to do that sort of.
Speaker G: She's right.
Speaker F: It's a different problem.
Speaker F: I mean, it's an interesting problem.
Speaker F: I mean, we've done stuff with numbers before.
Speaker F: And yeah, sometimes people, if you say 3981, sometimes people will say 3981 or 3891.
Speaker F: I don't think they'd say that.
Speaker F: Not very frequently.
Speaker F: But certainly could.
Speaker F: Yeah.
Speaker F: 3891 is probably how they do that.
Speaker G: So, I mean, this is something that Liz and I spoke about.
Speaker G: Nice.
Speaker G: This was something that Liz asked for specifically.
Speaker G: I think we need to defer to her.
Speaker F: Okay. Well, we're probably going to be collecting meetings for a while.
Speaker F: If we decide we still want to do some digits later, we might be able to do some different versions.
Speaker F: But this is the next suggestion.
Speaker F: Okay.
Speaker F: Okay. So, I guess, let me get my short thing out about the NSF.
Speaker F: I said this.
Speaker F: Actually, this is maybe a little side thing.
Speaker F: I sent to what I thought we had in some previous mail as the right joint thing to send to.
Speaker F: It was.
Speaker F: MTG, our CDR hyphen joint.
Speaker F: Yep.
Speaker F: But then I got some sort of funny mail saying that the moderator was going to.
Speaker G: That's because they set the one up at UW.
Speaker G: That's not on our side.
Speaker G: That's on the UW website.
Speaker G: Oh.
Speaker G: And so UW set it up as a moderated list.
Speaker G: Oh.
Speaker G: And I have no idea whether it actually ever goes to anyone.
Speaker G: So you might want to just mail to Mari.
Speaker F: No, I got a little excited notes from Mari and Jeff and so on.
Speaker F: Okay. Good.
Speaker G: Yeah.
Speaker G: So the moderator actually did repost it.
Speaker G: Yeah.
Speaker G: Because I had sent one earlier.
Speaker G: Actually, the same thing happened to me.
Speaker G: I had sent one earlier.
Speaker G: The message says you'll be informed and then I was never informed.
Speaker G: But I got replies from people indicating that they had gotten it.
Speaker G: Right.
Speaker G: It's just to prevent spam.
Speaker G: I see.
Speaker F: Yeah.
Speaker F: So, okay.
Speaker F: Anyway, I guess everybody here, you are on that list, right?
Speaker F: So you got to know.
Speaker F: Yeah.
Speaker F: Okay.
Speaker F: So this was a proposal that we put in before on more, more higher level issues in meetings from, I guess, higher level from my point of view.
Speaker F: Yeah.
Speaker F: And meeting mappings.
Speaker F: And so it was a proposal for the ITR program, the Information Technology Research Program, as part of the National Science Foundation.
Speaker F: It's the second year of there doing these grants.
Speaker F: There are a lot of them, some of them anyway, but they're larger grants than usual, small NSF grants.
Speaker F: So they're very competitive and they have a first phase where you put in pre-proposals and we got through that.
Speaker F: And so the next phase will be, you will actually be doing a larger proposal.
Speaker F: And I hope to be doing very little of it, which was also true for the pre-proposals.
Speaker F: So, there are a bunch of people working on it.
Speaker G: When's the full proposal, Tim?
Speaker F: I think April 9th or something.
Speaker F: So that's about a month.
Speaker G: And they said, end of business day, you could check on the reviewer forms.
Speaker F: Tomorrow.
Speaker F: Tomorrow, March 2nd.
Speaker G: Might be in a day off all week.
Speaker G: I guess that's a good thing, because I mean, I got my papers on.
Speaker F: So it's amazing you showed up at this meeting.
Speaker G: It is, it is actually quite amazing.
Speaker J: Let me just see the reviewers comment.
Speaker J: Yeah.
Speaker F: Yeah.
Speaker F: My favorite is when one reviewer says, this should be far more detailed.
Speaker F: And the next reviewer says, there's way too much detail.
Speaker F: Yes.
Speaker G: This is way too general.
Speaker G: The other reviewer says this is way too specific.
Speaker G: Yeah.
Speaker G: It's way too hard.
Speaker F: Way too easy.
Speaker F: We'll see.
Speaker G: Maybe there'll be something useful.
Speaker G: Well, it sounded like the first gate was pretty easy.
Speaker G: Is that right?
Speaker G: That they didn't reject a lot of the pre-proposals.
Speaker F: Do you know anything about the numbers?
Speaker J: No.
Speaker J: I don't think that's true.
Speaker J: He said the next phase will be very competitive because we didn't want to weed out much in the first phase.
Speaker J: Well, I have to see what the numbers are.
Speaker J: Yeah.
Speaker J: But they have to weed out enough so that they have enough reviewers.
Speaker F: Right.
Speaker F: So maybe they weed out as much as they want.
Speaker F: But it's usually pretty.
Speaker F: Yeah, it's certainly not.
Speaker F: I'm sure that it's not down to one and two or something.
Speaker F: Right.
Speaker F: It was left.
Speaker F: I'm sure it's.
Speaker G: How many awards are there?
Speaker F: Well, there's different numbers of awards for different size.
Speaker F: They have three size grants.
Speaker F: Let's find this one.
Speaker F: Let's see.
Speaker F: The small ones are less than 500,000 total over three years.
Speaker F: And that they have a fair number of them.
Speaker F: And the large ones are.
Speaker F: Oh, I forget.
Speaker F: I think more than.
Speaker F: More than a million and a half, more than two million or something like that.
Speaker F: And we're in the middle.
Speaker F: Middle category.
Speaker F: I think we're.
Speaker F: I forget it was.
Speaker F: But.
Speaker F: I don't remember.
Speaker F: But it's probably.
Speaker F: I don't remember.
Speaker F: But it's probably along the line.
Speaker F: I could be wrong in this.
Speaker F: It would probably along lines of 15, that they'll find the 20.
Speaker F: I mean, when they, do you know how many they funded when they've been in, in, in Chucks?
Speaker F: They got.
Speaker F: I see.
Speaker F: Yeah.
Speaker G: I thought it was smaller and that was like four or five.
Speaker G: Well, they fun.
Speaker F: They, yeah.
Speaker F: I mean, we'll find out one more.
Speaker F: Yeah.
Speaker F: I mean, last time I think they just had two category small and big.
Speaker F: And this time they came up with a middle one.
Speaker F: So it'll, they'll be more of them that they fund than of the big.
Speaker J: If we end up getting.
Speaker J: What will it be in the context in terms of where will the money go to what we'll be doing with it?
None: Exactly.
Speaker G: We say in the proposal.
Speaker J: I mean, I was far enough.
Speaker F: You know, none of it will go for the yachts that we've been talking about.
Speaker F: Well, you know, I mean, it's just for the research.
Speaker J: It's extending the research, right?
Speaker G: Because the other higher level stuff than we've been talking about for a record.
Speaker F: Yeah.
Speaker F: Yeah.
Speaker F: Yeah.
Speaker F: The other things that we have been working on with the communicator, especially with the newer things, with the more acoustically oriented things, are lower level.
Speaker F: And this is dealing with mapping on the level of the conversation, mapping the conversation to different kind of planes.
Speaker F: So.
Speaker F: But so it's all stuff that none of us are doing right now.
Speaker F: None of us are funded for.
Speaker F: So it's, so it's, it would be new.
Speaker J: So assuming everybody's busy now.
Speaker J: I mean, it's going to be higher more students.
Speaker F: Well, there's evenings in this room.
Speaker F: Yeah, there would be, there would be cars.
Speaker F: And there would be expansion.
Speaker F: But also, there's always, for everybody, there's always things that are dropping off, grants that are renting or other things.
Speaker F: So there's a continual need to bring in new things.
Speaker F: But there definitely would be new students and so forth, both of them.
Speaker G: Are there any students in your class who are expressing interest?
Speaker F: Not clear yet.
Speaker F: Other than the one who's already here.
Speaker F: I mean, we got, yeah, two in the, two in the, two in the class already here.
Speaker F: And then, and then there's a third who's doing a project here.
Speaker F: But he won't be in the country that long.
Speaker F: Maybe another one.
Speaker F: Yeah, actually, there's one other guy who's looking at that, that, that, Jeremy, I think.
Speaker F: Anyway, yeah, that's, that's how I was going to say is that, that's, you know, that's nice and we're sort of proceeding to next step.
Speaker F: And it'll mean some more work, you know, in March and getting a proposal out.
Speaker F: And then it's, you know, we'll see what happens.
Speaker F: The last one was that he had their whistle naming.
Speaker G: Yeah, it just, we've been cutting up sound files and, for both digits and for doing recognition.
Speaker G: And Liz had some suggestions on naming and it just brought up the whole issue that hasn't really been resolved about naming.
Speaker G: So one thing she would like to have is for all the names to be the same length.
Speaker G: So that sorting is easier.
Speaker G: Same number of characters so that when you're sorting file names, you can easily extract out bits and pieces that you want.
Speaker G: That's easy enough to do.
Speaker G: And I don't think we have so many meetings that that's a big deal just to change the names.
Speaker G: So that means instead of calling it MR1, MR2, you call it MRM001, MRM002, things like that, just so that they're all the same length.
Speaker C: But you know, when you do things like that, you can always, as long as you have, you can always search from the beginning of the end of the string.
Speaker G: The problem is that there are a lot of fields.
Speaker G: Right, so we have, we're going to have the speaker ID, the session, information on the microphones, information on the channels and all that.
Speaker G: And so if each one of those is a fixed length, the sorting comes a lot easier.
Speaker B: She wanted to keep the same length across different meetings also.
Speaker B: So like the NSA meeting links, file names are going to be the same length as the media recording meeting names.
Speaker G: And as I said, we just don't have that many that that's a big deal.
Speaker G: And so at some point we have to sort of take a few days off, like the transcribers have a few days off, make sure no one's touching the data and reorganize the file structures.
Speaker G: And when we do that, we can also rationalize some of the naming.
Speaker C: I would think though that the transcribes themselves wouldn't need to have such lengthy names.
Speaker C: So I mean, you're dealing with a different domain there, I mean, with starting n times and all that, channels and stuff.
Speaker G: So the only thing we would change with that is just the directory names, I would change them to match.
Speaker G: So instead of being MR1, it would be MRM001, but I don't think it's a big deal.
Speaker G: So for the meetings, we were thinking about three letters and three numbers for meeting IDs, for speakers, M or F, and then three numbers.
Speaker G: And that also brings up the point that we have to start assembling a speaker database so that we get those links back and forth and keep it consistent.
Speaker G: And then the microphone issues, we want some way of specifying more than looking in the key file, what channel and what mic, what channel, what mic, and what broadcast, or I don't know how to say it.
Speaker G: So with this one, it's this particular headset, with this particular transmitter, as a wireless, and you know, that one is a different headset and different channel.
Speaker G: And so we just need some naming conventions on that. And that's going to be come especially important once we start changing the microphone setup.
Speaker G: We have some new microphones that I'd like to start trying out once I test them, and then we'll need to specify that somewhere.
Speaker G: So I was just going to do a fixed list of microphones and types.
Speaker F: As I said, yeah. I'm sure it's such a short agenda list, I guess I will ask how are the transcription skills.
Speaker C: But the news is that I switched to start my news sense. I switched to doing the channel by channel transcriptions to provide a tighter time bins for partly for use in Teal's work and also its environments and other people in the project.
Speaker C: And I discovered in the process a couple of interesting things, which one of them is that it seems that there are time lags involved in doing this using an interface that has so much more complexity to it.
Speaker C: And I wanted to maybe ask Chuck to help me with some of the questions and efficiency. I was thinking maybe the best way to do this in the long run, maybe to give them single channel parts and then piece them together later.
Speaker C: And I have a script I can piece them together. So it's like I know that I can take them apart and put them together and end up with the representation, which is where the real power of that interface is.
Speaker C: And it may be it's faster to transcribe the channel at a time with only one sound violin, one set of utterances to check through.
Speaker F: I'm a little confused. I thought that that one of the reasons we thought we were so much faster than than the other transcription thing was that that we were using the mix.
Speaker C: Oh yes. Okay. But with the mix, when you have an overlap, you only have a choice of one start and end time for that entire overlap, which means that you're not tightly tuning the individual parts of that overlap by your foot speaker.
Speaker C: So someone may have only said two words in that entire big chunk of overlap. And for purposes of things like, well, some things like training the speech non speech segmentation thing.
Speaker C: It's necessary to have it more tightly tuned than that. And you know, it would be wonderful if it's possible then to use that algorithm to more tight be tying all the channels after that.
Speaker C: But I don't know exactly where that's going at this point, but I was experimenting with doing this by hand. And I really do think that it's wise that we've had them start the way we have with working off the mix signal, having the interface that doesn't require them to do the type, the type in every single channel.
Speaker C: The entire interaction. I did discover a couple other things by doing this though. And one of them is that once in a while a back channel will be overlooked by the transgarber as you might expect because when a back channel could well happen in a very densely populated overlap.
Speaker C: And if we're going to study types of our laps, which is what I want to do analysis of that, then that really tests require listening to every single channel all the way through the entire length for all the different speakers. Now, for only four speakers, that's not going to be too much time. But if it's nine speakers, then that is more time.
Speaker C: So it's like, you know, kind of wondering. And I think again, it's like this, it's really valuable that he was working on the speech non-speech segmentation because maybe we can close it out without having to actually go to the time that would take to listen to every single channel from start to finish the every single need.
Speaker H: Yeah, but those spectacles will always be a problem. I think, especially if they're really short and they're not very loud. And so it can, it will always happen that also the automatic detection system will miss some of them.
Speaker C: Also, then maybe the answer is to listen, especially densely in places of overlap, just so that there is not a overlooked because of that and count on accuracy during the sparse faces.
Speaker C: Because there are large spaces at a, that's a good point. There are large spaces where there's no overlap at all. Someone's giving a presentation.
Speaker C: Yeah. Whatever. That's a good, that's a good point. And let's see, there was only a thing I was going to say.
Speaker C: I think it's really interesting data to work with. I have to say it's very enjoyable. Really, not a problem spending time with these data.
Speaker C: I'm not just because I'm in there.
Speaker F: Well, I think it's a short meeting. You're still on the midst of what you're doing from the script last time, I assume.
Speaker D: I have a result, but I'm continuing working with the mix side now, after the last experience.
Speaker D: And I tried to adjust to improve in our own city, the test, that I implement.
Speaker D: But I have a problem because I get very much harmonics now.
Speaker D: I'm only possible harmonics. And now I'm trying to find some kind of help using the energy to distinguish between possible harmonics and other frequency picks.
Speaker D: And I have to talk with you, with the group, about the instantaneous frequency, because I have an algorithm.
Speaker D: And I get similar results, like the paper that I am following. But the rules that people use in the paper to distinguish the harmonics doesn't work well.
Speaker D: And I'm not sure that the way to obtain the instantaneous frequency is right. It's not right.
Speaker D: I haven't enough feeling to distinguish what happened.
Speaker F: I'd like to talk with you about it. If I don't have enough time, and you want to discuss with someone else, besides us, that you might want to talk to, might be Stefan.
Speaker J: I'm not going to do experience for this. The experience is not enough.
Speaker D: I don't process the fundamental. I calculate the phase derivate using the F50. The algorithm said that if you change the frequency X using the instantaneous frequency, you can find how in several frequencies, the rest of the frequency picks, the frequency picks, the frequency harmonic, and if you compare the instantaneous frequency of the continuous filters, the use to get the instantaneous frequency is probably to you can find that instantaneous frequency for continuous output continuous filters are brilliant.
Speaker F: I'd have to look at that and think about it. I haven't worked with that either. The simple-minded way I suggested was what Doug was just saying is that you can make a sieve.
Speaker F: Let's hypothesize that it's this frequency. Maybe you could use some other cute methods to shortcut it by making some guesses.
Speaker F: I mean, you could make some guesses from the autocorrelation or something, but then, given those guesses, try only looking at the energy at multiples of that frequency and seeing how much it takes to one that's maximum.
Speaker D: Using the energy of the multiples of frequencies. You have to do some kind of low pass filter before you do that. I don't use.
Speaker D: I know many people use low pass filter to get the pitch.
Speaker J: I'm going to try the vocal track, the response of the vocal track. Just looking at the energy and those, that's the harmonic.
Speaker F: The thing is that this is for a...
Speaker F: I don't need to get rid of it. I mean, that'd be nice, but I don't know if it's essential. I mean, because I think the main thing is that you're trying... what are you doing this for?
Speaker F: You're trying to distinguish between the case where there are more than one speaker. And the case where there's only one speaker.
Speaker F: You're not distinguishing voice to non-voice. If you don't care about that, see if you also want to determine whether it's on voice, then I think you want to look at high frequencies also because the fact that there's more energy in high frequencies is going to be sort of obvious cue that it's on voice.
Speaker F: But other than that, I guess, as far as the one person versus two persons, it would be primarily low frequency phenomenon.
Speaker F: And if you look at the low frequencies, yes, the higher frequencies are going to be a spectral slope. The higher frequencies would be lower energy.
Speaker D: I would be there for the next week, all my results about the Dharmonicity and we try to comment on this case here because I have enough feeling to understand what happened with so many peaks.
Speaker D: I see Dharmonics in many times, but there are a lot of peaks that have no harmonics.
Speaker D: I have to discover what is the best way to...
Speaker F: I don't think you're not going to be able to look at every frame. I really thought that the best way to do it, and I'm speaking with no experience on this to a good point, but my impression was the best way to do it was however you use instantaneous frequency, however you clope with your candidates, you want to see how much of the energy is in that.
Speaker F: It's supposed to all of the total energy. And if it's voiced, I guess so. So I think maybe you do need a voiced and voiced determination too, but if it's voiced and the fraction of the energy that's in the harmonic sequence that you're looking at is relatively low, then it should be more likely to be an overlap.
Speaker D: This is the idea I had to compare the ratio of the energy of the harmonics with the total energy in the spectrum, try to get the ratio to distinguish between overlap and speech.
Speaker F: But you're looking at... let's take a second of this. You're looking at the phase derivative in what domain... I mean this is in bands or...
Speaker D: No, no, no. Just overall...
Speaker D: The band is from 0 to 4 kilohertz. You just take the instantaneous frequency.
Speaker D: I use two methods. One, basically, on FFT to FFT to obtain the... or to study the harmonics from the spectrum directly, and to study the energy and the multiples of frequency. And another... another algorithm I have is the instantaneous frequency.
Speaker D: I use FFT to calculate the phase derivative in the time. I mean, I have two algorithms. But in my opinion, the instantaneous frequency, the behavior was very interesting, because I saw how the spectrum concentrates around the harmonic.
Speaker D: When I apply the rule of the instantaneous frequency of the continuous filter, the rule that people propose in the paper doesn't work, and I don't know why.
Speaker F: The instantaneous frequency wouldn't give you something more like the central frequency of the way most of the energy is. I mean, I think, why would it correspond to pitch?
Speaker D: I'm not sure. I try to... First, I calculate using the FFT, I get the spectrum, I represent all the frequency.
Speaker D: When I obtain the instantaneous frequency, and I change the disease using the instantaneous frequency here.
Speaker D: I use a scaling along that axis according to the instantaneous frequency. I use this frequency. The range is different, and the resolution is different.
Speaker D: More or less, seeing like this. And the paper said that these frequencies are probably harmonics.
Speaker D: But they used a rule based on the... Because to calculate the instantaneous frequency, they used a handing window.
Speaker D: They said that if these big harmonics, the instantaneous frequency of the continuous filters are very near.
Speaker D: I don't know what is the distance. I try to put different distance, to put different length of the window, different frequency.
Speaker F: I guess I'm not following enough. I'm not going to have time to do any of these.
Speaker C: I get it in the return of the transition. There's one third thing I wanted to raise is an issue, which is how to handle breaths.
Speaker C: The reason I asked the question is, aside from the fact that there are times when we took code, the fact that I have the indication from down ellis in the email that I sent to you.
Speaker C: I think that the question is, whether it would be possible to eliminate them from the audio signal, which would be the ideal situation.
Speaker C: I don't think it would be ideal. We're dealing with real speech. We're trying to have these real as possible and breaths are part of real speech.
Speaker C: We're hearing you breathing as if you're hearing our ear. It's like, I mean, breath is natural, but not...
Speaker G: I think the BDA application would have to cope with breath. The BDA might not have to, but more people than just BDA users are interested in this corpus.
Speaker G: We could remove it, but I think we don't want to remove it from the corpus in terms of delivering it because people will want it in there.
Speaker F: If it gets in the way of what somebody is doing with it, then you might want to have some method which will allow you to block it, but it's real data.
Speaker F: If there's a little bit of noise out there and somebody is talking about something they're doing, that's part of what we accept is part of a real meeting.
Speaker F: We have the fan and the projector up there. This is actual stuff that we want to work with.
Speaker C: This is very interesting because it shows very clearly the contrast between speech recognition research and discourse research.
Speaker C: Discourse in linguistic research is what's communicative. Once in a while, breath is communicative, but very rarely.
Speaker C: I had a discussion with Chuck about the data structure and the idea is that the transcripts will get stored as a master.
Speaker C: There will be a master transcript which has in it everything that's needed for both of these uses.
Speaker C: The one that's used for speech recognition will be processed by a script. Don's been writing scripts and two processes for speech recognition side.
Speaker C: Discourse side will have this side over here. The discourse side will have a script which will strip away the things which are non-communicative.
Speaker C: Let's think about the practicalities of how we get to that master copy with reference to breaths. What I would wonder is would it be possible to encode those automatically?
Speaker C: Could we get a breath detector?
Speaker G: Just to save the transcribers.
Speaker C: You just have no idea. If you're getting a breath several times every minute and just simply the keystrokes it takes to negotiate to put the boundaries in to type it in, it's just a huge amount of time.
Speaker C: You want to be sure it's used and you want to be sure it's done as efficiently as possible and it's done automatically. That would be ideal.
Speaker F: What if you put it in but put the boundaries?
Speaker C: You just know it's between these other things. The time boundaries could mark off words from non-words. That would be extremely time effective if that's sufficient.
Speaker F: If it's too hard for us to annotate the breaths per se, we are going to be building up models for these things and these things are somewhat self-aligning.
Speaker F: If we say there is some kind of a thing which we call a breath or a breath in or breath out, the models will learn that sort of thing.
Speaker F: But you do want them to point them at some region where the breaths really are.
Speaker C: That would maybe include a pause as well and that wouldn't be a problem.
Speaker F: There's this dynamic tension between marking absolutely everything and marking just a little bit and touting on the statistical methods.
Speaker F: Basically the more we can mark the better. But if there seems to be a lot of effort for a small amount of reward in some area, this might be one like this.
Speaker F: Although I'd be interested to get input from those Andreas on this to see if they've got lots of experience with breaths in...
Speaker F: I have lots of experience breathing.
Speaker F: Well, yes, they do but we can handle that without it here. But you're going to say something about that.
Speaker J: I think one possible way to handle it is that as the transcribers are going through and if they get a hunk of speech, they're going to transcribe it.
Speaker J: They're going to transcribe it because there's words in there or whatnot.
Speaker J: If there's a breath in there, they could transcribe that.
Speaker C: That's what they've been doing.
Speaker C: So within overlap segments.
Speaker J: But if there's a big hunk of speech, let's say on Morgan's mic where he's not talking at all, don't worry about that.
Speaker J: So what we're saying is there's no guarantee that...
Speaker J: So for the chunks that are transcribed, everything's transcribed.
Speaker J: But outside of those boundaries, there could have been stuff that wasn't transcribed.
Speaker J: So you just... somebody can't rely on that data and say that's perfectly clean data.
Speaker J: Do you see what I'm saying?
Speaker J: I haven't said, don't tell the transcribe anything that's outside of...
Speaker F: That sounds like a reasonable compromise.
Speaker I: And that's quite a corresponds to the way I try to train the speech and speech detector.
Speaker I: I really try not to detect those breaths, which are not with a speech-strung, but with just an silence region.
Speaker I: So they hopefully won't be mocked in those channel-specific files.
Speaker F: I wanted to comment a little more just for clarification about this business, about the different purposes.
Speaker F: In a way, this is a really key point.
Speaker F: That for speech recognition research, it's not just a minor part.
Speaker F: In fact, I would say the core thing that we're trying to do is to recognize the actual meaningful component.
Speaker F: The meaningful components in the midst of other things that are not meaningful.
Speaker F: So it's critical, it's not just incidental, it's critical for us to get these other components that are not meaningful.
Speaker F: Because that's what we're trying to pull the other out. That's our problem.
Speaker F: If we had only linguistically relevant things, if we only had changes in the spectrum that were associated with words with different spectral components, and we didn't have noise, we didn't have convolutional errors, we didn't have extraneous behaviors and so forth, moving your head and all these sorts of things, then actually speech recognition isn't that bad right now.
Speaker F: I mean, you know, it's the technology has come along pretty well.
Speaker F: The reason we so complain about it is because when you have more realistic conditions, then things fall apart.
Speaker C: Okay, I guess what I was wondering is what at what level does the breathing aspect enter into the problem?
Speaker C: Because if it were likely that a PDA would be able to be built, which would get rid of the breathing, so it wouldn't even have to be processed at this computational, it would have to be computationally processed, rid of it.
Speaker C: But if there were likely on the frontier, a good breath extractor, then...
Speaker C: So that's a research question, you know?
Speaker F: I've seen that's what I wouldn't know. And we don't either.
Speaker F: So the thing is, right now it's just data that we're collecting, and so we don't want to presuppose that people will be able to get rid of particular degradation, because that's actually the research that we're trying to feed.
Speaker F: So, you know, maybe in five years it'll work really well, and it'll only mess up 10% of the time, but then we would still want to account for that 10%.
Speaker C: I guess there's another aspect which is that as we've improved our micro-technique, we have a lot less breadth in the more recent recordings. So it's in a way it's an artifact that there's so much on the earlier ones.
Speaker J: I see.
Speaker J: One of the... just to add to this, one of the ways that we will be able to get rid of breath is by having models, I mean that's what a lot of people do now.
Speaker J: In order to build a model, you need to have some amount of it, right?
Speaker J: I don't think we need to worry a lot about breaths that are happening outside of a conversation.
Speaker J: You don't have to go and search for them to mark them at all, but they're there while they're transcribing some comfort that put them in possible.
None: Okay.
Speaker C: It's also the fact that they do for a lot in one channel to the other because of the way the microphone suggests.
Speaker F: Should we do a diddys?
Speaker F: Yep.
Speaker G: Okay, this is transcript. L173, 4368, 136593, 7308, 591761, 56010, 6395, 038510, 868, 64651, 878, 873, 482434, 6432, 0259, 42086, 289, 671, 2916, 4399, 76303, 277, 556, 390.
Speaker J: Transcript L-257, 6200, 0129, 0236, 7034, 6368, 463, 236, 418, 0649, 58, 2316, 775, 907, 724, 4, 014, 264, 845, 1, 877, 45, 872, 2, 495, 41015, 5105, 022, 0206, 6556, 1, 858,
Speaker D: 1, 888, 1, 877, 1, 847, 234, 603, 939, 25, 666, 640, 3, 8, 8, 8, 8, 975, 3-4, 6-0-3, 9-39-25, 6-6, 6-4-0, 3-5-13, 9-8, 6-5-3, 3-7-1-3-0, 5-6-0-1, 4-9-8-7, 7-29, 6-1-1, 4-6-3, 4-3-5, 1-6-9-1-0, 0-2, 9-0, 0-0, 3-1, 7-3.
Speaker I: Transcript L559, 4-2, 9-1, 8-8, 4-2, 2-9, 5-2, 3-5, 2-8, 3-6, 3-8, 6-8, 4-9, 1-4, 7-1, 5-6, 4-8-3, 7-2, 4-1-8-0, 0-1-1-3, 5-2-1-6, 3-4, 7-5-07, 3-5-20, 1-19-1, 0-9, 7-0-5-1, 2-5-1-3, 5-2-7, 3-4-6-3, 5-3-0-7, 4-0-1-2, 1-8-0, 5-9-5, 9-3-9.
Speaker F: Transcript L493, 9-3-4, 6-8, 5-2-6-8, 0-4-3, 8-6-4, 2-3-9-5, 4-9-2, 1-1-0, 8-7-1-0, 9-5-3, 0-3-0, 6-8-4, 8-5-7-1, 5-0-9-2, 9-3-8-9, 8-3-2-1, 3-1-2-9, 2-7-3-6, 9-9-8-9, 9-5-7-4, 7-3-0-5-4-3-5-3-3-1, 6-6-1-3-9-5-5-4-2, 6-5-1-9-5-8-9-3-8-5.
Speaker A: Transcript L-31, 3-271-9-6-6-5, 6-08-3-1-6-8-8-5-1-1, 7-09-4-9-2-1-6-9-7, 4-7-01-4-9-9-0-2-5, 3-7-9-2-4-2-7-9-0, 8-6-1-6-4-0-7-3-8, 5-8-6-4-4-3-8-7-7, 2-8-7-6-8-5-9-9-2, 4-8-6-8-5, 0-8-1-6-3-9-1-6-8-9.
Speaker C: Transcript L-6-2-1, 0-2-3-1-9-5-8-5-4, 1-5-0-1-1-8-8-3, 9-6-6-3-2-5-7-8-7-9, 2-0-6-4-3-4-6-6-0, 3-6-2-3-6-8-3-5-2-4, 1-7-2-6-1-9-7-2-6-5, 8-9-8-7-3-2-2-1-3-8, 2-3-3-7-7-1-9-5, 4-8-8-7-6-1-3-5-3-3-5-7, 9-5-8-5-1-5-4-2-2.
