None: Challenge?
Speaker B: Challenge.
Speaker F: So we think we're going to.
Speaker F: Okay, good.
Speaker F: Alright, going again.
Speaker F: So we're going to go around this before and do our digits.
Speaker F: Transcript, 1311-1330.
Speaker F: 323-4765.
Speaker F: 531-6241.
Speaker F: 677-890-94003.
Speaker F: 0158-17353.
Speaker F: 268-03624307.
Speaker F: 4.
Speaker F: 5069-4, 74857-9615-07802.
Speaker F: 090-90-604001.
Speaker F: 2.
Speaker B: I'm reading transcript.
Speaker B: 1391-1410-677-890-698.
Speaker B: 01319-16237-34.
Speaker B: 4.
Speaker B: 5.
Speaker B: 6.
Speaker B: 847-920-75.
Speaker B: 03696.
Speaker B: 0931003.
Speaker B: 1.
Speaker B: 2.
Speaker B: 3097.
Speaker B: 5267-983.
Speaker B: 6706.
Speaker C: Okay, I've got transcript 1271-1290.
Speaker C: 1710-281-207134.
Speaker C: 509-6080-7386-888818813.
Speaker C: 934-0394-042-021-2038232.
Speaker C: 42816-535-8308.
Speaker C: 659-1076.
Speaker C: 7662-879.
Speaker C: 0057804-141.
Speaker D: Transcript number 1291-1310.
Speaker D: 23902-38152-47467-5670-9401-00.
Speaker D: 07260.
Speaker D: 1604-645-23407-26440-74188-885-63989-00391.
Speaker A: Transcript number 1171-1190.
Speaker A: 83435-9439-0309691.
Speaker A: 203-556-086-304-0117882-89551-00214-36702-479-5905-7351.
Speaker A: 479-5905-7356-669-465-7.
Speaker H: Okay, this is Barry Chen and I'm reading transcript 1351-1370.
Speaker H: 4507-8928-71489-859-369-7159-047301-00106-243467-544-677-789-844-6389-00.
Speaker H: 544-677-899-899-001-1247-349-253-5008-48.
Speaker I: Transcript 1371-13905-6705-9263-305-0455-049162-30002-192-466-7208-5-0386-9088-7408-8408.
Speaker I: 3245-456-923.
Speaker F: Yeah, you don't actually need to say the name.
Speaker F: I'll probably bleep that out.
Speaker F: Okay.
Speaker F: These are not mine.
Speaker F: Oh.
Speaker F: Okay.
Speaker F: Not that there's anything defamatory about 857.
Speaker F: Okay.
Speaker F: So here's what I have.
Speaker F: I was just trying to add things I think that we should do today.
Speaker F: It's what I have for an agenda and so far.
Speaker F: We should talk a little bit about plants for the field trip next week.
Speaker F: A number of string field trip to OGI.
Speaker F: And mostly, first of all, about the logistics for it.
Speaker F: Then maybe later on, the meeting we should talk about, actually, might accomplish.
Speaker F: And then, kind of go around to see what people have been doing and talk about that progress record, essentially.
Speaker F: And then another topic I had was that Dave here said, give me something to do.
Speaker F: And I have failed so far on doing that and so we can discuss that a little bit, find some holes and some things that someone could use and help with these things.
Speaker F: I'm going to move into that current here.
Speaker F: Okay.
Speaker F: Always count on that.
Speaker F: That's a really good question.
Speaker F: So.
Speaker F: And then talk a little bit about disks and resource issues that started to work out.
Speaker F: And then anything else that he has that isn't in that list?
Speaker B: I was just wondering, does this mean the battery is dying and I should change it?
Speaker F: I think that means the battery is okay.
Speaker B: Oh, okay.
Speaker B: Yeah, that's good.
Speaker B: Because it's full.
Speaker F: All right.
Speaker F: Full of electrons.
Speaker F: Okay.
Speaker F: Okay.
Speaker F: So I'm going to start this with this mundane thing.
Speaker F: It was kind of my bright idea to have us take a plane that leaves at 7.20 in the morning.
Speaker F: Oh, yeah.
Speaker F: This is a reason I did it was because otherwise for those of us who have to come back the same day is really not much of a visit.
Speaker F: So the issue is how we ever accomplished that.
Speaker F: What part of the time do you live in?
Speaker H: I live in the corner of campus.
Speaker H: Okay.
Speaker H: South East corner.
Speaker F: Okay.
Speaker F: So would it be easier?
Speaker F: Those of you who are not used to this area can be very tricky to get to the airport at 6.30.
Speaker F: So would it be easier for you if you came here and I drove you?
Speaker F: Yeah, bridge.
Speaker F: Yeah.
Speaker F: Okay.
Speaker F: Okay.
Speaker F: So if everybody can get here at 6.
Speaker F: Yeah.
Speaker F: I'm afraid you can do that.
Speaker F: I guess.
Speaker F: Yeah.
Speaker F: Anyway.
Speaker C: So was that the enough time?
Speaker F: Yeah.
Speaker F: So I'll just pull up in front at 6.
Speaker F: And yeah, that'll be plenty of time.
Speaker F: It won't be bad traffic that time of day.
Speaker C: I guess once you get past the bridge.
Speaker C: That would be the opening.
Speaker C: Yeah.
Speaker C: Once you get past the turn off to the day bridge.
Speaker F: Well, the turn off bridge.
Speaker F: Yeah.
Speaker F: Won't even do that.
Speaker F: I mean, just go down.
Speaker F: Yeah.
Speaker F: Okay.
Speaker F: And the mountain with the King 988, 888.
Speaker F: So it's about 30 minutes to get there.
Speaker F: So it leaves us 50 minutes before the plane.
Speaker F: Yeah.
Speaker F: Great.
Speaker F: Okay.
Speaker F: So that'll, I mean, still not going to be really easy.
Speaker F: Well, particularly for the Burian, we're not staying overnight.
Speaker F: We're not bringing anything.
Speaker F: We're going to take a little bit.
Speaker F: We're going to have a paper.
Speaker F: We're going to do a little bit.
Speaker F: Don't bring a foot locker and we'll be okay.
Speaker F: So staying overnight, I figured he wouldn't need a great big.
Speaker F: Oh, yeah.
Speaker F: Anyway.
Speaker F: Okay.
Speaker F: Six a.m.
Speaker F: Front.
Speaker F: Six a.m. in front.
Speaker F: I'll be here.
Speaker F: I'll give you my phone number.
Speaker F: I'll give you a few minutes.
Speaker F: Wait a minute.
Speaker F: Wait a minute.
Speaker F: Wait a minute.
Speaker F: Wait a minute.
Speaker F: Wait a minute.
Speaker F: Wait a minute.
Speaker F: Wait a minute.
Speaker F: Wait a minute.
Speaker F: Wait a minute.
Speaker F: Wait a minute.
Speaker F: Okay.
Speaker F: That was the real, real important stuff.
Speaker F: I figured maybe wait on the potential goals for the meeting until we talk about what's been going on.
Speaker F: So what's been going on?
Speaker F: Let me start over here.
Speaker I: Well, the duration of the French test data.
Speaker I: Well, this is a digital French database, which is a microphone speech.
Speaker I: Don't suffer to make it worse.
Speaker I: I've had a noise to one part, which is actually the Aurora 2 noises.
Speaker I: So this is the training part.
Speaker I: And the remaining part are useful testing with one or two kind of noises.
Speaker I: So this is almost ready.
Speaker I: I'm preparing the HTK baseline for this task.
None: Okay.
Speaker F: So the HTK baseline, so this is using Melcadstrand.
Speaker F: Yeah.
Speaker F: Again, I guess the plan is to then give in this.
Speaker F: What's the plan again?
Speaker F: The plan we've been doing.
Speaker F: So just remind me of what you were going to do.
Speaker F: You just described what you've been doing.
Speaker F: Yeah.
Speaker F: So if you could remind me of what you're going to be doing.
Speaker I: Yeah.
Speaker I: Well, I'm like a cube.
Speaker I: I should definitely shoot.
Speaker I: Actually, we want to analyze three dimensions, the feature dimension, the training data dimension, and the test data dimension.
Speaker I: Well, what we want to do is first we have number for each task.
Speaker I: So we have the integer task, the Italian task, the French task, and the Finnish task.
Speaker I: So we have numbers with systems.
Speaker I: I mean, I mean, you run the work strain on the task data.
Speaker I: And then we have systems with neural networks trained on data from the same language, if possible, but using a more generic database, which is for the tickly balance.
Speaker F: So we had talked, I guess we had talked at one point about maybe the language ID corpus.
Speaker I: Yeah, but this corpus, there is the call OM and the callFranco.
Speaker I: The callFranco is for language identification.
Speaker I: Anyway, these corpus are telephone speech, so this could be a group of four.
Speaker I: Because the speech database are not telephone speech.
Speaker I: They are not sample to eight kilohertz, but they are not telephone bandwidth.
Speaker F: So it's funny, isn't it? I mean, because this whole thing is for telephone.
Speaker I: Yeah, but the idea is to compute the feature before sending them to the...
Speaker I: Well, you don't send speech use and features, compute, not the...
Speaker F: Yeah, I know, but the reason is...
Speaker F: So the point is that it's the features are computed locally, and so they aren't necessarily telephone bandwidth or telephone.
Speaker C: Did you happen to find out anything about the OGI multilingual database?
Speaker F: Yeah, that's what I meant.
Speaker F: I said, there's an OGI language ID, not the callFranco.
Speaker F: Yeah, there are also two other databases.
Speaker I: One, they call the multilingual database, and another one is 22 language.
Speaker I: But it's also a different speech. Oh, they are.
Speaker F: Well, but I'm not sure...
Speaker F: I mean, the bandwidth shouldn't be such an issue, right?
Speaker F: Because this is down sample filtered, right?
Speaker F: So it's just the fact that it's not telephone.
Speaker F: And there are so many other differences between these different databases.
Speaker F: I mean, some of the stuff's recorded in the car, some of the... I mean, there's many different acoustic differences.
Speaker F: So I'm not sure...
Speaker F: I mean, else we're going to include a bunch of car recordings in the training database.
Speaker F: I'm not sure if it's completely rules it out.
Speaker F: If our major goal is to have the metacontext, and you figure that there's going to be a mismatch in acoustic conditions, does it make it much worse to sort of add another mismatch if you will?
Speaker F: I guess the question is, how important is it to get multiple languages in there?
Speaker I: Yeah, but...
Speaker I: Yeah.
Speaker I: Well, actually, for the moment, if we do not want to use these different databases, we already have an English and French microphone speech.
Speaker F: So that's why I think we're using a sort of multilingual...
Speaker I: Yeah, for the multilingual part, we were thinking about using these three databases,
Speaker F: and the difference in the metacontext.
Speaker I: Actually, these three databases are generic databases. So for Italian, which is close to Spanish, French, the attitudes we have, digits training data, and also more general training data.
Speaker F: Well, we also have this broadcast news that we were talking about taking off the disk, which is this microchannel data for language.
Speaker I: Yeah, brups.
Speaker I: Yeah, there is also a timid.
Speaker I: Yeah, I would use timid.
Speaker F: Yeah, so it's probably a stuff around.
Speaker F: Okay, so anyway, the basic plan is to test this cube.
None: Yes, thank you.
Speaker F: The fillet filled in, yeah.
Speaker I: Okay, yeah, and brups, we were thinking that, perhaps, the cross-language issue is not so big of an issue, well-reviewed, brub-fusion, not focused too much on that cross-language stuff.
Speaker I: I mean, training on that on the language and testing for another language.
Speaker I: Perhaps the most important is to have neural networks trained on the target languages, with general database, general databases.
Speaker I: So that, well, the guy who has to develop an application with one language can use the net train on that language, or a generic net, but in other terms,
Speaker F: That's how you mean using the net. So if you're talking about for producing these discriminative features, you can't do that, because what they are asking for is a feature set.
Speaker F: Right, and so we're the ones who have been weird by doing this training.
Speaker F: But if we say, no, you have to have a different feature set for each language, I think this is very, very bad.
Speaker F: Oh, yeah.
Speaker F: Yeah, I mean, in principle, I mean, concessually, sort of like they want, well, they want a replacement for Melcafster.
Speaker F: So they say, okay, this is the year 2000.
Speaker F: We've got something much better than Melcafster, it's, you know, Gavaldi-Gook.
Speaker F: And so Gavaldi-Gook features, but these Gavaldi-Gook features are supposed to be good for any language.
Speaker F: Because you don't know who's going to call, and you know, I mean, so it's, it's, it's, how do you know what language?
Speaker F: So it picks up the phone.
Speaker F: So this is their English.
Speaker F: So it picks up the phone, right?
Speaker I: And you pick up the application, there is a target, the English one, the application.
Speaker F: So, yeah, you pick up the phone.
Speaker F: Yeah, you talk the phone and it sends features in.
Speaker F: Okay, so the phone doesn't know what your language is.
Speaker I: If it's the phone, but, but that's the image that they could be the, the server side.
Speaker F: It could be, but that's the image they have.
Speaker F: So that's, that's, I mean, one could argue all over the place about how things really will be in 10 years.
Speaker F: But the particular image that the cellular industry has right now is that it's distributed to be recognition where the probabilistic part and the semantics and so forth are all on the servers, and you compute features on the phone.
Speaker F: So that's, that's what we're involved.
Speaker F: You might, you might not agree that that's the way it will be in 10 years, but that's, that's what they're asking for.
Speaker F: So, so I think that the, it is an important issue whether it works cross-language.
Speaker F: Now it's the OGI folks perspective right now that probably that's not the biggest deal.
Speaker F: And that the biggest deal is the, you know, acoustic environment mismatch.
Speaker F: And they may very well be right, but I was hoping we could just do a task and determine if that was true.
Speaker F: That's true. We don't need to worry so much. Maybe, maybe we have a couple of languages in the training set, and that gives us enough breadth that, that, that the rest doesn't matter.
Speaker F: The other thing is this notion of training to, which I guess they're starting to look at up there, training to something more like articulate-toy features.
Speaker F: And if you have something that's just good for distinguishing different articulate-toy features, that should just be good across, you know, either into languages.
Speaker F: Yeah, so I don't, I don't, unfortunately, I don't, I see what you're coming, where you're coming from, I think, but I don't think we can, you know,
Speaker I: so we really have to do tests with real cross-language, I mean, for instance, training on English and testing in Italian, or we can train, or else, can we train a net on a range of languages, which can include the test, the target language.
Speaker F: Yeah, so there's, there's, this is complex. So, ultimately, is this saying, I think it doesn't fit within their image that you switch nets based on language.
Speaker F: Now, can you include the target language?
Speaker F: From a pure standpoint, it'd be nice not to, because then you can say, because surely someone is going to say at some point, okay, so you put in the German and the Finnish, now what do you do when somebody has Portuguese?
Speaker F: You know, and however, you aren't, it isn't actually a constraint in this evaluation.
Speaker F: So I would say, if it looks like there's a big difference to put it in, then we'd make note of it, and then we'd probably put in the other, because we have so many other problems in trying to get things to work well here, that it's not so bad as long as we know it, and we say, look, we did do this.
Speaker C: So ideally, what you'd want to do is you'd want to run it with and without the target language in a training set for a wide range of languages.
Speaker C: And that way you can say, well, you know, we're going to build it for what we think are the most common ones, but if that somebody is with a different language, you know, here's what, here's what's likely to happen.
Speaker F: And the truth is that it's not like there are, I mean, although there are thousands of languages, from the point of view of cellular companies, there aren't.
Speaker F: There's, you know, there's 50, there's something, you know, so, and they aren't, you know, the exception of Finnish, which I guess is pretty different from most things.
Speaker F: It's, most of them are like at least some of the others, I guess, that's why I guess this vanishes like the town.
Speaker F: I guess Finnish is a little bit like Hungarian, I suppose, right?
Speaker F: I don't know.
Speaker F: I don't know.
Speaker F: I know that, I mean, I'm not like this, but I guess Hungarian and Finnish, and one of the languages from the former Soviet Union is sort of the same family, but it's just this film.
Speaker F: The countries that are pretty far apart from one another, people are rode in on horses.
Speaker H: Okay.
Speaker H: Oh, my turn.
Speaker H: Okay.
Speaker H: Let's see, I spent the last week looking over Stefan Schulder, and understanding some of the data.
Speaker H: I reinstalled the HTK, the free version.
Speaker H: So, everybody's now using 3.0, which is the same version that OGI is using.
Speaker H: Oh, good.
Speaker H: Yeah, so, without any licensing big deals or anything like that.
Speaker H: And so, we've been talking about this cube thing, and it's beginning more and more looking like the, the Borg cube thing.
Speaker H: It's really gargantuan.
Speaker H: But, I...
Speaker F: I'm not going to be a similar one.
Speaker H: I'm not a resistant person.
Speaker H: Exactly.
Speaker H: Yeah, so, I've been looking at a Timmit stuff.
Speaker H: The stuff that we've been working on with Timmit, trying to get a, a labels file so we can train up a net on Timmit, and test the difference between this net train on Timmit and a net trained on digits alone, and seeing if it hurts or helps.
Speaker F: And again, just to clarify, when you're talking about training, have you been talking about training, have you been net for Tandem?
Speaker F: Yeah, yeah, I'm very...
Speaker F: And the inputs are POP and Delta.
Speaker H: Well, the inputs are one dimension of the cube, which we've talked about.
Speaker H: It being PLP, MFCCs, J-Rasta, LVA.
Speaker F: Yeah, but your initial things you're making one choice there.
Speaker H: Yeah, just PLP.
Speaker H: I haven't decided on an initial thing.
Speaker H: Probably something like PLP.
Speaker F: Yeah.
Speaker F: So, you take PLP and you...
Speaker F: You use HDK with it with the transform features using the neural net that's trained, and the training could either be from digits itself or from Timmit.
Speaker F: Right.
Speaker F: And then the testing would be these other things which might be foreign language.
Speaker F: Right.
Speaker F: I see.
Speaker F: I get in the picture about the cube.
Speaker F: Okay.
Speaker F: Okay.
Speaker F: I mean, those listening to this will not have a picture either, so I guess I'm not in worse off.
Speaker F: Somebody just told me the cube.
Speaker F: It sounds...
Speaker F: I get it.
Speaker F: I think I get it.
Speaker C: Yeah.
Speaker C: So, when you said that you're getting the labels for Timmit, what do you mean?
Speaker H: Oh, I'm just transforming them from the standard Timmit transcriptions to do a nice long, huge P-file.
Speaker C: Were the digits hand labeled for phones?
Speaker C: Or were they those labels automatically?
Speaker H: Oh, yeah. Those were automatically derived by Dan using embedded training in the alignment.
Speaker F: Oh, but which Dan?
Speaker H: Alice.
Speaker H: Okay.
None: Yeah.
Speaker G: So.
Speaker C: I was just wondering because that test...
Speaker C: I think you're doing this test because you wanted to determine whether or not having general speech performs as well as having specific speech.
Speaker C: That's right.
Speaker F: Well, especially when you go over the different languages again, because you would...
Speaker F: The different languages have different words from the physics of it.
Speaker C: I was just wondering if the fact that Timmit, who is in a hand labeled stuff from Timmit, might be confused the results that you get.
Speaker F: I think it would, but on the other hand, it might be better.
Speaker C: But if it's better, it may be better because it was handed.
Speaker F: Yeah, so probably use it.
Speaker F: I mean, you know, I guess I'm Sonic Cavalier, but I mean, I think the point is you have a bunch of labels and hand-marked...
Speaker F: I guess actually Timmit was not entirely hand-marked.
Speaker F: It was automatically first and then corrected.
Speaker F: But it might be a better source.
Speaker F: So, you're right. It would be another interesting scientific question to ask.
Speaker F: Is it because it's a broad source or because it was, you know, carefully?
Speaker F: And that's something you could ask.
Speaker F: But given limited time, I think the main thing is if it's a better thing for you to run across languages on this training tandem system.
Speaker C: What about the differences in the phone sets?
Speaker H: Between languages?
Speaker C: No, between Timmit and the physics.
Speaker H: Oh, right.
Speaker H: Well, there's a mapping from the 61 phonemes in Timmit to 56, the XE56.
Speaker H: And then the digits phonemes, there's about 22 or 24 of them.
Speaker H: Out of that 56?
Speaker H: Out of that 56.
Speaker H: So, it's definitely broader.
Speaker I: But actually the issue of phonemes, phonemapics, will arise when we will do several languages.
Speaker I: Because some phonemes are not in every languages.
Speaker I: So, we plan to develop a subset of phonemes that includes all the phonemes of training languages.
Speaker I: Using the two words, kind of 100 of boots.
Speaker I: Yeah, super set.
Speaker I: Yeah, super set.
Speaker A: I look for some per form.
Speaker A: For English, American English.
Speaker A: And the language who have more phonemes are the English of the language.
Speaker A: But, for example, in Spain, in the Spanish half, several phonemes that does not appear in the English and we are too complete.
Speaker A: But for that, in this, we must do a lot of work because we need to generate the transcription for the database that we have.
Speaker E: Other than the languages, there are reasons not to use Timmit phonemes.
Speaker E: Because it's larger.
Speaker E: Is it supposed to be X-E?
Speaker H: Oh.
Speaker H: I mean, why map the 61 to 56?
Speaker H: I don't know.
Speaker F: I figured if that happened to start with you or was it, or was there, yeah, so it's what you did that.
Speaker F: But I think basically there were several phonemes that were just hardly over there.
Speaker C: Yeah, and I think some of them, they were making distinctions between silence at the end and silence at the beginning.
Speaker C: And really, most silence.
Speaker C: And it was things like that that got it mapped down.
Speaker F: Yeah, especially a system like ours, which is a discriminative system.
Speaker F: You know, you really ask him to map the learn.
Speaker C: There's not much different.
Speaker C: The ones that are gone, I think, or they also, they also, and Timmit had like a glottal stop, which basically a short period of silence.
Speaker C: Well, we have that now too.
Speaker C: I don't know.
Speaker F: It's actually pretty common that a lot of the recognition systems people use have things like, like say, 39, simple symbols, right?
Speaker F: And then they get the variety by by bringing in the context.
Speaker F: And that context.
Speaker F: So we actually have them usually large number.
Speaker F: We'll be 10 to use here.
Speaker F: So actually, maybe now you've got me sort of intrigued, but there's...
Speaker F: Can you describe what's on the cube?
Speaker H: I think that's a good idea to talk about the whole cube, and maybe we can cut out sections in the cube for people to work on.
Speaker H: Okay.
Speaker F: So even the one in the quarter doesn't, since we're not running a video camera, we'll get this.
Speaker F: If you use a board, it'll help us anyway.
Speaker F: Okay, point out one of the limitations of this.
Speaker F: You've got to worry less, right?
Speaker H: Yeah, worry less.
Speaker H: Can you walk around too? No.
Speaker H: Okay, well, basically the cube will have three dimensions.
Speaker H: First dimension is the features that we're going to use.
Speaker H: And the second dimension is the training corpus.
Speaker H: And that's the training on the discriminant neural net.
Speaker H: And the last dimension happens...
Speaker F: Yeah, so the training for HDK is always...
Speaker F: That's always set up for the individual test, right?
Speaker F: There's some training data and some test data.
Speaker F: Right, right.
Speaker H: This is for ANN only.
Speaker H: And the training for the HDK models is always fixed for whatever language you're testing on.
Speaker H: And then there's the testing corpus.
Speaker H: So then I think it's probably instructive to go and show you the features that we were talking about.
Speaker H: So let's see.
Speaker H: It's a healthy L.
Speaker H: Okay, what?
Speaker H: BLP.
Speaker H: FST.
Speaker I: MSG.
Speaker H: MSG.
Speaker I: J-Rasta.
Speaker H: J-Rasta.
Speaker H: And J-Rasta L.E.
Speaker I: J-Rasta L.E.
Speaker I: J-Rasta L.E.
Speaker H: Multiband.
Speaker H: Multiband.
Speaker I: So there would be multiband before...
Speaker I: Before the tour, I mean...
Speaker H: Yeah, just the multiband features, right?
Speaker H: Yeah.
Speaker I: So something like the CT with advanced.
Speaker I: And then multiband after networks, meaning that you would have...
Speaker I: You run networks, the script that you run, the script for each band.
Speaker I: And using the outputs of these networks or the linear outputs.
Speaker I: Something like that.
Speaker C: Yeah.
Speaker C: What about no?
Speaker C: Oh.
Speaker C: You don't include that because it's part of the base.
Speaker F: We do have a baseline system that's the smell capture, right?
Speaker I: But...
Speaker I: Not for the A and M.
Speaker I: Okay.
Speaker I: So yeah, we could add a CT.
Speaker F: Probably should.
Speaker F: At least conceptually, you know, it doesn't mean you actually have to do it.
Speaker F: Conceptually, it makes sense, so it's a baseline.
Speaker C: It'd be an interesting test just to have...
Speaker C: Just to do MFCC with the neural mat.
Speaker C: And everything else the same.
Speaker C: Compare that with just that MFCC without the net.
Speaker C: Yeah.
Speaker H: I think Dan did some of that in his previous Aurora experiments.
Speaker H: And with the net, it's wonderful.
Speaker H: And now the net's just baseline.
Speaker F: I think OGI folks have been doing it too.
Speaker F: Because I think they're for a bunch of their experiments.
Speaker F: They used a no-cam study.
Speaker F: Yeah, actually.
Speaker F: Of course, that's there.
Speaker F: It's here.
Speaker H: Okay.
Speaker H: Okay.
Speaker H: For the training corpus, we have the digits from the various languages.
Speaker H: English, Spanish, French, what else do we have?
Speaker I: The finish.
Speaker I: The finish.
Speaker A: Where did that come from?
Speaker A: Italy, no.
Speaker A: Italy, yes.
Speaker A: Italian.
Speaker C: Is that distributed with Aurora or Aurora?
Speaker I: So English, the finish and Italian are Aurora.
Speaker I: And Spanish and French is something that we can use in addition to Aurora.
Speaker I: What?
Speaker F: Yes, the German brother, Spanish and stuff on both the French.
Speaker H: Okay.
Speaker H: And...
Speaker H: Oh, yeah.
Speaker H: Is it French French or Belgian?
Speaker H: It's French French.
Speaker A: French French.
Speaker A: French French.
Speaker A: French French.
Speaker F: I think that is more important than French French.
Speaker F: Yeah, probably.
Speaker F: Yeah.
Speaker F: Yeah, everybody always insists the Belgium is absolutely pure French.
Speaker F: But he says those pre-easions talk about it.
Speaker F: He likes Belgian fries too.
Speaker H: And then we have a broader corpus like Timit.
Speaker H: Timit's so far.
Speaker H: Spanish.
Speaker H: Spanish stories.
Speaker A: I buy you tea.
Speaker H: What about TI digit?
Speaker H: All these Aurora data is derived from TI digits.
Speaker H: Basically, they corrupted it with different kinds of noises at different SNR levels.
Speaker F: And I think Stefan was saying there's some broader material in the French also.
Speaker F: Yeah, we could use it.
None: Okay.
Speaker A: French data.
Speaker A: Spanish stories.
Speaker H: Spanish.
Speaker H: Spanish something.
Speaker H: Yeah.
Speaker H: Okay.
Speaker D: The Aurora people actually corrupted themselves or just specifies a signal on signal noise ratio.
Speaker H: They corrupted it themselves, but they also included the noise files for us, right?
Speaker H: Yeah.
Speaker H: So we can go ahead and corrupt other things.
Speaker F: I'm just curious to come here.
Speaker F: I mean, I couldn't tell if you were joking or is it Mexican Spanish?
Speaker F: No, no, no.
Speaker A: Oh, no, no.
Speaker F: Spanish is Spanish.
Speaker F: Spanish is Spanish.
Speaker F: Spanish is Spanish.
Speaker H: Okay.
None: Spanish is Spanish.
Speaker F: Spanish is Spanish.
Speaker F: Spanish is Spanish.
Speaker F: Yeah, we're really covered.
Speaker F: Okay.
Speaker F: Yeah, no different.
Speaker I: Yes.
Speaker I: From Paris.
Speaker I: Oh, from Paris.
Speaker I: Yeah.
Speaker H: Timit's from lots of different places.
Speaker F: It's from Texas.
Speaker E: It's from Texas.
Speaker F: It's not really from the US either.
Speaker H: Okay.
Speaker H: And within the training corpus, we're thinking about training with noise.
Speaker H: So incorporating the same kinds of noises that Aurora is incorporating in their training corpus.
Speaker H: I don't think we were given the unseen noise conditions.
Speaker F: I think what they were saying was that for this next test, there's going to be some of the cases where they have the same type of noises you were given beforehand, and in some cases where you're not.
Speaker H: Okay.
Speaker F: So presumably that'll be part of the topic of analysis of the test results.
Speaker F: How will you do when it's matching noise?
Speaker F: How will you do when it's not?
Speaker F: Right.
Speaker H: I think that's right.
Speaker H: So I guess we can't train on the unseen noise conditions.
Speaker H: Well, it matters.
Speaker F: Not seen.
Speaker H: It matters, yeah.
Speaker G: Yeah.
Speaker F: I mean, it does seem to me that a lot of times when you train with something that's at least a little bit noisy, it can help you out in other kinds of noise, even if it's not matching, just because there's some more variance that you've built into things.
Speaker F: But exactly how well your work will, how near it is to what you have at the time.
Speaker F: Okay, so that's your training corpus and then your testing corpus?
Speaker H: The testing corpus are just the same one that's oral testing.
Speaker H: And that includes the English, Italian, Finnish.
Speaker H: We're going to get German, right?
Speaker F: Well, so yeah, the final test on the English.
Speaker H: The Spanish perhaps?
Speaker H: Oh, yeah, we can test on the Spanish.
Speaker I: But the oral test.
Speaker F: Oh, yeah.
Speaker F: Oh, there's Spanish testing in Europe.
Speaker I: Not yet, but...
Speaker A: Yeah, it's providing.
Speaker I: They're preparing it.
Speaker I: Well, according to Hineki, it would be good at this at the end of November.
Speaker F: Okay, so I think like seven things in each column, so that's 343 different systems that are going to be developed.
Speaker F: There's three of you.
Speaker B: 100.
Speaker B: What about noise conditions?
Speaker B: What?
Speaker F: Don't we need to put in the column for noise conditions?
Speaker H: You're just trying to be difficult.
Speaker H: Well, when I put these testings on there, I'm assuming there's three tests.
Speaker H: Type A, type B and type C.
Speaker H: And they're all going to be tested with one training of the HTK system.
Speaker H: Test all three different types of noise conditions.
Speaker H: Test A is like a match, noise.
Speaker H: Test B is a slightly mismatched.
Speaker H: And test C is a mismatched channel.
Speaker B: And do we do all our training on clean data?
Speaker H: No, no, no.
Speaker H: We're going to be training on the noise files that we do have.
Speaker F: So, yeah, so I guess the question is how long does it take to do a training?
Speaker F: I mean, it's not totally crazy.
Speaker F: I mean, a lot of these are built-in things.
Speaker F: We know we have programs that compute POP, we have MSG, we have JVET.
Speaker F: You know, a lot of these things, which is kind of how people take in a huge amount of developments, just trying it out.
Speaker F: So we actually can't be quite a few experiments.
Speaker F: But how long does it take?
Speaker F: We think one of these trainings.
Speaker C: That's a good question.
Speaker C: What about combinations of them?
Speaker F: Oh, yeah, that's right.
Speaker F: I mean, because so, for instance, I think the major advantage of MSG, yeah.
Speaker F: What's the point?
Speaker F: The major advantage of MSG, I see that we've seen in the past, is combined with POP.
Speaker H: Now this is turning into a fourth dimensional queue.
Speaker C: Oh, you just select multiple things on the one dimension.
Speaker H: Oh, yeah, okay.
Speaker F: Yeah, so I mean, you don't want to see seven cheeses too.
Speaker F: POP is 21.
Speaker F: Different combinations.
Speaker D: It's not a complete set of combinations, don't we?
Speaker D: What?
Speaker D: It's not a complete set of combinations, though.
Speaker G: Yeah, I hope not.
Speaker H: Yeah, that would be...
Speaker F: Yeah, so POP and MSG, I think we definitely want to try, because we've had a lot of good experience with putting this together.
Speaker C: When you do that, you're increasing the size of the inputs to the net.
Speaker C: Do you have to...
Speaker C: Well, so it doesn't increase the number of trainings.
Speaker C: I'm just wondering about number of parameters in the net.
Speaker C: Do you have to worry about keeping that the same length?
Speaker F: I don't think so.
Speaker E: There's a computation limit, doesn't it?
Speaker E: Yeah, I mean, just more computation.
Speaker E: Excuse me?
Speaker E: Isn't there like a limit on the computation, or latency, or something like that, for a large amount?
Speaker F: Oh, yeah, we haven't talked about that in a new then, and all that.
Speaker F: Yeah.
Speaker F: So it's not really a limit.
Speaker F: What it is is that there's...
Speaker F: There's a...
Speaker F: Just...
Speaker F: If you're using a megabyte, then I'll say that's very nice, but of course, you will remember growing a cheap cell phone.
Speaker F: And I think the computation isn't so much a problem.
Speaker F: I think it's more than that, right?
Speaker F: And the expensive cell phones, expensive handhelds, and so forth, are going to have lots of memory.
Speaker F: So it's just that, as people see, the cheap cell phones is being the biggest market.
Speaker F: So...
Speaker F: But yeah, I was just realizing that actually it doesn't explode out.
Speaker F: It's not only through the cell, but it's...
Speaker F: But it doesn't really explode out the number of trainings, because these are all trained individually.
Speaker F: And so if you have all of these nets trained in some place, then you can combine their outputs and do the care transformation and so forth.
Speaker F: And so what it uploads out is the number of test things.
Speaker F: And the number of times you do that last part, that last part I think is so...
Speaker F: It's got to be pretty quick.
Speaker F: And it's just running the data through...
Speaker C: Well, you got to do the care transformation.
Speaker C: What about a net that's trained on multiple languages?
Speaker C: Is that just separate nets for each language, then combined, or is that actually one net trained?
Speaker I: Probably one net.
Speaker I: One would think one.
Speaker F: But I don't think we tested that.
Speaker I: So in the broader training course, we can use the three or combinations of two languages.
Speaker C: In one net.
Speaker F: Yeah, so I guess the first thing is, if we know how long a training takes, if we can train up all these combinations, then we can start working on testing of them individually and in combination.
Speaker F: And putting them in combination, I think is not as much a computationally as they're training with the net in the first place.
Speaker I: It's not too much.
Speaker I: But there is a testicle, so it's nice training.
Speaker F: It's a lot of things to learn.
Speaker F: How long does it take for an APK training?
Speaker I: It's around six or so, I think.
Speaker A: For training at this thing?
Speaker A: For the Italian, maybe one day.
Speaker A: Running on what?
Speaker F: I don't know what is the value.
Speaker F: I don't know what the value is.
Speaker E: I don't know.
Speaker E: I don't know.
Speaker I: It's not so long because the data is about 30 yards of speech.
Speaker F: There's no way we can begin to do any significant amount here unless we use multiple machines.
Speaker F: What machines are fast, what machines are used a lot, are we still using P-Make?
Speaker F: Once you get the basic thing set up, you have all these combinations.
Speaker I: I would say two days.
Speaker A: I think you folks are probably the only ones using it.
Speaker F: It's faster to do it on the spurred board.
Speaker F: It's still a little faster on the spurred board.
Speaker H: Adam did some testing.
Speaker H: You run on a spurred and then you can do other things on your computer.
Speaker F: You could set up 10 different jobs on spurred boards and have 10 other jobs running on different computers.
Speaker F: It's going to take that sort of thing.
Speaker F: I kind of like this because we have very limited time.
Speaker F: We have quite a bit of computational resource available.
Speaker F: We can look across the institute and now a little things are being used.
Speaker F: We've gotten before about voice-down voice silence, detection features, and I think it's a great thing to go to.
Speaker F: I like about this.
Speaker F: This is what you're thinking of doing in short terms.
Speaker F: Adam sort out about what's the best way to really attack this as a mass problem in terms of using many machines.
Speaker F: We can then present to them what it is that we're doing.
Speaker F: We can pull things out of this list that we think they are doing sufficiently.
Speaker H: How they go to the net trade region?
Speaker H: For the net trade on digits, we have been using 400 border hidden units.
Speaker H: The digit's nets will be correspond to about 20 phonemes.
Speaker H: We're actually broader classes, actually finer classes.
Speaker A: Carmen, did you have something else to add?
Speaker A: I tried to do a different thing with the HTG program.
Speaker F: I don't know what is better if you look at us or J. Rasta.
Speaker F: J. Rasta is more complicated.
Speaker F: J. Rasta is more complicated.
Speaker F: There are more ways that it can go wrong.
Speaker A: I think to recognize the Italian digit with the NetWorps and also to try another NetWorps with the Spanish digit.
Speaker A: The data base was at difficult work last week with the level of time that I have the difference with the level of time.
Speaker F: I'm sorry.
Speaker A: The Spanish level was in different formats for the program to train the NetWorps.
Speaker A: You just have to be properly converting the labels.
Speaker A: I don't know what I asked to do.
Speaker A: I think that with LiveCat I can transfer to ACS format.
Speaker A: I want to put ACS format to ACS format and then use LiveCat to do that.
Speaker F: It seems like there are some peculiarities of the dimensions that are getting sorted out.
Speaker F: We have a lot more computation.
Speaker F: I was thinking two things.
Speaker H: I thought of this as not in stages but more on the time axis.
Speaker F: I was thinking of how much you can realistically do.
Speaker F: I was thinking of how much you can do.
Speaker F: I still think we could do a lot of it.
Speaker H: The second thing was about scratch space.
Speaker F: I want to clarify my point about that.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker C: I want to clarify my point about how much you can do.
Speaker I: I want to clarify my point about how much you can do.
Speaker I: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: I want to clarify my point about how much you can do.
Speaker F: The last topic I had here was Dave's fine offer to do something.
Speaker F: I was thinking perhaps if additionally to all this experiment with this not really research,
Speaker I: it's not really research, but it's running programs. The closer look at the speed noise detection or voice sound detection.
Speaker C: The thing that Sue Neill was talking about with the labels, the thing that was running through these models very quickly,
Speaker F: and maybe that's the only problem I have with it is the same reason why I thought it would be a good thing to do. Let's fall back to that.
Speaker F: That's good.
Speaker F: What an additional clever person could help with when we're really in crunch or time.
Speaker F: So over the years, if he's interested in voice and voice silencing, he could do a lot.
Speaker F: I think it's a good thing to do with the holidays and the middle of it to get a lot done.
Speaker F: The very fact that it is just work and it's running programs and so forth is exactly why it's possible that some piece of it could be handed to someone to do because it's not.
Speaker F: That's a question. We don't have to solve it right this second, but we can think of some piece that's well defined that he could help with.
Speaker C: What about training up a multilingual map?
Speaker A: It's good to have the label between them and Spanish or something like that.
Speaker F: So what we were just saying was that I was arguing for if possible coming up with something that really was development and wasn't research because we have a time crunch.
Speaker F: So if there's something that would save some time for someone else to do, then we should think of that first.
Speaker F: So I think that's the only way to do a core job is to do a core job.
Speaker F: So I think that's the only way to do a core job is to do a core job.
Speaker F: And the other tricky thing is that we are, even though we don't have a strict prohibition on memory size and computation and complexity, clearly there's a limitation to it.
Speaker F: So I think that's one of the very important things to do with the organization, at least some kind of harmonic, or something.
Speaker F: This is another whole thing, take a while to develop.
Speaker F: And then one of the things along with current speech recognition is that we really use the whole way of the harmonious information.
Speaker F: So I think the other suggestion just came up was what about having worked on the multi-lingual super-set and coming up with that and training and that on that.
Speaker F: Is that our multiple database? What would you think? What would this task consist of?
Speaker I: Yeah, it would consist in creating the super-set, modifying the labels for matching the super-set.
Speaker F: So you're creating the changing labels on timid or on multiple languages? Yeah, with the treated languages.
Speaker H: So you have to create a mapping from each language to the super-set.
Speaker H: So you have a machine readable IPA sort of thing. And they have a website that Stefan was showing us has all the English phonemes and their Sampa correspondent phonemes.
Speaker H: And then they have Spanish and German have all sorts of languages mapping to the Sampa phonemes.
Speaker A: You're comparing the Sampa's version to Albaist.
Speaker C: international. No, it's it's saying that you use a special die critical stuff which you can't you can't print out a husky. So the sample is just mapping. Got it. What does OGI
Speaker F: have done anything about this issue? Do they have do they have any kind of
Speaker I: superset they already have? I don't think so well. They they're going actually the other way defining funny blisters. So they just throw the speech from all
Speaker C: different languages together then clustered in the 60 or 50 or whatever. I think they've
Speaker I: not done it during multiple language yet but what they did is to training English nets with all the phonemes and then training English nets with kind of 17 I think it was 70 automatic root glasses. Yeah automatically dried but yeah I think so. And the result was that apparently when testing on cross language it was better but you didn't add didn't have all the results when you showed me that
Speaker F: but so that doesn't make an interesting question though. Is there some way that we should tie into that? Right I mean if if in fact that is a better thing to do should we have the training with our own categories and now we're saying well how do we have to cross language in one way is to come to the superset but they're trying to come up with clusters.
Speaker F: Do we think there's something wrong with that? I think it does something wrong with
Speaker I: okay well because well for a moment we're testing on digits perhaps you using broad phonemes classes it's okay for classifying your digits but as soon as you will have more words words can differ with only a single phonemes which could be the same class. So
Speaker F: right although you are not using this for the future generation. Yeah but you will ask the net to put
Speaker C: a one for the plus so you're saying there may not be enough information coming out of a net to
Speaker D: help discriminate the words. Like most confusions are with it from classes. I think
Speaker E: Larry was saying like obstetricians are only confused with obstetricians. So maybe we could look
Speaker H: at articulatory. Did they not do that? I don't think so. They were looking at both
Speaker F: phonemes but they were talking about it but that sort of a question of what they did because that's the other route to go. So suppose you don't really market it to really market your features you really want to look at the acoustics and see where we can get them. So the second class way of doing it is to look at the phones that are labeled and translate them into acoustic and articulatory features. It won't really be right you won't have these
Speaker C: old and laughing things. So the targets of the net are these articulatory features. Right. But that implies that you can have more than one honor at a time.
Speaker F: That's right. You either do that or you have an fulfillment.
Speaker F: I see. And I don't know if our software, if the version of the quick net that we're using allows for that.
Speaker H: Do you know? It allows for multiple targets being one.
Speaker H: We have gotten soft targets to work.
Speaker F: To work that way. Yeah. Okay. So that's another thing that could be that.
Speaker F: We could just translate instead of translating to a superset just translate to a articulatory feature. So the articulatory features are training that.
Speaker F: The fact even though it's a smaller number, it's still fine because you have the combinations.
Speaker F: So in fact it has every distinction in it.
Speaker F: But you should go across on your just.
Speaker C: We do an interesting thing. It's very not a fact too.
Speaker C: We could, if you had the phone labels you could replace them by their articulatory features and then feed in a vector with those things turned on based on what there's supposed to be for each phone.
Speaker C: Let's see if it's a big win. Do you know what I'm saying?
Speaker C: So I mean if your net is going to be outputting a vector of basically well it's going to have probabilities but let's say they were ones and zeros then you know for each.
Speaker C: I don't know if you know this for your testing data but if you know for your test data what the string of phones is and you have them aligned then you can just instead of going through the net just create the vector for each phone and feed that in to see if that data helps.
Speaker C: Let me think about this as I was talking with TNIC and he said that there was a guy at AT&T who spent 18 months working on a single feature and because they had done some cheating experiments.
Speaker F: This was the guy that we were just talking that we saw in campus who's the lawyer's fault.
Speaker F: He was on our hands.
Speaker F: Right okay.
Speaker C: So he was doing it.
Speaker C: And they had done a cheating experiment or something right?
Speaker C: He didn't mention that part.
Speaker C: But he said that I guess before they had him work on this they had done some experiment where if they could get that one feature right it dramatically improved the results I was thinking you know if you think about this that it would be interesting experiment just to see you know if you did get all of those right.
Speaker F: Should be because if you get all of them in there that defines all of the phones that's the squirtle I'm saying.
Speaker F: Right and you've got all the phones right so that doesn't help us.
Speaker F: Oh yeah it would be an interesting cheating experiment because we are using it in this funny way we're converting it into features.
Speaker C: And then you also don't know what error they've got on the HTK side you know it sort of gives you the best you could hope for.
Speaker E: The soft training of the nets still requires the vector to sum to one.
Speaker E: This one up to one.
Speaker E: So you can't really feed it like two articulatory features that are on at the same time with ones because it will kind of normalize them down to one half or something like that.
Speaker I: But the rest you have destroyed the binon.
Speaker I: Not many areas.
Speaker I: You know you can use it.
Speaker H: It's a six.
Speaker H: No it's actually sigmoid x.
Speaker I: So if you choose sigmoids.
Speaker H: I think apparently the linear outputs.
Speaker E: Linear outputs?
Speaker E: No what you want.
Speaker E: If you're going to do a KL transfer mod.
Speaker H: Right.
Speaker H: Right.
Speaker H: But during the training we're trained on sigmoid x and then at the end just chop off the final non-miniarity.
Speaker F: We're up there.
Speaker F: Oh no.
Speaker F: OK.
None: Really fun.
None: Yes.
