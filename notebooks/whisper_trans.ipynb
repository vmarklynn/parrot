{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Speech to Text\n",
    "\n",
    "This is an example of speech to text modules and using whisper to do transcriptions.\n",
    "\n",
    "In this project we not only transcribe, but also summarize the transcriptions meeting summaries, indeintify speakers etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../services.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"../services.py\"\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import whisper, pytube, hashlib, os, datetime\n",
    "from pytube import YouTube\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from  mangorest.mango import webapi\n",
    "import pyannote\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Segment, Annotation, Timeline\n",
    "import json\n",
    "from django.http import HttpResponse\n",
    "\n",
    "# Taken from https://github.com/yinruiqing/pyannote-whisper\n",
    "class PyanWhisper:\n",
    "    PUNC_SENT_END = ['.', '?', '!']\n",
    "        \n",
    "    def diarize_text(transcribe_res, diarization_result):\n",
    "        timestamp_texts = PyanWhisper.get_text_with_timestamp(transcribe_res)\n",
    "        spk_text = PyanWhisper.add_speaker_info_to_text(timestamp_texts, diarization_result)\n",
    "        res_processed = PyanWhisper.merge_sentence(spk_text)\n",
    "        return res_processed\n",
    "\n",
    "    def get_text_with_timestamp(transcribe_res):\n",
    "        timestamp_texts = []\n",
    "        for item in transcribe_res['segments']:\n",
    "            start = item['start']\n",
    "            end = item['end']\n",
    "            text = item['text']\n",
    "            timestamp_texts.append((Segment(start, end), text))\n",
    "        return timestamp_texts\n",
    "    \n",
    "    def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "        spk_text = []\n",
    "        for seg, text in timestamp_texts:\n",
    "            spk = ann.crop(seg).argmax()\n",
    "            spk_text.append((seg, spk, text))\n",
    "        return spk_text\n",
    "    \n",
    "    def merge_cache(text_cache):\n",
    "        sentence = ''.join([item[-1] for item in text_cache])\n",
    "        spk = text_cache[0][1]\n",
    "        start = text_cache[0][0].start\n",
    "        end = text_cache[-1][0].end\n",
    "        return Segment(start, end), spk, sentence\n",
    "    \n",
    "    def merge_sentence(spk_text):\n",
    "        merged_spk_text = []\n",
    "        pre_spk = None\n",
    "        text_cache = []\n",
    "        for seg, spk, text in spk_text:\n",
    "            if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "                merged_spk_text.append(PyanWhisper.merge_cache(text_cache))\n",
    "                text_cache = [(seg, spk, text)]\n",
    "                pre_spk = spk\n",
    "            elif text[-1] in PyanWhisper.PUNC_SENT_END:\n",
    "                text_cache.append((seg, spk, text))\n",
    "                merged_spk_text.append(PyanWhisper.merge_cache(text_cache))\n",
    "                text_cache = []\n",
    "                pre_spk = spk\n",
    "            else:\n",
    "                text_cache.append((seg, spk, text))\n",
    "                pre_spk = spk\n",
    "        if len(text_cache) > 0:\n",
    "            merged_spk_text.append(PyanWhisper.merge_cache(text_cache))\n",
    "        return merged_spk_text\n",
    "\n",
    "    def write_to_txt(spk_sent, file):\n",
    "        with open(file, 'w') as fp:\n",
    "            for seg, spk, sentence in spk_sent:\n",
    "                line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sentence}\\n'\n",
    "                fp.write(line)\n",
    "\n",
    "# our models                \n",
    "model = whisper.load_model(\"base\", device=\"cuda\")\n",
    "diarizer = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
    "                                    use_auth_token=\"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\")\n",
    "\n",
    "bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "def transcribe_file(file =\"/Users/snarayan/Desktop/data/audio/index.mp4\", **kwargs):\n",
    "    result = model.transcribe(file)\n",
    "    return result\n",
    "\n",
    "    \n",
    "\n",
    "def splitIntoParas(tr, nLinesPerPara=4):\n",
    "    n= nLinesPerPara\n",
    "    l=tr.get('segments', [])\n",
    "    ret = \"\"\n",
    "    for i,j in enumerate(l[::n]):\n",
    "        a, b = i*n, i*n + n\n",
    "        o = \"\".join([j['text'] for j in l[a:b]])\n",
    "        ret += o.strip() + \"\\n\\n\";\n",
    "        #print(f'{a}-{b} {o} \\n')\n",
    "        \n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "test_url = \"https://www.youtube.com/watch?v=DuSDVj9a4WM&list=PLEpvS3HCVQ5_ZlyF1_i-WSwBzLoDLxoc9\"\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------    \n",
    "@webapi(\"/scribe/transcribe_youtube/\")\n",
    "def transcribe_youtube( url = test_url , force_download=False, force_transribe=False, **kwargs):    \n",
    "    h = hashlib.md5(url.encode())\n",
    "    file = \"/tmp/\" + str(h.hexdigest()) + \".mp4\"\n",
    "    \n",
    "    if (force_download or not os.path.exists(file)):  \n",
    "        file = YouTube(url).streams.filter(only_audio=True).first().download(filename=file)\n",
    "\n",
    "    print( f\"File: {file}\")\n",
    "    if (force_transribe or not os.path.exists(file +\".txt\")):  \n",
    "        print( f\"Calling transcription: {file}.txt\")\n",
    "        tr = model.transcribe(file)\n",
    "        ret = splitIntoParas(tr)\n",
    "        with open(file +\".txt\", \"w\") as f:\n",
    "            f.write(ret)\n",
    "        with open(file +\".json\", \"w\") as f:\n",
    "            f.write(str(tr))\n",
    "            \n",
    "        transcription = ret\n",
    "    else:\n",
    "        with open(file +\".txt\", \"r\") as f:\n",
    "            transcription = f.read()\n",
    "        \n",
    "    return transcription;\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------    \n",
    "@webapi(\"/parrot/transcribe_wavinput/\")\n",
    "def transcribe_wavinput(url, **kwargs):\n",
    "    print(\"Hi: \" + url)\n",
    "    if url.method == 'POST':\n",
    "        file = request.FILES['file']\n",
    "        print(\"I'm in\")\n",
    "    # ret = \"\\n\\n My Name is: \" + n + \"\\n\"\n",
    "    # for g in kwargs:\n",
    "    #     if (g ==\"request\"):\n",
    "    #         continue;\n",
    "    #     ret += g + \" \" + kwargs.get(g) + \"\\n\"\n",
    "    \n",
    "    # return ret\n",
    "#--------------------------------------------------------------------------------------------------------    \n",
    "@webapi(\"/parrot/uploadfile\")\n",
    "def uploadfile(request,  **kwargs):\n",
    "    par = dict(request.GET)\n",
    "    par.update(request.POST)\n",
    "\n",
    "    DESTDIR =\"/tmp/parrot/\"\n",
    "    print(\"uploadfile : \", DESTDIR, kwargs)\n",
    "    \n",
    "    if (not os.path.exists(DESTDIR)):\n",
    "        os.makedirs(DESTDIR)\n",
    "    \n",
    "    \n",
    "    ret = \"Files:\\n\"\n",
    "    for f in request.FILES.getlist('file'):\n",
    "        content = f.read()\n",
    "        filename = f\"{DESTDIR}{str(f)}\"\n",
    "        print(f\"++ Save file {filename} Content: {len(content)} :\")\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(content)\n",
    "        ret += filename + \"\\n\"\n",
    "\n",
    "    print(\" Retuning \", ret )\n",
    "    return ret\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------    \n",
    "@webapi(\"/parrot/processfile\")\n",
    "def processfile(request, force_transribe=False, **kwargs):\n",
    "    print(\"processing file: \", kwargs)\n",
    "\n",
    "    ret = uploadfile(request, **kwargs)\n",
    "    f = ret.split('\\n')[1]\n",
    "\n",
    "\n",
    "    print( f\"Calling transcription: {f}\")\n",
    "    result = model.transcribe(f)\n",
    "    diarization = diarizer(f)\n",
    "    final_result = PyanWhisper.diarize_text(result, diarization) \n",
    "    ret = \"\"\n",
    "    # Write to a new file\n",
    "    with open(f +\".txt\", \"w\") as new_f:\n",
    "        for seg, spk, sent in final_result:\n",
    "            line = f'{spk}:{sent}\\n'\n",
    "            new_f.write(line)                               \n",
    "            ret += line\n",
    "        transcription = ret\n",
    "    \n",
    "    input_ids = tokenizer.encode(transcription, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bart.generate(input_ids)\n",
    "    # Generate Summary\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\\n\" + summary)\n",
    "    response = {'transcription': transcription, 'summary': summary}\n",
    "    return HttpResponse(json.dumps(response), content_type='application/json')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myName' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m myName(\u001b[39m\"\u001b[39m\u001b[39mbabui\u001b[39m\u001b[39m\"\u001b[39m, an\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msada\u001b[39m\u001b[39m'\u001b[39m, j\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mewuiorwerwoe\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'myName' is not defined"
     ]
    }
   ],
   "source": [
    "myName(\"babui\", an='sada', j=\"ewuiorwerwoe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "dt1 = datetime.datetime.now() \n",
    "#tr = transcribe_youtube()\n",
    "dt2 = datetime.datetime.now() \n",
    "\n",
    "#print(tr, dt2 - dt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "whisperpyanndjango",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "46fee864bb45ac05443617bef70632a4a93ed12da25561ee732060b0c7acf590"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
