{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../summarizer_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"../summarizer_service.py\"\n",
    "\n",
    "from django.http import HttpResponse\n",
    "from mangorest.mango import webapi\n",
    "import whisper, hashlib, os, datetime, json, torch\n",
    "from transformers import pipeline\n",
    "import keybert\n",
    "import math\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Remove timelines and return the result in this format:\n",
    "    {SPEAKER}: {SENTENCES}\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        parts = line.split('|')\n",
    "        speaker = parts[1].strip().split(':')[0]\n",
    "        content = parts[1].strip().split(':')[1].strip()\n",
    "        result.append(f\"{speaker}: {content}\")\n",
    "    return '\\n'.join(result)   \n",
    "\n",
    "#-----------------------------models------------------------------------------------------------------------               \n",
    "summarizer = pipeline(\"summarization\", \"vmarklynn/bart-large-cnn-samsum-icsi-ami\", truncation=True)\n",
    "kw_model = keybert.KeyBERT(model='all-mpnet-base-v2')\n",
    "#-----------------------------------------------------------------------------------------------------               \n",
    "\n",
    "@webapi(\"/parrot/summarize_text/\")\n",
    "def summarizeText(request, **kwargs):\n",
    "    post_data = request.POST.dict()\n",
    "    transcription = post_data.get('transcription')\n",
    "    text = post_data.get('text')\n",
    "    wordCount = post_data.get('wordCount')\n",
    "    \n",
    "    input_cleanned_text = preprocess(transcription)\n",
    "    print(\"\\n\\n\", input_cleanned_text, \"\\n\\n\")\n",
    "    print( \"min_length: \", math.ceil(int(wordCount) * 0.1))\n",
    "    print(\"\\n\\nSummarizing...\")\n",
    "    summary = summarizer(input_cleanned_text, min_length = math.ceil(int(wordCount) * 0.1))[0]['summary_text']\n",
    "    print(\"\\n\", summary, \"\\n\")\n",
    "    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(1, 1), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_1= list(dict(keywords).keys())\n",
    "    print(keywords_list_1)\n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(2, 2), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_2= list(dict(keywords).keys())\n",
    "    print(keywords_list_2)    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(3, 3), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)    \n",
    "    keywords_list_3 = list(dict(keywords).keys())\n",
    "    print(keywords_list_3)\n",
    "    \n",
    "    response = {'transcription': transcription, 'summary': summary, \n",
    "                'keywords_list_1': keywords_list_1, 'keywords_list_2': keywords_list_2,\n",
    "                'keywords_list_3': keywords_list_3,}\n",
    "    return HttpResponse(json.dumps(response), content_type='application/json')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------               \n",
    "@webapi(\"/parrot/summarize_summary/\")\n",
    "def summarizeSummary(request, **kwargs):\n",
    "    post_data = request.POST.dict()\n",
    "    summary_input = post_data.get('summary')\n",
    "    wordCount = post_data.get('wordCount-summ')\n",
    "    \n",
    "    print( \"min: \", math.ceil(int(wordCount) * 0.1), \"max: \", math.ceil(int(wordCount) * 0.2))\n",
    "    print(\"\\n\\nSummarizing again...\")\n",
    "    summary = summarizer(summary_input, min_length = math.ceil(int(wordCount) * 0.1))[0]['summary_text']\n",
    "    print(\"\\n\", summary, \"\\n\")\n",
    "    \n",
    "    response = {'summary': summary}\n",
    "    return HttpResponse(json.dumps(response), content_type='application/json')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------               \n",
    "\n",
    "@webapi(\"/parrot/summarize_text_v2/\")\n",
    "def summarizeText(request, **kwargs):\n",
    "    post_data = request.POST.dict()\n",
    "    transcription = post_data.get('transcription')\n",
    "    text = post_data.get('text')\n",
    "    wordCount_input = post_data.get('wordCount-input')\n",
    "    \n",
    "    input_cleanned_text = preprocess(transcription)\n",
    "    # print(\"\\n\\n\", input_cleanned_text, \"\\n\\n\")\n",
    "    print( \"word count input length: \", wordCount_input)\n",
    "    print(\"\\n\\nSummarizing...\")\n",
    "    summary = summarizer(input_cleanned_text, min_length = wordCount_input)[0]['summary_text']\n",
    "    print(\"\\n\", summary, \"\\n\")\n",
    "    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(1, 1), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_1= list(dict(keywords).keys())\n",
    "    print(keywords_list_1)\n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(2, 2), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_2= list(dict(keywords).keys())\n",
    "    print(keywords_list_2)    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(3, 3), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)    \n",
    "    keywords_list_3 = list(dict(keywords).keys())\n",
    "    print(keywords_list_3)\n",
    "    \n",
    "    response = {'transcription': transcription, 'summary': summary, \n",
    "                'keywords_list_1': keywords_list_1, 'keywords_list_2': keywords_list_2,\n",
    "                'keywords_list_3': keywords_list_3,}\n",
    "    return HttpResponse(json.dumps(response), content_type='application/json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yonglong/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from django.http import HttpResponse\n",
    "from mangorest.mango import webapi\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", \"vmarklynn/bart-large-cnn-samsum-icsi-ami\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = \"\"\"\n",
    "SPEAKER_00: Yeah, the universal ones.\n",
    "SPEAKER_01: So presumably that might be an idea too.\n",
    "SPEAKER_01: But it's also 25, you need a lot of new features.\n",
    "SPEAKER_01: Yeah, yeah.\n",
    "SPEAKER_01: I mean what, 25 euros is about?\n",
    "SPEAKER_01: I don't know, 15 pounds or so?\n",
    "SPEAKER_01: And that's quite a lot for a more control.\n",
    "SPEAKER_00: Well, my first thought would be most remote controls are grey or black.\n",
    "SPEAKER_00: As you said, they come with a TV, so it's normally just your basic grey, black, remote controls as a function.\n",
    "SPEAKER_00: So maybe we could think about colour.\n",
    "SPEAKER_00: That might make it a bit different from the rest at least.\n",
    "SPEAKER_00: And as you say, we need to have some kind of gimmick.\n",
    "SPEAKER_00: So I thought maybe something like...\n",
    "\"\"\"\n",
    "original_word_count = 119"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting errors if letting users set the minimum word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length=500 must be inferior than your max_length=142.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unfeasible length constraints: the minimum length (500) is larger than the maximum length (142)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[0;32m----> 2\u001b[0m summary \u001b[39m=\u001b[39m summarizer(dialogue, min_length \u001b[39m=\u001b[39;49m user_input)[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msummary_text\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m summary\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text2text_generation.py:265\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    242\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    167\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[1;32m    168\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[1;32m    169\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[1;32m    170\u001b[0m     ):\n\u001b[1;32m    171\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1084\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1077\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1078\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m     )\n\u001b[1;32m   1083\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1084\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1091\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1090\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1091\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1092\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1093\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:992\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    991\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 992\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    993\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    994\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 187\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    188\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1291\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1284\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBoth `max_new_tokens` and `max_length` have been set but they serve the same purpose -- setting a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m limit to the generated output length. Remove one of those arguments. Please refer to the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m documentation for more information. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1288\u001b[0m     )\n\u001b[1;32m   1290\u001b[0m \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mmin_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m generation_config\u001b[39m.\u001b[39mmin_length \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mmax_length:\n\u001b[0;32m-> 1291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1292\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnfeasible length constraints: the minimum length (\u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mmin_length\u001b[39m}\u001b[39;00m\u001b[39m) is larger than\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1293\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m the maximum length (\u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mmax_length\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1294\u001b[0m     )\n\u001b[1;32m   1295\u001b[0m \u001b[39mif\u001b[39;00m input_ids_seq_length \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mmax_length:\n\u001b[1;32m   1296\u001b[0m     input_ids_string \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unfeasible length constraints: the minimum length (500) is larger than the maximum length (142)"
     ]
    }
   ],
   "source": [
    "user_input = 500\n",
    "summary = summarizer(dialogue, min_length = user_input)[0]['summary_text']\n",
    "summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we change the max length implicitly? ---> then it becomes a text generation model when user input exceeds the original word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 501, but you input_length is only 255. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Firstly, the group discussed the price of the remote control, which was 25.5 Euros. Secondly, the team discussed the design of the new remote control. Thirdly, they talked about the colour of the product. Lastly, they decided to design the product with a lot of new features, such as the ability to use the remote as a remote control with a TV. Finally, they agreed to have some kind of gimmick to make the product stand out from other remote controls, like the colour and the shape of the design. The team decided to use a yellow remote control as well as a red one, which would make it more user-friendly and appealing to the younger generation. The meeting ended with a discussion about the budget and the target group. The group decided to keep the price at 25 Euros and the size of the project at 25 million Euros. They also agreed that the product should be made of plastic and the price should not exceed 25 Euros. The final decision was made by the group members, and the team would make a final decision on the product design later. They decided that the colour should be different from that of the other remotes, and that the design should be simple and user-friendlier, like a yellow or a green one. The project manager concluded that the team should design a product with some special features to make it stand apart from the other ones, like colour and shape, to attract the younger group members and to sell it to the general market.   in the end, the project manager made a decision about the price, and it was decided that 25 Euros would be the most reasonable.  and the budget would be 25 Euros, but the price would not be much higher than that.  than the previous one.  for the product, as the team members thought that the project would be a success, and they would try to sell the product in the international market. Finally the team agreed on a price of 25 million euros and the product would be priced at 25 Euro. The target group would be under the age group of 25 and the market size would be between 25 and 30.5 million Euros, as they would be able to sell more remote controls with a variety of functions. The budget would also be around 25 million Euro. \\xa025 million Euro and the profit would be around 20 million Euro, which is the profit.  The team also decided to make a profit of 12.5 Euro and sell it in the'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = 500\n",
    "summary = summarizer(dialogue, min_length = user_input, max_length = user_input + 1)[0]['summary_text']\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5115531045578c37682d962fe3244651dfde701d1aa72ed856a496a3ac6ab995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
