{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing our summarization service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile \"../summarizer_service.py\"\n",
    "\n",
    "from django.http import HttpResponse\n",
    "from mangorest.mango import webapi\n",
    "import whisper, hashlib, os, datetime, json, torch\n",
    "from transformers import pipeline\n",
    "import keybert\n",
    "import math\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Remove timelines and return the result in this format:\n",
    "    {SPEAKER}: {SENTENCES}\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        parts = line.split('|')\n",
    "        speaker = parts[1].strip().split(':')[0]\n",
    "        content = parts[1].strip().split(':')[1].strip()\n",
    "        result.append(f\"{speaker}: {content}\")\n",
    "    return '\\n'.join(result)   \n",
    "\n",
    "#-----------------------------models------------------------------------------------------------------------               \n",
    "summarizer = pipeline(\"summarization\", \"vmarklynn/bart-large-cnn-samsum-acsi-ami-v2\", truncation=True)\n",
    "kw_model = keybert.KeyBERT(model='all-mpnet-base-v2')\n",
    "#-----------------------------------------------------------------------------------------------------               \n",
    "\n",
    "@webapi(\"/parrot/summarize_text/\")\n",
    "def summarizeText(request, **kwargs):\n",
    "    post_data = request.POST.dict()\n",
    "    transcription = post_data.get('transcription')\n",
    "    text = post_data.get('text')\n",
    "    wordCount = post_data.get('wordCount')\n",
    "    \n",
    "    input_cleanned_text = preprocess(transcription)\n",
    "    print(\"\\n\\n\", input_cleanned_text, \"\\n\\n\")\n",
    "    # print( \"min: \", math.ceil(int(wordCount) * 0.1), \"max: \", math.ceil(int(wordCount) * 0.25))\n",
    "    print(\"\\n\\nSummarizing...\")\n",
    "    summary = summarizer(input_cleanned_text)[0]['summary_text']\n",
    "    print(\"\\n\", summary, \"\\n\")\n",
    "    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(1, 1), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_1= list(dict(keywords).keys())\n",
    "    print(keywords_list_1)\n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(2, 2), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_2= list(dict(keywords).keys())\n",
    "    print(keywords_list_2)    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(3, 3), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)    \n",
    "    keywords_list_3 = list(dict(keywords).keys())\n",
    "    print(keywords_list_3)\n",
    "    \n",
    "    response = {'transcription': transcription, 'summary': summary, \n",
    "                'keywords_list_1': keywords_list_1, 'keywords_list_2': keywords_list_2,\n",
    "                'keywords_list_3': keywords_list_3,}\n",
    "    return HttpResponse(json.dumps(response), content_type='application/json')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------               \n",
    "@webapi(\"/parrot/summarize_summary/\")\n",
    "def summarizeSummary(request, **kwargs):\n",
    "    post_data = request.POST.dict()\n",
    "    summary_input = post_data.get('summary')\n",
    "    wordCount = post_data.get('wordCount-summ')\n",
    "    \n",
    "    print( \"min: \", math.ceil(int(wordCount) * 0.1), \"max: \", math.ceil(int(wordCount) * 0.25))\n",
    "    print(\"\\n\\nSummarizing again...\")\n",
    "    summary = summarizer(summary_input, min_length = math.ceil(int(wordCount) * 0.1), max_length = math.ceil(int(wordCount) * 0.25))[0]['summary_text']\n",
    "    print(\"\\n\", summary, \"\\n\")\n",
    "    \n",
    "    response = {'summary': summary}\n",
    "    return HttpResponse(json.dumps(response), content_type='application/json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "for c in \"http_proxy https_proxy HTTP_PROXY HTTPS_PROXY CURL_CA_BUNDLE\".split():\n",
    "    if (c not in os.environ or not os.environ[c]):\n",
    "        continue;\n",
    "    print(os.environ[c])\n",
    "    os.environ[c] = ''\n",
    "#sys.path.append(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../summarizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"../summarizer.py\"\n",
    "\n",
    "from transformers import pipeline\n",
    "import os, keybert\n",
    "\n",
    "\"\"\"\n",
    "Assumes input is as follows:\n",
    "    text='''\n",
    "    0:00:00 - 0:00:06 | SPEAKER_01: Yeah, we had a long \n",
    "    0:00:06 - 0:00:10 | SPEAKER_01: Morgan wants to make it hard.\n",
    "    0:00:10 - 0:00:13 | None: The counter is not moving.\n",
    "    0:00:13 - 0:00:16 | SPEAKER_01: It doesn't.\n",
    "    0:00:16 - 0:00:18 | SPEAKER_00: I didn't even check yesterday.\n",
    "    0:00:18 - 0:00:20 | SPEAKER_01: It didn't move\n",
    "    0:00:20 - 0:00:22 | SPEAKER_01: I don't know if \n",
    "    0:00:22 - 0:00:24 | SPEAKER_01: Channel 3?\n",
    "    '''\n",
    "\n",
    "Remove timelines and return the result in this format:\n",
    "{SPEAKER}: {SENTENCES}\n",
    "\"\"\"\n",
    "def cleanup(text):\n",
    "    result = []\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        parts = line.split('|')\n",
    "        speaker = parts[1].strip().split(':')[0]\n",
    "        content = parts[1].strip().split(':')[1].strip()\n",
    "        result.append(f\"{speaker}: {content}\")\n",
    "    return '\\n'.join(result)   \n",
    "\n",
    "#-----------------------------models------------------------------------------------------------------               \n",
    "summarizer = pipeline(\"summarization\", \"vmarklynn/bart-large-cnn-samsum-acsi-ami-v2\", truncation=True)\n",
    "kw_model = keybert.KeyBERT(model='all-mpnet-base-v2')\n",
    "#-----------------------------------------------------------------------------------------------------               \n",
    "def summarizeText(transcription, wordCount=1024):\n",
    "    text = cleanup(transcription)\n",
    "    summary = summarizer(text)[0]['summary_text']\n",
    "    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(1, 1), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_1= list(dict(keywords).keys())\n",
    "    print(keywords_list_1)\n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(2, 2), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_2= list(dict(keywords).keys())\n",
    "    print(keywords_list_2)    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(3, 3), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)    \n",
    "    keywords_list_3 = list(dict(keywords).keys())\n",
    "    print(keywords_list_3)\n",
    "    \n",
    "    ret = { 'summary': summary, \n",
    "            'keywords_list_1': keywords_list_1, \n",
    "            'keywords_list_2': keywords_list_2,\n",
    "            'keywords_list_3': keywords_list_3,}\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 08:38:02,919 sentence_transformers.SentenceTransformer INFO: Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "2023-08-24 08:38:03,824 sentence_transformers.SentenceTransformer INFO: Use pytorch device: cpu\n",
      "Your max_length is set to 142, but you input_length is only 133. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=66)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "['bleep', 'counter', 'channel', 'speaker_00', 'speaker_01']\n",
      "['bleep things', 'people bleep', 'counter moving', 'hard counter', 'speaker_01 channel']\n",
      "['people bleep things', 'counter moving speaker_01', 'make people bleep', 'bleep things speaker_01', 'speaker_01 doesn speaker_00']\n"
     ]
    }
   ],
   "source": [
    "import parrot.summarizer as summarizer\n",
    "\n",
    "text='''\n",
    "0:00:00 - 0:00:06 | SPEAKER_01: Yeah, we had a long discussion about how easy we want to make it for people to bleep things out.\n",
    "0:00:06 - 0:00:10 | SPEAKER_01: Morgan wants to make it hard.\n",
    "0:00:10 - 0:00:13 | None: The counter is not moving.\n",
    "0:00:13 - 0:00:16 | SPEAKER_01: It doesn't.\n",
    "0:00:16 - 0:00:18 | SPEAKER_00: I didn't even check yesterday.\n",
    "0:00:18 - 0:00:20 | SPEAKER_01: It didn't move yesterday either when I started it.\n",
    "0:00:20 - 0:00:22 | SPEAKER_01: I don't know if it doesn't look like both.\n",
    "0:00:22 - 0:00:24 | SPEAKER_01: Channel 3?\n",
    "'''\n",
    "ret = summarizer.summarizeText(text)\n",
    "#ct = cleanup(text)\n",
    "#print(ct)\n",
    "#summary = summarizer(ct)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5115531045578c37682d962fe3244651dfde701d1aa72ed856a496a3ac6ab995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
