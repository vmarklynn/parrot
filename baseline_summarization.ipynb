{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from transformers import pipeline\n",
    "\n",
    "# Verify GPU is active\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '\/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "# Utility functions\n",
    "def run_with_gpu(function, *extra_args, gpu=True, which_gpu=\"\/GPU:0\"):\n",
    "    \"\"\"\n",
    "    Runs functions with CUDA accelerator\n",
    "    \"\"\"\n",
    "    if gpu:\n",
    "        with tf.device(which_gpu):\n",
    "            return function(*extra_args)\n",
    "    else:\n",
    "        return function(*extra_args)"
   ],
   "execution_count":1,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Found GPU at: \/device:GPU:0\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "\/opt\/python\/envs\/my310\/lib\/python3.10\/site-packages\/tqdm\/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https:\/\/ipywidgets.readthedocs.io\/en\/stable\/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"KJGBxqSfJ8lifI18i061TN",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Taken from https:\/\/github.com\/yinruiqing\/pyannote-whisper\n",
    "# Converts rttm data from pyannote.audio to create transcription\n",
    "from pyannote.core import Segment, Annotation, Timeline\n",
    "\n",
    "\n",
    "def get_text_with_timestamp(transcribe_res):\n",
    "    timestamp_texts = []\n",
    "    for item in transcribe_res['segments']:\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        text = item['text']\n",
    "        timestamp_texts.append((Segment(start, end), text))\n",
    "    return timestamp_texts\n",
    "\n",
    "\n",
    "def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "    spk_text = []\n",
    "    for seg, text in timestamp_texts:\n",
    "        spk = ann.crop(seg).argmax()\n",
    "        spk_text.append((seg, spk, text))\n",
    "    return spk_text\n",
    "\n",
    "\n",
    "def merge_cache(text_cache):\n",
    "    sentence = ''.join([item[-1] for item in text_cache])\n",
    "    spk = text_cache[0][1]\n",
    "    start = text_cache[0][0].start\n",
    "    end = text_cache[-1][0].end\n",
    "    return Segment(start, end), spk, sentence\n",
    "\n",
    "\n",
    "PUNC_SENT_END = ['.', '?', '!']\n",
    "\n",
    "\n",
    "def merge_sentence(spk_text):\n",
    "    merged_spk_text = []\n",
    "    pre_spk = None\n",
    "    text_cache = []\n",
    "    for seg, spk, text in spk_text:\n",
    "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = [(seg, spk, text)]\n",
    "            pre_spk = spk\n",
    "\n",
    "        elif text[-1] in PUNC_SENT_END:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = []\n",
    "            pre_spk = spk\n",
    "        else:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            pre_spk = spk\n",
    "    if len(text_cache) > 0:\n",
    "        merged_spk_text.append(merge_cache(text_cache))\n",
    "    return merged_spk_text\n",
    "\n",
    "\n",
    "def diarize_text(transcribe_res, diarization_result):\n",
    "    timestamp_texts = get_text_with_timestamp(transcribe_res)\n",
    "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization_result)\n",
    "    res_processed = merge_sentence(spk_text)\n",
    "    return res_processed\n",
    "\n",
    "\n",
    "def write_to_txt(spk_sent, file):\n",
    "    with open(file, 'w') as fp:\n",
    "        for seg, spk, sentence in spk_sent:\n",
    "            line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sentence}\\n'\n",
    "            fp.write(line)"
   ],
   "execution_count":2,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"8me9LMzTcjFz3FZrskjRkS",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def baseline_summary(path_to_source):\n",
    "    \"\"\"\n",
    "    :param path_to_source Path to raw .txt files.\n",
    "    Creates a summary and writes in the result of the summary in .json format\n",
    "    \"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=\"philschmid\/bart-large-cnn-samsum\", truncation=True)\n",
    "\n",
    "    if not os.path.exists(\".\/baseline_sum\"):\n",
    "        os.mkdir(\".\/baseline_sum\")\n",
    "    for filename in glob.glob(f\"{path_to_source}*.txt\"):\n",
    "        txt_raw = filename.split(\"\/\")[2].split(\".\")[0]\n",
    "        result_dict = {}\n",
    "        with open(filename, encoding=\"unicode_escape\") as f:\n",
    "            read_data = f.read()\n",
    "            result_dict[\"filename\"] = filename\n",
    "            result_dict[\"transcript\"] = read_data\n",
    "            result_dict[\"summary\"] = run_with_gpu(summarizer, read_data)\n",
    "        with open(f\".\/baseline_sum\/{txt_raw}.json\", \"w\") as fp:\n",
    "            json.dump(result_dict, fp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "baseline_summary(\".\/data\/\")\n",
    "\n"
   ],
   "execution_count":3,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "\rDownloading (…)lve\/main\/config.json:   0%|          | 0.00\/1.63k [00:00<?, ?B\/s]\rDownloading (…)lve\/main\/config.json: 100%|██████████| 1.63k\/1.63k [00:00<00:00, 1.03MB\/s]\n",
      "\rDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00\/1.63G [00:00<?, ?B\/s]\rDownloading (…)\"pytorch_model.bin\";:   3%|▎         | 52.4M\/1.63G [00:00<00:03, 442MB\/s]\rDownloading (…)\"pytorch_model.bin\";:   6%|▋         | 105M\/1.63G [00:00<00:03, 441MB\/s] \rDownloading (…)\"pytorch_model.bin\";:  10%|▉         | 157M\/1.63G [00:00<00:03, 456MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  13%|█▎        | 210M\/1.63G [00:00<00:03, 462MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  16%|█▌        | 262M\/1.63G [00:00<00:03, 447MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  20%|█▉        | 325M\/1.63G [00:00<00:02, 479MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  24%|██▍       | 388M\/1.63G [00:00<00:02, 503MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  27%|██▋       | 440M\/1.63G [00:00<00:02, 480MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  30%|███       | 493M\/1.63G [00:01<00:02, 467MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  34%|███▎      | 545M\/1.63G [00:01<00:02, 426MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  37%|███▋      | 598M\/1.63G [00:01<00:02, 444MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  41%|████      | 661M\/1.63G [00:01<00:02, 471MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  44%|████▍     | 713M\/1.63G [00:01<00:01, 463MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  47%|████▋     | 765M\/1.63G [00:01<00:01, 456MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  50%|█████     | 818M\/1.63G [00:01<00:01, 461MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  54%|█████▍    | 881M\/1.63G [00:01<00:01, 496MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  58%|█████▊    | 944M\/1.63G [00:01<00:01, 515MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  61%|██████▏   | 996M\/1.63G [00:02<00:01, 496MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  65%|██████▍   | 1.05G\/1.63G [00:02<00:01, 482MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  68%|██████▊   | 1.10G\/1.63G [00:02<00:01, 425MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  72%|███████▏  | 1.16G\/1.63G [00:02<00:00, 462MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  75%|███████▍  | 1.22G\/1.63G [00:02<00:00, 471MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  78%|███████▊  | 1.27G\/1.63G [00:02<00:01, 348MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  81%|████████  | 1.31G\/1.63G [00:02<00:00, 336MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  83%|████████▎ | 1.35G\/1.63G [00:03<00:00, 311MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  86%|████████▌ | 1.39G\/1.63G [00:03<00:00, 328MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  89%|████████▉ | 1.45G\/1.63G [00:03<00:00, 357MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  92%|█████████▏| 1.50G\/1.63G [00:03<00:00, 388MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  95%|█████████▌| 1.55G\/1.63G [00:03<00:00, 414MB\/s]\rDownloading (…)\"pytorch_model.bin\";:  99%|█████████▊| 1.60G\/1.63G [00:03<00:00, 423MB\/s]\rDownloading (…)\"pytorch_model.bin\";: 100%|██████████| 1.63G\/1.63G [00:03<00:00, 432MB\/s]\n",
      "\rDownloading (…)okenizer_config.json:   0%|          | 0.00\/300 [00:00<?, ?B\/s]\rDownloading (…)okenizer_config.json: 100%|██████████| 300\/300 [00:00<00:00, 183kB\/s]\n",
      "\rDownloading (…)olve\/main\/vocab.json:   0%|          | 0.00\/798k [00:00<?, ?B\/s]\rDownloading (…)olve\/main\/vocab.json: 100%|██████████| 798k\/798k [00:00<00:00, 2.37MB\/s]\rDownloading (…)olve\/main\/vocab.json: 100%|██████████| 798k\/798k [00:00<00:00, 2.35MB\/s]\n",
      "\rDownloading (…)olve\/main\/merges.txt:   0%|          | 0.00\/456k [00:00<?, ?B\/s]\rDownloading (…)olve\/main\/merges.txt: 100%|██████████| 456k\/456k [00:00<00:00, 1.35MB\/s]\rDownloading (…)olve\/main\/merges.txt: 100%|██████████| 456k\/456k [00:00<00:00, 1.35MB\/s]\n",
      "\rDownloading (…)cial_tokens_map.json:   0%|          | 0.00\/239 [00:00<?, ?B\/s]\rDownloading (…)cial_tokens_map.json: 100%|██████████| 239\/239 [00:00<00:00, 170kB\/s]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"dqJQzKF7e37KJFnJa4c7Zg",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"G5rmhMenJjlKZY83pr1VIB",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"my310",
   "packages":[
    {
     "name":"tensorflow",
     "version":"2.11.0",
     "source":"PIP"
    },
    {
     "name":"pyannote.audio",
     "version":"2.1.1",
     "source":"PIP"
    },
    {
     "name":"openai-whisper",
     "version":"20230124",
     "source":"PIP"
    },
    {
     "name":"ffmpeg-python",
     "version":"master",
     "url":"https:\/\/github.com\/kkroening\/ffmpeg-python@master",
     "source":"GIT"
    }
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}