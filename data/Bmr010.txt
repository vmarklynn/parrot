0:00:00	SPEAKER_06
 Okay, we seem to be recording.

0:00:05	SPEAKER_06
 All right.

0:00:06	SPEAKER_06
 Sorry about not pre-doing everything the lunch went a little later than I was expecting.

0:00:13	SPEAKER_06
 Chuck.

0:00:14	SPEAKER_05
 Chuck was selling too many jokes.

0:00:18	SPEAKER_05
 Yeah, pretty much.

0:00:20	SPEAKER_05
 Does anybody have an agenda?

0:00:25	SPEAKER_07
 Nope.

0:00:26	SPEAKER_01
 Well, I've sent a couple of items.

0:00:30	SPEAKER_01
 I thought somebody had a lot of your...

0:00:32	SPEAKER_05
 Yeah, we only watched the end of the video.

0:00:35	SPEAKER_05
 No, why don't we talk about practical things?

0:00:37	SPEAKER_01
 Well, I can give you an update on the transcription effort.

0:00:41	SPEAKER_01
 Great.

0:00:42	SPEAKER_01
 Maybe raise the issue of microphone procedures with reference to the finalyness of the recordings.

0:00:51	SPEAKER_04
 Okay, transcription, microphone issues.

0:00:55	SPEAKER_01
 And then, maybe ask these guys, we have great steps forward in terms of the non-speech speech pre-segmenting of the signal.

0:01:03	SPEAKER_01
 Okay.

0:01:04	SPEAKER_06
 Well, we have steps forward.

0:01:06	SPEAKER_06
 Yeah, we prefer this.

0:01:10	SPEAKER_05
 Okay.

0:01:11	SPEAKER_08
 We don't know if it's the results.

0:01:15	SPEAKER_06
 I have a little bit of I-RAM stuff, but I'm not sure if that's of general interest.

0:01:21	SPEAKER_05
 I-RAM?

0:01:22	SPEAKER_05
 I-RAM.

0:01:23	SPEAKER_05
 I-RAM by-gram.

0:01:25	SPEAKER_05
 Yeah, let's see where we are at 330.

0:01:28	SPEAKER_02
 Since I have to leave this usually at 330, can we do the interesting stuff first?

0:01:33	SPEAKER_02
 I'm sorry.

0:01:34	SPEAKER_02
 Well, I'm sorry.

0:01:35	SPEAKER_05
 I'm sorry.

0:01:36	SPEAKER_05
 Yeah, now you get to tell us what's the least specified.

0:01:38	SPEAKER_02
 Well, I guess the work that's been done on segmentation.

0:01:43	SPEAKER_01
 I think that was a good thing to start with.

0:01:45	SPEAKER_04
 Okay.

0:01:47	SPEAKER_04
 And the other thing, which I'll just say very briefly, that maybe relates to that a little bit, which is that one of the suggestions that came up in a brief meeting I had the other day when I was in Spain with Lola Parto and Javier Ferreros, who was here before, was when I started with what they had before, but adding the non-silence boundaries.

0:02:19	SPEAKER_04
 So in what Javier did before, when they were doing, he was looking for speaker change points.

0:02:30	SPEAKER_04
 As a simplification, he originally did this only using silence as a punitive speaker change point.

0:02:42	SPEAKER_04
 And he did not say, look at points where you were changing broads, for that class, for instance.

0:02:49	SPEAKER_04
 And for broadcast news, that was fine.

0:02:53	SPEAKER_04
 Here obviously it's not.

0:02:55	SPEAKER_04
 And so one of the things that they were pushing and discussing with me is why you're spending so much time on the feature issue, when perhaps if you sort of deal with what you were using before, and then just broad did it instead of just using silence as punitive change point.

0:03:18	SPEAKER_04
 So then you've got, you already have the superstructure with Gaussian and HM and simple HM and so forth.

0:03:26	SPEAKER_04
 So there was a little bit of a difference of opinion because I thought it was interesting to look at what features are useful.

0:03:35	SPEAKER_04
 But on the other hand, I saw that they had a good point that if we had something that worked for many cases before, maybe starting from there a little bit because ultimately we're going to end up with some kind of structure like that where you have some kind of simple HM, you're testing the hypothesis that there is a change.

0:03:54	SPEAKER_04
 So anyway, just reporting that.

0:03:59	SPEAKER_04
 So yeah, why don't we do this speech?

0:04:03	SPEAKER_03
 Speech?

0:04:04	SPEAKER_03
 Speech?

0:04:05	SPEAKER_03
 Okay.

0:04:06	SPEAKER_03
 So what we basically did so far was using the mixed file to detect speech or non-speech or something like that, and what I did so far is I just used our old Munich system which is a nation-based system with Gaussian mixtures for speech and non-speech.

0:04:27	SPEAKER_03
 And it was a system which used only one Gaussian for silence and the one Gaussian for speech and now I added multi-mixer possibility for speech and non-speech and I did some training on one dialogue which was transcribed by, yeah, we did a non-speech transcription atom, Dave and I, we did for that dialogue and I trained it on that and I did some pre-segmentations for Jane and I'm not sure how good they are or what other transcribes say.

0:05:03	SPEAKER_03
 They can use it or?

0:05:05	SPEAKER_01
 They think it's a terrific improvement and it really just makes it a world of difference.

0:05:11	SPEAKER_01
 And you also did something in addition which was for those in which there were quiet speakers in the mix.

0:05:19	SPEAKER_03
 Yeah, yeah, that was one one thing.

0:05:23	SPEAKER_03
 Why I added more mixtures for the speech?

0:05:25	SPEAKER_03
 So I saw that there were loudly speaking speakers and quietly speaking speakers and so I did two mixtures, one for the loud speakers and one for the quiet speech.

0:05:36	SPEAKER_06
 And did you hand label who was loud and who was quiet?

0:05:38	SPEAKER_03
 I did that for five minutes of one dialogue.

0:05:41	SPEAKER_03
 That was enough to train the system and so it adapts on while running.

0:05:46	SPEAKER_02
 So what kind of a front end processing did you do?

0:05:51	SPEAKER_03
 It's just our old, mionic loudness-based spectrum on male scale, 20, 20, 20, 20 bands and on loudness and for additional features which is energy, loudness, modified loudness and zero crossing grade.

0:06:08	SPEAKER_03
 So it's 24 features.

0:06:12	SPEAKER_01
 And you also provided me with several different versions which I compared.

0:06:15	SPEAKER_01
 And so you change parameters, do you want to say something about the perverse?

0:06:19	SPEAKER_03
 You can specify the minimum length of speech and silence portions which you want.

0:06:25	SPEAKER_03
 And so I did some modifications in those parameters basically changing the minimum length for silence to have more or less silence portions in the literature.

0:06:43	SPEAKER_06
 So this would work well for pauses and utterance boundaries and things like that before overlap I imagine that doesn't work at all, that you'll have plenty of sections that are.

0:06:53	SPEAKER_01
 That's true, but it saves so much time.

0:06:56	SPEAKER_01
 The transgoers just enormous, enormous savings.

0:06:58	SPEAKER_04
 That's great.

0:06:59	SPEAKER_04
 Just one quickly still on the features.

0:07:02	SPEAKER_04
 So you have these 24 features, a lot of my spectral features.

0:07:05	SPEAKER_04
 Is there a transformation, principal components, transformation or something?

0:07:09	SPEAKER_03
 No, we're just, we're sourced too.

0:07:11	SPEAKER_03
 Originally we did that, but we saw when we used it for our closed talking microphone which, yeah, for our recognition as a mionic, we saw that it's not as necessary.

0:07:21	SPEAKER_03
 It works as well without the LDA or something.

0:07:25	SPEAKER_04
 Okay, no, I was curious.

0:07:27	SPEAKER_04
 I think it's a big deal for this application.

0:07:29	SPEAKER_04
 Yeah, yeah.

0:07:32	SPEAKER_01
 Okay, then there's another thing that also Tilo has involved in, which is, okay, and also Dave Galbart.

0:07:37	SPEAKER_01
 So this is this problem.

0:07:40	SPEAKER_01
 So we had this meeting, also Adam, before you went away.

0:07:45	SPEAKER_01
 Regarding the representation of overlaps, because it's present, because of the limitations of the interface we're using, overlaps are not being encoded by the transgoers in this complete and detailed away as it might be.

0:08:02	SPEAKER_01
 And as might be desired, I think would be desired in the corpus, ultimately.

0:08:05	SPEAKER_01
 So we don't have start and end points at each point where there's an overlap.

0:08:08	SPEAKER_01
 We just have the overlaps encoded in the simple bin.

0:08:12	SPEAKER_01
 Well, okay, so the limits of the interface are such that we were, in this meeting, we were entertaining how we might either expand the interface or find other tools which already do what would be useful, because what would ultimately be ideal in my view, and I think, and I had the sense that it was consensus, is that a thoroughgoing musical score notation would be the best way to go, because you can have multiple channels.

0:08:39	SPEAKER_01
 There's a single timeline.

0:08:40	SPEAKER_01
 It's very clear, flexible, and all those nice things.

0:08:42	SPEAKER_01
 Okay, so I spoke, I had a meeting with Dave Galbart, and he had excellent ideas on how the interface could be modified to do this kind of representation.

0:08:54	SPEAKER_01
 But in the meantime, you were checking into the existence of already existing interfaces, which might already have these properties, so do you want to see something about that?

0:09:04	SPEAKER_03
 Yes.

0:09:05	SPEAKER_03
 I talked with the many guys from Ludwig-Manns-Millianz University who do a lot of transcribing and transliterations, and they basically said they have a tool they developed themselves and they can't give away if it's to our own and it's not supported.

0:09:27	SPEAKER_03
 But Zanipur, who was at CMU, who was formerly at Munich, and is now with CMU, she said she has something which she uses to do a trans-transferrations, a channel simultaneously, but it's running under Windows.

0:09:48	SPEAKER_03
 So, I'm not sure if we can use it.

0:09:51	SPEAKER_03
 She said she would give it to us, it wouldn't be a problem, and I've got some kind of manual down in my office.

0:09:57	SPEAKER_06
 Well, maybe we should get it, and if it's good enough, we'll arrange Windows machines to be available.

0:10:02	SPEAKER_01
 I also wanted to be sure, I've seen this is called PROT, the RAA, which I just mean speech

0:10:09	SPEAKER_03
 and speech. Yeah, but I'm not sure that's the right thing for us.

0:10:13	SPEAKER_01
 In terms of it being Windows versus Windows, but I'm just wondering if it's not.

0:10:16	SPEAKER_06
 No, no PROT.

0:10:17	SPEAKER_01
 Oh, I see.

0:10:18	SPEAKER_01
 Oh, I see, so PROT may not be.

0:10:20	SPEAKER_03
 That's not PROT, it's called trans-edit.

0:10:22	SPEAKER_03
 I see.

0:10:23	SPEAKER_04
 The other thing, to keep in mind, I mean, we've been very concerned to get all this rolling so that we would actually have data.

0:10:35	SPEAKER_04
 But I think our outside sponsor is actually going to kick in and ultimately, that path could be smoothed out.

0:10:42	SPEAKER_04
 I don't know if we have a long-term need to do lots and lots of transcribing.

0:10:47	SPEAKER_04
 I think we had a very quick need to get something out, and we'd like to be able to do some later because it's interesting.

0:10:53	SPEAKER_04
 It's interesting.

0:10:54	SPEAKER_04
 But as far with any luck, we'll be able to wind down the link to PROT.

0:11:00	SPEAKER_06
 What our decision was is that we'll go ahead with what we have with a not very fine time scale on the overlaps and do what we can later to clean that up if we need to.

0:11:11	SPEAKER_01
 I was just thinking that if it were possible to bring that in like this week, then when they're encoding the overlaps, it would be nice for them to be able to specify when the start points and then points of overlaps.

0:11:27	SPEAKER_01
 They're making really quick progress.

0:11:29	SPEAKER_01
 That's great.

0:11:31	SPEAKER_01
 So my goal was, my charge was to get 11 hours by the end of the month and it'll be, I'm clear that we'll be able to do that.

0:11:38	SPEAKER_06
 That's great.

0:11:39	SPEAKER_06
 And did you forward Morgan, Brian's thing?

0:11:43	SPEAKER_01
 I sent it to, who can I send that to?

0:11:46	SPEAKER_01
 I sent it to a list and I thought I sent it to the...

0:11:48	SPEAKER_01
 Meeting the court list?

0:11:49	SPEAKER_01
 Oh yeah, okay, so you probably forgot.

0:11:50	SPEAKER_01
 So Brian did tell me that in fact what you said that they are making progress and that he's going to check the output of the first transcription.

0:12:02	SPEAKER_04
 I mean, basically it's all a different world, and basically he's on it.

0:12:07	SPEAKER_01
 Oh, that's just a new development.

0:12:10	SPEAKER_01
 So this is, so it will happen.

0:12:12	SPEAKER_01
 Super.

0:12:13	SPEAKER_04
 Okay.

0:12:14	SPEAKER_04
 I mean, basically it's just saying one of our best people is on it.

0:12:16	SPEAKER_04
 It just doesn't happen to hear anymore someone else's face.

0:12:19	SPEAKER_02
 But about the need for transcription, I mean, don't we, didn't we previously decide that the IBM transcripts would have to be checked anyway?

0:12:28	SPEAKER_02
 Yes.

0:12:29	SPEAKER_02
 Positive, augmented, so I think having a good tool is worth something.

0:12:32	SPEAKER_02
 Yeah, okay.

0:12:33	SPEAKER_02
 That's a good point.

0:12:35	SPEAKER_06
 Yeah, Dave Galbert did volunteer and since he's not here, I'll repeat it to at least modify transcriber, which if we don't have something else that works, I think that's a pretty good way of doing.

0:12:46	SPEAKER_06
 And we discussed on some methods of doing it.

0:12:47	SPEAKER_06
 My approach originally, and I've already hacked on it a little bit, it was too slow because I was trying to display all the waveforms, but he pointed out that you don't really have to, and then it's a good point, that if you just display the mix waveform and then have a user interface for editing the different channels, that's perfectly sufficient.

0:13:04	SPEAKER_01
 Yeah, exactly, and just keep those things separate.

0:13:06	SPEAKER_01
 And Dan Ellis's hack already allows them to be able to display different waveforms to clarify overlabs the things, so that's what it is.

0:13:13	SPEAKER_06
 They can only display one, but they can listen to different ones.

0:13:16	SPEAKER_01
 Well, yes, but what I mean is that from transcribers perspective, those two functions are separate, and Dan Ellis's hack handles the choice, the ability to choose different waveforms from moment to moment.

0:13:31	SPEAKER_06
 But only to listen to, not to look at.

0:13:34	SPEAKER_06
 The waveform you're looking at doesn't change.

0:13:36	SPEAKER_01
 That's true, yeah, but that's okay, because they're focused on it anyway.

0:13:40	SPEAKER_01
 And then the hack to preserve the overlap better would be one which creates different output files for each channel, which then would also serve Liz's request of having single channel, separable, cleanly, easily separable, transcriber tied to single channel audio.

0:14:00	SPEAKER_04
 Have folks from this been in contact with you?

0:14:04	SPEAKER_01
 Not directly.

0:14:05	SPEAKER_01
 If I could have gotten it over list, I don't think so.

0:14:08	SPEAKER_04
 Okay, well, holidays may have interrupted things, because they seem to want to get absolutely clear on standards for transcription standards and so forth.

0:14:18	SPEAKER_04
 Oh, this was supposed to be for December.

0:14:20	SPEAKER_04
 Right, because they're presumably going to start recording next month.

0:14:25	SPEAKER_06
 Okay, okay.

0:14:27	SPEAKER_06
 So we should definitely get with them then and agree upon a format.

0:14:32	SPEAKER_06
 I don't remember your email on that, so Wes, I had not in the loop on that.

0:14:36	SPEAKER_04
 Yeah, I don't think I mailed anybody.

0:14:39	SPEAKER_04
 I just think I told them the contact gene.

0:14:41	SPEAKER_04
 Okay.

0:14:42	SPEAKER_04
 That's right.

0:14:43	SPEAKER_04
 That's the point person on it.

0:14:46	SPEAKER_06
 Yeah, I think that's right.

0:14:49	SPEAKER_04
 So yeah, maybe I'll hang in there a little bit about it.

0:14:53	SPEAKER_04
 Okay.

0:14:54	SPEAKER_01
 Keeping the conventions absolutely simple as possible.

0:14:57	SPEAKER_04
 Yeah, because with any luck, there will actually be collections at Columbia and collections at UW and Dan.

0:15:06	SPEAKER_04
 Dan is very interested in doing this.

0:15:08	SPEAKER_04
 Right.

0:15:09	SPEAKER_04
 Yeah.

0:15:10	SPEAKER_06
 Well, I think it's important both for the notation and machine representation to be the same.

0:15:14	SPEAKER_06
 Yeah.

0:15:15	SPEAKER_01
 So there was also this email from Dan regarding the speech nonsense presentation.

0:15:19	SPEAKER_01
 Yeah.

0:15:20	SPEAKER_01
 I don't know if we want to end Dan.

0:15:24	SPEAKER_01
 Dave Gowart is interested in pursuing the aspect of using amplitude as basis for the separation.

0:15:32	SPEAKER_01
 Oh, yeah.

0:15:33	SPEAKER_04
 He was a correlation.

0:15:34	SPEAKER_04
 I mean, we hit cross correlation.

0:15:35	SPEAKER_04
 I mentioned this a couple times before the commercial devices that do voice active miking.

0:15:42	SPEAKER_04
 Basically, look at the at the energy and each of the mics.

0:15:46	SPEAKER_04
 You basically compare the energy here to some function of all of the mics.

0:15:50	SPEAKER_04
 So by doing that, rather than setting any absolute threshold, you actually do pretty good selection of who's talking.

0:16:00	SPEAKER_04
 And those systems work very well.

0:16:03	SPEAKER_04
 So people use them in panel discussions and so forth with sound reinforcement.

0:16:09	SPEAKER_04
 And this, why the guy knew we built them like 20 years ago.

0:16:16	SPEAKER_04
 So the techniques worked pretty well.

0:16:20	SPEAKER_01
 It's fantastic.

0:16:21	SPEAKER_01
 Because here's one thing that we don't have right now.

0:16:22	SPEAKER_01
 And that is the automatic channel identifier.

0:16:27	SPEAKER_01
 That would help in terms of encoding of overlaps, the transcribers would have less than just dangling to do if that were available.

0:16:35	SPEAKER_01
 Yeah.

0:16:36	SPEAKER_04
 So I think basically you can look at some, you have to play around a little bit to figure out what the right statistic is, but you compare each microphone to some statistic based on the overall.

0:16:47	SPEAKER_04
 We also have the advantage of having distant mics too.

0:16:50	SPEAKER_06
 Now using the close talking I think would be much better with the...

0:16:58	SPEAKER_04
 I don't know.

0:16:59	SPEAKER_04
 If I was actually working on it, sit there and play around with it and get a feeling for it.

0:17:04	SPEAKER_04
 But you certainly want to use the close talking.

0:17:08	SPEAKER_04
 Right.

0:17:09	SPEAKER_04
 At least.

0:17:10	SPEAKER_04
 I don't know if the other would add some other helpful dimension or not.

0:17:15	None
 Okay.

0:17:16	SPEAKER_08
 What is the difference that classes to code the overlap?

0:17:23	SPEAKER_08
 You would use?

0:17:24	SPEAKER_01
 To code some types of overlap?

0:17:27	SPEAKER_01
 Yeah.

0:17:28	SPEAKER_01
 So, and I mean that it wasn't transcribed.

0:17:31	SPEAKER_01
 We worked up a typology.

0:17:34	SPEAKER_01
 And...

0:17:35	SPEAKER_08
 It looked like you explained in the book.

0:17:38	SPEAKER_08
 Yes, exactly.

0:17:39	SPEAKER_01
 That hasn't changed.

0:17:40	SPEAKER_01
 So, that's basically a two-tiered structure where the first one is whether...

0:17:45	SPEAKER_01
 The person who's interrupt continues or not.

0:17:47	SPEAKER_01
 And then below that there's subcategories, have more to do with, you know, is it simply back channel or is it someone...

0:17:56	SPEAKER_01
 Completing someone else's thought or is it someone who introduced a new thought?

0:17:59	SPEAKER_06
 And I hope that if we do a forced alignment with the close talking mic, that will be enough to recover at least some of the time.

0:18:08	SPEAKER_06
 The time information of when the overlap occurred.

0:18:10	SPEAKER_08
 We hope.

0:18:11	SPEAKER_08
 Yeah, who knows.

0:18:12	SPEAKER_02
 Who's going to do that?

0:18:16	SPEAKER_02
 Who's going to do forced alignment?

0:18:18	SPEAKER_06
 Well, IBM was going to.

0:18:21	SPEAKER_06
 And I imagine they still plan to, but, you know, I haven't spoken with them about that recently.

0:18:25	SPEAKER_04
 Well, my suggestion now is that all of these things to contact my...

0:18:30	SPEAKER_01
 Okay, this is wonderful to have a direct contact like that.

0:18:33	SPEAKER_01
 Well, let me ask you this.

0:18:34	SPEAKER_01
 It occurs to me.

0:18:35	SPEAKER_01
 One of my transcribers told me today that she'll be finished with one meeting by...

0:18:40	SPEAKER_01
 Well, she sent tomorrow, but then she said, but, you know, let's just say maybe the day after just we just on the same side.

0:18:48	SPEAKER_01
 I consent Brian, the transcript.

0:18:51	SPEAKER_01
 I know these are...

0:18:53	SPEAKER_01
 I consent him that it would be possible or a good idea or not to try to do a forced alignment on what we're on the way we're encoding overlaps now.

0:19:03	SPEAKER_04
 I'll just talk to him about it.

0:19:05	SPEAKER_04
 I mean, you know, basically he's just a colleague, friend, and...

0:19:11	SPEAKER_04
 And, you know, the organization always did one.

0:19:13	SPEAKER_04
 I hope this was just a question of getting the right people connected to him at the time.

0:19:17	SPEAKER_06
 Is he on the mailing list?

0:19:20	SPEAKER_06
 Who should add him?

0:19:22	None
 Yeah, I don't know for sure.

0:19:23	SPEAKER_07
 Did something happen?

0:19:24	SPEAKER_07
 Or can he put on this or was he already on it?

0:19:27	SPEAKER_04
 No, I think it...

0:19:29	SPEAKER_04
 Yes, something happened.

0:19:31	SPEAKER_04
 I don't know what it was.

0:19:32	SPEAKER_04
 Yes, for war.

0:19:33	SPEAKER_01
 That would be like, that would be great.

0:19:36	SPEAKER_05
 Right.

0:19:37	SPEAKER_05
 So, where would...

0:19:41	SPEAKER_05
 Maybe a brief...

0:19:45	SPEAKER_05
 Well, let's...

0:19:46	SPEAKER_05
 Why don't we talk about microponations?

0:19:47	SPEAKER_05
 Yeah.

0:19:48	SPEAKER_06
 That would be great.

0:19:49	SPEAKER_06
 So one thing is that I did look on Sony's for replacement for the mics...

0:19:53	SPEAKER_06
 for the head-worn ones, because they're so uncomfortable.

0:19:56	SPEAKER_06
 But I think I need someone who knows more about mics than I do, because I couldn't find a single other model that seemed like it was.

0:20:02	SPEAKER_06
 It seemed like it would fit the connector, which seems really unlikely to me.

0:20:06	SPEAKER_06
 Does anyone like no stores or...

0:20:10	SPEAKER_06
 know about mics who would know the right questions to ask?

0:20:13	SPEAKER_04
 Oh, I probably would.

0:20:15	SPEAKER_04
 I mean, my knowledge is just 20 years out of date, but some of it still disconnects.

0:20:19	SPEAKER_04
 So, where...

0:20:20	SPEAKER_07
 You couldn't find the right connector that go into these things?

0:20:23	SPEAKER_06
 Yep.

0:20:24	SPEAKER_06
 When I looked, they listed one microphone, and that's it.

0:20:26	SPEAKER_06
 As having that type of connector.

0:20:28	SPEAKER_06
 And guess is that Sony maybe uses a different number for their connector than everyone else does.

0:20:34	SPEAKER_06
 And so...

0:20:35	SPEAKER_06
 Oh, let's look at it.

0:20:36	SPEAKER_06
 It seems really unlikely to me that there's only one.

0:20:39	SPEAKER_01
 And there's no adapter for it?

0:20:40	SPEAKER_01
 It seems like it would be a...

0:20:42	SPEAKER_06
 Okay.

0:20:43	SPEAKER_06
 Does that who knows?

0:20:44	SPEAKER_06
 Who are we buying these for?

0:20:48	SPEAKER_06
 I have it downstairs.

0:20:49	SPEAKER_04
 I don't remember off the top of my head.

0:20:51	SPEAKER_04
 Yeah, we can try and look at that together.

0:20:53	SPEAKER_06
 And then...

0:20:55	SPEAKER_06
 Just in terms of how you wear them.

0:20:57	SPEAKER_06
 I had thought about this before.

0:20:59	SPEAKER_06
 When you use a product like Dragon Dictate, they have a very extensive description about how to wear the microphone and so on.

0:21:05	SPEAKER_06
 But I felt that in a real situation, we were very seldom going to get people to really do it.

0:21:11	SPEAKER_06
 And maybe it wasn't worth concentrating on.

0:21:14	SPEAKER_04
 Well, I think that that's a good back-off position that I was saying earlier.

0:21:19	SPEAKER_04
 We are going to get some recordings that are imperfect and pay that's life.

0:21:24	SPEAKER_04
 But I think that it doesn't hurt the naturalness of the situation to try to have people wear the microphones properly if possible.

0:21:35	SPEAKER_04
 Because the natural situation is really what we have with the microphones on the table.

0:21:42	SPEAKER_04
 Oh, that's true.

0:21:43	SPEAKER_04
 In the target applications, we're talking about people aren't going to be wearing headmine mics anyway.

0:21:47	SPEAKER_04
 So this is just for these headmine mics are just for use with research.

0:21:50	SPEAKER_04
 And it's going to make, you know, if Andreas plays around with language modeling.

0:21:56	SPEAKER_04
 He's not going to be want to be used up by people breathing into the microphones.

0:22:01	SPEAKER_06
 Well, I'll dig through the documentation to Dragon Dictate and see if they still have the little form.

0:22:07	SPEAKER_04
 But it does form.

0:22:08	SPEAKER_02
 It's interesting.

0:22:11	SPEAKER_02
 I talked to some IBM guys last January.

0:22:15	SPEAKER_02
 I think I was there. So people who are working on their V-Avoise vacation product.

0:22:22	SPEAKER_02
 And they said the breathing is really a terrible problem for them to not recognize breathing as speech.

0:22:32	SPEAKER_02
 So anything to reduce breathing is a good thing.

0:22:37	SPEAKER_06
 It seemed to me when I was using Dragon that it was really microphone placement helped an enormous amount.

0:22:43	SPEAKER_06
 So you want it enough to the side so that when you exhale through your nose, the wind doesn't hit the mic.

0:22:50	SPEAKER_06
 Everyone's adjusting their microphone of course.

0:22:53	SPEAKER_06
 And then just close enough so that you get good volume.

0:22:56	SPEAKER_06
 So you know, wearing it right about here seems to be about the right way to do it.

0:23:00	SPEAKER_04
 I remember when I used a prominent laboratory's speech recognizer.

0:23:08	SPEAKER_04
 This is a bonus as well, I guess, about 12 years ago or something.

0:23:14	SPEAKER_04
 And they were perturbed with me because I was breathing in instead of breathing out.

0:23:19	SPEAKER_04
 And they had models for the Markov models for the breathing out that they didn't have for breathing in.

0:23:28	SPEAKER_01
 How do I wonder, is whether it's possible to have to maybe use the display at the beginning to be able to judge how correctly.

0:23:36	SPEAKER_01
 Or how do I do that?

0:23:39	SPEAKER_06
 I mean, when it's on, you can see it.

0:23:43	SPEAKER_06
 You can definitely see it.

0:23:45	SPEAKER_06
 Absolutely.

0:23:47	SPEAKER_06
 And so, you know, I've sat here and watched sometimes the breathing and the bar going up and down and thinking, I can say something.

0:23:56	SPEAKER_06
 I don't want to make people self-conscious.

0:23:58	SPEAKER_04
 Stop breathing.

0:23:59	SPEAKER_04
 It's going to be imperfect.

0:24:01	SPEAKER_04
 And you can do some, you know, first-order thing about it, which is to have people move it away from being just directly in front of the middle, but not too far away.

0:24:11	SPEAKER_04
 And then, you know, I think there's not much because you can't, you know, interfere.

0:24:16	SPEAKER_04
 You can't fine tune the meeting that much.

0:24:18	SPEAKER_01
 Right.

0:24:19	SPEAKER_01
 That's true.

0:24:20	SPEAKER_01
 It just seems like if something simple like that can be tweaked and the quality goes, you know, dramatically up in my people.

0:24:28	SPEAKER_06
 And then also, the position of the mic also, if it's more directly, you'll get better volume.

0:24:33	SPEAKER_06
 So, like, there's this pretty far down, the lower mouth.

0:24:37	SPEAKER_01
 My feedback from the transcribers is he is always close to Chris McLaren, and just been fantastic.

0:24:42	SPEAKER_06
 I don't know why that is.

0:24:44	SPEAKER_01
 You're also, your volume is greater, but still, I mean, they say...

0:24:49	SPEAKER_06
 I've been eating a lot.

0:24:52	SPEAKER_04
 It makes it their job extremely easy.

0:24:59	SPEAKER_01
 I could say something about the why I don't you want to.

0:25:03	SPEAKER_01
 About what?

0:25:04	SPEAKER_01
 About the transcribers or anything?

0:25:06	SPEAKER_02
 Well, why don't we do that?

0:25:08	SPEAKER_02
 But just to one more remark concerning the SRI recognizer, it is useful to transcribe and then ultimately train models for things like breadth and also the laughter is very, very important.

0:25:21	SPEAKER_02
 So, in your transcribers mark them.

0:25:28	SPEAKER_02
 Mark very audible breaths and laughter especially.

0:25:32	SPEAKER_01
 They are putting... so in curly brackets, they put inhale or breath.

0:25:36	SPEAKER_01
 Oh, great.

0:25:37	SPEAKER_01
 And then in curly brackets, they say laughter.

0:25:39	SPEAKER_01
 Now, they're not being awfully precise.

0:25:42	SPEAKER_01
 So, the two types of laughter, they're not being distinguished.

0:25:45	SPEAKER_01
 One is, when sometimes someone will start laughing one there in the middle of a sentence.

0:25:50	SPEAKER_01
 And then the other one is when they finish the sentence and then they laugh.

0:25:54	SPEAKER_01
 So, I did some double checking to look through.

0:25:59	SPEAKER_01
 You need to have extra complications like time tags indicating the beginning and ending up laughing.

0:26:06	SPEAKER_01
 That's a lot of different than that.

0:26:08	SPEAKER_01
 What they're doing is, in both cases, just saying Craig is laughing at it.

0:26:11	SPEAKER_02
 As long as there is an indication that there was laughter somewhere between two words, I think that's a fish worth the most.

0:26:17	SPEAKER_02
 Actually, the recognition of laughter once you know, it's pretty good.

0:26:22	SPEAKER_02
 So, as long as you can stick a tag in there that indicates that there was laughter, that would probably be a very interesting, prismatic feature.

0:26:34	SPEAKER_01
 And I'm going to ask one thing about that.

0:26:36	SPEAKER_01
 So, if they laugh between two words, you'd get it in between the two words.

0:26:41	SPEAKER_01
 But if they laugh across three or four words, you get it after those four words, does that matter?

0:26:46	SPEAKER_02
 Well, the thing that's hard to deal with is when they speak while laughing.

0:26:52	SPEAKER_02
 And I don't think that we can do very well with that.

0:26:57	SPEAKER_02
 But that's not as frequent as just laughing between speaking.

0:27:04	SPEAKER_06
 So, do you treat breath and laughter as phonetically or as word models or what?

0:27:11	SPEAKER_02
 We tried both. Currently, we use special words.

0:27:18	SPEAKER_02
 There's actually a word for, it's not just breathing, but all kinds of mouth stuff.

0:27:24	SPEAKER_02
 And then laughter is a special word.

0:27:28	SPEAKER_06
 How would we do that with the hybrid system?

0:27:32	SPEAKER_06
 So, train a phone in the neural net?

0:27:35	SPEAKER_02
 Yeah. Oh, and each of these words has a dedicated phone.

0:27:38	SPEAKER_02
 So, the mouth noise word has just a single phone.

0:27:44	SPEAKER_06
 Right, so, in the hybrid system, we could train the net with a laughter phone and a breath sound phone.

0:27:50	SPEAKER_04
 So, it's the same thing, right? I mean, you could say, well, we now think that laughter should have three states.

0:27:55	SPEAKER_04
 So, some units, three states, different states, and then you would have three.

0:28:00	SPEAKER_02
 Do whatever you want.

0:28:01	SPEAKER_02
 The pronunciation, the pronunciations are somewhat non-standard. They actually are. It's just a single phone in the pronunciation, but it has a self-loop on it.

0:28:12	SPEAKER_02
 So, it can...

0:28:13	SPEAKER_02
 To go on forever.

0:28:14	SPEAKER_06
 And how do you handle it in the language model?

0:28:17	SPEAKER_02
 It's just a word.

0:28:18	SPEAKER_02
 It's just a word in the language of the other word.

0:28:21	SPEAKER_02
 Yeah.

0:28:22	SPEAKER_02
 We also tried absorbing these both laughter and actually also noise.

0:28:31	SPEAKER_01
 So, it's...

0:28:33	SPEAKER_01
 Yes. Okay.

0:28:37	SPEAKER_02
 Anyway, we also tried absorbing that into the pause model. I mean, the model that matches the stuff between words.

0:28:44	SPEAKER_02
 And it didn't work as well.

0:28:47	SPEAKER_05
 Yeah. Okay.

0:28:49	SPEAKER_06
 Can you hand me your digit form?

0:28:52	SPEAKER_06
 Sure.

0:28:53	SPEAKER_06
 It's on a mark that you did not read digits.

0:28:56	SPEAKER_04
 Say hi for me.

0:28:58	None
 Yeah.

0:29:01	SPEAKER_01
 You didn't get me to thinking that. I'm not really sure which is more frequent, whether laughing.

0:29:07	SPEAKER_01
 I think maybe an individual thinks that people are more prone to laughing when they're speaking.

0:29:11	SPEAKER_01
 Yeah. I think...

0:29:12	SPEAKER_06
 I was noticing that with Dan and the one that we...

0:29:15	SPEAKER_06
 And Dan and the one that we're doing, we're not claiming to be getting the representation of mankind in these recordings.

0:29:22	SPEAKER_04
 We have very, very tiny sample of speech researchers.

0:29:33	SPEAKER_06
 Yeah.

0:29:34	SPEAKER_04
 It was really nice.

0:29:36	SPEAKER_04
 So...

0:29:39	SPEAKER_04
 Why we just... since we're on this main line, we just continue with what you're going to see about the transcriptions.

0:29:46	SPEAKER_01
 Okay. I'm really very...

0:29:48	SPEAKER_01
 I'm extremely fortunate with people who apply new...

0:29:51	SPEAKER_01
 Are transcribing for us. They are really perceptive.

0:29:56	SPEAKER_01
 And I'm not just saying that...

0:29:59	SPEAKER_07
 Because they're going to be transcribing it. That's the...

0:30:02	SPEAKER_07
 No, they're super.

0:30:03	SPEAKER_07
 Okay, turn the mic off and let's talk.

0:30:05	SPEAKER_01
 I know. I'm serious. They're just super.

0:30:09	SPEAKER_01
 So I brought them in and trained them in pairs because I think people can raise questions.

0:30:14	SPEAKER_01
 That's a good idea.

0:30:15	SPEAKER_01
 They think about different things and they think of different.

0:30:18	SPEAKER_01
 And I trained them on about a minute or two of the one that was already transcribed.

0:30:24	SPEAKER_01
 This also gives me a sense of...

0:30:27	SPEAKER_01
 I can use that later with ribbons to intercut your liability kind of issues.

0:30:31	SPEAKER_01
 And the main thing was to get them used to the conventions and the idea of the...

0:30:36	SPEAKER_01
 The size of the unit versus how long it takes to play it back.

0:30:39	SPEAKER_01
 So it's sort of calibration issues.

0:30:42	SPEAKER_01
 And then just set them loose.

0:30:44	SPEAKER_01
 And they're...

0:30:46	SPEAKER_01
 They all have already background and using computers.

0:30:49	SPEAKER_01
 They're trained in linguistics.

0:30:52	SPEAKER_01
 Oh, no, is that good or bad?

0:30:54	SPEAKER_01
 So one of them said, well, you know, he really said, not really.

0:30:58	SPEAKER_01
 And so what should I do with that?

0:31:01	SPEAKER_01
 And I said, oh, for our purposes, I do have a convention if it's a non-canonical...

0:31:05	SPEAKER_01
 That one, I think, you know, with Eric's work, I sort of figure we can just treat that as a variant.

0:31:10	SPEAKER_01
 But I told them if there's an obvious speech here, like I said in one thing and I gave my example.

0:31:16	SPEAKER_01
 I said, my phone instead of my phone, didn't bother...

0:31:21	SPEAKER_01
 I knew when I said it. I remember thinking, oh, that's not correctly pronounced.

0:31:25	SPEAKER_01
 But I thought it's not worth fixing because often when you're speaking, everybody knows what you mean.

0:31:31	SPEAKER_01
 But I have a convention that if it's obviously a non-canonical pronunciation, a speech error within the realm of resolution that you can tell in this American-English speaker, you know that I didn't mean to say microphone.

0:31:43	SPEAKER_01
 Then you put a little tick at the beginning of the word and that just signals that this is not standard and then curly-brack is a prong error.

0:31:51	SPEAKER_01
 And other than that, it's a word level. But when you know the fact that they noticed, you said, not end.

0:31:59	SPEAKER_01
 What shall I do with that? I mean, they're very receptive. And several of them are trained in IDEA.

0:32:03	SPEAKER_01
 They really could do phonetic transcription if we wanted to.

0:32:07	SPEAKER_04
 Right. Well, where were they when they wanted to do with some small subset of the whole thing?

0:32:14	SPEAKER_04
 I certainly would want to do everything.

0:32:16	SPEAKER_01
 I'm also thinking these people are terrific. Cool. I mean, so I told them that we don't know if this will continue past the end of the month.

0:32:24	SPEAKER_01
 And I also think they know that the data source is limited and I may not be able to keep them employed till the end of the month, even though I hope to.

0:32:33	SPEAKER_04
 The other thing we could do actually is use them for a more detailed analysis of the overall.

0:32:42	SPEAKER_06
 That would be so super. I mean, this is something that we were talking about. We could get a very detailed overlap if they were willing to transcribe each meeting four or five times.

0:32:51	SPEAKER_06
 Right one for each participant. So they could by hand.

0:32:55	SPEAKER_04
 Well, that's one way to do that. But I would say the other thing is just go through for the overlaps.

0:32:59	None
 Yeah.

0:32:59	SPEAKER_04
 So instead of doing phonetic transcription for the whole thing, which we know from the Steve's experience with the source transcription is very, very, very, tank consuming. And it took them, I don't know how many months to get four hours.

0:33:15	SPEAKER_04
 So that hasn't been really our focus. We can consider it. But I mean, the other thing is this is something so much time thinking about overlaps is maybe get much more detailed analysis of the overlaps.

0:33:28	SPEAKER_04
 But anyway, I'm open to our consideration. I don't want to say that I feel I'm open to a consideration of what are some other kinds of detailed analysis that would be most useful.

0:33:38	SPEAKER_04
 And I think this year we actually do it. It says we have due to variations in funding. We seem to be doing very well on money for this this year next year with any have much less.

0:33:57	SPEAKER_04
 You mean 2001 calendar year or I mean calendar year 2001. Yeah, so it's it's we don't want to hire a bunch of people a lot of time staff because the funding that we've gotten is sort of a big chunk for this year.

0:34:11	SPEAKER_04
 But having temporary people doing some specific thing is actually perfect.

0:34:16	SPEAKER_01
 Wonderful. And then school start in the 60 on the 16th. Some of them will have to cut back their hours.

0:34:21	SPEAKER_01
 Yeah, they were people time now or some of them are. Wow.

0:34:25	SPEAKER_01
 Well, I wouldn't say 40 hour weeks. No, but what I mean is I shouldn't say that way because it does sound like 40 hour weeks. No, I would say they're probably they don't have they don't have other things that are taking away.

0:34:35	SPEAKER_01
 I don't see how someone to do 40 hours a week on transcription. No, you're right. It's it's too taxing but they're putting in a lot.

0:34:43	SPEAKER_01
 And I checked them over. I haven't checked them all but just spot checking. They're fantastic. I think it would be transcribing.

0:34:49	SPEAKER_04
 I mean, for Ron Tay volunteer to do some of that. The first first stuff he did was transcribing Chuck.

0:34:59	SPEAKER_04
 And he said, you know, I was thought Chuck spoke really well.

0:35:03	SPEAKER_01
 Well, you know, I also thought Liz has this, you know, and I do also this, this interest in the types of overlaps that are involved.

0:35:13	SPEAKER_06
 So would be great choices for doing coding of that type if we wanted or whatever. So I think it would also be interesting to have a couple of the meetings have more than one transcriber do because I'm curious about interanotator agreement.

0:35:29	SPEAKER_01
 Yeah, I think that's a good idea. And there's also in my mind, I think on Andreas was leading to the topic the idea that we haven't yet seen the type of transcript that we get from IBM.

0:35:45	SPEAKER_01
 And it may just be, you know, pristine, but on the other hand, given the lesser interface, because this is, you know, we've got a good interface. We've got great headphones.

0:35:55	SPEAKER_04
 It could be that they will, there's one that being a kind of first pass or something like that.

0:36:00	SPEAKER_04
 Maybe a lot of them because again, they probably are going to do these in one which will also be.

0:36:05	SPEAKER_01
 That's true. Although you have to, don't you have to start with the close enough approximation of the verbal part.

0:36:12	SPEAKER_04
 Well, that's, that's available, right? I mean, so the argument is that if your statistical system is good, it will in fact clean things up.

0:36:22	SPEAKER_04
 So it's got its own objective criterion. And so in principle, you could start up with something that was kind of raw.

0:36:30	SPEAKER_04
 I mean, give an example of something we used to do at one point back with, like this area, in the other times as we would take, take a word and have a canonical pronunciation.

0:36:44	SPEAKER_04
 And if there's five phones in a word, break up the word into five equal length pieces, which is completely rough.

0:36:53	SPEAKER_04
 Yeah.

0:36:54	SPEAKER_04
 I mean, the timing is off all over the place, just about any word.

0:36:57	SPEAKER_04
 It's okay.

0:36:58	SPEAKER_04
 But it's okay. You start off with that and the statistical system then lines things. Eventually, you get something that doesn't really look to bad.

0:37:04	SPEAKER_04
 Oh, excellent.

0:37:05	SPEAKER_04
 So, so I think using a good aligner actually can help a lot.

0:37:11	SPEAKER_04
 But, you know, they both help each other. If you have a better starting point than it helps the aligner, if you have a good aligner, it helps the human taking less time to graduate.

0:37:24	SPEAKER_04
 So, excellent.

0:37:26	SPEAKER_01
 I guess there's another aspect too. And I don't know. This is very possibly different topic.

0:37:32	SPEAKER_01
 But, just let me say, with reference to this idea of higher order organization within meetings. So, like, you know, the topics that are covered during a meeting with reference to the other uses of the data.

0:37:45	SPEAKER_01
 So, being able to find where so and so talked about such and such.

0:37:50	SPEAKER_01
 Then, I mean, I did sort of a rough pass on encoding, like, episode, like, level things on the transcribed meeting.

0:38:04	SPEAKER_01
 Already transcribed meeting.

0:38:06	SPEAKER_01
 I don't know if, or if that's something that we want to do with each meeting, sort of like a manifest when you get a box full of stuff. Or if that's, I mean, I don't know what level of detail would be most useful.

0:38:24	SPEAKER_01
 I don't know if that's something that I should do when I look over it. Or if we want someone else to do, or whatever, at this issue of the contents of the meeting in an outline form.

0:38:35	SPEAKER_04
 Meaning, really, my thing.

0:38:44	SPEAKER_06
 I think just whoever is interested can do that. So, someone wants to use that data.

0:38:51	SPEAKER_04
 I think it's a little short here. We've been trying to finish. Well, you know, the thing that sort of I wanted to do these digits and haven't heard from the beginning.

0:39:02	SPEAKER_06
 We could skip the digits. We don't have to read digits each time.

0:39:09	SPEAKER_04
 I think, you know, another, another, what you did, it's more than that is good.

0:39:14	SPEAKER_04
 So, I'd like to do that. I think you maybe, did you prepare some all thing you wanted to see? Or is prepared?

0:39:22	SPEAKER_08
 Yeah. How long?

0:39:25	SPEAKER_08
 I think it's fast because I have the results of the study of the training energy without lowering.

0:39:36	SPEAKER_08
 I'm just trying to do the medium, the average, dividing by the variance.

0:39:45	SPEAKER_08
 The last meeting, I don't know if you remain, we have problems with the parameters, with representations, the parameters.

0:39:57	SPEAKER_08
 Because the values of the peaks in the signal look like that's a follow to the energy in the signal.

0:40:06	SPEAKER_08
 And it was probably not with the scale. With what?

0:40:10	SPEAKER_08
 Scale.

0:40:12	SPEAKER_08
 And I change the score and we can see the variance.

0:40:18	SPEAKER_04
 But the bottom line is still not separating out.

0:40:21	SPEAKER_04
 That's an option.

0:40:27	SPEAKER_04
 There's no point in going through all that.

0:40:30	SPEAKER_04
 I think we have to start.

0:40:33	SPEAKER_04
 There's two suggestions really, which is what we said before, is that it looks like, at least you have probably the obvious way to normalize so that the energy is anything like a reliable indicator of the overlap.

0:40:50	SPEAKER_04
 I'm still low. I think that's a low funny.

0:40:53	SPEAKER_04
 It seems like it should be.

0:40:56	SPEAKER_04
 But you don't want to keep knocking at it if you're not getting any result of that.

0:41:01	SPEAKER_04
 But I mean the other things that we talked about is pitch related things and hominicity related things, which also should be some kind of reasonable indicator.

0:41:15	SPEAKER_04
 But a completely different tack on it is the one that was suggested by your colleagues in Spain, which is to say, don't worry so much about the features.

0:41:27	SPEAKER_04
 That is to say use, as you're doing with speech, non-speech use of very general features.

0:41:33	SPEAKER_04
 And then look at it more from the aspect of models.

0:41:38	SPEAKER_04
 Have a couple of mark-up models.

0:41:41	SPEAKER_04
 And try to determine when is the layer you're in an overlap when you're not in overlap.

0:41:47	SPEAKER_04
 And let the statistical system determine what's the right way to look at the data.

0:41:52	SPEAKER_04
 I think it would be interesting to find individual features put them together.

0:41:58	SPEAKER_04
 I think that you'd end up with a better system overall, but given the limitation in time, given the fact that heavier system already exists, doing this sort of thing.

0:42:07	SPEAKER_04
 But its main limitation is that again, it's only working at silences, which maybe that's a better place to go.

0:42:18	SPEAKER_08
 I think that the possibility can be that fellow working with a new class, not only non-speech and speech, that in the speech class, dividing speech from speaker and overlapping to do a fast experiment to prove that this general feature can solve the problem.

0:42:51	SPEAKER_08
 How far is...

0:42:53	SPEAKER_08
 I have prepared the pitch tracker now.

0:42:57	SPEAKER_08
 I hope the next week we have some results.

0:43:03	SPEAKER_08
 We will see the parameter of pitch tracking with the problem.

0:43:09	SPEAKER_04
 Have you ever looked at the heavier speech segmenter?

0:43:15	SPEAKER_04
 No.

0:43:16	SPEAKER_04
 Maybe you could show...

0:43:18	SPEAKER_04
 Yeah, sure.

0:43:20	SPEAKER_04
 The limitation there again was that he was only using it to look at silences as a punitive split point between speakers.

0:43:30	SPEAKER_04
 But if you included broad classes, then principle maybe you could cover the overlap cases.

0:43:37	SPEAKER_03
 But I'm not too sure if we can really represent overlap with the detector I use, up to now, the speech.

0:43:47	SPEAKER_03
 I think that's right.

0:43:50	SPEAKER_06
 I think that the heavier speech might be able to.

0:43:52	SPEAKER_06
 It doesn't have the same HMM modeling, which is a drawback.

0:43:58	SPEAKER_04
 It's just a Gaussian for each.

0:44:07	SPEAKER_06
 Yeah.

0:44:08	SPEAKER_06
 And then you choose optimal splitting.

0:44:13	SPEAKER_04
 What does it have? Does it have any temporal...

0:44:16	SPEAKER_06
 Maybe I'm misremembering, but I did not think it had a mark-off.

0:44:21	SPEAKER_04
 I guess I don't remember either.

0:44:24	SPEAKER_04
 It's been a while.

0:44:26	SPEAKER_08
 Yeah.

0:44:27	SPEAKER_08
 I could have looked at it.

0:44:28	SPEAKER_08
 You can see the same word with the mark-off.

0:44:31	SPEAKER_08
 Yeah, I didn't think so.

0:44:32	SPEAKER_04
 He just computes a Gaussian over a Gaussian.

0:44:36	SPEAKER_06
 And so I think it would work fine for detecting overlap.

0:44:43	SPEAKER_06
 What he does is, as a first pass, he does a guess at where the divisions might be.

0:44:50	SPEAKER_06
 And he overestimates.

0:44:51	SPEAKER_06
 And that's just a data reduction step so that you're not trying it every time interval.

0:44:56	SPEAKER_06
 And so those are the punitive places where he tries.

0:44:59	SPEAKER_06
 And right now he's doing that with silence.

0:45:01	SPEAKER_06
 And that doesn't work with the meeting recorder.

0:45:04	SPEAKER_06
 So if we use another method to get a first pass, I think it would probably work.

0:45:09	SPEAKER_06
 It's a good method.

0:45:10	SPEAKER_06
 As long as the segments are long enough.

0:45:12	SPEAKER_06
 That's the other problem.

0:45:13	SPEAKER_04
 Okay, so let me go back to what you had.

0:45:16	SPEAKER_04
 The other thing one to do is...

0:45:19	SPEAKER_04
 So you have two categories.

0:45:22	SPEAKER_04
 And the mark-off model is reached.

0:45:24	SPEAKER_04
 Good, you have a third category?

0:45:27	SPEAKER_04
 So you have non-speech, single-person speech, and multiple-person speech?

0:45:32	SPEAKER_04
 He has a non-speech board, actually.

0:45:34	SPEAKER_01
 Don't you have any other categories on the board?

0:45:37	SPEAKER_04
 And you have a mark-off model for each?

0:45:41	SPEAKER_03
 I'm not sure.

0:45:43	SPEAKER_03
 I'm not about adding another class to it.

0:45:46	SPEAKER_03
 It's not too easy, I think, the transition between a different class, in the system I have now.

0:45:53	SPEAKER_03
 But it could be possible, I think, in principle.

0:45:58	SPEAKER_03
 Yeah, I mean, this is all pretty gross.

0:46:00	SPEAKER_04
 I mean, the reason why I was suggesting originally to look at the features is because I thought, well, we're doing something we haven't done before.

0:46:07	SPEAKER_04
 We should at least look at the space, understand.

0:46:10	SPEAKER_04
 It seems like if two people, two or more people talk at once, should get louder.

0:46:14	SPEAKER_04
 Yeah.

0:46:15	SPEAKER_04
 And there should be some discontinuity and pitch contours.

0:46:20	SPEAKER_04
 And there should overall be a smaller proportion of the total energy that is explained by any particular harmonic sequence in the spectrum.

0:46:32	SPEAKER_04
 So those are all things that should be there.

0:46:34	SPEAKER_04
 So far, Jose has been, although I was told I should be calling you a paper, but anyway, has been exploring largely the energy issue.

0:46:52	SPEAKER_04
 As with a lot of things, it's not as simple as it sounds.

0:46:57	SPEAKER_04
 And there's a lot of energy, as it helps you see residual energy, is it delta of those things?

0:47:04	SPEAKER_04
 Obviously, just a simple number.

0:47:07	SPEAKER_04
 Absolute number isn't going to work, so it should be compared to watch.

0:47:11	SPEAKER_04
 There'd be a long window for the normalizing factor, a short window for what you're looking at, or how short should they be.

0:47:19	SPEAKER_04
 So that he's been playing around with a lot of these different things, and so far, at least has not come up with any combination that really gave you an indicator.

0:47:27	SPEAKER_04
 So I still have a hunch that it's in their someplace, but it may be given that you've limited time here, it just may not be the best thing to focus on some range.

0:47:38	SPEAKER_04
 So pitch related and harmonic related.

0:47:43	SPEAKER_04
 But it seems like if we just want to get something to work, that there's a suggestion of, they were suggesting going to mark up models, but in addition, there's an expansion of what have you idea of.

0:47:56	SPEAKER_04
 And one of those things, looking at the statistical component, even if the features that you give it are maybe not ideal for it, it's just a general filter bang or a faster one or something.

0:48:07	SPEAKER_04
 It's in there somewhere probably.

0:48:10	SPEAKER_08
 What did you just think about the possibility of using the Javier Sokhovur?

0:48:15	SPEAKER_08
 I mean, the bike criterion, to train the Gaussian using the mark by hand to train overlapping zone and SP zone.

0:48:32	SPEAKER_08
 I mean, I think that interesting experiment could be to prove that if we suppose that the first step, I mean the classified world world, classified from Javier or classified from Filo, what happened with the second step?

0:48:53	SPEAKER_08
 I mean, what happened with the clustering process using the Javier?

0:49:03	SPEAKER_08
 I mean, it's enough to operate or to distinguish between overlapping zone and SP zone.

0:49:14	SPEAKER_08
 If we develop and classify and the second step doesn't work, we have another problem.

0:49:26	SPEAKER_06
 I had tried doing it by hand at one point with a very short sample, and it worked pretty well, but I haven't worked with it a lot.

0:49:33	SPEAKER_06
 So I took a hand segmented sample and I added 10 times the amount of numbers at random, and it did pick out pretty good boundaries, but this was just very anecdotal sort of thing.

0:49:44	SPEAKER_08
 It's possible with my segmentation by Khan that we have information about the overlapping.

0:49:49	SPEAKER_06
 Right, so if we fed the hand segmentation to Javier's and it doesn't work, then we know something's wrong.

0:49:56	SPEAKER_08
 I think that's probably worth a while doing.

0:49:59	SPEAKER_08
 Do you know where our software is?

0:50:08	SPEAKER_06
 Do you use it?

0:50:10	SPEAKER_06
 I have as well, so if you need help, let me know.

0:50:17	SPEAKER_06
 Transcript 295129706030970801502.05884.

0:50:31	SPEAKER_06
 1 6 2 8 5 8 3 2 3 3 0 3 1 5 4 5 0 9 9 7 1 1 2 8 4 0 0 9 4 0 7 1 0 1 2 4 1 5 3 1 2 6 7 2

0:50:52	SPEAKER_04
 correct that 6 7 2 1 0 8 6. Transfer 2 8 7 1-2 8 9 0 3 3 8 4 4 6 5 2 5 8 0 6 7 8 0 0 1 4 0 1 8 1 3 1 1 6 2 5 3 4 6 8 1 3 4 5 0 6 0 1 7 1 1 2 8 3 3 6 0 8 0 9 6 5 0 7 3 8 0 8 6 9 1 2 2

0:51:35	SPEAKER_07
 transcript 2 8 3 1-2 8 5 0 1 8 6 7 0 6 5 2 3 4 0 6 1 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 1 3 2 0 8 4 5 5 0 6 7 8 0 7 8 9 0 0 5 1 3 3 5 0 0 5

0:52:14	SPEAKER_03
 OK, transcript 2 7 9 1-2 8 1 0 0 0 0 2 3 7 3 3 2 4 7 5 3 0 2 6 7 8 0 8 0 1 0 7 0 3 1 5 0 3 6 2 8 0 0 0 3 4 5 0 2 0 0 7 2 0 3 3 1 7 8 9 5 1 2 2 0 9 8 5 3 9 7 8

0:52:57	SPEAKER_08
 2 3 3 3 3 3 3 4 3 4 4 4 4 1 1 0 5 6 7 7 7 7 8 9 0 1 2 9 4 8 5 9 2 2 3 3 49567979 8350 02795108254

0:53:36	SPEAKER_01
 Transcript number 2771 2790 909 899 0150 1228 845 2500 3824 465608 819566 0502187 07029 1 2 3 0 0 5 3 6 3524 7764 862809

