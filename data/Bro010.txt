0:00:00	SPEAKER_04
 Okay, we're on.

0:00:07	SPEAKER_05
 Okay, what are we talking about today?

0:00:14	SPEAKER_01
 I don't know.

0:00:18	SPEAKER_01
 We have news from the conference talk.

0:00:23	SPEAKER_01
 This was program for yesterday.

0:00:30	SPEAKER_05
 Oh, I'm sorry.

0:00:33	SPEAKER_05
 Now I know what you're talking about.

0:00:34	SPEAKER_05
 No one told me anything.

0:00:38	SPEAKER_04
 This was the talk where they were supposed to try to decide.

0:00:41	SPEAKER_04
 To decide what to do.

0:00:44	SPEAKER_05
 That would have been a good thing to find out for this meeting.

0:00:47	SPEAKER_05
 I have no idea.

0:00:52	SPEAKER_05
 So, let's assume for right now we're just kind of plugging on ahead because even if they tell us that the worlds are different, we're still interested in doing what we're doing.

0:01:06	SPEAKER_05
 So what are you doing?

0:01:09	SPEAKER_01
 Well, we've a little bit worked on trying to see what were the bugs and the problems with the latencies.

0:01:23	SPEAKER_01
 We took the LDA filters and redesigned new filters using recursive filters.

0:01:33	SPEAKER_05
 So when you say we did something similar as that.

0:01:36	SPEAKER_05
 I'm sorry.

0:01:37	SPEAKER_01
 Who is doing that?

0:01:42	SPEAKER_01
 So we took the filters, the fear filters, and we designed our filters that have the same frequency response.

0:01:57	SPEAKER_01
 Well, similar.

0:01:59	SPEAKER_01
 But that have shorter delays.

0:02:03	SPEAKER_01
 So they had two filters, one for the low frequency bands and another for the high frequency bands.

0:02:08	SPEAKER_01
 So we redesigned two filters and the low frequency bands has 64 milliseconds of delay and the high frequency band filter has something like 11 milliseconds compared to the 200 milliseconds of the high-high filters.

0:02:25	SPEAKER_01
 But it's not yet tested.

0:02:27	SPEAKER_01
 So we have the filters but we still have to implement the routine that does recursive filtering.

0:02:33	SPEAKER_05
 You had a discussion with Sonia about this though?

0:02:35	SPEAKER_05
 No.

0:02:38	SPEAKER_05
 You should talk with them.

0:02:41	SPEAKER_05
 No, the whole problem happened before was coordination.

0:02:45	SPEAKER_05
 So you need to discuss with them what we're doing because they could be doing the same thing.

0:02:53	SPEAKER_01
 I don't know if that's what they were trying to do.

0:02:58	SPEAKER_01
 I'm trying to do something different like taking while using filter that takes only a past.

0:03:05	SPEAKER_01
 This is just a little bit different.

0:03:07	SPEAKER_01
 But I will send him an email and tell him exactly what we are doing.

0:03:12	SPEAKER_05
 Yeah.

0:03:15	SPEAKER_05
 We just have to be in contact more.

0:03:17	SPEAKER_05
 I think that the fact that we did that was, had that thing with the latency.

0:03:21	SPEAKER_05
 It was a decade of the fact that there wasn't enough communication.

0:03:25	SPEAKER_01
 Yeah.

0:03:30	SPEAKER_01
 Well, there is one remark about these filters that they don't have a linear phase.

0:03:39	SPEAKER_01
 So, but I don't know.

0:03:42	SPEAKER_01
 Rapid.

0:03:43	SPEAKER_01
 Rapid doesn't hurt because the phase is almost linear.

0:03:49	SPEAKER_01
 And so, yeah, for the delay, I give you air.

0:03:56	SPEAKER_01
 It's a computing done.

0:03:58	SPEAKER_01
 The five-hertz modulation frequency, which is the most important for speech.

0:04:07	SPEAKER_01
 This is the first thing.

0:04:10	SPEAKER_05
 So that would be a reduction of 136 milliseconds.

0:04:15	SPEAKER_05
 Which, what was the total we ended up with?

0:04:19	SPEAKER_01
 330.

0:04:21	SPEAKER_01
 So that would be within.

0:04:23	SPEAKER_01
 Yeah, but there are other points actually, which will perhaps add some more delay.

0:04:30	SPEAKER_01
 Is that some other stuff in the process where perhaps not very correct.

0:04:38	SPEAKER_01
 Like the down sampling, which has simply dropping frames.

0:04:44	SPEAKER_01
 So we will try also to add a nice down sampling having a filter.

0:04:49	SPEAKER_01
 That, that, well, a low pass filter at 25 hertz.

0:04:54	SPEAKER_01
 Because when we look at the LDA filters, well, they are basically low pass, but they leave a lot of what's above 25 hertz.

0:05:05	SPEAKER_01
 And so, yeah, this would be another filter, which would add 10 milliseconds again.

0:05:12	SPEAKER_01
 Yeah, and then the third thing is that basically the way online normalization was done is just using this recursion on the feature stream.

0:05:32	SPEAKER_01
 But this is a filter, so it has also a delay.

0:05:36	SPEAKER_01
 And when we look at this filter actually has a delay of 85 milliseconds.

0:05:42	SPEAKER_01
 So if we want to be very correct, so if we want to the estimation of the mean to be, well, the right estimation of the mean, we have to take 85 milliseconds in the future.

0:05:59	SPEAKER_05
 That's a little bit of a problem.

0:06:03	SPEAKER_01
 But, well, when we add up everything, it would be at 65 plus 10 plus for the down sampling plus 85 for the online normalization.

0:06:18	SPEAKER_01
 So it's, yeah, plus 80 for the neural net and PCA.

0:06:24	SPEAKER_01
 So it would be around 240.

0:06:27	SPEAKER_01
 What's the allowable?

0:06:33	SPEAKER_05
 250.

0:06:36	SPEAKER_05
 Which there's some discussion of.

0:06:39	SPEAKER_04
 What were they thinking of changing it to?

0:06:41	SPEAKER_05
 Well, the people who we had very low latency wanted to be low, very narrow latency bound.

0:06:50	SPEAKER_05
 And people have longer latency down.

0:06:53	SPEAKER_05
 So fortunately, we're the main ones with long latency.

0:06:57	SPEAKER_01
 But yeah, and basically the best proposed on that something like 30 or 40 milliseconds.

0:07:04	SPEAKER_05
 So they were basically, I mean, they were more or less trading computation for performance.

0:07:10	SPEAKER_05
 And we were trading latency for performance.

0:07:14	SPEAKER_05
 And they were dealing with noise explicitly and we weren't.

0:07:18	SPEAKER_05
 So I think of it as complimentary.

0:07:20	SPEAKER_05
 I think the best systems.

0:07:23	SPEAKER_05
 So everything that we did in a way was it was just adamantly insisting on going in with the brain damage system.

0:07:30	SPEAKER_05
 Which is something actually we've done a lot over the last 15 years.

0:07:34	SPEAKER_05
 We say, well, this is the way we do it.

0:07:38	SPEAKER_05
 And then we do it.

0:07:39	SPEAKER_05
 And then someone else does something that's straightforward.

0:07:43	SPEAKER_05
 So this was a test that largely had as with noise.

0:07:46	SPEAKER_05
 And we did absolutely nothing explicitly to handle the added noise.

0:07:51	SPEAKER_05
 We just trained up systems to be more discriminant.

0:07:56	SPEAKER_05
 And we did this Rastelike filtering, which just on the log domain, and it's attending to handle convolutional noise.

0:08:04	SPEAKER_05
 We actually did nothing about that.

0:08:07	SPEAKER_05
 So the spectral subtraction schemes, a couple of places, seemed to do a nice job.

0:08:17	SPEAKER_05
 And so we're talking about putting some of that in, still keeping some of our stuff.

0:08:21	SPEAKER_05
 I think you should be able to end up with a system that's better than both.

0:08:24	SPEAKER_05
 But clearly, the way that we're operating for this other stuff does involve some latency.

0:08:28	SPEAKER_05
 To get rid of most of that latency, you get down to 40 or 50 milliseconds.

0:08:32	SPEAKER_05
 We'd have to throw out most of what we're doing.

0:08:35	SPEAKER_05
 And I don't think there's any good reason for it.

0:08:38	SPEAKER_05
 And the application actually, I mean, you're speaking to a recognizer.

0:08:42	SPEAKER_05
 I'm a remote server.

0:08:44	SPEAKER_05
 And having a quarter second versus some processing to clean it up doesn't seem like it's that.

0:08:52	SPEAKER_05
 You could deal.

0:08:53	SPEAKER_05
 These aren't large vocabulary things, so the decoder should take a really long time.

0:08:57	SPEAKER_04
 And I don't think anybody's going to notice the difference between a quarter of a second of latency and 30 milliseconds of latency.

0:09:04	SPEAKER_05
 No.

0:09:05	SPEAKER_05
 What does- What was your experience when you were doing stuff with the surgical?

0:09:11	SPEAKER_05
 Microscopes and so forth.

0:09:15	SPEAKER_05
 How long was it from when somebody finished an utterance to when something started happening?

0:09:27	SPEAKER_04
 We had a silence detector, so we would look for the end of an utterance based on the silence detector.

0:09:35	SPEAKER_04
 And I can't remember now if the top of my head, how many frames of silence we had to detect before we would declare it to be the end of an utterance.

0:09:46	SPEAKER_04
 But it was, I would say it was probably around the order of 250 milliseconds.

0:09:51	SPEAKER_05
 Yeah.

0:09:52	SPEAKER_05
 And that's when you'd start doing things.

0:09:55	SPEAKER_04
 Yeah, we did the backtrace at that point to get the answer.

0:09:58	SPEAKER_04
 Of course, I didn't take too long at that point.

0:10:00	SPEAKER_04
 It was pretty quick.

0:10:01	SPEAKER_05
 So you had a quarter second delay before plus some little processing time.

0:10:10	SPEAKER_05
 And then the microscope would start moving or something.

0:10:14	SPEAKER_05
 Right.

0:10:15	SPEAKER_05
 And there's physical inertia there, so probably the motion itself.

0:10:19	SPEAKER_04
 And it felt to the users that it was instantaneous.

0:10:24	SPEAKER_04
 I mean, as fast as talking to a person, I don't think anybody ever complained about the delay.

0:10:30	SPEAKER_05
 So you would think as long as it's under half a second or something.

0:10:33	SPEAKER_04
 Yeah.

0:10:34	SPEAKER_04
 I don't remember the exact numbers, but something like that.

0:10:40	SPEAKER_04
 I don't think you can really tell a person, I don't think a person can tell the difference between a quarter of a second and a hundred milliseconds.

0:10:50	SPEAKER_04
 Yeah.

0:10:51	SPEAKER_04
 I'm not even sure if you can tell the difference between a quarter of a second and a half a second.

0:10:54	SPEAKER_04
 Yeah.

0:10:55	SPEAKER_04
 I mean, it just feels so quick.

0:10:57	SPEAKER_05
 Yeah, if you said, what's the shortest route to the opera?

0:11:06	SPEAKER_05
 And it took half a second to get back to you.

0:11:09	SPEAKER_05
 Yeah.

0:11:10	SPEAKER_05
 I mean, it might be too abrupt.

0:11:13	SPEAKER_05
 You might have to put it in.

0:11:15	SPEAKER_04
 It may feel different than talking to a person because when we talk to each other, we tend to step on each other's utterances.

0:11:21	SPEAKER_04
 Yeah.

0:11:27	SPEAKER_04
 So like if I'm asking you a question, you may start answering before I'm even done.

0:11:30	SPEAKER_04
 Right.

0:11:31	SPEAKER_05
 So it would probably feel different, but I don't think it would feel slow.

0:11:36	SPEAKER_05
 All right.

0:11:37	SPEAKER_05
 Not anyway.

0:11:38	SPEAKER_05
 I mean, I think we could cut, we know else we could cut down on the neural net time by, by, by, uh, playing around a little bit going more into the past or something like that.

0:11:45	SPEAKER_05
 We talked about that.

0:11:47	SPEAKER_04
 How far away do you think the neural net is going to relate and see from the neural net caused by how far ahead you're looking?

0:11:53	SPEAKER_05
 And there's also, well, there's neural net and there's also this multi-frame, uh, KLT.

0:12:02	SPEAKER_04
 Was it in the recurrent neural nets where they weren't looking ahead at all?

0:12:07	SPEAKER_05
 They weren't looking ahead much.

0:12:10	SPEAKER_04
 They looked ahead a little bit.

0:12:13	SPEAKER_05
 Yeah, I mean, you could do this with a recurrent net.

0:12:18	SPEAKER_05
 But you also could just, I mean, we have an experiment with this, but I imagine you could predict a label from more in the past than in the future.

0:12:35	SPEAKER_05
 We've done some stuff with that before.

0:12:37	SPEAKER_05
 I think it works okay.

0:12:39	SPEAKER_04
 We've always had, usually we use the symmetric windows, but yeah, but we played a little bit with asymmetric.

0:12:46	SPEAKER_05
 You can do it.

0:12:49	SPEAKER_05
 So, that's what you're busy with.

0:12:54	SPEAKER_05
 Oh, yeah.

0:12:57	SPEAKER_00
 Also, we were thinking to apply the spectral saturation from Ericsson and to change the contextual RKLT for LDA.

0:13:15	SPEAKER_04
 Change the way?

0:13:17	SPEAKER_00
 The contextual RKLT.

0:13:19	SPEAKER_04
 I'm missing that last word.

0:13:21	SPEAKER_00
 KLT.

0:13:22	SPEAKER_00
 KLT, oh, KLT.

0:13:24	SPEAKER_00
 I'm just using LDA, this community.

0:13:29	SPEAKER_01
 Yeah.

0:13:30	SPEAKER_00
 How do you mean?

0:13:35	SPEAKER_04
 What is the advantage of that?

0:13:38	SPEAKER_01
 Well, it's that, well, for a moment we have something that's discriminant and nonlinear.

0:13:43	SPEAKER_01
 And the other is linear, but it's not discriminant.

0:13:46	SPEAKER_01
 I thought, well, it's a linear transformation.

0:13:51	SPEAKER_05
 So, at least just to understand maybe what the difference was between how much you were getting from just putting the frames together and how much you're getting from the discriminative, what the nonlinear already does for you or doesn't do for you.

0:14:03	SPEAKER_05
 Just to understand it a little better, I guess.

0:14:07	SPEAKER_01
 What, yeah.

0:14:10	SPEAKER_01
 Actually, what we want to do, perhaps it's to replace 212, something that's discriminant, but linear.

0:14:18	SPEAKER_01
 And to see if it's fitting proof of work or the non-discriminating of transformation.

0:14:25	SPEAKER_01
 And if the new run net is better than this.

0:14:28	SPEAKER_05
 Yeah, well, that's why I'm honest.

0:14:30	SPEAKER_05
 To see whether, whether having a neural net really buys you anything.

0:14:33	SPEAKER_05
 I mean, it did look like it buys you something over just the KLT, but maybe it's just discrimination.

0:14:39	SPEAKER_05
 And maybe, yeah, maybe the nonlinear discrimination isn't necessary.

0:14:44	SPEAKER_05
 Good to know.

0:14:46	SPEAKER_05
 But the other part you're saying was the spectrosotraction.

0:14:49	SPEAKER_05
 So, just kind of, at what stage do you do that?

0:14:52	SPEAKER_05
 Do you, doing that?

0:14:55	SPEAKER_01
 So, it would be on the...

0:14:58	SPEAKER_01
 An undemanding.

0:15:00	SPEAKER_00
 Yeah.

0:15:01	SPEAKER_00
 Okay, so just do that on the phone before.

0:15:03	SPEAKER_00
 We were thinking to do before after the video.

0:15:06	SPEAKER_00
 Or we don't know exactly when we spent it before after the video.

0:15:13	SPEAKER_05
 So, you know that the way that there's one thing that would be no good to find out about from this conference call is that what they were talking about, what they're proposing doing, was having a third party run a good VAD and determine boundaries.

0:15:32	SPEAKER_05
 Yeah.

0:15:33	SPEAKER_05
 And then given those boundaries, then everybody do the recognition.

0:15:38	SPEAKER_05
 The reason for that was that if someone, one group put in the VAD and another didn't, or one had a better VAD than the other, since they're not viewing that as being part of the task and that any manufacturer would put a bunch of effort on having some kind of good speech-science detection still wouldn't be perfect.

0:16:03	SPEAKER_05
 But I mean, the argument was let's not have that be part of this task.

0:16:07	SPEAKER_05
 Let's separate that out.

0:16:10	SPEAKER_05
 And so, I guess they argued about that yesterday.

0:16:14	SPEAKER_05
 And I'm sorry, I don't know the answer, but we should find out.

0:16:17	SPEAKER_05
 I'm sure we'll find out soon what they decided.

0:16:23	SPEAKER_05
 So, yeah, so there's a question of the VAD, but otherwise it's on the Mel filter bank energy, I guess.

0:16:31	SPEAKER_05
 You're doing the thing.

0:16:33	SPEAKER_05
 And you're just attracting in the power domain, or magnitude domain, probably power domain.

0:16:42	SPEAKER_01
 I guess it's power domain.

0:16:44	SPEAKER_01
 I don't remember exactly.

0:16:46	SPEAKER_01
 So it's before everything else.

0:16:48	SPEAKER_05
 If you look at the theory, it should be in the power domain.

0:16:51	SPEAKER_05
 But I've seen implementations for people to do the magnitude domain.

0:16:55	SPEAKER_05
 They ask people why and they strike their shoulders and say, oh, it works.

0:17:04	SPEAKER_05
 And I guess this is mysterious.

0:17:06	SPEAKER_05
 I mean, people who do this a lot, I guess, have developed little tricks of the trade.

0:17:10	SPEAKER_05
 I mean, there's this, you don't just subtract the estimate of the noise spectrum, you subtract it at times or less.

0:17:18	SPEAKER_01
 Good.

0:17:20	SPEAKER_01
 Yeah.

0:17:21	SPEAKER_01
 And generally, this, so you have the estimation of the power spectrum of the noise.

0:17:28	SPEAKER_01
 You multiply this by a factor, which is dependent on the SNR.

0:17:32	SPEAKER_01
 So when the signal level is more important, compared to this noise level, the coefficient is small around one.

0:17:44	SPEAKER_01
 But when the signal level is a small compared to the noise level, the coefficient is more important.

0:17:55	SPEAKER_01
 And this reduce, actually, the musical noise, which is more important during silence portions.

0:18:01	SPEAKER_01
 The energy is small.

0:18:04	SPEAKER_01
 So there are tricks like this.

0:18:06	SPEAKER_01
 Yeah.

0:18:09	SPEAKER_04
 Is the estimate of the noise spectrum a running estimate?

0:18:16	SPEAKER_05
 Yeah.

0:18:18	SPEAKER_05
 Well, that's, I mean, that's what differs from different tasks and different spectral subtraction methods.

0:18:25	SPEAKER_05
 I mean, if you have a fair assurance that the noise is quite stationary, then the smartest thing to do is use as much data as possible to estimate the noise, get a much better estimate and subtract it off.

0:18:41	SPEAKER_05
 But if it's varying at all, which is going to be the case from almost any real situation, you have to do it online with some forgetting factors.

0:18:51	SPEAKER_04
 So do you, is there some long window that extends into the past over which you calculate the average?

0:18:59	SPEAKER_05
 Well, there's a lot of different ways of computing the noise spectrum.

0:19:02	SPEAKER_05
 So one of the things that Hans Gunter Hirsch did, and other people, actually, he's, isn't anyone, I guess, was to take some period of speech and in each band to develop a histogram.

0:19:20	SPEAKER_05
 So to get a decent histogram of these energies takes at least a few seconds, really.

0:19:26	SPEAKER_05
 But you can do it with a smaller amount, but it's pretty rough.

0:19:31	SPEAKER_05
 And in fact, I think the NIST standard method of determining signal noise ratio is based on this.

0:19:38	SPEAKER_05
 So a couple of seconds?

0:19:40	SPEAKER_05
 No, it's based on this kind of method, this histogram method.

0:19:43	SPEAKER_05
 So you have a histogram.

0:19:44	SPEAKER_05
 Now, if you have signal and you have noise, you basically have these two bumps in histogram.

0:19:50	SPEAKER_05
 She could approximate its two Gaussian's.

0:19:53	SPEAKER_04
 But don't they overlap sometimes?

0:19:55	SPEAKER_05
 Oh, yeah.

0:19:56	SPEAKER_05
 So you have a mixture of two Gaussian's.

0:19:58	SPEAKER_05
 Yeah.

0:19:59	SPEAKER_05
 Right, you can use EM to figure out what it is.

0:20:01	SPEAKER_05
 So basically now you have this mixture of two Gaussian's.

0:20:03	SPEAKER_05
 You know what they are, and I mean, sorry, you estimate what they are.

0:20:07	SPEAKER_05
 And so this gives you what the signal is and what the noise energy is in that band and the spectrum.

0:20:13	SPEAKER_05
 And then you look over the whole thing and then you have a noise spectrum.

0:20:16	SPEAKER_05
 So Hans-Gunther Hirsch and others have used that kind of method.

0:20:20	SPEAKER_05
 And the other thing to do is which is sort of more trivial and obvious is to determine through magical means that there's no speech in some period and see what the spectrum is.

0:20:33	SPEAKER_05
 And you know, that's strictly due, it has mistakes.

0:20:38	SPEAKER_05
 And if you've got enough time, this other method appears to be somewhat more reliable.

0:20:43	SPEAKER_05
 A variant on that for just determining signals and noise ratio is to just, you can do an iterative thing, yeah, I'm like thing to determine means only, I guess it is EM still, but just determine the means only.

0:21:06	SPEAKER_05
 Don't worry about the variances.

0:21:08	SPEAKER_04
 And then just use those mean values as being the noise ratio in that band.

0:21:14	SPEAKER_04
 Depending on where the window was that you used to calculate the signal and noise ratio.

0:21:22	SPEAKER_05
 Yeah, sure.

0:21:23	SPEAKER_05
 But, that's certainly because if you start looking to the future, right?

0:21:26	SPEAKER_04
 Okay, well that was my question.

0:21:27	SPEAKER_05
 Yeah, I mean, if you just, if you, at the beginning you have some gas.

0:21:33	SPEAKER_05
 Yes, some gas.

0:21:36	SPEAKER_05
 Yeah, interesting question. I wonder how they did.

0:21:38	SPEAKER_01
 Actually, it's if you want to have good estimation on non-stationary noise, you have to look in the future.

0:21:45	SPEAKER_01
 I mean, if you take your window and build your Instagram on this window, what you can expect is to have an estimation of the noise in the middle of the window.

0:21:55	SPEAKER_01
 Not at the end, so.

0:21:56	SPEAKER_01
 Well, yeah, but what does, what does our hotel do?

0:22:01	SPEAKER_01
 They just look in the past, I guess it works because the noise are almost stationary.

0:22:07	SPEAKER_05
 Well, the thing, I mean, you talk about non-stationary noise, but I think that sexual subtraction is rarely, is not going to work really well for non-stationary noise.

0:22:15	SPEAKER_01
 Well, if you have a good estimation of the noise, yeah, because, what do you, let's talk about it.

0:22:20	SPEAKER_01
 That's hard to do.

0:22:21	SPEAKER_05
 Yeah, that's hard to do.

0:22:22	SPEAKER_05
 So, I think that what is, what's more common is that you're going to be helped with slowly varying or stationary noise.

0:22:32	SPEAKER_05
 That's what spectral subtraction will help with, practically speaking.

0:22:35	SPEAKER_05
 If it varies a lot, to get a good estimate, you need a few seconds of speech, even if it's centered.

0:22:42	SPEAKER_05
 If you need a few seconds to get a decent estimate, but it's changed a lot in a few seconds, then, you know, it's kind of a problem.

0:22:50	SPEAKER_05
 I mean, imagine five hertz is the middle of the speech modulation spectrum, right?

0:22:55	SPEAKER_05
 So imagine a jackhammer growing in five hertz.

0:22:58	SPEAKER_05
 I mean, good luck, so.

0:23:00	SPEAKER_01
 So in this case, yeah, sure, you can.

0:23:05	SPEAKER_01
 But I think, uh, earth does experiment with windows of, like, between 500 milliseconds in one second.

0:23:14	SPEAKER_01
 And, well, 500 was not so bad.

0:23:18	SPEAKER_01
 And he worked on non-stationary noise.

0:23:21	SPEAKER_01
 He's like, noise modulated with, well, with, with, with, and between modulation, so things like that.

0:23:29	SPEAKER_04
 So, what is, uh, windows centered around that?

0:23:34	SPEAKER_01
 Yeah, well, I think, yeah, when the paper we show that, actually, the estimation of the noise is delayed while it's, there is, you have to center the window.

0:23:44	SPEAKER_05
 Yeah.

0:23:45	SPEAKER_05
 No, I understand it's better to do, but I just think that, uh, for real noises, what, what's most likely to happen is that there'll be some things that are relatively stationary where you can use one or another spectral subtraction thing.

0:23:57	SPEAKER_05
 And, uh, there's a lot of things where it's not so stationary.

0:24:00	SPEAKER_05
 I mean, you can always pick something that, that falls between your methods.

0:24:05	SPEAKER_05
 But I don't know if, you know, sinusoidally, amplitude modulated noise is sort of a big problem in practice, I think.

0:24:14	SPEAKER_04
 Yeah.

0:24:15	SPEAKER_04
 We could probably get a really good estimate of the noise if we just went to the noise files and built the averages from them.

0:24:22	SPEAKER_01
 What do you, he's actually, but if the noise is stationary, perhaps we don't even need some kind of noise estimation algorithm, just take the beginning of the utterance.

0:24:39	SPEAKER_01
 Oh, yeah, sure.

0:24:40	SPEAKER_01
 I know, I don't know if people try this for, well, everybody seems to use some kind of adaptive, but, but, but, but, it's game, but, you know, stationary useful.

0:24:50	SPEAKER_05
 Very useful. Very slow adaptation.

0:24:52	SPEAKER_05
 Right. The word stationary is a very precise statistical meaning, but, you know, in, in signal processing, really, what we're talking about, I think, is things that change slowly compared with our processing techniques.

0:25:04	SPEAKER_05
 So, if you're driving along in a car, I, I would think that most of the time the nature of the noise is going to change relatively slowly.

0:25:12	SPEAKER_05
 It's not going to stay absolutely the same if you, if you check it out five minutes later, you may be in a different part of the road or whatever, but it's, it's using the local characteristics.

0:25:25	SPEAKER_05
 Time is probably going to work pretty well, but you could get hurt a lot if you just took something from the beginning of all the speech of, you know, an hour speech and that lighter.

0:25:35	SPEAKER_05
 So, they may be, you know, maybe overly complicated for, for this test, but, but, but, I don't know.

0:25:45	SPEAKER_05
 But what you're saying, you know, makes sense though. I mean, if possible, you should, you should make it center of the center of the window, but we're already having problems with these delay.

0:25:59	SPEAKER_05
 Yeah, it's a bit of a waste without it.

0:26:08	SPEAKER_04
 If they're going to provide a voice activity detector that will tell you the boundaries of the speech, then couldn't you just go outside those boundaries and do your estimate there?

0:26:21	SPEAKER_04
 Oh yeah.

0:26:24	SPEAKER_05
 Yeah, so I imagine that's what they're doing, right?

0:26:30	SPEAKER_05
 They're probably looking in non-speech sections and getting some, yeah, they have some kind of threshold on the previous estimate.

0:26:38	SPEAKER_01
 So, yeah, I think, I think, Ericsson used this kind of threshold.

0:26:46	SPEAKER_01
 Yeah, so they have an estimate of the noise level and they put a threshold like 6 or 10 dB above.

0:26:54	SPEAKER_01
 What's under this threshold is used to update the estimate.

0:26:58	SPEAKER_01
 Isn't that right?

0:27:00	SPEAKER_00
 I think so.

0:27:02	SPEAKER_00
 So, it's, yeah.

0:27:03	SPEAKER_01
 It's like saying what's under the threshold is silence.

0:27:06	SPEAKER_05
 Does trans talk harm do the same thing?

0:27:10	SPEAKER_01
 I, you know, perhaps?

0:27:13	SPEAKER_00
 No, I have no idea what that is.

0:27:25	SPEAKER_05
 Okay, we're done with that.

0:27:29	SPEAKER_05
 Let's see.

0:27:31	SPEAKER_05
 Maybe we talked about a couple of other things, briefly, just things that we've been chatting about.

0:27:37	SPEAKER_05
 And these meetings yet, so you're coming up with your Quiles proposal.

0:27:43	SPEAKER_05
 And when I just give a two, three minute summary of what you're about to be doing.

0:27:51	SPEAKER_02
 Two, three, you can be sure of that.

0:27:55	SPEAKER_02
 Well, I've talked to somebody already, but I'm looking into extending the work done by Larry Saw and John Allen and Mazinari.

0:28:06	SPEAKER_02
 They have a system that's a multi-band system, but the multi-band is a little different than the way that we've been doing multi-band in the past, where we've been taking subband features and training up these new formats and on phonetic targets and then combining them somehow down the line.

0:28:31	SPEAKER_02
 They're taking subband features and training up a detector that detects for these phonetic features.

0:28:43	SPEAKER_02
 For example, it presents a detector to detect sonorous.

0:28:50	SPEAKER_02
 And so what it basically is, there's at the lowest level, it's an AND gate.

0:28:59	SPEAKER_02
 So on each subband, you have several independent tests to test whether there's the existence of sonorous in a subband.

0:29:09	SPEAKER_02
 And then it's combined by a soft AND gate.

0:29:13	SPEAKER_02
 And then at the higher level, for every, if the higher level, there's a soft or gate.

0:29:21	SPEAKER_02
 If this detector detects the presence of sonorous in any of the subband, then the or gate at the top says, okay, well, this frame has evidence of sonorous.

0:29:35	SPEAKER_04
 What are some of the low-level detectors that they use?

0:29:38	SPEAKER_02
 Well, the low-level detectors are logistic regressions.

0:29:44	SPEAKER_05
 And the one other way, basically, is one of the units in our neural networks.

0:29:51	SPEAKER_05
 That's all it is.

0:29:52	SPEAKER_05
 It's a sigmite with weighted sum of TMP.

0:29:56	SPEAKER_05
 Is it trained by gradient?

0:29:59	SPEAKER_02
 Yes.

0:30:00	SPEAKER_02
 So he uses an EM algorithm to train up these parameters for logistic regression?

0:30:07	SPEAKER_05
 Well, actually, so he's using EM to get the targets.

0:30:12	SPEAKER_05
 So you have this AND gate, calling an AND gate, but it's a product, product rule thing at the output.

0:30:19	SPEAKER_05
 And then he uses, and then feeding into that, or, I'm sorry, it's nor at the output, isn't it?

0:30:26	SPEAKER_05
 Yeah, that's the product.

0:30:27	SPEAKER_05
 And then he has each of these AND things.

0:30:34	SPEAKER_05
 So they're little neural units, and they have to have targets.

0:30:42	SPEAKER_05
 So the targets come from the EM.

0:30:45	SPEAKER_04
 And so are each of these low-level detectors?

0:30:52	SPEAKER_04
 Are these something that you decide ahead of time, like I'm going to look for this particular feature, and look at this frequency, or what are they looking at?

0:31:01	SPEAKER_04
 What are their input?

0:31:02	SPEAKER_02
 Right, so the, okay, so, for each subband, there are basically several measures of SNR and correlation.

0:31:12	SPEAKER_02
 Oh, okay.

0:31:13	SPEAKER_02
 And he said there's like 20 of these per subband.

0:31:19	SPEAKER_02
 And for every subband, you just take ahead of time, I'm going to have like five independent logistic tests.

0:31:29	SPEAKER_02
 And you initialize these parameters in some way, and use the EM to come up with your training targets for the low-level detectors.

0:31:41	SPEAKER_02
 And then once you get that done, you train the whole thing on maximum likelihood.

0:31:48	SPEAKER_02
 And he showed that using this method to detect sonarances is very robust compared to two typical full band Gaussian mixture estimations of sonnets.

0:32:05	SPEAKER_02
 And so that's just one detector.

0:32:09	SPEAKER_02
 And you can imagine building many of these detectors on different features, you get enough of these detectors together, then you have enough information to do higher-level discrimination, example discriminating between phones, and then you keep working your way up and telling you to build a full recognizer.

0:32:28	SPEAKER_02
 So that's the direction I'm thinking about going.

0:32:33	SPEAKER_02
 Cool.

0:32:34	SPEAKER_05
 It has a number of properties that I really like.

0:32:39	SPEAKER_05
 I mean, one is going towards using narrow band information for phonetic features of some sort rather immediately going for the typical sound units.

0:32:54	SPEAKER_05
 Another thing I like about it is that this thing is going to be explicitly trained for a product of errors rule, which is what Alan keep pointing out that Fletcher is observed in the 20s for people listening to narrow band stuff.

0:33:10	SPEAKER_05
 That's Friday's time.

0:33:12	SPEAKER_05
 And the third thing I like about it is, and we've played around with this in a different kind of way a little bit, but it hasn't been a dominant way of evaporating, I think.

0:33:27	SPEAKER_05
 This issue of where the targets come from.

0:33:30	SPEAKER_05
 I mean, in our case, when we've been training at multi-band things, the way we get the targets for the individual bands is that we get the phonetic label of the sound.

0:33:44	SPEAKER_05
 But this is saying, that's maybe what our ultimate goal is, or not ultimate, but an ultimate goal is getting these small sound units. But along the way, what should we be training these intermediate things for?

0:34:03	SPEAKER_05
 I mean, because we don't know that this is a particularly good feature. There's no way someone in the audience yesterday was asking, well, couldn't you have people go through a market for the individual bands and say where they're in the first place on earth?

0:34:17	SPEAKER_05
 But I think having a bunch of people listening to critical band-wide, I think it would be possible. It's all going to sound like sign waves to you more or less. Well, narrow band.

0:34:33	SPEAKER_05
 I think it's very hard for someone to make that determination. So we don't really know how those should be labeled. It could be that you should not be paying that much attention to certain bands for certain sounds, in order to get the results.

0:34:54	SPEAKER_05
 So what we have been doing there to sort of mixing it all together is certainly much, much cruder than that. We train these things up on the final label. Now we have, I guess, done experiments. You've probably done stuff where you have done separate the therapies on the different force alignment on the subband labels.

0:35:17	SPEAKER_05
 Yeah, you've done that. Did that help at all? It helps for one iteration, but anything after that, it doesn't help. So that may or may not be helpful because in the sense that's the same sort of thing. You're taking global information and determining how you should.

0:35:37	SPEAKER_05
 But this is, I think, a little more direct. How do they measure the performance of their detector? Well, he's just actually looking at the confusions between sound and non-sound.

0:35:50	SPEAKER_05
 So he hasn't applied it to recognition, or if he did, he didn't talk about it. And one of the concerns in the audience actually was that he did a comparison to an old foil that asked the old standard recognizing it with, well, no filter bank at the front and HM admins and so forth.

0:36:13	SPEAKER_05
 And it didn't do nearly as well, especially in noise. But the good questions in the audience was, well, yeah, but that was great for that. I mean, the use of a very smooth, a spectral envelope is something that has evolved generally a good thing for speech recognition.

0:36:33	SPEAKER_05
 But if you knew that what you were going to do is detect sounder, it's not. So sounder, it's almost like voice down voice to accept, I guess, that the voice stops are also called up joints.

0:36:46	SPEAKER_05
 So it's, it's, but with the exception of the stops, I guess it's pretty much the same as voice down voice, right? So, so if you knew you were doing that, if you were doing something safe for a vocoder, you wouldn't use the same kind of features. You would use something that was sensitive to the periodicity and not just the envelope. And so in that sense, it was an unfair test.

0:37:12	SPEAKER_05
 So I think the question was right. It was, in that sense, an unfair test. Nonetheless, it was one that was interesting because this is what we are actually using for speech recognition, these sort of envelopes.

0:37:24	SPEAKER_05
 This says that perhaps even trying to use them the best way we can that we ordinarily do with Gaussian mixers and, did you mention so forth, you don't actually do that well on determining whether something is sounder or not, which means you're going to make errors between similar sounds that are sounder or instrument.

0:37:43	SPEAKER_04
 Didn't they also do some kind of an oracle experiment where they said if we could detect sounder and perfectly and then show how it would improve speech recognition, I thought I remember hearing about an experiment like that.

0:37:58	SPEAKER_05
 These same people? I don't remember that. That's, you're right. That's exactly the question to follow up this discussion. I suppose you did that. Got that right.

0:38:11	SPEAKER_01
 Yeah. What could be the other low level detectors and for other kind of features in addition to detecting sounder and sounder?

0:38:24	SPEAKER_02
 That's what you want to go for. So, what? Oh, build other other, other, other different different features.

0:38:35	SPEAKER_02
 Let's see. Yeah, I don't know. Easiest thing would be to go do some voicing stuff, but that's very similar to sounder.

0:38:55	SPEAKER_04
 When we talked with John O'Halla the other day, we made a list of some of the things like forcation, abrupt closure, our coloring, naizality, voicing.

0:39:08	SPEAKER_05
 Yeah, so is it half doesn't like that better? Now this was coming at it from a different angle, but maybe it's a good way to start with these things, which John felt that a human annotator would be able to reliably mark.

0:39:22	SPEAKER_05
 So the sort of things he felt would be difficult for a human annotator to reliably mark would be tongue position.

0:39:29	SPEAKER_04
 Please, thanks. Yeah. Yeah. There's also things like stress.

0:39:35	SPEAKER_05
 But stress doesn't fit in this thing of coming up with features that will distinguish words from one another.

0:39:43	SPEAKER_05
 It's a good thing to mark and will probably help us ultimately.

0:39:46	SPEAKER_04
 Yeah, there's a few cases where it can like permit, permit, but that's not very common in English. In other languages, it's more important.

0:39:57	SPEAKER_05
 Well, yeah, but either case you'd write P, R, I, T, you get the word right.

0:40:02	SPEAKER_04
 No, I'm saying, I thought you were saying that stress doesn't help you distinguish between words.

0:40:08	SPEAKER_04
 Oh, I see what you're saying. As long as you get the secret. We're talking about transcriptionists.

0:40:13	SPEAKER_04
 Yeah, yeah, yeah. Yeah. Yeah. Right. So where it could help is maybe at a higher level. Right. Yeah. Understandings. Yeah. Yeah. Yeah.

0:40:20	SPEAKER_05
 Exactly. But that's this afternoon's meeting. Yeah. That's what I understand. That's what I'm saying.

0:40:27	SPEAKER_05
 Yeah. So that's, yeah, that's, you know, the neat thing.

0:40:34	SPEAKER_02
 So, oh, how that's going to help you with these transcription of the leading data?

0:40:41	SPEAKER_04
 Well, I don't know. We, we sort of didn't get that far. We just talked about some possible features that could be marked by humans.

0:40:52	SPEAKER_04
 And because of having maybe some extra transcribe or time, we thought we could go through and mark some portion of the data for that.

0:41:00	SPEAKER_05
 Yeah. That's not an immediate problem. We have a lot of extra transcribe.

0:41:05	SPEAKER_05
 Right. But I'm long term, I guess, Chuck is going to continue to dialogue with John.

0:41:12	SPEAKER_04
 Well, we'll end up doing some. I'm definitely interested in this area too.

0:41:17	SPEAKER_04
 A acoustic feature. Okay. Stuff.

0:41:20	SPEAKER_05
 Yeah. I think it's an interesting, interesting way to go.

0:41:24	SPEAKER_05
 I said, it's an amazing thing. There's never good things.

0:41:29	SPEAKER_05
 So, we're going to talk maybe two, three minutes about what we've been talking about, right?

0:41:36	SPEAKER_03
 Yeah. Okay. So, we're interested in methods for far-makes, speech recognition, mainly methods to deal with the reverberation in the far-makes signals.

0:41:48	SPEAKER_03
 So, one approach would be, say, MSG and PLP, like was used in Aurora 1, and there are other approaches which actually attempt to remove the reverberation instead of being robust to it like MSG.

0:42:04	SPEAKER_03
 And so, we're interested in comparing the performance of a robust approach like MSG with these speech enhancement or do reverberation approaches.

0:42:17	SPEAKER_03
 And it looks like we're going to use the meeting recorded digits that after that.

0:42:24	SPEAKER_01
 And the dear reverberation algorithm. Can you give some more details on this? Use one microphone.

0:42:31	SPEAKER_03
 Several microphones. Okay. Well, there was something that was done by a guy named Carla, so I forget his last name. He worked with Hinat, who Evan Donnell. It was like Rasta in the sense of it was deconvolution by filtering, except he used a longer time window like a second maybe. And the reason for that is Rasta's time window is too short to include the whole reverberation.

0:43:02	SPEAKER_03
 I don't know what you call the reverberation response. If you see what I mean. The reverberation filter for my mouth to that mic is like, it's too long in the time domain for the Rasta filtering to take care of it.

0:43:18	SPEAKER_03
 And then there are a couple of other speech enhancement approaches which haven't been tried for speech recognition yet, but have just been tried for enhancement, which have the assumption that you can do LPC analysis of the signal you get at the far microphone.

0:43:37	SPEAKER_03
 And the all-pull filter that you get out of that should be good. It's just the excitation signal that is going to be distorted by the reverberation.

0:43:49	SPEAKER_03
 And so you can try and reconstruct a better excitation signal and feed that through the all-pull filter and get enhanced speech with reverberation produced.

0:44:04	SPEAKER_05
 There's also this echo cancellation stuff that's sort of chasing. So when we're seeing these digits now, we do have a close microphone signal. And then there's the distant microphone signal.

0:44:20	SPEAKER_05
 And you could, as a kind of baseline, say, okay, given that we have both of these, we should be able to do a cancellation so that we essentially identify the system in between the linear time and the varying system between the two microphones and re-cancel it out to some reasonable approximation.

0:44:45	SPEAKER_05
 That's not a practical thing. If you don't have a close mic, we thought that by making a good baseline. It still won't be perfect because there's no noise.

0:45:00	SPEAKER_05
 But there are single microphone methods that I think people have done for this kind of due reverberation. Do you know any references to any? Because I was, I literally thought that I guess when people are working with single microphones, they are more trying to do, well, not very,

0:45:29	SPEAKER_01
 well, there is the event and work, but also trying to find the deconvolution filter, but not in the time domain, but in the stream of features. But there's someone working on this in mons, so perhaps, yeah, we should try to, it's working on this on trying to, on reverberation.

0:46:03	SPEAKER_05
 The first paper on this is going to have great references. I can tell you what I mean. I always go to have references, especially when reviewers read it or one of the authors, and you're okay, you cited me.

0:46:21	SPEAKER_01
 Well, it did deconvolution, and it did some fancier things like training different network and different reverberation conditions, and then trying to find the best one.

0:46:36	SPEAKER_05
 The other thing that Dave was talking about earlier was multiple mic things, where they're all distant. So, I mean, there's all this work in the race, but the other thing is, what can we do that's clever, that can take some advantage of only two mics, particularly if there's an obstruction between them.

0:46:57	SPEAKER_05
 It creates a shadow, which is helpful, it's part of why you have such good direction. We've two ears, even though they're not several feet apart, for most people, since.

0:47:12	SPEAKER_05
 That could help though. That's what it's for. It's basically heads for, separate ears.

0:47:30	SPEAKER_04
 Okay, I think that's all we have this week, and I think it's digit time. Actually, for some reason, the digit forms are blank. I think that maybe due to the fact that Adam ran out of digits, and didn't have time to regenerate it.

0:47:48	SPEAKER_04
 Well, this is no real reason to write our names on here. If you want to put your credit card numbers, or do we need the names for the other stuff? Yeah, I do need your names and the time and all that, because we put that into the key files.

0:48:03	SPEAKER_05
 Okay, that's why we have the forms. Okay, yeah, I didn't notice this. I was sitting here. I was in a box reading.

0:48:12	SPEAKER_05
 So, I guess we're done. I'll do my credit card later.

