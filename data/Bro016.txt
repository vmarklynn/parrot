0:00:00	SPEAKER_04
 Let's see. Test? Test? Yeah? Okay. Channel one. Hello.

0:00:09	SPEAKER_00
 Test.

0:00:12	SPEAKER_04
 I was saying he can be here next week, once day through Friday, through, I'm sorry, and I won't be here Thursday and Friday, but my suggestion is that, at least for this meeting, people should go ahead. I've seen him go be here.

0:00:27	SPEAKER_04
 You know, we don't have any check accent. Yeah, that's...

0:00:30	SPEAKER_04
 As far as I know, so. Okay.

0:00:33	SPEAKER_04
 There we go.

0:00:37	SPEAKER_04
 So other than reading digits, what's our agenda?

0:00:48	SPEAKER_06
 I don't really have anything new than working on meeting recorder stuff.

0:00:55	SPEAKER_04
 Okay. Do you think that would be the case for next week, also, or is...

0:01:03	SPEAKER_04
 what's your projection on?

0:01:10	SPEAKER_04
 Because the one thing that seems to me, we really should try, if you hadn't tried it before, because it hadn't occurred to me, it was sort of obvious thing, is adjusting the scaling and insertion.

0:01:24	SPEAKER_04
 L.T. sort of stuff.

0:01:26	SPEAKER_06
 I did play with that, actually, a little bit.

0:01:29	SPEAKER_06
 What happens is, when you get to the noisy stuff, you start getting lots of insertions.

0:01:35	SPEAKER_06
 Right.

0:01:36	SPEAKER_06
 And so I've tried playing around a little bit with the insertion penalties, things like that.

0:01:43	SPEAKER_06
 Yeah.

0:01:44	SPEAKER_06
 It didn't make a whole lot of difference.

0:01:46	SPEAKER_06
 Like, for the well-matched case, it seemed like it was pretty good.

0:01:50	SPEAKER_06
 I could do more playing with that, though.

0:01:53	SPEAKER_04
 But you were looking at Mel Capstrom.

0:01:56	SPEAKER_04
 Right.

0:01:59	SPEAKER_06
 Oh, you're talking about for our features.

0:02:02	SPEAKER_04
 Right. So, I mean, it's not the direction that you were working with, that we were saying, what's the best you can do with Mel Capstrom.

0:02:10	SPEAKER_04
 But they raised a very valid point, which I guess...

0:02:14	SPEAKER_04
 So, to first order, I mean, you have other things you're going to do.

0:02:17	SPEAKER_04
 But the first order, I would say, that the conclusion is, that if you do some munking around with the exact HTK training, and with how many states and so forth, that it doesn't particularly improve the performance.

0:02:37	SPEAKER_04
 In other words, that even though it sounds pretty dumb, just applying the same number of states to everything, or, no matter what language, isn't so bad.

0:02:46	SPEAKER_04
 Right.

0:02:47	SPEAKER_04
 And I guess you hadn't gotten to all the experiments you wanted to do with number of Gaussian's.

0:02:51	SPEAKER_04
 Right.

0:02:53	SPEAKER_04
 But let's just...

0:02:56	SPEAKER_04
 If we had to draw a conclusion on the information we have so far, we'd say something like that, right?

0:03:01	SPEAKER_04
 So, the next question to ask, which is, I think the one that Andreas was addressing himself to in the lunch meeting, is, we're not supposed to adjust the back end.

0:03:11	SPEAKER_04
 But anybody using the system would.

0:03:14	SPEAKER_04
 Yeah.

0:03:16	SPEAKER_04
 If you were just adjusting the back end, how much better would you do in noise?

0:03:23	SPEAKER_04
 Because the language scaling and search and penalty is over, they're probably set to be about right for milk capstrom.

0:03:31	SPEAKER_04
 But they're probably not at all set right for these things, particularly these things that look over larger time windows, in one way or another with LDA and KLT and neural nets, all these things.

0:03:47	SPEAKER_04
 In the past, we always found that we had to increase the insertion penalty to correspond to such things.

0:03:53	SPEAKER_04
 So, I think that's kind of a first order thing that we should try.

0:03:59	SPEAKER_06
 So, the experiment is to run our front end like normal with the default insertion penalties and so forth, and then tweak that a little bit and see how much of a difference it makes.

0:04:14	SPEAKER_04
 By our front end, I mean, take some version that Stefan has that is our current best version of something.

0:04:25	SPEAKER_04
 I mean, don't want to do this over 100 different things that they've tried, but for some version, he said he's a good one.

0:04:34	SPEAKER_04
 So, how much does it improve if you actually adjust that?

0:04:45	SPEAKER_04
 But it is interesting to say you have for the noise, how about for the mismatched or the medium mismatched conditions?

0:04:54	SPEAKER_04
 Have you, when you adjusted those numbers for milk capstrom, did it?

0:04:59	SPEAKER_06
 I don't remember off the top of my head.

0:05:05	SPEAKER_06
 Yeah, I didn't even write them down.

0:05:08	SPEAKER_06
 I don't remember I would hate to.

0:05:10	SPEAKER_06
 Well, I did write down.

0:05:14	SPEAKER_06
 So, when I was doing, I just wrote down some numbers for the well-matched case.

0:05:19	SPEAKER_06
 Yeah.

0:05:21	SPEAKER_06
 Looking at, I wrote down with the deletions, the substitutions, insertions, or four different numbers of states per phone.

0:05:29	SPEAKER_06
 Yeah.

0:05:30	SPEAKER_06
 But that's all I wrote down.

0:05:33	SPEAKER_06
 Okay.

0:05:34	SPEAKER_06
 I would need to do that.

0:05:37	SPEAKER_04
 Okay, so.

0:05:38	SPEAKER_04
 I can do that for next week.

0:05:40	SPEAKER_04
 Yeah.

0:05:41	SPEAKER_04
 And, yeah, also, sometimes if you run behind some of these things, maybe we can get someone else to do it, you can super-rise or something.

0:05:51	SPEAKER_04
 But I think it'd be good to know that.

0:05:53	SPEAKER_06
 Yeah.

0:05:54	SPEAKER_06
 I just need to get a friend and, uh, select for me, or you point me to some files.

0:05:58	SPEAKER_06
 Yeah.

0:05:59	SPEAKER_06
 You've already come to it.

0:06:00	SPEAKER_06
 All right.

0:06:01	SPEAKER_01
 Okay.

0:06:02	SPEAKER_06
 I probably will have time to do that and time to play a little bit with the silence.

0:06:17	SPEAKER_01
 Yeah.

0:06:18	SPEAKER_06
 Model.

0:06:19	SPEAKER_06
 Maybe I can have that for next week, maybe next week.

0:06:26	SPEAKER_04
 Yeah.

0:06:27	SPEAKER_04
 Yeah.

0:06:28	SPEAKER_04
 Because I mean, the other, that in fact, might have been part of what, uh, the difference was, at least part of it, that we were seeing, you know, we were seeing the SRI system was so much better than the tandem system.

0:06:43	SPEAKER_04
 Part of it just be that the SRI system, they always adjust these things to be so optimized.

0:06:50	SPEAKER_06
 I wonder if there's anything that we could do to the front end that would affect the insertion?

0:06:56	SPEAKER_06
 Yes.

0:06:57	SPEAKER_04
 I think you can.

0:06:58	SPEAKER_04
 Okay.

0:06:59	SPEAKER_04
 Well, um, uh, part of what's going on, um, is the, uh, the range of values.

0:07:12	SPEAKER_04
 So if you have something that has a much smaller range or a much larger range, and taking the appropriate root, you know, if something is kind of like the equivalent of a bunch of probabilities multiplied together, you can take a root of some sort of, like, seven probabilities together, you can take the seventh root, or something, or it's in the log domain, divided by seven, but, um, that has a similar effect because it changes the scale of the numbers, if the differences between different candidates from the acoustic model, as opposed to what's coming from the language model.

0:07:50	SPEAKER_06
 That's changing the value of your insertion.

0:07:54	SPEAKER_04
 Yeah. I mean, it's more directly like the, the language scaling or the, the model scaling or acoustic scaling, but you know that those things have kind of a similar effect to the insertion penalty anyway, a slightly different way of, of handling it.

0:08:08	SPEAKER_06
 So, um, so if we know what the insertion penalty is, we can get an idea about what range our numbers should be on.

0:08:15	SPEAKER_06
 I think so, yeah.

0:08:17	SPEAKER_04
 Yeah, so that's why I think that's another reason, another curiosity, as to why it would, in fact, be kind of neat to find out if we're way off.

0:08:25	SPEAKER_04
 I mean, the other things aren't, we're seeing, I'm sure you've already looked at this in these noisy cases.

0:08:30	SPEAKER_04
 We are seeing lots of insertions, right? The insertion number is quite high.

0:08:34	SPEAKER_04
 I know the VAD takes pretty care part of that.

0:08:37	SPEAKER_02
 I've seen that with the male capster. I don't know about the Aurora front end, but I think it's much more balanced with, uh, when the front end is more robust.

0:08:45	SPEAKER_02
 Yeah, I could look at it.

0:08:48	SPEAKER_02
 At this, yeah.

0:08:49	SPEAKER_04
 Yeah, what's the typical number?

0:08:51	SPEAKER_04
 I don't, I don't know.

0:08:53	SPEAKER_04
 Okay. I'm sure it's more balanced, but it wouldn't surprise me if there's still, I mean, in the, the old system is used to do.

0:09:03	SPEAKER_04
 I remember numbers kind of like insertions being half the number of deletions as being, and both numbers being, tend to be on the small side, comparing to, uh, substitutions.

0:09:14	SPEAKER_06
 Well, this, the whole problem with insertions was what I think, um, we talked about when the guy from OGI came down at one time, and, and that was when people were saying what we should have a, uh, voice activity detector.

0:09:30	SPEAKER_06
 Right. That, because all that stuff that we're getting, the silence that's getting through is causing insertions.

0:09:35	SPEAKER_06
 Right.

0:09:37	SPEAKER_06
 I've mentioned there's still a lot.

0:09:40	SPEAKER_04
 Yeah, and it may be less of a critical thing. I mean, the fact that some get by, maybe less of a critical thing if you, uh, get things in the right range.

0:09:49	SPEAKER_04
 So I mean, the insertions is, is a symptom.

0:09:52	SPEAKER_04
 It's a symptom that there's something wrong with the range, but there's a, your, your, your substitutions tend to go up as well.

0:09:59	SPEAKER_04
 So I, I think that, uh, the most obvious thing is just the insertions.

0:10:03	SPEAKER_04
 I don't know if it's, but, uh, um, if you're operating in the wrong range, I mean, that's why just in general, if you change what these, these penalties and scaling factors are, you reach some point that's, uh, that's a minimum.

0:10:18	SPEAKER_04
 So, um, we do have to do well over a range of different conditions, some of which are noisier than others.

0:10:28	SPEAKER_04
 But, um, I think we may get a better handle on that if we, if we see, um, I mean, it's, if we actually could pick a, a more stable value for the range of these features.

0:10:41	SPEAKER_04
 It, um, could, uh, even though it's, it's, it's true that in a real situation, you can, in fact, adjust the, these, these scaling factors and the back end.

0:10:58	SPEAKER_04
 And it's artificial here that we're not adjusting those.

0:11:01	SPEAKER_04
 You certainly don't want to be adjusting those all the time.

0:11:04	SPEAKER_04
 And if you have a nice front end that's roughly the right range, I remember after we got our stuff more or less together in the previous systems, we built that we tended to set those scaling factors at kind of a standard level.

0:11:16	SPEAKER_04
 And we would rarely adjust them again, even though you could get a, for an evaluation, you can get an extra point or something if you tweaked it a little bit.

0:11:23	SPEAKER_04
 But once we knew what, roughly the right operating range was, it was pretty stable.

0:11:27	SPEAKER_04
 And, uh, we might just not even be in the right operating range.

0:11:34	SPEAKER_06
 So with the, uh, what a good idea to try to map it into the same range that you get in the well matched case.

0:11:37	SPEAKER_06
 So if we computed what the range was and well matched, and then when we get our noisy conditions out, we try to make it have the same range.

0:11:44	SPEAKER_04
 No, you don't want to change it for different conditions.

0:11:49	SPEAKER_04
 No, no, I, what, what I'm saying.

0:11:52	SPEAKER_06
 Oh, I wasn't interested in changing it for different conditions. I was just saying that when we pick a range, we want to pick a range that we map our numbers into.

0:12:00	SPEAKER_06
 Yeah.

0:12:01	SPEAKER_06
 We should probably pick it based on the range that we get in the well matched case.

0:12:06	SPEAKER_06
 Otherwise, I mean, what range are we going to choose to map everything into?

0:12:13	SPEAKER_04
 Well, it depends how much we want to do, game ismanship and how much we want to do.

0:12:17	SPEAKER_04
 I mean, if it can be actually, even if you want to be played on the game ismanship side, it can be kind of tricky.

0:12:24	SPEAKER_04
 So I mean, what you would do is set the, set the scaling factors so that you got the best number for this 0.45 times the, you know, and so on.

0:12:37	SPEAKER_04
 But they might change that those weightings.

0:12:40	SPEAKER_04
 So I just sort of think we need to explore the space. Let's take a look at a little bit.

0:12:47	SPEAKER_04
 We may just find that that we're way off.

0:12:50	SPEAKER_04
 Maybe we're not.

0:12:51	SPEAKER_04
 You know, that's with these other things that may turn out the, it's kind of reasonable.

0:12:56	SPEAKER_04
 But then, I mean, Andreas gave very reasonable response and he's probably not going to be the only one who's going to say this in the future of, you know, people, people within this tight-knit community who are doing this evaluation are accepting.

0:13:09	SPEAKER_04
 More or less that these are the rules.

0:13:12	SPEAKER_04
 But people outside of it are looking at the broader picture are certainly going to say, well, wait a minute, you're doing all this standing on your head in the front end when all you could do is just adjust this in the back end with one knob.

0:13:25	SPEAKER_04
 And so we have to at least, I think, determine that that's not true, which would be okay.

0:13:31	SPEAKER_04
 Or determine that it is true, in which case we want to adjust that and then continue with what we're doing. And as you say, as you point out, finding ways to then compensate for that in the front end, also then becomes a priority for this particular test.

0:13:48	SPEAKER_04
 So you don't have to do that.

0:13:51	SPEAKER_04
 Okay.

0:13:56	SPEAKER_04
 So, what's new with you?

0:14:02	SPEAKER_02
 So there's nothing new.

0:14:07	SPEAKER_04
 What's old with you? It's developed.

0:14:10	SPEAKER_04
 I'm sorry.

0:14:11	SPEAKER_04
 Okay, what's old with you? That is developed over the last week.

0:14:16	SPEAKER_02
 So, if we're mainly working on the report, on the report of the work that was already done,

0:14:36	SPEAKER_06
 that's all. Anything new on the thing that you're working on with the... What was that?

0:14:52	SPEAKER_06
 The voicing detected.

0:14:56	SPEAKER_00
 What's going on now? What are you doing?

0:15:08	SPEAKER_00
 We try to use the variance, the difference between the effect spectrum and the male filter band spectrum. Also, the other parameters relates with the autocorrelation function.

0:15:26	SPEAKER_00
 Energy and the variance also helps the autocorrelation function.

0:15:32	SPEAKER_04
 So, that's what you were describing, I guess a week or two ago.

0:15:38	SPEAKER_00
 We don't have result of the aurora jet. We need to try and run network.

0:15:44	SPEAKER_04
 So, you're training neural networks now?

0:15:46	SPEAKER_04
 No, not yet.

0:15:48	SPEAKER_04
 So, what's going on?

0:15:50	SPEAKER_00
 Well, I work in the report too, because we have a lot of results, embedded in spares, and what necessary to look at the directory to give some structure.

0:16:04	SPEAKER_04
 So, yeah, if I can summarize, basically what's going on is that you're going over a lot of material that you've generated in a furious fashion of generating many results and doing experiments and trying to pull it together into some coherent form to be able to...

0:16:19	SPEAKER_02
 Yeah, basically we've stopped experimenting. We're just trying to think of some kind of technical report.

0:16:29	SPEAKER_06
 Is this a report that's for aurora?

0:16:33	SPEAKER_06
 No.

0:16:35	SPEAKER_06
 It's like a tech report for XC.

0:16:37	SPEAKER_00
 Yes.

0:16:39	SPEAKER_04
 So, my suggestion though is that you not necessarily finish that, but that you put it all together so that you've got a clearer structure to it, you know what things are, you have things documented, you've looked things up that you needed to look up so that such a thing can be written.

0:17:04	SPEAKER_04
 And when do you leave again?

0:17:09	SPEAKER_00
 July, first of July.

0:17:11	SPEAKER_04
 First of July, okay. And that you figure on actually finishing it in June, because you're going to have another much results to fit in there anyway.

0:17:24	SPEAKER_04
 And right now it's kind of important that we actually go forward with experiments. So I think it's good to pause and together everything together and make sure it's in good shape so that other people can get access to it so that it can go into a report in June.

0:17:39	SPEAKER_04
 But I think to really work on fine tuning the report at this point is probably a bad timing.

0:17:48	SPEAKER_02
 Yeah. Well, we didn't, we just planned to work on one week on this report, no more anyway.

0:17:55	SPEAKER_04
 I really want to add other things later anyway, because you're, this is more to go.

0:18:02	SPEAKER_06
 Yeah, well, so I don't know there are small things that we started to do, but maybe discovering anything that makes you scratch your head as you write this report.

0:18:14	SPEAKER_06
 Like, why did we do that? Why did we do this?

0:18:19	SPEAKER_02
 Yeah. Yeah.

0:18:23	SPEAKER_02
 Actually, there were some tables that were also with partial results. We just noticed that I get a ring of result that for some conditions we didn't have everything.

0:18:39	SPEAKER_02
 Yeah, yeah, we have extracted actually the noises from a speech that car.

0:18:44	SPEAKER_02
 So we can train neural network with speech and these noises.

0:18:51	SPEAKER_02
 It's difficult to say what it will give because when we look at the, or at the IDG experiments, there are these three conditions that have different noises and apparently the system performs as well on the scene noises, on the scene noises.

0:19:14	SPEAKER_02
 But I think there's something we have to try anyway. So adding the noises from, from the speech that car.

0:19:28	SPEAKER_04
 That's, that's, that's permitted.

0:19:39	SPEAKER_02
 Well, OGI did that.

0:19:45	SPEAKER_02
 At some point, they did that for the first activity.

0:19:53	SPEAKER_06
 Could you say it again? What exactly did they do?

0:19:59	SPEAKER_02
 They use some parts of the Italian database to train the voice activity sector, I think.

0:20:07	SPEAKER_04
 Yeah, I guess the thing is, yeah, I guess that's a matter of interpretation.

0:20:11	SPEAKER_04
 The rules, as I understand it, is that in principle the Italian and Spanish and the English, no Italian and the Finnish, and the English were development data. And what you could adjust things.

0:20:28	SPEAKER_04
 And the German and Danish were the evaluation data. And then when they finally actually evaluated things, they used everything.

0:20:37	SPEAKER_04
 Yeah, that's right.

0:20:40	SPEAKER_04
 And it is true that the performance on the German was, I mean, the improvement wasn't so good. The raw performance was really pretty good.

0:20:52	SPEAKER_04
 So, and it doesn't appear that there's strong evidence that even though things were somewhat tuned on those three or four languages, that going to a different language really hurt you. And the noises were not exactly the same, right, because it was taken from a different...

0:21:11	SPEAKER_04
 I mean, they were different drives. I mean, it was actual different cars and so on.

0:21:16	SPEAKER_04
 So, it's somewhat tuned. It's tuned more than, you know, you really like to have something that needed no particular noise at all, maybe just some white noise or something like that, at most.

0:21:30	SPEAKER_04
 But that's not really what this contest is. So, I guess it's okay.

0:21:37	SPEAKER_04
 That's something I'd like to understand before we actually use something from it, because it would...

0:21:42	SPEAKER_06
 It's probably something that, you know, the experiment designers didn't really think about, because I think most people aren't doing trained systems or, you know, systems that are like ours, but they actually use the data to build models, I mean, just doing things like all the processing.

0:22:02	SPEAKER_04
 Well, it's true, except that that's what we used in Aurora 1, and then they designed the things for Aurora 2, knowing that we were doing that.

0:22:09	SPEAKER_04
 That's true.

0:22:11	SPEAKER_06
 And they didn't forbid us, right, to build models on the data.

0:22:15	SPEAKER_04
 No, but I think that it probably would be the case that if, say, we trained on Italian data, and then we tested on Danish data, and it did terribly.

0:22:26	SPEAKER_04
 That it would look bad, and I think someone would notice, and would say, well, look, this is not generalizing. I would hope they would.

0:22:35	SPEAKER_04
 But it's true, you know, maybe there's parameters that other people have used, you know, that they have tuned in some way for other things.

0:22:49	SPEAKER_04
 So it's, we should, maybe that's maybe a topic, especially if you talk with him when I'm not here, that's a topic you should discuss with he may check it's okay.

0:23:00	SPEAKER_06
 The speakers or each of the training utterances.

0:23:05	SPEAKER_02
 What do you mean?

0:23:07	SPEAKER_04
 Social security number.

0:23:11	SPEAKER_02
 I think it.

0:23:13	SPEAKER_02
 Made a female just me up at least.

0:23:19	SPEAKER_06
 What kind of information do you mean?

0:23:21	SPEAKER_06
 Well, I was thinking about things like, you know, gender, you know, gender specific nets and both the track link on normalization.

0:23:30	SPEAKER_06
 Things like that.

0:23:31	SPEAKER_06
 I don't know what information we have about the speakers that we could try to take advantage of.

0:23:40	SPEAKER_04
 Right. I mean, again, if you had the whole system you were optimizing, that would be easy to see.

0:23:45	SPEAKER_04
 But if you're supposedly just using a fixed back end and you're just coming up with a feature vector, I'm not sure.

0:23:57	SPEAKER_04
 I mean, having the two nets, suppose you detected that was male, female, you can look at different both in as separate streams or something.

0:24:12	SPEAKER_04
 Maybe.

0:24:15	SPEAKER_06
 I don't know. I was just wondering if there was other information we could exploit.

0:24:24	SPEAKER_04
 Yeah, it's interesting thought maybe having something along, I mean, you can't really do vocal track normalization.

0:24:39	SPEAKER_04
 It's something that had some of that effect. Yeah.

0:24:57	SPEAKER_06
 No, I had no idea. I had thought it was too much about it really. It just something that popped into my head just now.

0:25:12	SPEAKER_06
 Normalization, you know, you have some sort of a general speech model, maybe just a mixture of galsians that you evaluate every utterance against.

0:25:30	SPEAKER_06
 And then you see where each utterance, like the likelihood of each other, and to divide the range of the likelihoods up into discrete bins.

0:25:33	SPEAKER_06
 And then each bins got some knob.

0:25:36	SPEAKER_04
 Yeah, but just listen to yourself. I mean, that really doesn't sound like a real time thing with less than 200 milliseconds latency that were you're not adjusting the statistical engine at all.

0:25:49	SPEAKER_04
 Yeah, well, not just expensive. I don't see how you could possibly do it. You can't look at the whole utterance and do anything.

0:25:57	SPEAKER_04
 Each frame comes in and it's got to go out the other end.

0:26:00	SPEAKER_04
 So whatever it was, it would have to be sort of on a per frame basis. Yeah. I mean, you can do fairly quickly. You can do male female stuff.

0:26:12	SPEAKER_04
 But as far as, I mean, like I thought, maybe I ended a thing with a vocal track normalization, ways back, maybe other people did too, with trying to identify third formant, average third formant, using that as an indicator of.

0:26:25	SPEAKER_04
 So, you know, third formant, if you imagine that the first order, what happens with changing vocal track is that the formants get moved out by some proportion.

0:26:36	SPEAKER_04
 So if you had a first formant that was 100 hertz before, if the 50, if the vocal track is 50% shorter, then it would be out at 750 hertz and so on.

0:26:46	SPEAKER_04
 So that's a move of 250 hertz, whereas the third formant, which might have started off at 2500 hertz, might be out to 3750, you know, so you frequently get less distinct higher formants.

0:27:00	SPEAKER_04
 It's still third formant is kind of a reasonable compromise.

0:27:04	SPEAKER_04
 So I think, if I recall correctly, they did something like that.

0:27:08	SPEAKER_04
 But that doesn't work for just having one frame or something. That's more like looking at third formant over a turn or something like that.

0:27:20	SPEAKER_04
 So, but on the other hand, male females is a much simpler categorization than figuring out a factor to squish or expand the spectrum.

0:27:31	SPEAKER_04
 You could imagine that, just like we're saying, voiced and voiced is good to know. Male female is good to know also.

0:27:40	SPEAKER_04
 But you have to figure out a way to incorporate it on the fly.

0:27:46	SPEAKER_04
 I mean, I guess, as you say, one thing you could do is simply have the male and female output vectors, you know, net strain only on males and drain only on females.

0:27:58	SPEAKER_04
 But I don't know if that would really help because you already have males and females. It's fitting into one net.

0:28:08	SPEAKER_06
 Is it balanced in terms of gender data?

0:28:12	SPEAKER_02
 Do you know? Almost.

0:28:16	SPEAKER_04
 Okay. You were saying before?

0:28:27	SPEAKER_02
 Yes. So this noise.

0:28:31	SPEAKER_02
 Yeah, the MSG.

0:28:37	SPEAKER_02
 There is something perhaps you could spend some days to look at this thing because it seems that when we train networks on, let's say on timid with MSG features, they look as good as network strain on BLP.

0:28:54	SPEAKER_02
 But when they are used on the speech data, it's not the case. The MSG features are much worse. And so maybe they are more sensitive to different recording conditions.

0:29:12	SPEAKER_04
 Yeah. But let me ask you this. What's the, do you know, recall of the insertions were higher with MSG?

0:29:25	SPEAKER_02
 I don't know. I cannot tell. But it's the error rate is higher.

0:29:30	SPEAKER_04
 Yeah, we should always look at insertions, solutions and substitutions. So MSG is very, very different. And BLP is very much like milk hamstring.

0:29:41	SPEAKER_04
 MSG is very different from both of them. So if it's very different, then this is the sort of thing. I mean, I'm really glad Andreas brought this point up. I sort of forgotten to discuss it.

0:29:53	SPEAKER_04
 We always have to look at how these adjustments affect things. And even though we're not allowed to do that, again, we maybe could reflect that back to our use of the features.

0:30:05	SPEAKER_04
 So if it, if in fact, the problem might be that the range of the MSG features is quite different, the range of the BLP or milk hamstring. And you might want to change that.

0:30:16	SPEAKER_02
 But yeah, but it's after, well, it's tandem features. So yeah.

0:30:28	SPEAKER_02
 Yeah, we have estimation of first, post-serials. Yeah, with BLP and with MSG as input. So why not? Well, that means they're between zero and one.

0:30:39	SPEAKER_04
 But it doesn't necessarily, you know, they could be, doesn't tell you what the variance of the things is.

0:30:58	SPEAKER_04
 So you're taking log of these things. We could be knowing what the sum of the probabilities are. It doesn't tell you what the sum of the logs are.

0:31:13	SPEAKER_02
 So, yeah, so we should look at the likelihood. Or what? Well, the log props. Yeah. Or what, you know, what you're the thing you're actually looking at.

0:31:32	SPEAKER_06
 So your, your values that are actually being fed into HTK. What do they look like? So the, for the tandem system, the values that come out of the net don't go through the sigmide, right? They're sort of the cream on linearity values.

0:31:48	SPEAKER_06
 Right. So they're kind of like log probabilities. So that's what goes into HTK.

0:31:55	SPEAKER_04
 Almost. Then you actually do a KLT. They are normalized after that. Are they?

0:32:12	SPEAKER_04
 No. No. Okay. So, right. So the question is, yeah, whatever they are at that point, are they something for which taking a square root or cube root or four-thread or something like that is going to be a good or a bad thing?

0:32:39	SPEAKER_04
 So, and that's something that nothing else after that is going to, things are going to scale it. You know, subtract things from it, scale it from it, but nothing will have that same effect.

0:32:55	SPEAKER_06
 So, anyway, if the log probs that are coming out, whether the MSG are really big, the standard insertion penalties are going to have very little effect compared to, you know, smaller set of log probs.

0:33:11	SPEAKER_04
 Now, again, you don't really look at that. It's something that, and then it's going through this transformation that's probably pretty close to, whatever the KLT is doing, but it's probably pretty close to what a discrete close-any transformation is doing. But still, it's not going to probably radically change the scale of things.

0:33:28	SPEAKER_04
 I would think, and yeah, maybe entirely off, and it may be, at least it may be quite different for MSG than it is for milk, up to MPLP. So that would be, so the first thing to look at without adjusting anything would just be to go back to the experiment, look at the substitutions, insertions, and relations.

0:33:47	SPEAKER_04
 And if there's a fairly large effect of the ratio between insertions and relations for the two cases, then that would be an indicator that might be in that direction.

0:34:04	SPEAKER_02
 Yeah, but my point was more that it works sometimes. Yeah, but sometimes it doesn't work.

0:34:18	SPEAKER_04
 And it works on the TID Jits and the speech that Gary doesn't work. Yeah, but some problems are harder than others. And sometimes there's enough evidence for something to work, and then it's harder to break.

0:34:37	SPEAKER_04
 But it could be that when you say it works, maybe we could be doing much better even if the TID Jits. Yeah, well, there is also the spectroscopy section, which I think maybe we should try to integrate it.

0:35:06	SPEAKER_02
 Yeah. Right. But I think that would involve to use a big bunch of the system of Ericsson.

0:35:32	SPEAKER_02
 Because the spectroscopy section then it's followed by other kind of processing that's dependent on the speech on silence.

0:35:48	SPEAKER_02
 And there is kind of spectral flattening after if it's silence. And I think it's important to reduce this musical noise and this increase of variance during silence portions.

0:36:04	SPEAKER_02
 So, this would involve to take almost everything from the this proposal and then just add some kind of fun like normalization and the neural network.

0:36:22	SPEAKER_04
 Okay, well, this would be I think something for discussion with Henik next week. Right. So, how are things going with what you're doing?

0:36:38	SPEAKER_05
 Well, it took a lot of time just getting my taxes out of the way. I'm starting to write code now from my work, but I don't have any results yet. It would be good for me to talk to Henik, I think when he's here. Do you know what his schedule will be like?

0:36:54	SPEAKER_04
 He'll be around for three days. Okay, so we'll have a lot of time. So, he'll be talking with everybody in this room.

0:37:08	SPEAKER_06
 But you said you won't be here next Thursday?

0:37:10	SPEAKER_04
 Not Thursday and Friday, so it would be a faculty retreat. So, I'll try to connect with him and people as I can on Wednesday.

0:37:26	SPEAKER_04
 How did taxes go? Next go, okay. Yeah.

0:37:32	SPEAKER_04
 Yeah, that's one of the big advantages of not making much money. The taxes are easier.

0:37:40	SPEAKER_06
 Unless you're getting money into countries. I think you're going to want to cut.

0:37:50	SPEAKER_04
 Can't do what? Can't do what's a cut? You have to do two returns. For 2000, I did. Yeah. Oh, yeah. That's right.

0:38:06	SPEAKER_05
 I'll still have a bit of Canadian income, but it'll be less complicated because I will not be considered a resident of Canada anymore, so I won't have to declare my American income on my Canadian return.

0:38:24	SPEAKER_03
 Very? Oh, right. Continuing looking at phonetic events. And it was Tuesday. I've been meeting with John and Halal and Chuck to talk some more about these neck events.

0:38:46	SPEAKER_03
 I came up with a plan of attack. Oh, well, I don't want to say something about what it is.

0:39:00	SPEAKER_03
 Okay, well, we're all gathered here together. I hope I can wave my hands. So once I'm thinking of getting a set of acoustic events to be able to distinguish between phones and words and stuff.

0:39:22	SPEAKER_03
 Once we figure out a set of these events that can be hand labeled or derived from hand labeled phone targets, we can take these events and do some cheating experiments where we feed these events into an SRI system and evaluate its performance on a switchboard task.

0:39:51	SPEAKER_03
 Can you give an example of an event? Sure. I can give you an example of 20 odd events. So in this paper, I'm talking about funding recognition using acoustic events.

0:40:07	SPEAKER_03
 So things like vacation or news. Who's paper?

0:40:23	SPEAKER_04
 From University of Hamburg and Bielfeld.

0:40:39	SPEAKER_06
 I think there's a difference between acoustic features and acoustic events. And I think of acoustic features as being things that linguists talk about.

0:40:55	SPEAKER_06
 So stuff that's not based on data. So they talk about features for phones like its height, its tenseness, laxness, things like that, which may or may not be all that easy to measure in the acoustic signal versus an acoustic event, which is just something in the acoustic signal that is fairly easy to measure.

0:41:24	SPEAKER_06
 So it's a little different.

0:41:28	SPEAKER_04
 When we did the spam work, we had this notion of an auditory event called an event with an A at the front.

0:41:44	SPEAKER_04
 And the idea was something that occurred that is important to a bunch of neurons somewhere. So a sudden change or a relatively rapid change in some spectral characteristic will do sort of this.

0:41:59	SPEAKER_04
 And there's certainly a bunch of places where you know that neurons are going to fire because something novel has happened. That was the main thing that we were focusing on there. But there's certainly other things beyond what we talked about there that aren't just sort of rapid changes.

0:42:14	SPEAKER_06
 It's kind of like the difference between top down and bottom up. I think of the acoustic, you know, phonetic features as being top down. And you look at the phone and you say this phone is supposed to be, you know, have this feature, this feature in this feature.

0:42:28	SPEAKER_06
 Whether that those features show up in the acoustic signal is sort of irrelevant. Whereas an acoustic event goes the other way. Here's the signal. Here's some event. And that, you know, that may map to this phone sometimes and sometimes it may not. It just depends on the context and things like that.

0:42:45	SPEAKER_03
 So, yeah. Okay. Using these events, we could perform these cheating experiments and how good they are in terms of phoneme recognition or work recognition.

0:43:07	SPEAKER_03
 And then from that point on, I would design robust event detectors in a similar spirit that Saul has done with his graphical models and this probabilistic and or model that he uses.

0:43:28	SPEAKER_03
 I tried to extend it to account for other phenomena like CMR, co-multulation, release. And maybe also the best of the ways to modify the structure of these models in a data driven way, similar to the way that Jeff, Jeff, builds his work.

0:43:55	SPEAKER_03
 And while I'm doing these event detectors, you know, I can measure my progress by comparing the error rates in clean and noisy conditions to something like neural nets.

0:44:09	SPEAKER_03
 And so once we have these event detectors, put them together and feed the outputs of the event detectors into the SRI, HMM system and test it on switchboard or maybe even Aurora stuff.

0:44:34	SPEAKER_03
 And that's pretty much the big picture of the plan.

0:44:43	SPEAKER_04
 By the way, there's a couple people who are going to be here. I forget I already told you this, but a couple people who are going to be here for six months. It's Professor Kolmeier from Germany who's quite big in the hearing aid signal processing area.

0:45:04	SPEAKER_04
 And Michael Klanchman who's worked with him who also looks at auditory properties inspired by various brain function things. So I think they'll be interesting to talk to in this sort of issue is these detectors are developing.

0:45:26	SPEAKER_04
 He looks at interesting things in different ways of looking at specter in order to get very speech properties out.

0:45:40	SPEAKER_04
 Okay, well, short meeting with it. Okay. And that's what we're doing. I encourage you to go ahead and meet next week with HINIK.

0:45:56	SPEAKER_04
 All right, I'll start. Okay, I'm doing transcript L76 032 36 5550 7058 592 4657 17 8034 6015 544 445 088 3666 601726 971 235 1588 4821 8042 3770 528 578 8674

0:46:44	SPEAKER_02
 Transcript L-77 5 845 474 163 130 287 452 121 161422 3891 4838 1740 651 8676 293 3132 6134 24

0:47:11	SPEAKER_06
 0243 214 1337 001950 7956 transcript L-78 1543589276 449 746 646 787 337 618 2 689 033 313 375 175 536 1141 3007 510 782 461 64382 2503

0:47:59	SPEAKER_03
 transcript L-79 885 2526 17 445 191 288 4 13168 4 3004 668 3 9 8 6 4 8 6 6 6 7 5 0 3 1 4 1 7 0 8 5 9 5 0 3 7 6 0 2 6 0 8 2 3 0 7 1 9 8 7 5 7 4 2 8 8 3 7 1 6 8

0:48:46	SPEAKER_05
 transcript L-80 9 5 6 6 4 3 9 7 8 3 0 2 6 4 3 6 1 2 8 9 3 3 4 4 0 5 7 9 8 1 3 9 8 8 8 0 1 0 2 0 9 9 9 5 8 9 8 1 8 9 5 4 8 7 9 6 1 8 7 8 8 3 0 9 6 7 6 2 9 0 5 7 5 6 0 7 2 9 7

0:49:28	SPEAKER_00
 transcript L-81 7915 908 2 16168 6 4013 4 405 5 6 4 2 1 9 4 2 1 0 5 1 2 7 2 1 2 9 5 8 8 6 3 9 2 5 8 4 3 6 0 5 3 0 2 3 3 2 6 2 4 3 6 5 2 6 2 4 2 8 5 1 4 5

