0:00:00	SPEAKER_01
 Okay, so the one, one thing I knew I wanted to talk about was about sort of last minute stuff to try to get some recognition results.

0:00:13	SPEAKER_05
 Recognition results?

0:00:14	SPEAKER_01
 Yeah, so on needing data.

0:00:19	SPEAKER_01
 And so I'm not sure exactly what you're doing already and the stuff I talked today.

0:00:24	SPEAKER_05
 But recognition on the antelis segments, which was about using the closed-talk microphone.

0:00:34	SPEAKER_05
 Okay, so I know you wanted the five field.

0:00:37	SPEAKER_01
 Right, so we have some stuff with no overlap for which there would be near-near field results.

0:00:46	SPEAKER_01
 We wanted to get the far field results for that.

0:00:48	SPEAKER_01
 And then this real long shot thing would be that we'd apply Dave's processing to potentially training and test data and do the look at the same thing.

0:01:02	SPEAKER_01
 And in talking this morning with Chuck and Dave, one thought was to use, we couldn't remember how different the numbers were, but if you just worked with males only and use the short training, there, I think Chuck's recollection was that when he was doing the future stuff, it took maybe a day and a half to do the training.

0:01:21	SPEAKER_01
 Yeah.

0:01:22	SPEAKER_05
 Yeah.

0:01:23	SPEAKER_05
 That's about right.

0:01:26	SPEAKER_05
 Actually, it should probably be, depending on who else is using machines, but we have a machine.

0:01:39	SPEAKER_05
 Probably.

0:01:40	SPEAKER_01
 How much worse is the short training set than the large one in terms of ultimate performance?

0:01:47	SPEAKER_05
 Like, 3%, 3%, 4%, 4% absolute.

0:01:56	SPEAKER_01
 So that should be fine for this, I think.

0:01:59	SPEAKER_01
 So you have short training results for the close case?

0:02:07	SPEAKER_05
 Not for meetings, because we didn't train.

0:02:10	SPEAKER_05
 We didn't ever recognize with the small models on meeting data.

0:02:19	SPEAKER_05
 But I have the model, so I could.

0:02:22	SPEAKER_01
 So how do you know it's 3% on Hub 5?

0:02:26	SPEAKER_01
 I see.

0:02:27	SPEAKER_01
 Yeah, but we have the model, so we could get that number.

0:02:31	SPEAKER_01
 And so the question is, what?

0:02:33	SPEAKER_05
 I mean, the recognition also takes a lot of time, so we might want to restrict it to maybe a few meetings if you want to do a full comparison.

0:02:49	SPEAKER_01
 It has to be enough so that, I mean, it's not overlap only.

0:02:56	SPEAKER_01
 And it has to be enough to be sort of comparable to what you folks were seeing and what you reported already.

0:03:04	None
 I mean.

0:03:05	SPEAKER_05
 Well, do we have the process data that's also?

0:03:13	SPEAKER_01
 No, he has to create that.

0:03:16	SPEAKER_01
 So we have a whole parallel set of things over here, which are all with digits.

0:03:23	SPEAKER_01
 And Dave has been working with that, and there's all those issues.

0:03:28	SPEAKER_01
 But I know that if I go in with something that's not just digits, it would be good.

0:03:34	SPEAKER_01
 And so we already have these results that you, I mean, on a particular test set that you reported at HLT.

0:03:46	SPEAKER_01
 Right.

0:03:47	SPEAKER_01
 It'd be nice to have something more than that.

0:03:49	SPEAKER_01
 And we had talked about was having distant.

0:03:53	SPEAKER_01
 And then if we could on top of that, I mean, so this is going to be a lot worse.

0:03:58	SPEAKER_01
 Right.

0:03:59	SPEAKER_01
 Whatever comparison we want one would assume.

0:04:01	SPEAKER_01
 But we don't know how much worse, which is certainly one interesting thing.

0:04:06	SPEAKER_01
 And then Dave, I think we figured that it would probably take a day or two to compute the, well, how many hours to train?

0:04:20	SPEAKER_05
 I did retrain.

0:04:21	SPEAKER_05
 I recently retrained for another reason on the full training set.

0:04:28	SPEAKER_05
 And that took only two days.

0:04:33	SPEAKER_05
 Yeah.

0:04:34	SPEAKER_05
 So it's actually conceivable to do use the full training set.

0:04:39	SPEAKER_01
 Yeah, but we also have to do this other processing.

0:04:41	SPEAKER_01
 So having a smaller training set, if it's only a few percent difference, it might be.

0:04:45	SPEAKER_01
 Oh, you were just doing it.

0:04:46	SPEAKER_01
 But how big is the small training set?

0:04:53	SPEAKER_05
 Do you remember?

0:04:57	SPEAKER_05
 I know something like something between 30 and 50 hours.

0:05:12	SPEAKER_05
 Maybe I forget the.

0:05:13	SPEAKER_01
 It's around there.

0:05:15	SPEAKER_01
 And Mayo is roughly half of that or.

0:05:17	SPEAKER_01
 Or is that only me?

0:05:19	SPEAKER_05
 Actually, I don't know.

0:05:22	SPEAKER_05
 I can look it up.

0:05:24	SPEAKER_05
 It's just, I don't know, just remember the number.

0:05:28	SPEAKER_02
 We could only just do the Mayo only, right?

0:05:31	SPEAKER_05
 Or well, the Mayo account for most of this meeting that I need help.

0:05:35	SPEAKER_05
 Yeah.

0:05:36	SPEAKER_05
 Yeah, I would say we do only males.

0:05:39	SPEAKER_00
 Yeah.

0:05:40	SPEAKER_01
 So, yeah, so that's certainly part of the issue is that right now he hasn't written his stuff for efficiency.

0:05:48	SPEAKER_01
 It's in Matt Lab and so on.

0:05:51	SPEAKER_01
 And it's not an impossible amount of time.

0:05:54	SPEAKER_01
 We were guesstimating.

0:05:55	SPEAKER_01
 It was like one and a half times faster than real time or something.

0:06:00	SPEAKER_01
 So if there's 30 hours of data, you can calculate, you can do with the enhancement in a day, or something.

0:06:10	SPEAKER_01
 But if we were dealing with 200 hours or something, I think it would be prohibitive.

0:06:14	SPEAKER_05
 No, it's definitely less than 100 hours, for sure.

0:06:18	SPEAKER_05
 Yeah.

0:06:19	SPEAKER_05
 It's probably actually, I think it's around 30 hours.

0:06:25	SPEAKER_05
 Yeah.

0:06:26	SPEAKER_01
 It's a much gender.

0:06:27	SPEAKER_01
 Yeah.

0:06:28	SPEAKER_01
 Yeah.

0:06:29	SPEAKER_01
 So I mean, it's a bit of a push, but it seems like, okay, we've got some models, we've got some training data, we have software that works.

0:06:34	SPEAKER_01
 He's got a method that helps with other tasks.

0:06:40	SPEAKER_01
 It appears to be debugged.

0:06:44	SPEAKER_02
 So one thing I was wondering is, did you already do that middle one, or should we redo that one too?

0:06:50	SPEAKER_05
 No, I didn't do that.

0:06:52	SPEAKER_05
 We haven't even cut the waveforms.

0:06:55	SPEAKER_05
 Yeah, that's what I was going to say next.

0:06:57	SPEAKER_02
 We have to cut the, so Morgan is the plan to just pick one of the Carfield mics.

0:07:04	SPEAKER_05
 And there's a bit of a course, whether you want to use what segmentations you want to use.

0:07:11	SPEAKER_05
 David just, sorry, Dom just created a new version of the first meetings that we have previously recognized, but with different segmentations.

0:07:24	SPEAKER_05
 So it would be nice, I mean, if the results are comparable to what we had before, to use those segmentations, because then we could claim that everything's automatic.

0:07:39	SPEAKER_02
 Do you know when he'll have the comparisons?

0:07:44	SPEAKER_05
 Well, as I said, I just started the recognizer.

0:07:55	SPEAKER_05
 It will probably be a couple of hours before we'll start that.

0:07:58	SPEAKER_01
 Oh, okay.

0:07:59	SPEAKER_02
 Because I don't think the new data will be ready for a couple of days.

0:08:07	SPEAKER_01
 Me and the training.

0:08:08	SPEAKER_05
 The training.

0:08:09	SPEAKER_05
 But the segmentations matter for the filtering, right, because for the test.

0:08:17	SPEAKER_05
 But first, of course, you wouldn't want to process the training data because you want to get that started.

0:08:22	SPEAKER_05
 Yeah.

0:08:23	SPEAKER_01
 I mean, it'd be really great if it was all automatic, but I think that given the pressure of time, since you're going to find out in the short amount of time, that's great.

0:08:33	SPEAKER_01
 But if it doesn't work out, I think it would rather charge ahead with the older segmentations.

0:08:38	SPEAKER_01
 And we were going to use one of the PCMs.

0:08:42	SPEAKER_01
 I don't know.

0:08:43	SPEAKER_01
 Probably whatever one you've been using for the digits, is it this one?

0:08:47	SPEAKER_01
 F.

0:08:48	SPEAKER_01
 Which, that's F?

0:08:49	SPEAKER_01
 That's F.

0:08:50	SPEAKER_02
 How do you know?

0:08:51	SPEAKER_02
 It's the second nearest machine.

0:08:54	SPEAKER_01
 Oh.

0:08:55	SPEAKER_01
 The answer.

0:08:57	SPEAKER_01
 Okay.

0:08:58	SPEAKER_01
 All right.

0:08:59	SPEAKER_01
 So.

0:09:00	SPEAKER_01
 Let's go with the whole family.

0:09:03	SPEAKER_01
 Okay.

0:09:04	SPEAKER_02
 So, I just want to make sure I understand what we need to run.

0:09:11	SPEAKER_02
 Let's see.

0:09:15	SPEAKER_02
 So it's, okay.

0:09:16	SPEAKER_02
 So if we're talking about, let's assume that we're going to use the new segmentations.

0:09:21	SPEAKER_02
 We need to run recognition, just looking at the no overlap column.

0:09:28	SPEAKER_02
 Basically, we have to do recognitions for all three of those cases, right?

0:09:34	SPEAKER_02
 Because we're going to be using the just the male model, short training set for the male.

0:09:39	SPEAKER_02
 So we need to have results for all three of those.

0:09:41	SPEAKER_05
 Even though we have...

0:09:42	SPEAKER_05
 Maybe we should limit ourselves to the meeting recorder.

0:09:45	SPEAKER_05
 Meetings.

0:09:46	SPEAKER_02
 Okay.

0:09:47	SPEAKER_05
 If you were going to cut out the notes, I would take a switch.

0:09:51	SPEAKER_01
 Well, maybe.

0:09:52	SPEAKER_01
 But I mean, how long does it take?

0:09:54	SPEAKER_05
 Actually, for the test.

0:09:55	SPEAKER_05
 For the test, the meetings take longer because this is one speaker who talks a lot.

0:09:59	SPEAKER_05
 And so, the...

0:10:01	SPEAKER_05
 No, it's because for all the adaptation and normalization steps, you cannot...

0:10:06	SPEAKER_05
 You have to...

0:10:07	SPEAKER_05
 You cannot chop it up in small pieces.

0:10:12	SPEAKER_05
 So you're sort of limited by how long the longer speaker is speaking.

0:10:18	SPEAKER_05
 So how much data there is from the speaker who talks the most.

0:10:23	SPEAKER_05
 So you parallelize across different speakers, but if you have a bunch of speakers who speak very little and then one speaks a lot, then effectively everybody waits for the longest one to process.

0:10:37	SPEAKER_01
 But what was your result for that we had at the HLT?

0:10:41	SPEAKER_01
 Was that a combination?

0:10:42	SPEAKER_05
 That was both types of meetings, but there were only two robustus meetings and four or fives.

0:10:47	SPEAKER_02
 And we're redoing the baseline.

0:10:48	SPEAKER_02
 We need workers.

0:10:49	SPEAKER_02
 It would be okay, right?

0:10:51	SPEAKER_02
 Right.

0:10:52	SPEAKER_02
 So we're going to just limit ourselves to this.

0:10:54	SPEAKER_02
 How long would it take to run recognition if we did that?

0:10:59	None
 I don't have...

0:11:03	SPEAKER_05
 I mean, is it like a day or is it a few hours of rough?

0:11:10	SPEAKER_05
 For everything?

0:11:11	SPEAKER_05
 For all the meetings?

0:11:12	SPEAKER_02
 Yeah, let's say we just did the meeting recorder meetings for our test set.

0:11:17	SPEAKER_05
 It's probably more than a day, but probably less than two.

0:11:25	SPEAKER_02
 Oh, really?

0:11:26	SPEAKER_02
 I didn't realize each test at that long.

0:11:27	SPEAKER_05
 Well, no, I mean for all the meetings.

0:11:30	SPEAKER_05
 Because it's...

0:11:32	SPEAKER_05
 So you're doing each meeting takes something like...

0:11:41	SPEAKER_05
 Again, when we ran these, we were sort of short of machines.

0:11:46	SPEAKER_05
 I don't know.

0:11:49	SPEAKER_05
 I would estimate maybe four hours per meeting.

0:11:53	SPEAKER_05
 Something like that.

0:11:54	SPEAKER_03
 Four hours per meeting.

0:11:56	SPEAKER_03
 Right.

0:11:57	SPEAKER_01
 Yeah, but if you...

0:11:59	SPEAKER_01
 So if you do have dozen meetings, that's about a day.

0:12:03	SPEAKER_01
 We also have more machines now.

0:12:05	SPEAKER_05
 Right.

0:12:06	SPEAKER_05
 So that's why I'm saying I'm not sure how that would scale with more machines.

0:12:09	SPEAKER_01
 Yeah.

0:12:10	SPEAKER_01
 I mean, if we had about six hour test sets, not bad, right?

0:12:15	SPEAKER_00
 Six minutes.

0:12:16	SPEAKER_02
 Six meetings.

0:12:17	SPEAKER_02
 Okay.

0:12:18	SPEAKER_02
 Right.

0:12:19	SPEAKER_01
 I mean, a lot of the evaluations have been...

0:12:22	SPEAKER_01
 We have MR2C4.

0:12:23	SPEAKER_05
 I think there are four meeting recorder meetings that we worked with.

0:12:31	SPEAKER_01
 Four that you worked with?

0:12:32	SPEAKER_04
 Because at the same set as the alignment, I think it's five-five.

0:12:35	SPEAKER_04
 You're going to meetings?

0:12:36	SPEAKER_01
 Okay.

0:12:37	SPEAKER_01
 That would be okay, too.

0:12:38	SPEAKER_01
 I mean, so if they have a set that they worked with, then you got...

0:12:41	SPEAKER_01
 Did you do similarly in performance between them and the other meetings?

0:12:44	SPEAKER_01
 Or was it...

0:12:45	SPEAKER_01
 With a robustness?

0:12:46	SPEAKER_05
 Yeah.

0:12:47	SPEAKER_05
 The big variation is by whether it's a native speaker or not.

0:12:54	SPEAKER_05
 Yeah.

0:12:55	SPEAKER_05
 And whether it's...

0:12:58	SPEAKER_05
 I think that's the...

0:13:00	SPEAKER_05
 And then of course, what...

0:13:02	SPEAKER_05
 You know, whether it's lapel or...

0:13:05	SPEAKER_05
 Headset microphone.

0:13:07	SPEAKER_05
 And overlap or not.

0:13:09	SPEAKER_01
 Yeah.

0:13:10	SPEAKER_01
 So, maybe just the meeting recorder set of the...

0:13:14	SPEAKER_01
 And we can exclude...

0:13:15	SPEAKER_05
 We don't need to record.

0:13:16	SPEAKER_05
 It has the...

0:13:17	SPEAKER_05
 Not natives, because we know that...

0:13:19	SPEAKER_05
 I mean, in fact, we excluded them previously.

0:13:21	SPEAKER_01
 Yeah, so we want to do the same thing.

0:13:23	SPEAKER_02
 Okay, so...

0:13:24	SPEAKER_02
 Alright, so if we got a list of the segmentations for these five meeting recorder meetings, we could start the first two experiments going right away using the short male models.

0:13:39	SPEAKER_02
 So we can get those going while Dave is creating waveforms for the retraining the short male models.

0:13:49	SPEAKER_01
 Once we know which segmentation is reducing.

0:13:51	SPEAKER_02
 Right.

0:13:52	SPEAKER_02
 Okay.

0:13:53	SPEAKER_02
 Okay.

0:13:54	None
 And then...

0:13:55	SPEAKER_02
 Okay.

0:13:56	SPEAKER_02
 So do we also want to run that bottom experiment without retraining the short male models on this thing?

0:14:04	SPEAKER_02
 Did you want that?

0:14:06	SPEAKER_01
 Yeah.

0:14:09	SPEAKER_01
 I agree that that would be an interesting thing to do, but I sort of regard it as secondary.

0:14:13	SPEAKER_01
 So if there's sort of machines sitting around and people sitting around and they're waiting for other things to finish, then sure.

0:14:20	SPEAKER_01
 But Chuck had been asking about that earlier.

0:14:23	SPEAKER_01
 It's kind of a control to know.

0:14:26	SPEAKER_01
 Because, I mean, you could imagine a fantasy in which you said that Dave's processing made the...

0:14:32	SPEAKER_01
 a far microphone like the near microphone in which case you shouldn't actually have to retrain.

0:14:38	SPEAKER_01
 But it's not really true.

0:14:40	SPEAKER_01
 It's sort of fantasy.

0:14:41	SPEAKER_01
 It does block up the data in some funny ways.

0:14:44	SPEAKER_01
 And so I'm kind of questioning that.

0:14:47	SPEAKER_01
 But...

0:14:48	SPEAKER_02
 Well, in a more basic level also it means that that third experiment, there are actually two differences between the other experiments, not one.

0:14:54	SPEAKER_02
 Right.

0:14:55	SPEAKER_02
 So it's hard to know.

0:14:56	SPEAKER_01
 It involves retraining and it involves a...

0:14:58	SPEAKER_01
 Right.

0:14:59	SPEAKER_01
 That's right.

0:15:01	SPEAKER_01
 I mean, the other thing which you might come into is if there was some problem in the retraining, maybe you just have some mechanical thing we do wrong.

0:15:13	SPEAKER_01
 Right.

0:15:15	SPEAKER_01
 That since Dave's experience was that it didn't help as much if you didn't retrain, that it does help some.

0:15:24	SPEAKER_01
 That we would hopefully see that.

0:15:27	SPEAKER_01
 So that's true.

0:15:29	SPEAKER_05
 So when you use original models and you just process the test set in this way, do you get any...

0:15:40	SPEAKER_05
 do you get decent performance or not?

0:15:42	SPEAKER_03
 I think for the far mic HDK system I was using, it did help somewhat.

0:15:49	SPEAKER_03
 I could recheck that but it was such a bad baseline that I don't know what that means.

0:15:54	SPEAKER_05
 Right.

0:15:55	SPEAKER_03
 Okay. Because the baseline word error rate was around 40% on digits.

0:16:00	SPEAKER_02
 On the far field.

0:16:02	SPEAKER_03
 Right.

0:16:07	SPEAKER_01
 Right.

0:16:08	SPEAKER_01
 So...

0:16:09	SPEAKER_05
 Okay, well I can get started on the...

0:16:12	SPEAKER_05
 well the first...

0:16:14	SPEAKER_05
 the one that already has a...

0:16:16	SPEAKER_05
 Oh, there.

0:16:17	SPEAKER_05
 We need to redo that with small models.

0:16:19	SPEAKER_05
 Right.

0:16:20	SPEAKER_05
 And then I have to ask, I guess, Dawn to cut the...

0:16:27	SPEAKER_05
 cut the segments for the two distant like...

0:16:33	SPEAKER_05
 So we would be using the same channel for each... for everything?

0:16:38	SPEAKER_05
 Yeah.

0:16:39	SPEAKER_05
 Okay.

0:16:40	SPEAKER_01
 I mean, do you have to rely on your segmentations at all to do the...

0:16:43	SPEAKER_05
 No, no, we would use the same segmentations but we need to extract...

0:16:47	SPEAKER_05
 Oh, okay.

0:16:50	SPEAKER_02
 So when you said you were going to start that top one, were you going to use the new segmentations?

0:16:59	SPEAKER_05
 Yeah.

0:17:01	SPEAKER_05
 If assuming that the performance turns out to be comparable with...

0:17:06	SPEAKER_05
 with the old experiments.

0:17:09	SPEAKER_05
 Right.

0:17:10	SPEAKER_05
 And the old segmentations.

0:17:12	SPEAKER_05
 Now there's the issue of...

0:17:15	SPEAKER_05
 Oh, okay. So there's the issue of speaker normalization.

0:17:18	SPEAKER_05
 So with the distant microphone, you wouldn't know which speaker is talking.

0:17:25	SPEAKER_05
 Right.

0:17:28	SPEAKER_01
 We talked about this before.

0:17:30	SPEAKER_01
 I think what we were saying was that...

0:17:34	SPEAKER_01
 the very fact that in both cases we're ignoring the overlap section means that...

0:17:42	SPEAKER_01
 we're to some extent finessing that.

0:17:45	SPEAKER_01
 So I think for the purposes of just determining whether a fire field microphone...

0:17:51	SPEAKER_01
 what the effect of the fire field microphone is we should do the same to both.

0:17:56	SPEAKER_05
 That's it.

0:17:57	SPEAKER_05
 So you want to cheat?

0:18:03	SPEAKER_01
 We want to incorporate certain data that would not be available during final tests...

0:18:10	SPEAKER_01
 under a full fair test of it, much as we are in the...

0:18:15	SPEAKER_05
 All the numbers we have so far.

0:18:17	SPEAKER_05
 We have speakers as...

0:18:20	SPEAKER_05
 in a way that's compatible with the closed-talking.

0:18:24	SPEAKER_01
 Yeah. We'd simply wanted to determine what's the difference in performance due to it being distant versus those.

0:18:29	SPEAKER_02
 So does that mean you turn off speaker normalization if you let in?

0:18:33	SPEAKER_03
 No, it means...

0:18:35	SPEAKER_05
 you group together the segments that by magic you know belong to one speaker.

0:18:44	SPEAKER_01
 I mean to a lesser extent you had that same magic the other way too, because you have leakage into other microphones, right?

0:18:50	SPEAKER_01
 But it's just you're using the fact that this is where this person is, right?

0:18:55	SPEAKER_05
 But it's just easier to do.

0:18:59	SPEAKER_05
 Well in the new test actually, that's not true.

0:19:02	SPEAKER_05
 Again, if these new segmentations work okay, then it's a fair...

0:19:10	SPEAKER_05
 it's a completely fair...

0:19:12	SPEAKER_01
 So how do you determine what you use to group together to be a...

0:19:16	SPEAKER_05
 You group together all the data coming in through one channel and where Pilos speech detector has determined that there is speech and that speech is deemed to come from that speaker, whether that's true or not.

0:19:31	SPEAKER_05
 So if you get some crosstalk from another microphone then you just processes it as if it were from that speaker.

0:19:38	SPEAKER_01
 The only other alternative would be to turn off speaker adaptation in both.

0:19:42	SPEAKER_05
 Well that's more of a problem.

0:19:44	SPEAKER_05
 I mean because it's...

0:19:45	SPEAKER_05
 you can just pretend it's some kind of...

0:19:49	SPEAKER_05
 I mean you can pretend it's all from one speaker and do all this processing the same but then you're going to get results that are worse on a kind of not doing proper speaker normalization and you're going to have...

0:20:01	SPEAKER_05
 So you could certainly do better than that by doing, for instance, cluster the segments which is what we do, say in a broadcast news system where you don't have speaker labels.

0:20:11	SPEAKER_05
 But that would be another processing step that I would have to debug first and so forth and so we want to avoid that.

0:20:19	SPEAKER_05
 So I agree with you.

0:20:20	SPEAKER_05
 We should do the...

0:20:22	SPEAKER_05
 you know, this sort of cheating experiment.

0:20:26	SPEAKER_01
 Yeah, and so that will tell us what the difference is between the mics and then in order to...

0:20:33	SPEAKER_01
 the other difference that we'd have to take care of is that, yeah, we don't have a mic that is particular to a person and so we'll have to do some clustering and that'll be another issue too.

0:20:51	SPEAKER_01
 But it... I could be wrong but it seems to me that the speaker, the level of degradation that you get from having the distant mic in a normal acoustic is much greater than what you get from, say, not applying speaker adaptation or applying speaker adaptation.

0:21:12	SPEAKER_01
 I think that the... I mean we'll see but I think that the kind of gains that we've seen from speaker adaptation and how five sort of things are like a few percent, right?

0:21:22	SPEAKER_05
 And it's not just speaker adaptation, it's the whole no feature normalization process.

0:21:27	SPEAKER_05
 It's all that is speaker-based.

0:21:30	SPEAKER_05
 You know, so we...

0:21:33	SPEAKER_05
 So in that, I'm...

0:21:37	SPEAKER_05
 you know, the most important, of course, is the capital means attraction.

0:21:42	SPEAKER_05
 Yeah, and that... I don't know if we... we never really... I don't remember because it's so far...

0:21:49	SPEAKER_05
 it's so long ago that we didn't do that on that per speaker basis.

0:21:52	SPEAKER_01
 It doesn't make that much difference, I think.

0:21:54	SPEAKER_01
 I would doubt that it would be a huge amount of difference for that.

0:21:57	SPEAKER_01
 So, I mean, I think that that difference would definitely be marginal.

0:22:01	SPEAKER_01
 I think the main thing is to do something, to do some extra means attraction at some level.

0:22:05	SPEAKER_01
 And so it's different about this processing, just that we're doing it a much longer time scale, right?

0:22:11	SPEAKER_01
 But...

0:22:16	SPEAKER_05
 And by the way, it's... actually, we're already...

0:22:20	SPEAKER_05
 if we use the same segmentations that we use for the first talking microphone, then the segmentations assume that we have access to all channels and cross...

0:22:28	SPEAKER_05
 That's right.

0:22:29	SPEAKER_05
...parallel them.

0:22:30	SPEAKER_05
 So there's no point in not using that knowledge for speaking about that.

0:22:35	SPEAKER_03
 I think also for the log-special means attraction, we want to know which speakers talking when, because we want to chain together the audio from one particular speaker to calculate the mean and subtract it, and we don't...

0:22:48	SPEAKER_01
 Right. Right.

0:22:50	SPEAKER_01
 Okay.

0:22:53	SPEAKER_01
 Yeah, I guess.

0:22:55	SPEAKER_01
 But I also think that, again, once we got into it, that using some kind of clustering, we probably work reasonably well there, too.

0:23:06	SPEAKER_01
 Certainly for the two microphone case, which we're not going to mess with, because it's another whole deal with the locality microphones, we ought to be able to at least tell that it appears that things are coming from a particular direction.

0:23:21	SPEAKER_01
 So we ought to be able to use that information as well.

0:23:25	SPEAKER_01
 So I think we might be able to do not too bad a job of separating out segments that appear to come from a single speaker, both in terms of acoustic similarity and in terms of direction.

0:23:40	SPEAKER_01
 So I mean, but that's another research thing to do, and probably won't get done the next week.

0:23:46	SPEAKER_05
 Right, so what is this schedule here?

0:23:49	SPEAKER_01
 Well, I mean, I'm leaving for the New Orleans meeting next Saturday, and we kind of nice to have some results, at least a day or two before that, so that I could figure out what to say.

0:24:03	SPEAKER_01
 I'll call you when you get there.

0:24:05	SPEAKER_02
 You'll have email, right?

0:24:08	SPEAKER_01
 Yeah.

0:24:10	SPEAKER_01
 Not to mention that Mari's putting together this report next week, too.

0:24:15	SPEAKER_01
 So what we were hoping was that over the weekend, we could do the calculation on the training set, and maybe we could, by the end of the weekend, we could have the top one, and then early next week do these.

0:24:32	SPEAKER_01
 If we had enough machines, maybe do them in parallel, so that by the middle of the week, we had some kind of result.

0:24:38	SPEAKER_01
 I mean, it's one of these Hail Mary kinds of things.

0:24:41	SPEAKER_01
 I mean, it might not work out, but I figured I may as well ask for it.

0:24:50	SPEAKER_05
 So I'll ask the other thing is, and I'll ask Don, which is easier to process in terms of creating the test data for the far microphone.

0:25:03	SPEAKER_05
 If it turns out that for some reason, that's easier for him to use the old segmentations, then we'll just use that, I figure.

0:25:19	SPEAKER_01
 Right.

0:25:20	SPEAKER_02
 So I don't want you to have to be burdened with doing a lot of stuff.

0:25:25	SPEAKER_02
 What can I do to...

0:25:29	SPEAKER_02
 You said it would be easy for you to do that top one there, and I guess Don can do the segmentations of the channel app.

0:25:40	SPEAKER_02
 I can certainly help with retraining the short-male models once we have the new data.

0:25:47	SPEAKER_01
 You have models or short-mails?

0:25:52	SPEAKER_05
 Right.

0:25:56	SPEAKER_05
 Let's see.

0:26:00	SPEAKER_05
 You could run the...

0:26:07	SPEAKER_05
 Basically, once the top one is done, you could easily rerun the whole set of experiments, manage the jobs and so forth.

0:26:21	SPEAKER_02
 The bottom of it would just be a matter of pointing at it and set a file and kicking it off.

0:26:26	SPEAKER_02
 So that would be...

0:26:27	SPEAKER_02
 Not the bottom of it, but the middle one would be really easy once you've got the top one going.

0:26:31	SPEAKER_02
 I could do that.

0:26:33	SPEAKER_02
 Right.

0:26:34	SPEAKER_02
 I guess I just need to get Don to...

0:26:38	SPEAKER_05
 So somehow, assuming he uses the new naming scheme, then he should call the waveforms...

0:26:48	SPEAKER_05
 So the waveforms have the meeting ID and the microphone.

0:26:55	SPEAKER_05
 I guess the channel and the microphone and the speaker...

0:27:03	SPEAKER_05
 some something that identifies the speaker.

0:27:07	SPEAKER_05
 So...

0:27:08	SPEAKER_02
 Keep it the same, but just change it all to the channel app.

0:27:11	SPEAKER_05
 Exactly.

0:27:12	SPEAKER_05
 So you still need to be able to distinguish the different speakers.

0:27:15	SPEAKER_05
 I'm going to go back to the point because if you want to do what we just discussed, the easiest way to do that would be to just make the channel app, but then keep the speaker names the same as they would be in the old, in the close talking version.

0:27:32	SPEAKER_02
 Okay.

0:27:33	SPEAKER_02
 And so that's something that Don would do when he creates the speaker.

0:27:36	SPEAKER_02
 Right.

0:27:37	SPEAKER_02
 Exactly.

0:27:38	None
 Okay.

0:27:39	SPEAKER_02
 So will you talk to him about that?

0:27:42	SPEAKER_05
 I'll get dark too.

0:27:46	SPEAKER_02
 And then the bottom one in terms of the test will be...

0:27:53	SPEAKER_02
 that will just be a copy of the one above it except for different models.

0:27:59	SPEAKER_03
 We also have to means attract the test data.

0:28:03	SPEAKER_02
 Okay.

0:28:05	SPEAKER_02
 So we need to run...

0:28:07	SPEAKER_02
 Okay.

0:28:08	SPEAKER_02
 Well, once we have the new...

0:28:12	SPEAKER_02
 Once I do that, second experiment will have the files.

0:28:16	SPEAKER_02
 And I can give you those to process.

0:28:18	SPEAKER_03
 Okay.

0:28:19	SPEAKER_03
 And so the way this means subtraction expects to work is it expects to have...

0:28:25	SPEAKER_03
 this continuous stream of audio data from a particular speaker to operate on.

0:28:29	SPEAKER_03
 And it goes along with the sliding window, calculating the mean, using the data in the window, and then subtracting that.

0:28:35	SPEAKER_02
 So I mean...

0:28:38	SPEAKER_02
 I'll create this continuous stream from the individual utterance file.

0:28:40	SPEAKER_03
 That's how I've been doing it just by concatenating files together.

0:28:43	SPEAKER_03
 And if these files...

0:28:45	SPEAKER_03
 And since their individual utterance files, long silence periods are removed, which is a good thing, because this method might estimate the mean badly if you had to face long silence periods.

0:28:55	SPEAKER_03
 But that does mean that I need as much...

0:28:58	SPEAKER_03
 I need twice as much disk space as the original set.

0:29:01	SPEAKER_03
 Because I need... Well, I'm running it because I need to create this intermediate set of these big files.

0:29:07	SPEAKER_03
 And then, finally, the mean subtracted little files.

0:29:12	SPEAKER_03
 And then I can get rid of the big files.

0:29:15	SPEAKER_03
 But while I'm doing the processing, I need twice as much disk space.

0:29:20	SPEAKER_02
 Okay. I'll check with Markam and see what happened with the disks.

0:29:25	SPEAKER_02
 He went to persona a couple of weeks ago and something.

0:29:28	SPEAKER_01
 On vacation. I'll check the tape.

0:29:31	SPEAKER_02
 You haven't seen new disks pop up, have you?

0:29:34	SPEAKER_04
 No, I was wondering if they're in the big roller-launchies then.

0:29:38	SPEAKER_01
 But they were like mushrooms.

0:29:41	SPEAKER_01
 Shake them, popping out.

0:29:43	SPEAKER_02
 He went to put them on and then something happened.

0:29:45	SPEAKER_02
 He sent a note around saying, oh, something...

0:29:48	SPEAKER_02
 He didn't work again. We'll have to schedule another time.

0:29:51	SPEAKER_02
 Nothing happens.

0:29:53	SPEAKER_02
 I'll check with David about that.

0:29:55	SPEAKER_01
 Okay.

0:29:57	SPEAKER_01
 Because we still have that other one going, which is the macro phone.

0:30:02	SPEAKER_03
 Right.

0:30:03	SPEAKER_03
 So, Andreas, in you, Dr. Speech Data, SRI-FogFive, there's this Hub5 training set.

0:30:10	SPEAKER_03
 Is that the long training set there?

0:30:12	SPEAKER_05
 That's everything.

0:30:14	SPEAKER_05
 So I can give you a list of the short version.

0:30:18	SPEAKER_03
 Okay. I think you already did, actually.

0:30:20	SPEAKER_03
 Okay.

0:30:21	SPEAKER_03
 And so say the microphone files that are included in this short training are just a subset of the microphone files, right?

0:30:28	SPEAKER_05
 That's right.

0:30:29	SPEAKER_03
 Okay. So when you did some TI Digits experience training on microphone.

0:30:36	SPEAKER_03
 But that's not necessarily any less data than the SRI Hub5 set.

0:30:41	SPEAKER_03
 It's not a subset of the short SRI Hub5 set, right?

0:30:47	SPEAKER_05
 No, it is.

0:30:50	SPEAKER_05
 Sorry. Can you repeat the question?

0:30:53	SPEAKER_03
 When you trained on microphone to do those Digits experiments, did you use the entire microphone purpose?

0:30:59	SPEAKER_05
 Only the portion that was in the Hub5 training set.

0:31:02	SPEAKER_01
 Oh. That was in Hub5 small training set.

0:31:05	SPEAKER_05
 Well, the Hub5 small training set contains as much microphone as the large training set for historical reasons.

0:31:11	SPEAKER_05
 Yeah.

0:31:13	SPEAKER_03
 Okay. So.

0:31:17	SPEAKER_05
 Sorry, you have that process, right?

0:31:19	SPEAKER_05
 Because you already did that.

0:31:20	SPEAKER_05
 Did you already do that experiment?

0:31:22	SPEAKER_03
 I got confused because I thought you were using the whole microphone set.

0:31:27	SPEAKER_03
 Okay. Well, if I just need to use that subset, I can get it processed.

0:31:32	SPEAKER_03
 I actually got, I think I got into it before and then I thought I was doing the wrong thing and I stopped and it shouldn't take that long to do it.

0:31:40	SPEAKER_05
 Right.

0:31:42	SPEAKER_05
 Okay. Have you need only the mail?

0:31:46	SPEAKER_02
 So basically, Dave, so for you to get your processing going, you need the list of the wave, I guess it'll be, you don't need to get the segmentations.

0:31:55	SPEAKER_02
 Yeah.

0:31:56	SPEAKER_02
 You're going to whether we're using anywhere the old from dawn.

0:31:59	SPEAKER_02
 And then from that, you need the, from the segmentations, you'll have the list of wave files that the short set is trained on.

0:32:08	SPEAKER_02
 And then you'll need this space.

0:32:10	SPEAKER_02
 And once you've got those things, then you can start your processing.

0:32:14	SPEAKER_02
 Yeah.

0:32:15	SPEAKER_02
 Okay.

0:32:17	SPEAKER_05
 There's this, this, it's sort of, it's not very nice to use the small train set for another reason, which is that the, you also add losing on, again, because you don't use all the data you have for one speaker.

0:32:33	SPEAKER_05
 So the normalizations you compute for your train speakers will be, uh, crumbier than the word in the large train set.

0:32:42	SPEAKER_05
 So, um, I have to solve, to make it really matching experiment, I have to find, uh, I have to use short models that were trained on normalizations that were also only estimated on the short set, which is, I think so, I, I have to check.

0:33:06	SPEAKER_05
 In any case, I could retrain short models within a few hours, actually, if I use, I wonder about that though.

0:33:13	SPEAKER_01
 I mean, because all we're doing, the only reason we're using a short training set is, is for speed.

0:33:20	SPEAKER_01
 And they're, we're not really making any claims about using a smaller training set.

0:33:25	SPEAKER_01
 So as long as we're not using any testing data from...

0:33:28	SPEAKER_05
 But the thing is, if, if we use, if we use the whole training set for normalizations, then David would have to process much more data, which, that's, that's one bottleneck for us, right?

0:33:41	SPEAKER_05
 Oh, you mean for, for his normalizations?

0:33:43	SPEAKER_01
 Yeah.

0:33:44	SPEAKER_01
 Oh, oh, oh, I'm trying.

0:33:45	SPEAKER_05
 Right. So you want to do the exact same thing.

0:33:47	SPEAKER_05
 Right. Well, you have apples and oranges.

0:33:50	SPEAKER_00
 Yeah.

0:33:57	SPEAKER_05
 It doesn't make, I don't think it makes that much of a difference.

0:33:59	SPEAKER_05
 It's just this little detail that, if you can take care of that, then you should.

0:34:05	SPEAKER_05
 I think I have, I have the model site.

0:34:08	SPEAKER_05
 I have, um...

0:34:11	SPEAKER_05
 Yeah, and if not, I can retrain those models very...

0:34:17	SPEAKER_02
 Oh, there's, there's one other issue.

0:34:20	SPEAKER_02
 And that is that David throws out speakers that have less than 12 seconds of training data.

0:34:28	SPEAKER_02
 And he said there were a few in the Macaphone set like that.

0:34:33	SPEAKER_02
 So do we need to wait to find out who he's going to throw out so that we create a new set of short models that don't include those speakers?

0:34:45	SPEAKER_02
 Uh...

0:34:46	SPEAKER_02
 Again, sorry, I messed up.

0:34:47	SPEAKER_02
 So, and the problem is that if we proceed like we just described, um, when he goes to create the new training data with his processing, he throws out some speakers, so the two training sets won't be identical.

0:35:04	SPEAKER_01
 Yeah, he throws out some speakers that are very small.

0:35:07	SPEAKER_05
 Yeah, I don't think it'll make a better.

0:35:10	SPEAKER_01
 Yeah, I think they've sewn a few.

0:35:12	SPEAKER_05
 In fact, I thought about throwing those out too, because when I heard how they'd speech there was for some of them, I thought they could only heard your models, because again, their normalizations will be all over the map.

0:35:23	SPEAKER_05
 And you won't get very, very clean models from the anyhow, so...

0:35:30	SPEAKER_02
 Do you think it's okay then?

0:35:31	SPEAKER_05
 Yeah, in fact, if you want to do this, just speed things up.

0:35:39	SPEAKER_05
 We can leave out the Macaphone data altogether.

0:35:43	SPEAKER_05
 That hurt...

0:35:44	SPEAKER_05
 Actually, oh no, sorry, not in the short, then you have to do little data.

0:35:48	SPEAKER_05
 Okay, sorry, forget that.

0:35:51	SPEAKER_05
 When you go to the large training set, then leaving on Macaphone actually sometimes helps you, because it's just not relevant to the meaning or to conversational speech anyway.

0:36:06	SPEAKER_05
 Okay, yeah, leave it out.

0:36:08	SPEAKER_05
 And in the event that I retrain, the short models, why don't you give me a list of the files that you throw out, and I'll throw them out too.

0:36:17	SPEAKER_05
 And then we have to complete the identical training conditions.

0:36:22	SPEAKER_02
 Actually, you should be able to figure out, Dave, right, once you know the segmentations, who you're going to, which speakers will get left out even before you run your process.

0:36:33	SPEAKER_02
 The segmentations?

0:36:34	SPEAKER_02
 Yeah, the segmentations from Don.

0:36:36	SPEAKER_05
 The segmentations are only... they only affect the test set.

0:36:39	SPEAKER_05
 We're talking about the training speakers.

0:36:41	SPEAKER_01
 No, the training's going through right now, see how long now.

0:36:45	SPEAKER_02
 Right, I'm just wondering how long it will take to get that information.

0:36:50	SPEAKER_05
 You already have it.

0:36:52	SPEAKER_05
 You already have it.

0:36:53	SPEAKER_03
 I have it for Macrophone already, I think.

0:36:58	SPEAKER_03
 And I think by tomorrow I'll have it for the rest.

0:37:04	SPEAKER_00
 All right.

0:37:13	SPEAKER_01
 That's that one, maybe.

0:37:19	SPEAKER_01
 We're looking at synthesizers.

0:37:22	SPEAKER_04
 You were like, yeah, I was doing something for the smart computer collection as Robo was taking his laptop back to Germany, so we needed a new synthesis machine.

0:37:32	SPEAKER_04
 And we have now a sun workstation in the library, which does the synthesis and the festivals.

0:37:36	SPEAKER_04
 So Roboch is...

0:37:38	SPEAKER_04
 His laptop, which we used for the smart computer collection for the synthesis.

0:37:44	SPEAKER_04
 And so he took it to Germany.

0:37:47	SPEAKER_04
 And so we couldn't do any data collection.

0:37:49	SPEAKER_04
 Is he gone now?

0:37:50	SPEAKER_04
 No, he's just gone to a smart computer workshop.

0:37:53	SPEAKER_04
 Oh, oh.

0:37:54	SPEAKER_04
 And so we have now the sun in the library, which can do that.

0:37:58	SPEAKER_04
 And I looked into the F-0 thing and talked to Liz.

0:38:03	SPEAKER_04
 And it seems that it's quite what she wants, but we'll have to think about the energy thing.

0:38:11	SPEAKER_01
 This was a business about coming up with something that was purely prasadic.

0:38:17	SPEAKER_01
 And so I'm just going to use pitch detector, drive a synthesizer.

0:38:22	SPEAKER_01
 And since it doesn't have a hook in it for modifying energy, you'll have a little box at the output that will modify the energy.

0:38:29	SPEAKER_01
 So...

0:38:33	SPEAKER_01
 I think, okay.

0:38:36	SPEAKER_02
 Are you interfacing to that thing with the C++ routines?

0:38:40	SPEAKER_02
 Is there another interface that you use?

0:38:42	SPEAKER_04
 For festival?

0:38:43	SPEAKER_04
 Yeah.

0:38:44	SPEAKER_04
 You can just use it from the...

0:38:46	SPEAKER_04
 Yeah, basically from the command line and defining the phones, whatever you want to have synthesized.

0:38:52	SPEAKER_04
 And if the F-0 targets and then it keeps on away from, and I want to manipulate the away from them.

0:39:00	SPEAKER_02
 Oh, great.

0:39:06	SPEAKER_01
 Okay.

0:39:07	SPEAKER_01
 Did it?

0:39:17	SPEAKER_02
 Okay.

0:39:23	SPEAKER_02
 Transcript L-288.

0:39:26	SPEAKER_02
 523-817-719.

0:39:29	SPEAKER_02
 423-158823.

0:39:33	SPEAKER_02
 766-608-2212.

0:39:37	SPEAKER_02
 6926-9249.

0:39:41	SPEAKER_02
 5736-010645.

0:39:45	SPEAKER_02
 666-9118.

0:39:48	SPEAKER_02
 022-676651.

0:39:52	SPEAKER_03
 Transcript L-290.

0:39:55	SPEAKER_03
 428-0231313823526059881.

0:40:05	SPEAKER_03
 1585994156.

0:40:10	SPEAKER_03
 887-792-722.

0:40:15	SPEAKER_03
 397-333226.

0:40:19	SPEAKER_03
 006-380331.

0:40:24	SPEAKER_03
 836-3646-7473.

0:40:29	SPEAKER_03
 592-008279.

0:40:35	SPEAKER_00
 Transcript L-280.

0:40:38	SPEAKER_00
 530-464438.

0:40:43	SPEAKER_00
 6384-9850-9939709093.

0:40:51	SPEAKER_00
 597-5.

0:40:53	SPEAKER_00
 021-5.

0:40:54	SPEAKER_00
 313-3.

0:40:56	SPEAKER_00
 816-6.

0:40:58	SPEAKER_00
 974-847-973.

0:41:03	SPEAKER_00
 274-159-1269.

0:41:08	SPEAKER_00
 2425-5719-00.

0:41:13	SPEAKER_00
 517-397-0618.

0:41:19	SPEAKER_05
 Transcript L-281.

0:41:22	SPEAKER_05
 043-180781459.

0:41:26	SPEAKER_05
 297-1466204.

0:41:31	SPEAKER_05
 036-597-6247.

0:41:35	SPEAKER_05
 5215-841166.

0:41:39	SPEAKER_05
 4621-4664.

0:41:43	SPEAKER_05
 9402-12332073.

0:41:48	SPEAKER_05
 9778-047295.

0:41:53	SPEAKER_05
 378-797-3466.

0:41:57	SPEAKER_04
 Transcript L-208.

0:42:00	SPEAKER_04
 292-269634.

0:42:05	SPEAKER_04
 6917-29778-2.

0:42:10	SPEAKER_04
 283-491-277.

0:42:14	SPEAKER_04
 490-8696432.

0:42:18	SPEAKER_04
 966-704940.

0:42:23	SPEAKER_04
 6387-2826-9059.

0:42:28	SPEAKER_04
 177-586-814.

0:42:32	SPEAKER_04
 566-656809.

0:42:38	SPEAKER_01
 Transcript L-287-18388-8155.

0:42:45	SPEAKER_01
 2309-336109.

0:42:49	SPEAKER_01
 368510-3452.

0:42:53	SPEAKER_01
 1143936702.

0:42:57	SPEAKER_01
 1571-00728896.

0:43:01	SPEAKER_01
 2246-5913.

0:43:05	SPEAKER_01
 0721-4598.

0:43:08	SPEAKER_01
 6454-116336.

0:43:14	SPEAKER_01
 It's all folks.

