0:00:00	SPEAKER_03
 Alright, we're off.

0:00:04	SPEAKER_04
 Just test test.

0:00:06	SPEAKER_04
 Just test me.

0:00:08	SPEAKER_04
 Yeah.

0:00:09	None
 Okay.

0:00:10	None
 Okay.

0:00:11	SPEAKER_04
 So, there's two sheets of paper in front of us.

0:00:24	SPEAKER_04
 This is the arm wrestling?

0:00:26	SPEAKER_00
 Yeah, we formed a coalition, actually.

0:00:29	SPEAKER_00
 We already made it into one.

0:00:31	SPEAKER_04
 Oh, good.

0:00:32	SPEAKER_04
 Excellent.

0:00:33	SPEAKER_04
 That's the best thing.

0:00:34	SPEAKER_04
 Tell me about it.

0:00:37	SPEAKER_02
 So, it's well, it's spectacular.

0:00:39	SPEAKER_02
 Subjection or we know, finish, right?

0:00:41	SPEAKER_02
 We're bending on the big bits.

0:00:46	SPEAKER_02
 Each risk where I'm at.

0:00:49	SPEAKER_02
 Perfect.

0:00:50	SPEAKER_02
 And then with the estimation of knowing depending on the arm, it's moving at a time.

0:00:57	SPEAKER_02
 It's working out frequency.

0:01:00	SPEAKER_02
 It's very simple.

0:01:02	SPEAKER_02
 And the best result.

0:01:06	SPEAKER_02
 But you see, you're on 50 bands with a winner filter.

0:01:13	SPEAKER_02
 And there is no noise addition.

0:01:15	SPEAKER_02
 It's good because it's difficult.

0:01:19	SPEAKER_02
 Are you looking at one or two?

0:01:26	SPEAKER_02
 Yeah, so the sheet that gives you 3.66.

0:01:30	SPEAKER_02
 The second sheet, this, about the same, it's the same idea, but it's working on med bands.

0:01:41	SPEAKER_02
 And it's a spectrosupportion instead of a winner filter.

0:01:45	SPEAKER_02
 And there is also a noise addition after cleaning up the main bits.

0:01:52	None
 Well, there is a term.

0:01:55	None
 Let me know.

0:01:57	SPEAKER_04
 Yeah, I mean, it's actually very similar.

0:02:05	SPEAKER_04
 I mean, if you look at databases, the one that has the smallest.

0:02:14	SPEAKER_04
 The one that has the smallest overall number is actually better on the finish and Spanish.

0:02:21	SPEAKER_04
 But it is worse on the, I mean, on the DI digits.

0:02:32	SPEAKER_04
 So, probably doesn't matter that much either way.

0:02:40	SPEAKER_04
 But when you say unified, do you mean it's one piece of software now?

0:02:45	SPEAKER_02
 So no, we are already setting up software.

0:02:50	None
 Should be ready.

0:02:54	SPEAKER_03
 So, what's happened?

0:02:55	SPEAKER_03
 I think I've missed something.

0:02:57	SPEAKER_04
 Okay, so a week ago, maybe you weren't around when you and I came close here.

0:03:01	SPEAKER_04
 You didn't go to your night?

0:03:02	SPEAKER_04
 Yeah, I didn't.

0:03:03	SPEAKER_04
 Oh, okay, so yeah, let's summarize.

0:03:05	SPEAKER_04
 And then if I summarize, somebody can tell me if I'm wrong, which will also be possibly helpful.

0:03:09	SPEAKER_04
 What if I just press here?

0:03:10	SPEAKER_04
 I hope this is still working.

0:03:15	SPEAKER_04
 We looked at, after coming back from Qualcomm, we had very strong feedback.

0:03:23	SPEAKER_04
 And I think it was, he didn't think there was an eye opinion also that, you know, we sort of spread out to look at a number of different ways of doing the only suppression.

0:03:33	SPEAKER_04
 But given limited time, it was sort of time to choose one.

0:03:40	SPEAKER_04
 And so the retail series hadn't really worked out that much.

0:03:46	SPEAKER_04
 The subspace stuff had not worked with so much.

0:03:51	SPEAKER_04
 So it sort of came down to spectral subtraction versus refiltoring.

0:03:55	SPEAKER_04
 We had a long discussion about how they were the same and how they were completely different.

0:04:00	SPEAKER_04
 And I mean, fundamentally, the same sort of thing, but the math is a little different so that there's an exponent difference in, you know, what's the ideal filtering, depending on how you construct the problem.

0:04:14	SPEAKER_04
 And I guess after that meaning sort of made more sense to me, because if you're dealing with powerspectra, then how are you going to choose your error?

0:04:26	SPEAKER_04
 And typically you'll do something like variance. So that means there'll be something like the square of the power spectra.

0:04:32	SPEAKER_04
 Whereas when you're doing the, looking at it the other way, you're going to be dealing with signals and you're going to have to looking at power of noise power that you're trying to reduce.

0:04:43	SPEAKER_04
 So there should be a difference of, you know, conceptually of a factor too in the exponent.

0:04:50	SPEAKER_04
 But there's so many different little factors that you adjust in terms of over subtraction and so forth that arguably you're, and the choice of do you operate on the Melvans or the operate on the FFT beforehand.

0:05:12	SPEAKER_04
 There are so many other choices to make that are almost, well, if not independent, certainly in addition to the choice of whether you, the respective subtraction or when you're filtering, that again, we sort of felt the gang should just sort of figure out which it is they want to do.

0:05:30	SPEAKER_04
 And then let's pick it, go forward with it. So that was, that was last week. And we said, take a week, go arm wrestle, you know, figure it out.

0:05:40	SPEAKER_04
 And the joke there was that each of them had specialized in one of them. And so they, so instead they went to your somebody and bonded and they can go to a single, single piece of software.

0:05:51	SPEAKER_04
 Another, another victory for international collaboration.

0:05:57	SPEAKER_03
 So so you guys have combined for you're going to be combining software.

0:06:01	SPEAKER_00
 Well, the piece of software has like plenty of options like you can pass command line arguments. So depending on that, it becomes either spec with subtraction or we're filtering.

0:06:10	SPEAKER_04
 Well, that's fine. But the important thing is that there is a piece of software that you that we all will be using. Yeah.

0:06:16	SPEAKER_02
 Yeah, it's just one piece of software.

0:06:18	SPEAKER_02
 And we want to make optimize parameters. Sure. But still so there is a piece of software.

0:06:33	SPEAKER_02
 How is how good is that? I don't have a sense of. It's just one person. Best system. It's between. We are single.

0:06:50	SPEAKER_04
 Yeah. But compared to the last evaluation, which we started for before, but we were considerably far behind. And the thing is this doesn't have neural net in yet, for instance.

0:06:59	SPEAKER_04
 So it's so it's it's not using our full ballot bag of tricks, if you will. And it is very close performance to the best thing that was there before.

0:07:11	SPEAKER_04
 But you know, looking at another way, maybe more importantly, we didn't have any explicit noise handling stationary dealing with we didn't explicitly have anything to deal with stationary noise. And now we do.

0:07:26	SPEAKER_03
 So will the neural net operate on the output from either the weener filtering or the spectral subtraction? Well, so so arguably what we should do, I mean, I gather you have, it sounds like you have a few more days of nailing things down with the software and so on.

0:07:43	SPEAKER_04
 But but arguably what we should do is, even though the software can do many things, we should for now pick a set of things, these things, I would guess.

0:07:54	SPEAKER_04
 And not change that and then focus on everything that's left. And I think you know that our goal should be by next week when he comes back to really just to have a firm path for the for the time he's gone of what things will be attacked.

0:08:14	SPEAKER_04
 So I would I would think that what we would want to do is not thoughts with this stuff for a while because what will happen is we'll change many other things in the system. And then we'll probably want to come back to this and possibly make some other choices.

0:08:27	SPEAKER_04
 But just conceptually where does the neural net do you want to run it on the output of the spectrally subtraction? Well, depending on its size, well, one question is, is it on the server side or is it on the terminal side?

0:08:41	SPEAKER_04
 If it's on the server side, we probably don't have to worry too much about size. So that's kind of an argument for that. We do still, however, have to consider its latency. So the issue is is, for instance, could we have a neural net that only looked at the past?

0:08:55	SPEAKER_04
 What we've done in the past is to use the neural net to transform all of the features that we use. So this is done early on. This is essentially, I guess it's more or less like a speech enhancement technique here.

0:09:13	SPEAKER_04
 We're just kind of creating new, if not new speech, at least new FFTs that have, which could be turned into speech, that have some of the noise removed. After that, we still do a mess of other things to produce a bunch of features.

0:09:29	SPEAKER_04
 And then those features are not now currently transformed by the neural net. And then the way that we had it in our proposal 2 before, we had the neural net transform features and we had the untransformed features, which, I guess you actually did linearly transform with KLT, but to our Thogon-Lyzen, but they were not processed through a neural net.

0:09:52	SPEAKER_04
 And Stefan's idea with that, as a recall, was that you'd have one part of the feature vector that was very discriminant in another part that wasn't, which would smooth things a bit for those occasions when the testing set was quite different than what you'd change your discriminant features for.

0:10:09	SPEAKER_04
 So all of that is still seems like a good idea. The thing is now we know some other constraints. We can't have unlimited amounts of latency. That's still being debated by people in Europe, but no matter how they end up there, it's not going to be unlimited amounts, so we have to be a little conscious of that.

0:10:32	SPEAKER_04
 So there's the neural net issue, there's the VAD issue, and there's the second stream thing. And I think last time we agreed that those are the three things that have to get focused on.

0:10:47	SPEAKER_03
 What was the issue with the VAD?

0:10:49	SPEAKER_04
 Well, better ones are good.

0:10:52	SPEAKER_03
 And so the default boundaries that they provide are okay, that's not all that great. I guess they still allow 200 milliseconds on either side, is that for the audience?

0:11:04	SPEAKER_02
 So the VAD issue is outside the beginning.

0:11:11	SPEAKER_02
 The speech pose, which is sometimes the speech that comes, you have to pose this down to 1.25 seconds. Wow! More than one second.

0:11:23	SPEAKER_02
 And it seems to us that this way of dropping the beginning and end is not... we can do better. Because this way of dropping the frames that improve for the baseline by 14% and so we already showed that we are currently in the end improve by 100% percent.

0:11:47	SPEAKER_03
 And so the top of the VAD that they provide? No.

0:11:53	SPEAKER_02
 Oh, okay. There's this 14.

0:12:18	SPEAKER_02
 And if we just take only the VAD probably is computed on the clean signal and apply them on the far-being test, the nuances, then results are much better.

0:12:36	SPEAKER_02
 In some case, the VAD is zero already, right?

0:12:40	SPEAKER_03
 So it means that there is still... how much latency does the VAD and... is the signal again?

0:12:52	SPEAKER_02
 Right now it's under undeniable, so it's 14 milliseconds plus the rank ordering should be...

0:13:02	SPEAKER_00
 So we have another 10 frames. The rank ordering, I'm sorry.

0:13:10	SPEAKER_00
 There's this... there's this... the... the filtering of the probabilities on the...

0:13:16	SPEAKER_04
 We don't think the media is going to be... we have 11.

0:13:34	SPEAKER_04
 So, yeah, I was just noticing on this that it makes reference to delay. So what's the... if you ignore... the VAD is sort of in parallel, isn't it?

0:13:48	SPEAKER_04
 With the... I mean, the additive with the... the LDA in the winter filtering.

0:13:54	SPEAKER_00
 Yeah, so what happened right now we removed that delay of the LDA.

0:13:58	SPEAKER_00
 So we're... I mean, if... so which is like... if we reduce the delay of VAD... so the... the final delay is now... is determined by the delay of the VAD.

0:14:08	SPEAKER_00
 Because the LDA doesn't have any delay. So if you reduce the delay of the VAD, I mean, it's like... effectively reducing the delay.

0:14:14	SPEAKER_03
 How much delay was on the LDA?

0:14:18	SPEAKER_00
 So the LDA and the VAD both had 100 milliseconds delay. So when they were in parallel, so which means you pick either one of them.

0:14:24	SPEAKER_00
 The biggest order. So right now that LDA delay is removed.

0:14:29	SPEAKER_04
 And there didn't seem to be any penalty for that. But there didn't seem to be any penalty for making it causal.

0:14:35	SPEAKER_00
 Oh no, it actually made it like 0.1% better or something.

0:14:40	SPEAKER_04
 It's just winter filter is 40 milliseconds today.

0:14:45	SPEAKER_00
 Yeah, so that's the one which Stefan was discussing like...

0:14:48	SPEAKER_00
 The smoothing. Yeah, you smoothed it and then delay the decision by.

0:14:55	SPEAKER_04
 Right, okay. So that's really not bad. So we may in fact... we'll see what they decide.

0:15:00	SPEAKER_04
 We may in fact have the latency time available to have an Earl Madden. I mean, it sounds like we probably will.

0:15:09	SPEAKER_04
 So that'd be good. Because it certainly always helped us before.

0:15:16	SPEAKER_03
 What amount of latency are you thinking about when you see it?

0:15:19	SPEAKER_04
 Well, they're disputing it. They're saying one group is saying 130 milliseconds and another group is saying 250 milliseconds.

0:15:27	SPEAKER_04
 250 is what it was before actually. So some people are lobbying to make it shorter.

0:15:36	SPEAKER_03
 Were you thinking of the 250 or the 137th position?

0:15:41	SPEAKER_04
 Well, when we find that out, it might change exactly how we do it is all.

0:15:46	SPEAKER_04
 I mean, how much effort do we put into making it causal?

0:15:49	SPEAKER_04
 I mean, I think the neural net will probably do better if it looks a little bit of the future.

0:15:54	SPEAKER_04
 But it will probably work to some extent to look only at the past.

0:15:58	SPEAKER_04
 And we limited machine and human time and effort and how much time should we put into that.

0:16:08	SPEAKER_04
 So it'll be helpful if we find out from the standards, folks, whether they're going to restrict that or not.

0:16:16	SPEAKER_04
 But I think at this point our major concern is making the performance better.

0:16:20	SPEAKER_04
 And if something has to take a little longer in latency in order to do it, that's a secondary issue.

0:16:29	SPEAKER_04
 But if we get told otherwise, then we may have to clamp down a bit more.

0:16:42	SPEAKER_00
 So one difference that was there is like we tried computing the delta and then doing the frame dropping.

0:16:50	SPEAKER_00
 The earlier system was do the frame dropping and then compute the delta on the.

0:16:56	SPEAKER_04
 So this kind of an adult. Yeah. Oh, so that's fixed in us.

0:17:01	SPEAKER_00
 Yeah. So we have now delta and then so the frame dropping is the last thing that we do.

0:17:07	SPEAKER_00
 So we have what we do is we compute the silence probability converted into that binary flag.

0:17:12	SPEAKER_00
 And then in the end you up sampled it to match the final features number of.

0:17:19	SPEAKER_00
 It seems to be helping on the well match condition.

0:17:21	SPEAKER_00
 So that's why this improvement I got from the last result.

0:17:24	SPEAKER_00
 So and it actually reduced a little bit on the high mismatch.

0:17:27	SPEAKER_00
 So the final weight is it's better because the well match is the weighted more than.

0:17:33	SPEAKER_04
 So I mean you were doing a lot of changes. Did you happen to notice how much.

0:17:38	SPEAKER_04
 Change was due to just this frame dropping problem.

0:17:41	SPEAKER_00
 You had something on it, right?

0:17:44	SPEAKER_02
 Just a friend dropping problem. Yeah, but it's difficult. Sometimes we would change to things together.

0:17:54	SPEAKER_04
 But it's around maybe it's less than my personal.

0:17:58	SPEAKER_04
 Yeah.

0:17:59	SPEAKER_04
 But like we're saying there's four or five things like that.

0:18:03	SPEAKER_04
 Pretty shocked. Soon you're talking real improvement.

0:18:09	SPEAKER_02
 And the proposal that you're not that flexible. So working on that.

0:18:14	SPEAKER_01
 Mm hmm.

0:18:17	SPEAKER_04
 Oh, that's a real good point.

0:18:21	SPEAKER_02
 You can be the same guy.

0:18:25	SPEAKER_04
 Might be hard if it's at the server side, right?

0:18:30	SPEAKER_02
 Well, we can be the friend dropping server side or we can't just be careful.

0:18:42	SPEAKER_03
 Okay.

0:18:45	SPEAKER_03
 You have.

0:18:47	SPEAKER_03
 So when you maybe I don't quite understand how this works, but.

0:18:52	SPEAKER_03
 Couldn't you just send all of the frames, but mark the ones that are supposed to be dropped.

0:18:56	SPEAKER_03
 Because you have a bunch more bandwidth, right?

0:18:59	SPEAKER_04
 Well, you could, you know, I mean, it it always seemed to us that it would be kind of nice to in addition to reducing insertions actually use a plus bandwidth.

0:19:09	SPEAKER_04
 But nobody seems to care about that in this evaluation.

0:19:14	SPEAKER_03
 That's why the net use.

0:19:17	SPEAKER_03
 If the net's on the server side, then it could use all of the frames.

0:19:22	SPEAKER_00
 Yes, it could be like, you mean you just transmit everything and then finally drop the frames after the neuralite, right?

0:19:28	SPEAKER_00
 Yeah, that's one thing which you could even mark them.

0:19:32	SPEAKER_00
 Yeah, right now we have the server.

0:19:34	SPEAKER_00
 Right now what we did is like we just we just have this additional bit which goes along the features saying it's currently it's a speech or a non speech.

0:19:43	SPEAKER_00
 So there is no frame dropping till the final features like including the deltas are computed.

0:19:47	SPEAKER_00
 And after the delta's are computed, you just pick up the ones that are marked silence and then drop them.

0:19:53	SPEAKER_04
 So be more or less the same thing with the neural net, I guess.

0:19:56	SPEAKER_00
 So that's what that's what this is doing right now.

0:20:00	SPEAKER_00
 Yeah.

0:20:02	SPEAKER_04
 Okay.

0:20:09	SPEAKER_04
 So what's that's good set of work that.

0:20:16	SPEAKER_00
 Just one more thing like should we do something more for the noise estimation because we still.

0:20:21	SPEAKER_04
 Yeah, I was wondering about that. I hate to written that down there.

0:20:26	None
 Actually, I did this experiment.

0:20:29	SPEAKER_02
 We just used in frames.

0:20:32	SPEAKER_02
 We take the first 15 frames of the judgements.

0:20:35	SPEAKER_04
 Yeah.

0:20:38	SPEAKER_02
 I tried just breaking the.

0:20:51	SPEAKER_02
 But of course, I didn't play.

0:21:00	SPEAKER_04
 Yeah, well, it's not surprising to be worse first time, but it does seem like some compromise between always depending on the first 15 frames and always depending on pause is a good idea.

0:21:26	SPEAKER_04
 Maybe you have to wait the estimate from the first 15 frames more heavily than was done in your first attempt.

0:21:38	SPEAKER_04
 Yeah.

0:21:41	SPEAKER_04
 I mean, do you have any way of assessing how well or how poorly the noise estimation is currently doing?

0:21:55	SPEAKER_02
 Yeah.

0:21:58	SPEAKER_00
 Was there any experiment with because I did the only experiment what I tried was I used the channel zero-watt for the noise estimation and frame dropping.

0:22:08	SPEAKER_00
 So I don't have a split like which one help more.

0:22:13	SPEAKER_00
 So it was the best result I could get.

0:22:19	SPEAKER_04
 So that's something you could do with this final system, right?

0:22:26	SPEAKER_04
 Just do this everything that is in this final system except use the channel zero for the noise estimation.

0:22:35	SPEAKER_00
 Yeah.

0:22:37	SPEAKER_04
 And then see how much better it gets.

0:22:40	SPEAKER_04
 If it's essentially not better than it's probably not worth.

0:22:45	SPEAKER_00
 Yeah, but the Ginters argument is slightly different. It's like even if I use a channel zero-watt, I'm just averaging the the pause spectrum.

0:22:54	SPEAKER_00
 But the Ginters argument is like if it is a non stationary segment, then he doesn't update the noise spectrum.

0:23:00	SPEAKER_00
 So it's like it tries to capture only the stationary part.

0:23:04	SPEAKER_00
 And so the averaging is like different from updating the noise spectrum only during stationary segments.

0:23:12	SPEAKER_00
 So the Ginters was arguing that even if you have a very good VAD averaging it like over the whole thing is not a good idea.

0:23:19	SPEAKER_00
 Because you are averaging the stationary and the non stationary and finally you end up getting something which is not really the same.

0:23:25	SPEAKER_00
 Anyway, you can't remove the stationary part from the signal.

0:23:30	SPEAKER_00
 No, using these messages.

0:23:32	SPEAKER_00
 Yeah, so you just update only the stationary components.

0:23:37	SPEAKER_00
 So that's still slight difference from what Ginters is trying in.

0:23:43	SPEAKER_04
 Well, yeah. And also there's just the fact that although we're trying to do very well in this evaluation, we actually would like to have something that worked well in general.

0:23:56	SPEAKER_04
 And relying on having 15 frames at the front or something is pretty.

0:24:02	SPEAKER_04
 I mean, you might not.

0:24:08	SPEAKER_04
 So it certainly be more robust to different kinds of input if you had at least some updates.

0:24:18	SPEAKER_04
 Well, what do you guys see as being what you would be doing in the next week given what's happened?

0:24:33	SPEAKER_00
 We have the VAD.

0:24:38	SPEAKER_00
 Was that VAD?

0:24:50	SPEAKER_02
 Okay.

0:25:17	SPEAKER_02
 Yeah.

0:25:29	SPEAKER_03
 So I don't remember what you said.

0:25:34	SPEAKER_03
 The answer to my question earlier.

0:25:37	SPEAKER_03
 Were you trained in that on after you've done the spectral subtraction or the different net?

0:25:42	SPEAKER_00
 Yeah, which is a new that's a neural it is some of the VAD net.

0:25:48	SPEAKER_00
 So that VAD was trained on the noisy features.

0:25:52	SPEAKER_00
 So right now we have like we have the cleaned up features so we can have a better VAD by turning the net on the clean up speeches.

0:26:02	SPEAKER_00
 But we need a VAD for noise estimation also.

0:26:13	SPEAKER_03
 Can you use the same net to do both for can use the same net that you that I was talking about to the VAD?

0:26:23	SPEAKER_00
 It actually comes at the very end.

0:26:26	SPEAKER_00
 So the net the final net I mean which is the feature net so that actually comes after a chain of like LDA plus everything.

0:26:33	SPEAKER_00
 So it's like it takes a long time to get a decision out of it and I can actually do it for final frame dropping but not for the V.

0:26:40	SPEAKER_00
 Noise estimation.

0:26:41	SPEAKER_04
 See the idea is that the initial decision to that that you're in silence or speech happens pretty quickly.

0:26:52	SPEAKER_03
 Is that used by somebody's own?

0:26:56	SPEAKER_03
 Yeah, that sort of fed forward and you say well flush everything it's not speech anymore.

0:26:59	SPEAKER_03
 I thought it would be used for doing frame dropping.

0:27:03	SPEAKER_04
 It is used.

0:27:06	SPEAKER_04
 Yeah, it's only used well it's used for frame dropping.

0:27:11	SPEAKER_04
 It's used for end of utterance because you know there's if you have more than 500 milliseconds of of non speech then you figure it's end of utterance.

0:27:21	SPEAKER_04
 Something like that so.

0:27:24	SPEAKER_02
 I see.

0:27:52	SPEAKER_04
 Yeah, so probably the VAD and maybe testing out the noise estimation a little bit and keeping the same method but seeing if the noise estimation could be improved.

0:28:10	SPEAKER_04
 It's sort of related issues.

0:28:12	SPEAKER_04
 It probably makes sense to move from there and then later on in the month I think we want to start including the neural at the end.

0:28:31	SPEAKER_04
 Okay, anything else?

0:28:44	SPEAKER_04
 Didn't fall.

0:28:49	SPEAKER_04
 Our effort would have been devastated.

0:28:54	SPEAKER_04
 So Henik is coming back next week?

0:29:04	SPEAKER_04
 No, no, he's dropped into the US.

0:29:11	SPEAKER_04
 So the idea was that we'd sort out where we're going next with this work before he left on his next trip.

0:29:25	SPEAKER_04
 Good.

0:29:28	SPEAKER_04
 Very you just got through your qual so I don't know if you have much to say but...

0:29:37	SPEAKER_01
 Now just looking into some of the things that John O'Halla and Henik gave as feedback as a starting point for the project.

0:29:51	SPEAKER_01
 In my proposal I was thinking about starting from a set of phonological features or a subset of them.

0:30:00	SPEAKER_01
 It might not be necessarily a good idea according to John. He said these phonological features are sort of figments of imagination also.

0:30:13	SPEAKER_04
 In conversational speech in particular, I think you can put them in pretty reliably in synthetic speech but we don't have too much trouble recognizing synthetic speech since we created in the first place.

0:30:24	SPEAKER_01
 Yeah, so a better way would be something more data-driven just looking at the data and seeing what's similar and what's not similar.

0:30:34	SPEAKER_01
 I'm taking a look at some of Sun Gita's work on traps.

0:30:42	SPEAKER_01
 She did something where the traps she clustered the temporal patterns of certain phonemes in average over many, many contexts and some things tended to cluster.

0:31:03	SPEAKER_01
 So I've stopped Constance clustered very well. Silence was by its own self and Vocalic was clustered.

0:31:15	SPEAKER_01
 So those are interesting things.

0:31:18	SPEAKER_03
 Now you're sort of looking to try to gather a set of these types of features.

0:31:23	SPEAKER_01
 Right, yeah. See where I could start off from a set of small features and continue to iterate and find a better set.

0:31:43	SPEAKER_04
 Okay, well, short meeting. It's okay. So next we hopefully will get Tina Kear to join us.

0:31:53	SPEAKER_03
 Digits.

0:31:57	SPEAKER_04
 Digits.

0:32:01	SPEAKER_04
 Okay, let me go ahead and get my glasses on so I can see. Okay. Transcript L-327.

0:32:13	SPEAKER_04
 821-067-40000-800-4176-1281-630-224-1912-650-869-4624-8919-1485-6845-388-383-80-4518-829135-0-234-44812-2

0:32:48	SPEAKER_02
 Transcript L-328-9060-3955-984-3165-343-114-4166-4194-33-7655-505-754-075-666-8 630-5487-701-812-831-5734-8703-68

0:33:25	SPEAKER_03
 Transcript L-329-9977-30368-7627-1700-0996-9388-96987-099 2126-937206-672-308-949-8032-631-489-6444-2669-318-791-3247

0:34:04	SPEAKER_00
 Transcript L-330-2360-9593-546-348-6675-3704-3844-866-3675-8705-5739-150-9016-22-409-277-701-9206-8967-896701-9206-8967-660 164-191-2428

0:34:39	SPEAKER_01
 Transcript L-331-3788-9100-623-270-385-138-183795-096-6610-197-541-2420-41-11-11-197-142-121-107-8 096-761-0197-541-240-41-5612-309-1-6057-6528-341-164739-4528-8507 Okay, and we're up.

