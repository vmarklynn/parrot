0:00:00	SPEAKER_02
 Channel 3.

0:00:02	SPEAKER_08
 Yes.

0:00:03	SPEAKER_08
 Okay.

0:00:04	None
 Channel 3.

0:00:05	SPEAKER_08
 Channel 3.

0:00:07	SPEAKER_06
 Okay.

0:00:09	SPEAKER_06
 Did you sell speech recognition last week?

0:00:12	SPEAKER_06
 Alright.

0:00:14	SPEAKER_06
 That's the image processing.

0:00:16	SPEAKER_06
 Yes, again.

0:00:18	SPEAKER_07
 We did it again, Morgan.

0:00:20	SPEAKER_06
 Alright.

0:00:22	SPEAKER_00
 Okay.

0:00:29	SPEAKER_06
 It's April 5th.

0:00:37	SPEAKER_06
 Actually, he next to be getting back in town shortly, if he hasn't already.

0:00:41	SPEAKER_06
 Is he going to come here?

0:00:43	SPEAKER_06
 Well, we'll drag him here.

0:00:45	SPEAKER_07
 So when you sit in town, you mean, Oregon?

0:00:48	SPEAKER_06
 Hey, man, you know this end of the world.

0:00:50	SPEAKER_06
 That's really what I meant.

0:00:52	SPEAKER_06
 He's been in Europe.

0:00:54	SPEAKER_06
 So.

0:00:57	SPEAKER_07
 I have something just fairly brief to report on.

0:01:08	SPEAKER_07
 I did some experiments, just a few more experiments before I had to go away for that week.

0:01:17	SPEAKER_07
 Was it last week or whatever?

0:01:20	SPEAKER_07
 So what I was starting playing with was, again, this is the HTK back end.

0:01:26	SPEAKER_07
 I was curious because the way that they train up the models, they go through about four sort of rounds of training.

0:01:36	SPEAKER_07
 And in the first round, they do, I think it's three iterations.

0:01:50	SPEAKER_07
 And for the last three rounds, they do seven iterations of re-estimation in each of those three.

0:01:57	SPEAKER_07
 And so, you know, that's part of what takes so long to train the back end for this.

0:02:03	SPEAKER_07
 And so, you know, it's the first one that's been in the first three rounds of training.

0:02:09	SPEAKER_07
 I guess you could say iterations.

0:02:12	SPEAKER_07
 The first one is three, then seven, seven, and seven.

0:02:16	SPEAKER_07
 And what these numbers refer to is the number of times that the HMM re-estimation is run.

0:02:24	SPEAKER_06
 It's this program called HE Rest.

0:02:27	SPEAKER_07
 So what happens is, at each one of these points, you increase the number of Gaussian's in the model.

0:02:36	SPEAKER_06
 Oh, right. This was the mix up.

0:02:39	SPEAKER_07
 Yes, the mix up.

0:02:41	SPEAKER_07
 And so, in the final one here, you end up with, for all of the digit words, you end up with three mixtures per state.

0:02:53	SPEAKER_07
 In the final thing. So I had done some experiments where I was, I want to play with the number of mixtures.

0:03:00	SPEAKER_07
 But I wanted to first test to see if we actually need to do this many iterations early on.

0:03:09	SPEAKER_07
 And so, I ran a couple of experiments where I reduced that to be three, two, two, five, I think.

0:03:20	SPEAKER_07
 And I got almost the exact same results.

0:03:23	SPEAKER_07
 But it runs much, much faster.

0:03:26	SPEAKER_07
 So I think it only took something like three or four hours to do the full training.

0:03:33	SPEAKER_07
 As opposed to...

0:03:34	SPEAKER_07
 As opposed to what, 16 hours or something like that?

0:03:38	SPEAKER_07
 I mean, you have to do an overnight, basically, the way it is set up now.

0:03:42	SPEAKER_07
 So, even if we don't do anything else, doing something like this could allow us to turn experiments around a lot faster.

0:03:49	SPEAKER_06
 And then when you have your final thing...

0:03:50	SPEAKER_07
 And when you have your final thing, we go back to this.

0:03:53	SPEAKER_07
 So, and it's a real simple change to make.

0:03:56	SPEAKER_07
 I mean, it's like one little text file you edit and change those numbers.

0:04:00	SPEAKER_07
 And you don't do anything else. And then you just run.

0:04:02	SPEAKER_07
 So it's a very simple change to make and it doesn't seem to hurt all that much.

0:04:06	SPEAKER_02
 So...

0:04:07	SPEAKER_02
 You run with three, two, two?

0:04:09	SPEAKER_07
 I have to look to see what the exact numbers were.

0:04:11	SPEAKER_07
 I thought it was like three, two, two, five. But I'll double check.

0:04:15	SPEAKER_07
 It was over a week ago that I did it, so I can't remember exactly.

0:04:20	SPEAKER_07
 But it's so much faster.

0:04:22	SPEAKER_07
 It makes a big difference.

0:04:24	SPEAKER_07
 So we could do a lot more experiments and throw a lot more stuff in there.

0:04:27	SPEAKER_07
 That's great.

0:04:28	SPEAKER_07
 Oh, the other thing that I did was...

0:04:30	SPEAKER_07
 I compiled the HTK stuff for the Linux boxes.

0:04:34	SPEAKER_07
 So we have this big thing that we got from IBM, which is a five processor machine, really fast.

0:04:41	SPEAKER_07
 But it's running Linux. So you can now run your experiments on that machine.

0:04:46	SPEAKER_07
 And you can run five at a time and it runs as fast as five different machines.

0:04:52	SPEAKER_07
 So I've forgotten now what the name of that machine is, but I can send an email around about it.

0:04:59	SPEAKER_07
 And so we've got it now.

0:05:01	SPEAKER_07
 HTK is compiled for both the Linux and for the sparks.

0:05:05	SPEAKER_07
 You have to make sure that in your.cshrc it detects whether you're running on a Linux or a spark and points to the right executables.

0:05:16	SPEAKER_07
 And you may not have had that in your.cshrc before if you were always just running the spark.

0:05:22	SPEAKER_07
 So I can tell you exactly what you need to do to get all of that to work.

0:05:28	SPEAKER_07
 But it'll really increases what we can run on.

0:05:31	SPEAKER_07
 So together with the fact that we've got these faster Linux boxes and it takes less time to do these, we should be able to crank through a lot more experiments.

0:05:42	SPEAKER_07
 So after I did that, then what I wanted to do was try increasing the number of mixers just to see how that affects performance.

0:05:51	SPEAKER_06
 Yeah, in fact, you could do something like keep exactly the same procedure and then add a fifth thing onto it.

0:06:01	None
 Exactly.

0:06:02	SPEAKER_00
 So at the middle where the arrows are showing, that's you're adding one more mixture per state?

0:06:09	SPEAKER_07
 Let's see, it goes from this, let's try to go backwards. At this point it's two mixtures per state.

0:06:21	SPEAKER_07
 So this just adds one except that actually for the silence model, it's six mixtures per state.

0:06:30	SPEAKER_07
 So it goes to two. And I think what happens here is...

0:06:39	SPEAKER_06
 Might be between a shared...

0:06:41	SPEAKER_07
 Yeah, I think that's what it is or something.

0:06:44	SPEAKER_07
 Yeah, it's...

0:06:47	SPEAKER_07
 I can't remember now what happens at that first one. I have to look it up and see.

0:06:53	SPEAKER_07
 Because they start off with an initial model, which is just this global model, and then they split it to the individuals.

0:07:01	SPEAKER_07
 And so it may be that that's what's happening here. I have to look it up and see. I don't exactly remember.

0:07:12	SPEAKER_07
 So that's it.

0:07:13	SPEAKER_06
 Right. So what else?

0:07:19	SPEAKER_02
 Yeah, there was a conference call this Tuesday.

0:07:26	SPEAKER_02
 I don't know yet what happened Tuesday, but the points that they were supposed to discuss is still things like the weights.

0:07:38	SPEAKER_06
 Oh, this is a conference call for Aurora participants, sort of thing. I see.

0:07:44	SPEAKER_06
 Do you know who was... since we weren't in on it, do you know who was in from OGI?

0:07:49	SPEAKER_06
 Was he involved or was it Sunil?

0:07:52	SPEAKER_06
 Yeah, right.

0:07:53	SPEAKER_03
 Oh, you don't know.

0:07:54	SPEAKER_03
 Okay.

0:07:55	SPEAKER_02
 All right.

0:07:58	SPEAKER_02
 Yeah.

0:07:59	SPEAKER_02
 So the points where the weights...

0:08:01	SPEAKER_02
 Oh, to weight, the different error rates that are obtained from different languages and conditions.

0:08:09	SPEAKER_02
 It's not clear that they will keep the same kind of weighting. Right now it's a weighting on improvements.

0:08:16	SPEAKER_02
 Some people are giving that it would be better to have weights on...

0:08:21	SPEAKER_02
 Well, to combine error rates before computing improvement.

0:08:25	SPEAKER_02
 And the fact is that right now for the English, they have weights... they combine error rates.

0:08:33	SPEAKER_02
 And the other language is they combine improvements. It's not the very consistent.

0:08:40	SPEAKER_02
 Yeah.

0:08:41	SPEAKER_02
 And so, well, this is a point.

0:08:47	SPEAKER_02
 And right now, actually, there is a thing also that happens with the current weight.

0:08:52	SPEAKER_02
 It's that very non-significant improvement on the well-matched case, resulting in huge differences in the final number.

0:09:02	SPEAKER_02
 So, perhaps they will change the weights to...

0:09:09	SPEAKER_07
 How should that be done?

0:09:11	SPEAKER_07
 I mean, it seems like there's a simple way...

0:09:16	SPEAKER_07
 This seems like an obvious mistake or something.

0:09:18	SPEAKER_06
 Well, I mean, the fact that it's inconsistent is an obvious mistake.

0:09:21	SPEAKER_06
 But the other thing... I don't know, I haven't thought of through, but one would think that each...

0:09:27	SPEAKER_06
 It's like, if you say, what's the best way to do an average, an arithmetic average or geometric average?

0:09:33	SPEAKER_06
 It depends what you want to show.

0:09:35	SPEAKER_06
 Each one is going to have a different characteristic.

0:09:38	SPEAKER_07
 So...

0:09:39	SPEAKER_07
 It seems like they should do like the percentage improvement or something, rather than the absolute improvement.

0:09:45	SPEAKER_07
 Well, they are doing that.

0:09:47	SPEAKER_06
 No, it is relative.

0:09:49	SPEAKER_06
 But the question is, do you average the relative improvements or do you average the error rates and take the relative improvement of that?

0:09:56	SPEAKER_06
 And the thing is, it's not just a period average because of these ratings.

0:10:01	SPEAKER_06
 It's a weighted average.

0:10:03	SPEAKER_02
 Yeah, and so, when you average the relative improvement, it tends to give a lot of importance to the when-match case, because the baseline is already very good.

0:10:17	SPEAKER_07
 Why don't they not look at improvements, but just look at your scores.

0:10:21	SPEAKER_07
 Figure out how to combine the scores with a weight or whatever, and then give you a score.

0:10:26	SPEAKER_07
 Here's your score.

0:10:27	SPEAKER_07
 And then they can do the same thing for the baseline system. Here's its score.

0:10:29	SPEAKER_07
 And then you can look at...

0:10:30	SPEAKER_06
 Well, that's what you're seeing as one of the things they could do.

0:10:33	SPEAKER_06
 Just when you get all done, I think that they...

0:10:36	SPEAKER_06
 I was in there, but I think they started off this process with an ocean that you should be significantly better than the previous standard.

0:10:44	SPEAKER_06
 And so they said, how much is significantly better and so they said, well, you should have half the errors or something that you had before.

0:10:54	SPEAKER_06
 So it's...

0:10:57	SPEAKER_06
 But it does seem like...

0:10:59	SPEAKER_06
 It does seem like it's more logical to combine them first and then do the...

0:11:03	SPEAKER_02
 Combining error rates?

0:11:04	SPEAKER_02
 Yeah.

0:11:05	SPEAKER_02
 Yeah.

0:11:06	SPEAKER_02
 Well, but there is still this problem of ways.

0:11:10	SPEAKER_02
 When you combine error rates, it tends to give more importance to the difficult cases.

0:11:15	SPEAKER_02
 And some people think that they have different opinions about this.

0:11:21	SPEAKER_02
 The people think that it's more important to look at 12-10% relative improvement on well-matched cases than 12-50% on the mismatched.

0:11:31	SPEAKER_02
 And other people think that it's more important to improve a lot on the mismatch.

0:11:36	SPEAKER_07
 It sounds like they don't really have a good idea about what the final application is going to be.

0:11:41	SPEAKER_06
 Well, you know, the thing is that if you look at the numbers on the more difficult cases, if you really believe that was going to be the predominant use, none of this would be good enough.

0:11:56	SPEAKER_06
 Nothing anybody...

0:11:57	SPEAKER_06
 Whereas you sort of, with some reasonable error recovery, could imagine in the better cases these systems working.

0:12:05	SPEAKER_06
 So, I think the hope would be that it would work well for the good cases and it would have reasonable soft degradation as you got to worse and worse conditions.

0:12:20	SPEAKER_07
 Yeah.

0:12:21	SPEAKER_07
 I guess what I'm...

0:12:23	SPEAKER_07
 I mean, I was thinking about it in terms of if I were building the final product and I was going to test to see which front end I wanted to use.

0:12:30	SPEAKER_07
 Try to wait things depending on the exact environment that I was going to be using the system in if I...

0:12:36	SPEAKER_06
 But no, no, no. I mean, this isn't the operating theater.

0:12:39	SPEAKER_06
 I mean, they don't really know, I think.

0:12:43	SPEAKER_07
 So, if they don't know, doesn't that suggest a way for them to go?

0:12:50	SPEAKER_07
 You assume everything's equal. I mean, you...

0:12:52	SPEAKER_06
 Well, I think one thing to do is to just not rely on a single number to maybe have three numbers.

0:13:00	SPEAKER_06
 And say, here's how much you improve the relatively clean case.

0:13:06	SPEAKER_06
 And here's a real match case and here's how much you...

0:13:11	SPEAKER_07
 So, not try to combine them.

0:13:13	SPEAKER_06
 Yeah, actually it's true. I've forgotten this.

0:13:15	SPEAKER_06
 But, well matched, it's not actually clean. What it is is just that...

0:13:22	SPEAKER_06
 The training and testing are similar.

0:13:24	SPEAKER_06
 So, I guess what you would do in practice is you try to get as many examples of similar sort of stuff as you could.

0:13:31	SPEAKER_06
 And then...

0:13:32	SPEAKER_06
 So, the argument for that being the more important thing is that you're going to try and do that.

0:13:38	SPEAKER_06
 But you want to see how badly it deviates from that when the...

0:13:42	SPEAKER_07
 So...

0:13:45	SPEAKER_07
 So, you should wait those other conditions very, you know, really small.

0:13:48	SPEAKER_07
 But, no, that's more of an information kind of thing.

0:13:51	SPEAKER_06
 That's an argument for it. Let me give you the opposite argument.

0:13:54	SPEAKER_06
 The opposite argument is you're never really going to have a good sample of all these different things.

0:13:58	SPEAKER_06
 Meaning, are you going to have examples with Windows open, half open, full open, going 70, 60, 50, 40 miles an hour on what kind of roads, with what passing you, with...

0:14:09	SPEAKER_06
 I think that you could make the opposite argument that the well-matched case is a fantasy.

0:14:15	SPEAKER_06
 So, I think the thing is that if you look at the well-matched case versus the medium and the mismatched case, we're seeing really, really big differences in performance, right?

0:14:29	SPEAKER_06
 And you wouldn't like that to be the case.

0:14:33	SPEAKER_06
 You wouldn't like this. As soon as you step outside, you know, a lot of the cases...

0:14:36	SPEAKER_07
 A lot of the cases... A little teach him to roll their window up.

0:14:39	SPEAKER_06
 I mean, these cases, if you go from the... I mean, remember the numbers right off, but if you go from the well-matched case to the medium, it's not an enormous difference in the training testing situation.

0:14:55	SPEAKER_06
 And it's a really big performance drop.

0:14:59	SPEAKER_06
 So, yeah, I mean, the reference one, for instance, this is back old on Italian, was like 6% of the error for the well-matched and 18% for the medium matched and 60% for the...

0:15:14	SPEAKER_06
 for how they mismatched.

0:15:17	SPEAKER_06
 And, you know, these other systems, we helped it out quite a bit, but still there's something like a factor or two or something between well-matched and medium matched.

0:15:26	SPEAKER_06
 And so, I think that if what you're... if the goal of this is to come up with robust features, it does mean...

0:15:33	SPEAKER_06
 so you could argue, in fact, that the well-matched is something you shouldn't be looking at at all.

0:15:38	SPEAKER_06
 That the goal is to come up with features that will still give you reasonable performance.

0:15:45	SPEAKER_06
 You know, it's again, gentle degradation, even though the testing condition is not the same as the training.

0:15:53	SPEAKER_06
 So, you know, I could argue strongly that something like the medium mismatch, which is, you know, not pathological, but...

0:16:00	SPEAKER_06
 What was the medium mismatch condition again?

0:16:04	SPEAKER_02
 Yeah, medium mismatch is everything with the far microphone, but trained on low-noisy condition, low speed and stop-car, and tested on high-speed conditions.

0:16:21	SPEAKER_06
 Right, so it's still the same microphone in both cases, but there's a mismatch between the car conditions.

0:16:31	SPEAKER_06
 And that's... you could argue that's a pretty realistic situation, and I'd almost argue for waiting that highest, but the way they have it now, I guess it's... they compute the relative improvement first and then average that with a waiting.

0:16:46	SPEAKER_06
 And so then that makes the highly matched, the really big thing.

0:16:53	SPEAKER_06
 So, since they have these three categories, it seems like the reasonable thing to do is to go across the languages and to come up with an improvement for each of those.

0:17:04	SPEAKER_06
 And say, okay, in the highly matched case, this is what happens, in the... the... so the medium, if this happens, in the highly mismatched, that happens.

0:17:16	SPEAKER_06
 And you should see a gentle degradation through that.

0:17:25	SPEAKER_06
 But I think that... I gather that in these meetings, it's really tricky to make anything... can he policy change?

0:17:38	SPEAKER_06
 Because everybody has their own opinion.

0:17:43	SPEAKER_04
 Yeah.

0:17:46	SPEAKER_02
 So, yeah. But there is probably a big change that we made is that the baseline, they want to have a new baseline, perhaps, which is the MFCC, but with voice activity detector.

0:18:03	SPEAKER_02
 And apparently, some people are pushing to still keep this 50% number, so they want to have at least 50% improvement on the baseline, but it should be a much better baseline.

0:18:21	SPEAKER_02
 And if we look at the result that's summing the sound, just putting the VAD in the baseline improves like more than 20%, which would mean that 50% on this new baseline is like more than 60% improvement.

0:18:37	SPEAKER_06
 So, nobody would be there, probably, right?

0:18:40	SPEAKER_02
 Right, nobody would be there.

0:18:42	SPEAKER_06
 Good. What to do.

0:18:46	SPEAKER_06
 So, who's VAD? Is this...

0:18:51	SPEAKER_02
 They didn't decide yet. I guess this was one point of the conference.

0:18:56	SPEAKER_02
 But...

0:19:00	SPEAKER_02
 Yeah.

0:19:04	SPEAKER_06
 I think that would be good. I mean, it's not that the design of the VAD isn't important, but it does seem to be a lot of work to do a good job on that, as well as being a lot of work to do a good job on the feature design.

0:19:22	SPEAKER_06
 So, if we can cut down on that, maybe we can make some progress.

0:19:28	SPEAKER_02
 But I guess, perhaps...

0:19:35	SPEAKER_02
 Yeah. So, one told that perhaps it's not fair to do that because to make a good VAD, you don't have enough to do the business features.

0:19:53	SPEAKER_02
 So, you really need to put more in the different things.

0:20:00	SPEAKER_06
 Yeah.

0:20:05	SPEAKER_07
 Sure. But I'm confused. What do you mean?

0:20:10	SPEAKER_06
 Yeah. But, let's say, MFCC, for instance, doesn't have anything related to the pitch. So, just for example. So, suppose you've got what you really want to do is put a good pitch detector on there, and if it gets a non-imbeguous...

0:20:25	SPEAKER_06
 Oh, I see.

0:20:26	SPEAKER_06
...if it gets a non-imbeguous result, then you definitely in a region with speech.

0:20:31	SPEAKER_07
 So, there's this assumption that the voice activity detector can only use the MFCC?

0:20:36	SPEAKER_02
 That's not clear, but...

0:20:38	SPEAKER_06
 Well, for the baseline.

0:20:40	SPEAKER_06
 Yeah. So, if you use other features, then it's just a question of what is your baseline?

0:20:45	SPEAKER_06
 What is it that you're supposed to do better than? And so, having the baseline be the MFCCs means that people could choose to pour their effort into trying to do really good VAD.

0:20:56	SPEAKER_06
 But they seem like two separate issues, right?

0:20:59	SPEAKER_06
 I mean, there's sort of separate. Unfortunately, there's coupling between them, which is part of what I think Stefan is getting to is that you can choose your features in such a way as to improve the VAD.

0:21:07	SPEAKER_06
 And you also can choose your features in such a way as to improve recognition.

0:21:11	SPEAKER_06
 But you should do both, right?

0:21:13	SPEAKER_06
 You should do both. And I think that this still makes...

0:21:16	SPEAKER_06
 I still think this makes sense as a baseline.

0:21:18	SPEAKER_06
 It's just saying, as a baseline, we know that we had the MFCCs before, lots of people have done voice activity detectors.

0:21:25	SPEAKER_06
 You might as well pick some voice activity detector and make that the baseline, just like you picked some version of HDK and made that the baseline.

0:21:32	SPEAKER_06
 And then let's try to make everything better. And if one of the ways you make it better is by having your features be better features for the VAD than that's so be it.

0:21:44	SPEAKER_06
 But at least you have a starting point that's...

0:21:48	SPEAKER_06
 Because some of the people didn't have a VAD at all, I guess, right? And then they look pretty bad.

0:21:55	SPEAKER_06
 And in fact, what they were doing wasn't so bad at all.

0:21:58	SPEAKER_07
 Yeah, it seems like you should try to make your baseline as good as possible.

0:22:02	SPEAKER_07
 And if it turns out that you can't improve on that, well, I mean, nobody wins and you just use MFCC, right?

0:22:09	SPEAKER_06
 Yeah, I mean, it seems like...

0:22:12	SPEAKER_06
 It should include sort of the current state of the art that you want to try and improve.

0:22:18	SPEAKER_06
 And MFCCs, or PLP or something, it seems like a reasonable baseline for the features.

0:22:24	SPEAKER_06
 And anybody doing this task is going to have some sort of voice activity detection at some level.

0:22:29	SPEAKER_06
 Some way they might use the whole recognizer to do it, rather than a separate thing.

0:22:34	SPEAKER_06
 But they'll have it on some level.

0:22:38	SPEAKER_07
 Seems like whatever they choose, they shouldn't purposefully brain damage a part of the system to make a worse baseline.

0:22:47	SPEAKER_06
 Well, I think people just... it wasn't that they purposefully brain damage, I think people hadn't really thought through about the VAD issue.

0:22:56	SPEAKER_06
 And then when the proposals actually came in, half of them had VADs and half of them didn't.

0:23:02	SPEAKER_06
 And half the did did well and half the didn't did poorly so.

0:23:07	SPEAKER_02
 Yeah, I'm sorry, we see what happened with this.

0:23:14	SPEAKER_02
 Yeah, so what happened since last week is quite from BOTGID's experiments on VAD on the baseline.

0:23:30	SPEAKER_02
 And these experiments also are using some kind of noise compensation, so spectra-suppraction.

0:23:40	SPEAKER_02
 And putting on nine normalization just after this.

0:23:45	SPEAKER_02
 So having spectra-suppraction, LDA filtering and on nine normalization.

0:23:49	SPEAKER_02
 So which is similar to the proposal one, but with spectra-suppraction in addition.

0:23:56	SPEAKER_02
 And it seems that on nine normalization doesn't help further when you have spectra-suppraction.

0:24:04	SPEAKER_07
 Is this related to the issue that you brought up a couple of meetings ago with the musical tones?

0:24:10	SPEAKER_02
 I have no idea because the issue I brought up was with a very simple spectra-suppraction approach.

0:24:17	SPEAKER_02
 The one that they use at OGI is one from the proposed or a proposal, which might be much better.

0:24:28	SPEAKER_02
 So yeah, I asked Sunil for more information about that, but I don't know yet.

0:24:39	SPEAKER_02
 And what's happened here is that we have this kind of new reference system which use a nice clean-down sampling of sampling, which uses a new filter that's much shorter, and which also gets the frequency bit of 64 hours, which was not done for the proposal.

0:25:02	SPEAKER_02
 When we say we have that, Sunil, have it now too or? No. No. Okay. Because we're still testing. So we have the result for just the features.

0:25:13	SPEAKER_02
 We are currently testing with putting the neural network into Kerti.

0:25:20	SPEAKER_02
 It seems to improve on the well-matched case, but it's a little bit worse on the mismatched, highly mismatched.

0:25:31	SPEAKER_02
 I mean, when we put the neural network, and with the current weight thing, I think it would be better because the one-matched case is better.

0:25:41	SPEAKER_05
 But how much worse since the weighting might change? How much worse is it on the other conditions when you say it's a little worse?

0:25:48	SPEAKER_02
 It's like... 10% relative.

0:25:57	SPEAKER_06
 Okay. But it has the latency is much shorter.

0:26:09	SPEAKER_02
 When I say it's worse, it's not... I compare proposal 2 to proposal 1.

0:26:15	SPEAKER_02
 Putting neural network compared to not having any neural network. This new system is better because as this 64 hours cutoff, clean, don't sampling, and what else? Yeah, good VAD.

0:26:42	SPEAKER_05
 But the latency... but you've got latency shorter now. Yeah. So it's better than the system that we had before.

0:26:53	SPEAKER_02
 Yeah, mainly because of the 64 hours and the good VAD.

0:27:04	SPEAKER_02
 And then I took this system and we put the old filters also. So we have this good system with good VAD, with the short filter and with the long filter.

0:27:16	SPEAKER_02
 And with the short filter it's not worse. So why is it?

0:27:24	SPEAKER_06
 Okay, so that's all fine. But what you're saying is that when you do these... let me try to understand. When you do these same improvements to proposal 1, that on the things are somewhat better in proposal 2 for the well-matched case and somewhat worse for the other two cases.

0:27:46	SPEAKER_06
 So now that these other things are in there, is it the case maybe that the additions of proposal 2 over proposal 1 are less important?

0:28:00	SPEAKER_02
 Yeah, probably.

0:28:01	SPEAKER_02
 Okay. So yeah, but it's a good thing anyway to have shorter delay. Then we try to do something like proposal 2, but using also have the G features.

0:28:21	SPEAKER_02
 So there is this G L T part which is just standard features and then junior 2 neural networks.

0:28:32	SPEAKER_02
 And it doesn't seem to help. However, we just have one result which is the Italian mismatch.

0:28:41	SPEAKER_05
 Okay. There was a start of some effort and something related to voicing or something.

0:28:55	SPEAKER_02
 Yeah. So basically we try to find good features that will be useful voicing detection.

0:29:05	SPEAKER_02
 But it's still on the... basically we're still paying with my laptop.

0:29:15	SPEAKER_07
 What sorts of features are you looking at?

0:29:19	SPEAKER_02
 So we would be looking at the variance of the spectrum of the excitation, something like this, which is really I have a voiced sound.

0:29:32	SPEAKER_07
 What does that mean? The variance of the spectrum of excitation?

0:29:38	SPEAKER_02
 Yeah. So basically the spectrum of excitation.

0:29:45	SPEAKER_06
 Okay. What you're calling the excitation is that recall is you're subtracting the male filter spectrum from the FFT spectrum.

0:29:59	SPEAKER_02
 So we have the male filter bank, we have the FFT.

0:30:03	SPEAKER_06
 So it's not really an excitation, but it's something that hopefully tells you something about the excitation.

0:30:08	SPEAKER_02
 Yeah, that's right.

0:30:11	SPEAKER_01
 Yeah.

0:30:13	SPEAKER_01
 There's some histograms.

0:30:16	SPEAKER_01
 Yeah, that's...

0:30:18	SPEAKER_02
 Yeah, so for a voiced portion we have something that has a mean around 0.3 and for a voiced portion the mean is 0.59.

0:30:31	SPEAKER_02
 But the variance seems quite...

0:30:34	SPEAKER_07
 How do you know... how did you get your voiced and unvoiced truth data?

0:30:39	SPEAKER_02
 We used a timet and we used the cannon guns between the phones and...

0:30:48	SPEAKER_01
 But if we look at one send, apparently it's good.

0:30:54	SPEAKER_02
 Yeah, but yeah.

0:30:58	SPEAKER_02
 So it's noisy timet, that's right.

0:31:04	SPEAKER_02
 It seems quite robust to know, so when we take mid-row, this parameter across time for the same values and that's very close.

0:31:15	SPEAKER_02
 Yes, so there is this, there will be also the...

0:31:21	SPEAKER_02
 Something like the maximum of the other relation functions.

0:31:25	SPEAKER_07
 Is this a trained system or is it a system where you just pick some thresholds?

0:31:30	SPEAKER_02
 How does it work?

0:31:34	SPEAKER_02
 Right now we're just trying to find some features.

0:31:38	SPEAKER_02
 Hopefully I think what we want to have is to put these features in kind of...

0:31:45	SPEAKER_02
 Well, to obtain a statistical model of these features, just to use an neural network.

0:31:52	SPEAKER_02
 Hopefully these features would help.

0:31:55	SPEAKER_07
 Because it seems like what you said about the mean of the voiced and the unvoiced, that seemed pretty encouraging.

0:32:02	SPEAKER_06
 Well, yes, except the variance was big.

0:32:04	SPEAKER_07
 Well, I don't know that I would trust that so much because you're doing these canonical mappings from timet lablings, right?

0:32:09	SPEAKER_07
 So really that's sort of a cartoon picture about what's voiced and unvoiced.

0:32:14	SPEAKER_07
 So that could be giving you a lot of variance.

0:32:18	SPEAKER_07
 It may be that you're finding something good and that the variance is sort of artificial because of how you're getting your truth.

0:32:25	SPEAKER_06
 Yeah, but another way of looking at it might be that, I mean, we are coming up with feature sets after all.

0:32:31	SPEAKER_06
 So another way of looking at it is that the melkepster, melke spectrum, melkepster, any of these variants, give you this smooth spectrum.

0:32:41	SPEAKER_06
 It's a spectral envelope.

0:32:43	SPEAKER_06
 By going back to the FFT, you're getting something that is more like the raw data.

0:32:50	SPEAKER_06
 So the question is, what characterization, and you're playing around with this, another way of looking at it, is what characterization of the difference between the raw data and the smooth version is something that you're missing that could help.

0:33:06	SPEAKER_06
 So I mean, looking at different statistical measures of that difference, coming up with some things and just trying them out and seeing if you add them onto the feature vector, is that make things better or worse in noise.

0:33:17	SPEAKER_06
 Where you really just, the way I'm looking at it is not so much you're trying to find the best world's best voiced unvoiced classifier, but it's more that, you know, try some different statistical characteristics of that difference back to the raw data.

0:33:30	SPEAKER_06
 Right.

0:33:31	SPEAKER_06
 And maybe there's something there that the system can use.

0:33:34	SPEAKER_02
 Yeah, but the more of use is that, the more of use is that, well, using the FFT, you just give you just information about if it's voiced or not voiced mainly.

0:33:51	SPEAKER_02
 Yeah.

0:33:53	SPEAKER_02
 This is why we started to look.

0:33:56	SPEAKER_06
 Well, that's the way what I'm arguing is that, yeah, I mean, what I'm arguing is that that that's gives you your intuition.

0:34:04	SPEAKER_06
 But in reality, it's, you know, there's all this, this overlap and so forth.

0:34:09	SPEAKER_06
 And, but what I'm saying is that maybe okay, because what you're really getting is not actually voiced or sound voiced, both for the fact the reason of the overlap and then, you know, structural reasons like the one the Chuck said, that in fact, well, the data itself is, that you're working with is not perfect.

0:34:27	SPEAKER_06
 So, I'm saying is maybe that's not a killer because you're just getting some characterization, one that's driven by your intuition about voiced and voiced certainly, but just some characterization of something back in the, in the, in the almost raw data rather than the smooth version.

0:34:44	SPEAKER_06
 Your intuition is driving you towards particular kinds of statistical characterizations of what's missing from the spectra envelope.

0:34:56	SPEAKER_06
 Obviously, something about the excitation.

0:35:00	SPEAKER_06
 And what is it about the excitation?

0:35:02	SPEAKER_06
 And, you know, and you're not getting the excitation anyway, you know, so, so I would almost take especially if these trainings and so forth or faster, but almost just take a scatter shot.

0:35:13	SPEAKER_06
 A scatter shot at a few different ways of look of characterizing that difference and you have one of them, but, and see, you know, which of them helps?

0:35:22	SPEAKER_07
 So, is the idea that you're going to take whatever features you develop and just add them on to the feature vector? Or what's the use of the voice, unvoiced detector?

0:35:33	SPEAKER_02
 I guess we don't know exactly yet, but, yeah. It's not part of a VAD system that you're doing?

0:35:45	SPEAKER_02
 Oh, okay.

0:35:47	SPEAKER_02
 Yeah, it could be, it could be a neural network that does voice and voice detection, so the big neural networks that does, on the specification.

0:36:07	SPEAKER_06
 But each one of the mixture components, I mean, you have variants only, so it's kind of like you're just multiplying together these, probably some individual features within each mixture.

0:36:22	SPEAKER_06
 So it's so...

0:36:25	SPEAKER_07
 I think it's an 8 thing. It seems like a good idea.

0:36:28	SPEAKER_06
 Yeah.

0:36:30	SPEAKER_06
 Yeah, I mean, I know that people doing some robustness things are always back, we're just doing, just being gross, just throwing in the FFT and actually it wasn't so bad.

0:36:49	SPEAKER_06
 So, and you know that it's got to hurt you a little bit to not have a smooth spectral envelope, so there must be something else that you get in return for that.

0:37:03	SPEAKER_07
 So, how does, maybe I'm going into much detail, but how exactly do you make the difference between the FFT and the smooth spectral envelope?

0:37:13	SPEAKER_07
 Well, yeah, how is that?

0:37:19	SPEAKER_02
 We just...

0:37:22	SPEAKER_01
 We have the 23 coefficient of after the FFT and we understand this coefficient between the other frequency range and the interpolation between the point is given for the triangular filter, the value of the triangular filter.

0:37:46	SPEAKER_01
 And these weight we obtained is mod.

0:37:50	SPEAKER_06
 So, you essentially take the values that you get in the triangular filter and extend them, just sort of like a rectangle.

0:37:57	SPEAKER_06
 That's at that value.

0:38:00	SPEAKER_02
 So, yeah, we have one point for one energy for a filter bank, which is the energy that's centroid on the surface.

0:38:08	SPEAKER_07
 So, you end up with a vector that's the same length as the FFT vector, and then you just compute differences and sum the differences.

0:38:19	SPEAKER_02
 And I think the variance is computed only from like 200 hertz to 1500, because...

0:38:31	SPEAKER_02
 Right.

0:38:32	SPEAKER_02
...1500, because yeah.

0:38:34	SPEAKER_02
...20000.

0:38:35	SPEAKER_02
 Above seems that some voices on can add also like noisy part on the African.

0:38:47	SPEAKER_06
 Yeah, but no, it's being sensed to look at.

0:38:49	SPEAKER_06
 So, this is...

0:38:50	SPEAKER_07
...basically.

0:38:51	SPEAKER_07
 Basically, this is comparing an original version of a signal to a smoothed version of the same signal.

0:38:56	SPEAKER_06
 Right. So, this is...

0:38:58	SPEAKER_06
 I mean, you could argue about whether it should be linear interpolation or zero-thord or...

0:39:03	SPEAKER_06
...anyway, something like this is what you're feeding your recognizer, typically.

0:39:08	SPEAKER_06
 Like which...

0:39:09	SPEAKER_06
 No, so the male capstream is the...

0:39:12	SPEAKER_06
...capstream of this spectrum, or the long spectrum, whatever.

0:39:17	SPEAKER_06
 You're subtracting in...

0:39:19	SPEAKER_06
...power domain or log domain.

0:39:21	SPEAKER_06
 Okay, so it's sort of like division.

0:39:23	SPEAKER_06
 You do that, you have this vector.

0:39:25	SPEAKER_06
 So, ratio?

0:39:26	SPEAKER_06
 Yeah.

0:39:28	SPEAKER_06
 But anyway...

0:39:31	SPEAKER_07
...and that's...

0:39:32	SPEAKER_07
 So, what's the intuition behind this kind of a thing?

0:39:35	SPEAKER_07
 I don't really know the signal processing well enough to understand what...

0:39:38	SPEAKER_07
...what is that doing?

0:39:40	SPEAKER_02
 Yeah, like the sub-exam.

0:39:42	SPEAKER_02
 What we would like to have is some spectrum of the excitation signal, which is for a very strong, ideally, a full strain.

0:39:52	SPEAKER_02
 And for a voiced, it's something that's more flat.

0:39:56	SPEAKER_02
 Right.

0:39:57	SPEAKER_02
 And the way to do this is that, well, we have the FFT because it's computing the system.

0:40:03	SPEAKER_02
 And we have the male filter bands.

0:40:06	SPEAKER_02
 And so, if we like remove the male filter band from the FFT, we have something that's close to the excitation signal.

0:40:18	SPEAKER_02
 Okay.

0:40:19	SPEAKER_02
 Something that's like a train of both strain, a voiced sound, and that's...

0:40:24	SPEAKER_02
 I see.

0:40:26	SPEAKER_07
 So, do you have a picture that...

0:40:27	SPEAKER_07
...is this for a voiced segment, this picture?

0:40:29	SPEAKER_07
 Yeah.

0:40:30	SPEAKER_07
 What does it look like for unvoiced?

0:40:32	SPEAKER_01
 No, I'm voicing over.

0:40:34	SPEAKER_01
 Oh, thanks.

0:40:36	SPEAKER_01
 So, you know...

0:40:38	SPEAKER_02
 This is the...

0:40:40	SPEAKER_01
 This is another voiced accent.

0:40:42	SPEAKER_01
 You know, is this part between the frequency that we are considered for the situation, for the difference, and this is the difference.

0:40:48	SPEAKER_07
 This is the difference, okay?

0:40:50	SPEAKER_02
 Yeah, because we begin in 15.

0:40:56	SPEAKER_07
 So...

0:40:58	SPEAKER_07
 Does the periodicity of this signal say something about the pitch?

0:41:03	SPEAKER_07
 The pitch?

0:41:04	SPEAKER_07
 Yeah.

0:41:05	SPEAKER_06
 That's like fundamental frequency.

0:41:06	SPEAKER_06
 So, I mean...

0:41:07	SPEAKER_06
 Day-on-see.

0:41:08	SPEAKER_06
 I mean, to first order, what you're doing, you can ignore all the details and all the ways which is these are complete lies, that what you're doing in future extraction for speech recognition is you have, in your head, a simplified production model for speech, in which you have a periodic, a great periodic source of strivings and filters.

0:41:31	SPEAKER_06
 First order for speech recognition, you say, I don't care about the source, right?

0:41:35	SPEAKER_06
 So, you just want to find out what the filters are.

0:41:37	SPEAKER_06
 The filters, roughly act like an overall resonances and so forth, that's prositing the excitation.

0:41:47	SPEAKER_06
 So, if you look at the spectral envelope, just the very smooth properties of it, you get something closer to that.

0:41:53	SPEAKER_06
 And the notion is, if you have the full spectrum of all the little minigritty details, that that has the effect of both, and it would be a multiplication in frequency domain, so that would be like an addition in log, a spectrum domain.

0:42:08	SPEAKER_06
 And so, this is saying, well, if you really do have that, so a vocal tract envelope, and you subtract that off, what you get is the excitation.

0:42:16	SPEAKER_06
 I call that lies because you don't really have that.

0:42:18	SPEAKER_06
 You just have some kind of signal processing trickery to get something that's kind of smooth.

0:42:23	SPEAKER_06
 It's not really what's happening in the vocal tract.

0:42:25	SPEAKER_06
 So, you're really getting the vocal excitation.

0:42:27	SPEAKER_06
 That's why I was referring to it in a more conservative way when I was saying, well, it's the excitation.

0:42:36	SPEAKER_06
 It's not really the excitation.

0:42:38	SPEAKER_06
 It's whatever it is that's different between...

0:42:40	SPEAKER_06
 So, standing back from that, you sort of say there is this very detailed representation.

0:42:44	SPEAKER_06
 You go to a smooth representation.

0:42:46	SPEAKER_06
 You go to a smooth representation because it typically generalizes better.

0:42:50	SPEAKER_06
 But, whenever you smooth, you lose something.

0:42:53	SPEAKER_06
 So, the question is, have you lost something you can use?

0:42:56	SPEAKER_06
 Probably you wouldn't want to go to the extreme of just saying, okay, our features that will be the FFT.

0:43:01	SPEAKER_06
 Because we really think we do gain something in robustness from going to something smoother.

0:43:05	SPEAKER_06
 But maybe there's something that we missed.

0:43:07	SPEAKER_06
 So, what is it?

0:43:08	SPEAKER_06
 And then you go back to the intuition that, well, you don't really get the excitation, but you get something related to it.

0:43:13	SPEAKER_06
 And as you can see from those pictures, you do get something that shows some periodicity in frequency.

0:43:19	SPEAKER_06
 And sometimes.

0:43:21	SPEAKER_06
 That's really nice.

0:43:23	SPEAKER_07
 So, you don't have one for unvoiced picture?

0:43:26	SPEAKER_01
 No, I don't think so.

0:43:28	SPEAKER_06
 But presumably you'll see something that won't have this kind of regularity and frequency.

0:43:36	SPEAKER_07
 I would like to see those pictures.

0:43:39	SPEAKER_07
 Yeah.

0:43:40	SPEAKER_06
 Yeah.

0:43:45	SPEAKER_07
 And so, you said this is pretty doing this kind of thing.

0:43:47	SPEAKER_07
 It's pretty robust to noise.

0:43:49	SPEAKER_02
 It seems, yeah.

0:43:51	SPEAKER_02
 The mean is different.

0:43:53	SPEAKER_01
 Because the histogram is different.

0:43:57	SPEAKER_02
 And I know that the kind of robustness to noise, so if you take this frame from the noisy utterance, and the same frame from the king utterance,

0:44:09	SPEAKER_07
 do you end up with a similar difference over here? Yeah.

0:44:13	SPEAKER_07
 Okay.

0:44:14	SPEAKER_01
 Cool.

0:44:15	SPEAKER_01
 Because here the same frame for the clean speed.

0:44:18	SPEAKER_01
 Oh, that's clean.

0:44:19	SPEAKER_01
 Okay.

0:44:20	SPEAKER_01
 There are differences because here the FFT is only with 256 points.

0:44:26	SPEAKER_01
 And this is with 512.

0:44:28	SPEAKER_01
 Okay.

0:44:29	SPEAKER_02
 This is kind of interesting also because if we use the standard frame length of 25 milliseconds, it happens is that for low pitched voice because of the frame length, you don't really have.

0:44:49	SPEAKER_02
 You don't clearly see this period of structure because of the first logo for each each of you.

0:44:56	SPEAKER_07
 So this one include is a longer.

0:44:58	SPEAKER_02
 It's like 50 milliseconds.

0:45:00	SPEAKER_02
 50 minutes.

0:45:01	SPEAKER_02
 Yeah.

0:45:02	SPEAKER_02
 But it's the same frame.

0:45:04	SPEAKER_07
 What's that time frequency trade off thing, right?

0:45:07	SPEAKER_07
 I see.

0:45:08	SPEAKER_07
 Sorry.

0:45:09	SPEAKER_07
 Oh, so is this the difference here?

0:45:13	SPEAKER_01
 No, this is the same frame.

0:45:16	SPEAKER_07
 Oh, that's the original.

0:45:19	SPEAKER_02
 Yeah, so with the short frame basically, the period is not enough to use these kind of neat things.

0:45:25	SPEAKER_01
 But yeah.

0:45:27	SPEAKER_02
 Yeah, so probably we'll have to use like long, long frames.

0:45:34	SPEAKER_07
 Oh, that's interesting.

0:45:36	SPEAKER_06
 Maybe.

0:45:39	SPEAKER_06
 Well, I mean, it looks better.

0:45:42	SPEAKER_06
 But the thing is if you're actually asking if you actually need to do very long and FFT and maybe pushing things.

0:45:53	SPEAKER_07
 Would you want to do this kind of difference thing after you do spectral subtraction?

0:46:01	SPEAKER_06
 Maybe.

0:46:13	SPEAKER_06
 The spectral subtraction is being done at what level is it being done at the level of FFT bins or at the level of a male spectrum or something.

0:46:24	SPEAKER_02
 I guess it depends.

0:46:26	SPEAKER_06
 I mean, how are they doing it?

0:46:28	SPEAKER_02
 I guess Erickson is still doing that.

0:46:35	SPEAKER_02
 So yeah, I'm not really...

0:46:43	SPEAKER_06
 So in that case, it might not make much difference at all.

0:46:46	SPEAKER_07
 It seems like you'd want to do it on the FFT bins.

0:46:49	SPEAKER_07
 Maybe.

0:46:50	SPEAKER_07
 I mean, if you were going to edit it for this purpose, that is.

0:46:55	SPEAKER_05
 Yeah.

0:47:01	SPEAKER_06
 Okay.

0:47:03	SPEAKER_06
 What else?

0:47:07	SPEAKER_02
 So we'll perhaps try to convince the JIP people to use a new FFT.

0:47:22	SPEAKER_06
 Okay.

0:47:25	SPEAKER_06
 Has anything happened yet on this business of having some sort of standard...

0:47:30	SPEAKER_02
 Sorry, sir.

0:47:32	SPEAKER_02
 Not yet, but I will go down.

0:47:36	SPEAKER_02
 No, they are.

0:47:37	SPEAKER_02
 I think they're more time because they're...

0:47:42	SPEAKER_07
 When is the next Aurora deadline?

0:47:49	SPEAKER_05
 Early June?

0:47:52	SPEAKER_05
 Late June?

0:47:53	SPEAKER_05
 Not early June?

0:47:56	SPEAKER_06
 Okay.

0:48:02	SPEAKER_06
 And he's been doing all the talking, but...

0:48:04	SPEAKER_06
 Yeah.

0:48:05	SPEAKER_06
 This is by the way a bad thing.

0:48:08	SPEAKER_06
 We're trying to get female voices in this record as well.

0:48:12	SPEAKER_06
 Make sure Carmen talks as well.

0:48:16	SPEAKER_06
 But is he pretty much been talking about what you're doing also?

0:48:20	SPEAKER_01
 I am doing this.

0:48:22	SPEAKER_05
 Yes.

0:48:23	SPEAKER_01
 I don't know.

0:48:25	SPEAKER_01
 I think that's for the recognition.

0:48:27	SPEAKER_04
 The meeting record that is better than that can be speaking to you.

0:48:36	SPEAKER_06
 Well, we'll get to Spanish voices sometime.

0:48:41	SPEAKER_06
 We do.

0:48:42	SPEAKER_06
 We want to recognize you too.

0:48:45	SPEAKER_01
 And the result for the TVG.

0:48:48	SPEAKER_01
 The video record that we did for each people.

0:48:52	SPEAKER_06
 No, we like...

0:48:55	SPEAKER_06
 We're in the...

0:48:57	SPEAKER_05
 We're in the Lord Hermansky work in the frame of mind.

0:49:00	SPEAKER_05
 We like higher rates.

0:49:03	SPEAKER_05
 That way there's lots of work to do.

0:49:06	SPEAKER_05
 That's...

0:49:09	SPEAKER_05
 Anything to...

0:49:11	SPEAKER_08
 Not much is new.

0:49:13	SPEAKER_08
 I talked about what I'm trying to do last time.

0:49:16	SPEAKER_08
 I said I was going to use Avandano's method of using a transformation to map from long analysis frames, which I used for removing reverberation to short analysis frames for feature calculation.

0:49:30	SPEAKER_08
 He has a trick for doing that involving viewing the DFT as a matrix.

0:49:37	SPEAKER_08
 But I decided not to do that after all because I realized to use that I'd need to have these short analysis frames get plugged directly into the feature computation somehow.

0:49:50	SPEAKER_08
 And right now I think our feature computation is set up to take audio as input in general.

0:49:57	SPEAKER_08
 So I decided that I'll do the reverberation removal on the long analysis windows and then just re-sensitize audio and then send that.

0:50:06	SPEAKER_06
 This is in order to use the SRI system.

0:50:11	SPEAKER_08
 Or even if I'm using R system I was thinking it might be easier to re-sensitize the audio because then I could just use FeeCalc as it is and I wouldn't have to change the code.

0:50:23	SPEAKER_06
 Yeah, I mean it's certainly in a short turn.

0:50:29	SPEAKER_05
 This sounds easier.

0:50:31	SPEAKER_05
 Yeah, I mean longer term if it turns out to be useful when I want to.

0:50:36	SPEAKER_06
 Right, that's true.

0:50:39	SPEAKER_06
 No, you may be putting other kinds of errors in from the recent analysis.

0:50:46	SPEAKER_08
 Okay, I don't know anything about recent analysis.

0:50:49	SPEAKER_05
 But I'm not sure if you're likely to think that is.

0:51:02	SPEAKER_05
 It's a reasonable way to go for an initial thing.

0:51:05	SPEAKER_05
 We can look at exactly what you end up doing and figure out if there's something that could be heard by the end part of the process.

0:51:14	SPEAKER_08
 Okay.

0:51:20	SPEAKER_08
 That's, that's it, that's it.

0:51:24	SPEAKER_06
 Anything to?

0:51:26	SPEAKER_00
 Well, I've been continuing reading.

0:51:29	SPEAKER_00
 I went off on a little tangent this past week looking at Modulations Spectrum stuff and learning a bit about what it is and the importance of it in speech recognition.

0:51:46	SPEAKER_00
 I found some neat papers, historical papers from Kanadera, Hermanski and Array.

0:51:56	SPEAKER_00
 And they did a lot of experiments where they take speech and they modify the, they measure the relative importance of having different portions of the Modulation Spectrum intact.

0:52:14	SPEAKER_00
 And they find that the spectrum between one and 16 hertz in the Modulation is important for speech recognition.

0:52:24	SPEAKER_06
 Sure, I mean this sort of goes back to earlier stuff by Drillman.

0:52:28	SPEAKER_06
 And the MSG features were sort of built up this notion.

0:52:32	SPEAKER_06
 But I guess I thought you had brought this up in the context of targets somehow.

0:52:38	SPEAKER_06
 Right.

0:52:40	SPEAKER_06
 But it's not, I mean they're sort of not in the same kind of category as a phonetic target or a syllabic target or a more like a feature or something.

0:52:46	SPEAKER_00
 I was thinking more like using them as the inputs to the detectors.

0:52:51	SPEAKER_06
 Oh, I see.

0:52:53	SPEAKER_06
 Well, that's sort of what MSG does.

0:52:55	SPEAKER_06
 Right. So it's, but, but, yeah.

0:53:01	SPEAKER_06
 We'll talk more about it later.

0:53:03	SPEAKER_06
 We can talk more about it later.

0:53:07	SPEAKER_05
 Would you dig it?

0:53:09	SPEAKER_05
 Let's do dig it.

0:53:13	SPEAKER_05
 You start.

0:53:15	SPEAKER_08
 Reading transcript L-5617686691.

0:53:25	SPEAKER_08
 7921.

0:53:28	SPEAKER_08
 20350125.

0:53:33	SPEAKER_08
 40564334.

0:53:39	SPEAKER_08
 9290-3114-8629.

0:53:45	SPEAKER_08
 4136256690.

0:53:51	SPEAKER_08
 4367-615298.

0:53:56	SPEAKER_08
 76633377823.

0:54:02	SPEAKER_08
 842-614627.

0:54:07	SPEAKER_06
 Transcript L-55.

0:54:10	SPEAKER_06
 Or transcript L-55.

0:54:14	SPEAKER_06
 687-715-075.

0:54:18	SPEAKER_06
 896-03865.

0:54:22	SPEAKER_06
 566-2002-96.

0:54:26	SPEAKER_06
 848-9164.

0:54:29	SPEAKER_06
 1686-24013.

0:54:33	SPEAKER_06
 3126-619960.

0:54:37	SPEAKER_06
 837-08080.

0:54:40	SPEAKER_06
 6236-4006-9743.

0:54:45	SPEAKER_07
 Transcript L-49.

0:54:48	SPEAKER_07
 884259-7450.

0:54:52	SPEAKER_07
 787-0106158.

0:54:57	SPEAKER_07
 742-503970.

0:55:02	SPEAKER_07
 858-034714.

0:55:06	SPEAKER_07
 06044-2001.

0:55:11	SPEAKER_07
 2293-3128.

0:55:15	SPEAKER_07
 8558-558-691.

0:55:20	SPEAKER_07
 358-294017.

0:55:24	SPEAKER_00
 Transcript L-50.

0:55:27	SPEAKER_00
 9067-3933.

0:55:32	SPEAKER_00
 08308-3481.

0:55:38	SPEAKER_00
 21365-3159.

0:55:43	SPEAKER_00
 4084305211.

0:55:48	SPEAKER_00
 924-584-5504.

0:55:52	SPEAKER_00
 2226168155.

0:55:58	SPEAKER_00
 707-087-8402-803-160507.

0:56:07	SPEAKER_02
 Transcript L-53.

0:56:11	SPEAKER_02
 5954-883914.

0:56:16	SPEAKER_02
 860310-9753.

0:56:21	SPEAKER_02
 55529-3365.

0:56:26	SPEAKER_02
 337-074710.

0:56:31	SPEAKER_02
 6418-3166.

0:56:35	SPEAKER_02
 576-89596.

0:56:39	SPEAKER_02
 228-3305595.

0:56:45	SPEAKER_02
 355-059-615-025.

0:56:52	SPEAKER_01
 Transcript L-54.

0:56:56	SPEAKER_01
 1431771032.

0:57:02	SPEAKER_01
 9882-488812.

0:57:08	SPEAKER_01
 1310576812.

0:57:15	SPEAKER_01
 628875912.

0:57:22	SPEAKER_01
 5272-8617498-0000709.

0:57:32	SPEAKER_01
 862-458892.

0:57:38	SPEAKER_01
 221-196783.

