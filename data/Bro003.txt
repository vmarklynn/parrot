0:00:00	None
 Challenge?

0:00:01	SPEAKER_01
 Challenge.

0:00:02	SPEAKER_05
 So we think we're going to.

0:00:07	SPEAKER_05
 Okay, good.

0:00:08	SPEAKER_05
 Alright, going again.

0:00:11	SPEAKER_05
 So we're going to go around this before and do our digits.

0:00:16	SPEAKER_05
 Transcript, 1311-1330.

0:00:20	SPEAKER_05
 323-4765.

0:00:25	SPEAKER_05
 531-6241.

0:00:30	SPEAKER_05
 677-890-94003.

0:00:37	SPEAKER_05
 0158-17353.

0:00:41	SPEAKER_05
 268-03624307.

0:00:46	SPEAKER_05
 4.

0:00:49	SPEAKER_05
 5069-4, 74857-9615-07802.

0:00:54	SPEAKER_05
 090-90-604001.

0:00:59	SPEAKER_05
 2.

0:01:02	SPEAKER_01
 I'm reading transcript.

0:01:04	SPEAKER_01
 1391-1410-677-890-698.

0:01:14	SPEAKER_01
 01319-16237-34.

0:01:23	SPEAKER_01
 4.

0:01:24	SPEAKER_01
 5.

0:01:25	SPEAKER_01
 6.

0:01:27	SPEAKER_01
 847-920-75.

0:01:33	SPEAKER_01
 03696.

0:01:36	SPEAKER_01
 0931003.

0:01:40	SPEAKER_01
 1.

0:01:42	SPEAKER_01
 2.

0:01:43	SPEAKER_01
 3097.

0:01:46	SPEAKER_01
 5267-983.

0:01:50	SPEAKER_01
 6706.

0:01:54	SPEAKER_02
 Okay, I've got transcript 1271-1290.

0:02:00	SPEAKER_02
 1710-281-207134.

0:02:08	SPEAKER_02
 509-6080-7386-888818813.

0:02:17	SPEAKER_02
 934-0394-042-021-2038232.

0:02:29	SPEAKER_02
 42816-535-8308.

0:02:36	SPEAKER_02
 659-1076.

0:02:40	SPEAKER_02
 7662-879.

0:02:46	SPEAKER_02
 0057804-141.

0:02:53	SPEAKER_03
 Transcript number 1291-1310.

0:02:58	SPEAKER_03
 23902-38152-47467-5670-9401-00.

0:03:12	SPEAKER_03
 07260.

0:03:15	SPEAKER_03
 1604-645-23407-26440-74188-885-63989-00391.

0:03:35	SPEAKER_00
 Transcript number 1171-1190.

0:03:45	SPEAKER_00
 83435-9439-0309691.

0:03:51	SPEAKER_00
 203-556-086-304-0117882-89551-00214-36702-479-5905-7351.

0:04:15	SPEAKER_00
 479-5905-7356-669-465-7.

0:04:27	SPEAKER_07
 Okay, this is Barry Chen and I'm reading transcript 1351-1370.

0:04:35	SPEAKER_07
 4507-8928-71489-859-369-7159-047301-00106-243467-544-677-789-844-6389-00.

0:05:00	SPEAKER_07
 544-677-899-899-001-1247-349-253-5008-48.

0:05:19	SPEAKER_08
 Transcript 1371-13905-6705-9263-305-0455-049162-30002-192-466-7208-5-0386-9088-7408-8408.

0:05:48	SPEAKER_08
 3245-456-923.

0:05:58	SPEAKER_05
 Yeah, you don't actually need to say the name.

0:06:01	SPEAKER_05
 I'll probably bleep that out.

0:06:02	SPEAKER_05
 Okay.

0:06:03	SPEAKER_05
 These are not mine.

0:06:05	SPEAKER_05
 Oh.

0:06:06	SPEAKER_05
 Okay.

0:06:07	SPEAKER_05
 Not that there's anything defamatory about 857.

0:06:15	SPEAKER_05
 Okay.

0:06:18	SPEAKER_05
 So here's what I have.

0:06:19	SPEAKER_05
 I was just trying to add things I think that we should do today.

0:06:23	SPEAKER_05
 It's what I have for an agenda and so far.

0:06:27	SPEAKER_05
 We should talk a little bit about plants for the field trip next week.

0:06:32	SPEAKER_05
 A number of string field trip to OGI.

0:06:38	SPEAKER_05
 And mostly, first of all, about the logistics for it.

0:06:42	SPEAKER_05
 Then maybe later on, the meeting we should talk about, actually, might accomplish.

0:06:49	SPEAKER_05
 And then, kind of go around to see what people have been doing and talk about that progress record, essentially.

0:06:58	SPEAKER_05
 And then another topic I had was that Dave here said, give me something to do.

0:07:05	SPEAKER_05
 And I have failed so far on doing that and so we can discuss that a little bit, find some holes and some things that someone could use and help with these things.

0:07:14	SPEAKER_05
 I'm going to move into that current here.

0:07:18	SPEAKER_05
 Okay.

0:07:19	SPEAKER_05
 Always count on that.

0:07:20	SPEAKER_05
 That's a really good question.

0:07:22	SPEAKER_05
 So.

0:07:23	SPEAKER_05
 And then talk a little bit about disks and resource issues that started to work out.

0:07:32	SPEAKER_05
 And then anything else that he has that isn't in that list?

0:07:39	SPEAKER_01
 I was just wondering, does this mean the battery is dying and I should change it?

0:07:45	SPEAKER_05
 I think that means the battery is okay.

0:07:48	SPEAKER_01
 Oh, okay.

0:07:49	SPEAKER_01
 Yeah, that's good.

0:07:50	SPEAKER_01
 Because it's full.

0:07:52	SPEAKER_05
 All right.

0:07:53	SPEAKER_05
 Full of electrons.

0:07:55	SPEAKER_05
 Okay.

0:07:56	SPEAKER_05
 Okay.

0:08:02	SPEAKER_05
 So I'm going to start this with this mundane thing.

0:08:07	SPEAKER_05
 It was kind of my bright idea to have us take a plane that leaves at 7.20 in the morning.

0:08:14	SPEAKER_05
 Oh, yeah.

0:08:15	SPEAKER_05
 This is a reason I did it was because otherwise for those of us who have to come back the same day is really not much of a visit.

0:08:25	SPEAKER_05
 So the issue is how we ever accomplished that.

0:08:32	SPEAKER_05
 What part of the time do you live in?

0:08:35	SPEAKER_07
 I live in the corner of campus.

0:08:38	SPEAKER_07
 Okay.

0:08:39	SPEAKER_07
 South East corner.

0:08:41	SPEAKER_05
 Okay.

0:08:42	SPEAKER_05
 So would it be easier?

0:08:43	SPEAKER_05
 Those of you who are not used to this area can be very tricky to get to the airport at 6.30.

0:08:51	SPEAKER_05
 So would it be easier for you if you came here and I drove you?

0:09:00	SPEAKER_05
 Yeah, bridge.

0:09:01	SPEAKER_05
 Yeah.

0:09:02	SPEAKER_05
 Okay.

0:09:03	SPEAKER_05
 Okay.

0:09:04	SPEAKER_05
 So if everybody can get here at 6.

0:09:06	SPEAKER_05
 Yeah.

0:09:07	SPEAKER_05
 I'm afraid you can do that.

0:09:08	SPEAKER_05
 I guess.

0:09:09	SPEAKER_05
 Yeah.

0:09:10	SPEAKER_05
 Anyway.

0:09:11	SPEAKER_02
 So was that the enough time?

0:09:16	SPEAKER_05
 Yeah.

0:09:17	SPEAKER_05
 So I'll just pull up in front at 6.

0:09:22	SPEAKER_05
 And yeah, that'll be plenty of time.

0:09:24	SPEAKER_05
 It won't be bad traffic that time of day.

0:09:26	SPEAKER_02
 I guess once you get past the bridge.

0:09:29	SPEAKER_02
 That would be the opening.

0:09:31	SPEAKER_02
 Yeah.

0:09:32	SPEAKER_02
 Once you get past the turn off to the day bridge.

0:09:35	SPEAKER_05
 Well, the turn off bridge.

0:09:36	SPEAKER_05
 Yeah.

0:09:37	SPEAKER_05
 Won't even do that.

0:09:38	SPEAKER_05
 I mean, just go down.

0:09:39	SPEAKER_05
 Yeah.

0:09:40	SPEAKER_05
 Okay.

0:09:41	SPEAKER_05
 And the mountain with the King 988, 888.

0:09:43	SPEAKER_05
 So it's about 30 minutes to get there.

0:09:47	SPEAKER_05
 So it leaves us 50 minutes before the plane.

0:09:50	SPEAKER_05
 Yeah.

0:09:51	SPEAKER_05
 Great.

0:09:52	SPEAKER_05
 Okay.

0:09:53	SPEAKER_05
 So that'll, I mean, still not going to be really easy.

0:09:56	SPEAKER_05
 Well, particularly for the Burian, we're not staying overnight.

0:10:00	SPEAKER_05
 We're not bringing anything.

0:10:02	SPEAKER_05
 We're going to take a little bit.

0:10:04	SPEAKER_05
 We're going to have a paper.

0:10:06	SPEAKER_05
 We're going to do a little bit.

0:10:08	SPEAKER_05
 Don't bring a foot locker and we'll be okay.

0:10:11	SPEAKER_05
 So staying overnight, I figured he wouldn't need a great big.

0:10:15	SPEAKER_05
 Oh, yeah.

0:10:16	SPEAKER_05
 Anyway.

0:10:17	SPEAKER_05
 Okay.

0:10:18	SPEAKER_05
 Six a.m.

0:10:19	SPEAKER_05
 Front.

0:10:20	SPEAKER_05
 Six a.m. in front.

0:10:22	SPEAKER_05
 I'll be here.

0:10:25	SPEAKER_05
 I'll give you my phone number.

0:10:28	SPEAKER_05
 I'll give you a few minutes.

0:10:30	SPEAKER_05
 Wait a minute.

0:10:31	SPEAKER_05
 Wait a minute.

0:10:32	SPEAKER_05
 Wait a minute.

0:10:33	SPEAKER_05
 Wait a minute.

0:10:34	SPEAKER_05
 Wait a minute.

0:10:35	SPEAKER_05
 Wait a minute.

0:10:36	SPEAKER_05
 Wait a minute.

0:10:37	SPEAKER_05
 Wait a minute.

0:10:38	SPEAKER_05
 Wait a minute.

0:10:39	SPEAKER_05
 Wait a minute.

0:10:40	SPEAKER_05
 Okay.

0:10:41	SPEAKER_05
 That was the real, real important stuff.

0:10:45	SPEAKER_05
 I figured maybe wait on the potential goals for the meeting until we talk about what's been going on.

0:10:53	SPEAKER_05
 So what's been going on?

0:10:56	SPEAKER_05
 Let me start over here.

0:11:00	SPEAKER_08
 Well, the duration of the French test data.

0:11:10	SPEAKER_08
 Well, this is a digital French database, which is a microphone speech.

0:11:14	SPEAKER_08
 Don't suffer to make it worse.

0:11:17	SPEAKER_08
 I've had a noise to one part, which is actually the Aurora 2 noises.

0:11:25	SPEAKER_08
 So this is the training part.

0:11:28	SPEAKER_08
 And the remaining part are useful testing with one or two kind of noises.

0:11:35	SPEAKER_08
 So this is almost ready.

0:11:42	SPEAKER_08
 I'm preparing the HTK baseline for this task.

0:11:47	None
 Okay.

0:11:48	SPEAKER_05
 So the HTK baseline, so this is using Melcadstrand.

0:11:56	SPEAKER_05
 Yeah.

0:11:58	SPEAKER_05
 Again, I guess the plan is to then give in this.

0:12:06	SPEAKER_05
 What's the plan again?

0:12:10	SPEAKER_05
 The plan we've been doing.

0:12:13	SPEAKER_05
 So just remind me of what you were going to do.

0:12:16	SPEAKER_05
 You just described what you've been doing.

0:12:18	SPEAKER_05
 Yeah.

0:12:19	SPEAKER_05
 So if you could remind me of what you're going to be doing.

0:12:21	SPEAKER_08
 Yeah.

0:12:22	SPEAKER_08
 Well, I'm like a cube.

0:12:27	SPEAKER_08
 I should definitely shoot.

0:12:28	SPEAKER_08
 Actually, we want to analyze three dimensions, the feature dimension, the training data dimension, and the test data dimension.

0:12:44	SPEAKER_08
 Well, what we want to do is first we have number for each task.

0:12:53	SPEAKER_08
 So we have the integer task, the Italian task, the French task, and the Finnish task.

0:12:58	SPEAKER_08
 So we have numbers with systems.

0:13:01	SPEAKER_08
 I mean, I mean, you run the work strain on the task data.

0:13:07	SPEAKER_08
 And then we have systems with neural networks trained on data from the same language, if possible, but using a more generic database, which is for the tickly balance.

0:13:26	SPEAKER_05
 So we had talked, I guess we had talked at one point about maybe the language ID corpus.

0:13:33	SPEAKER_08
 Yeah, but this corpus, there is the call OM and the callFranco.

0:13:40	SPEAKER_08
 The callFranco is for language identification.

0:13:43	SPEAKER_08
 Anyway, these corpus are telephone speech, so this could be a group of four.

0:13:54	SPEAKER_08
 Because the speech database are not telephone speech.

0:14:00	SPEAKER_08
 They are not sample to eight kilohertz, but they are not telephone bandwidth.

0:14:06	SPEAKER_05
 So it's funny, isn't it? I mean, because this whole thing is for telephone.

0:14:12	SPEAKER_08
 Yeah, but the idea is to compute the feature before sending them to the...

0:14:18	SPEAKER_08
 Well, you don't send speech use and features, compute, not the...

0:14:23	SPEAKER_05
 Yeah, I know, but the reason is...

0:14:26	SPEAKER_05
 So the point is that it's the features are computed locally, and so they aren't necessarily telephone bandwidth or telephone.

0:14:36	SPEAKER_02
 Did you happen to find out anything about the OGI multilingual database?

0:14:41	SPEAKER_05
 Yeah, that's what I meant.

0:14:43	SPEAKER_05
 I said, there's an OGI language ID, not the callFranco.

0:14:48	SPEAKER_05
 Yeah, there are also two other databases.

0:14:51	SPEAKER_08
 One, they call the multilingual database, and another one is 22 language.

0:14:57	SPEAKER_08
 But it's also a different speech. Oh, they are.

0:15:02	SPEAKER_05
 Well, but I'm not sure...

0:15:08	SPEAKER_05
 I mean, the bandwidth shouldn't be such an issue, right?

0:15:11	SPEAKER_05
 Because this is down sample filtered, right?

0:15:14	SPEAKER_05
 So it's just the fact that it's not telephone.

0:15:17	SPEAKER_05
 And there are so many other differences between these different databases.

0:15:22	SPEAKER_05
 I mean, some of the stuff's recorded in the car, some of the... I mean, there's many different acoustic differences.

0:15:28	SPEAKER_05
 So I'm not sure...

0:15:30	SPEAKER_05
 I mean, else we're going to include a bunch of car recordings in the training database.

0:15:35	SPEAKER_05
 I'm not sure if it's completely rules it out.

0:15:39	SPEAKER_05
 If our major goal is to have the metacontext, and you figure that there's going to be a mismatch in acoustic conditions, does it make it much worse to sort of add another mismatch if you will?

0:15:53	SPEAKER_05
 I guess the question is, how important is it to get multiple languages in there?

0:16:00	SPEAKER_08
 Yeah, but...

0:16:03	SPEAKER_08
 Yeah.

0:16:06	SPEAKER_08
 Well, actually, for the moment, if we do not want to use these different databases, we already have an English and French microphone speech.

0:16:17	SPEAKER_05
 So that's why I think we're using a sort of multilingual...

0:16:22	SPEAKER_08
 Yeah, for the multilingual part, we were thinking about using these three databases,

0:16:26	SPEAKER_05
 and the difference in the metacontext.

0:16:30	SPEAKER_08
 Actually, these three databases are generic databases. So for Italian, which is close to Spanish, French, the attitudes we have, digits training data, and also more general training data.

0:16:49	SPEAKER_05
 Well, we also have this broadcast news that we were talking about taking off the disk, which is this microchannel data for language.

0:16:57	SPEAKER_08
 Yeah, brups.

0:16:58	SPEAKER_08
 Yeah, there is also a timid.

0:17:00	SPEAKER_08
 Yeah, I would use timid.

0:17:03	SPEAKER_05
 Yeah, so it's probably a stuff around.

0:17:06	SPEAKER_05
 Okay, so anyway, the basic plan is to test this cube.

0:17:11	None
 Yes, thank you.

0:17:13	SPEAKER_05
 The fillet filled in, yeah.

0:17:16	SPEAKER_08
 Okay, yeah, and brups, we were thinking that, perhaps, the cross-language issue is not so big of an issue, well-reviewed, brub-fusion, not focused too much on that cross-language stuff.

0:17:31	SPEAKER_08
 I mean, training on that on the language and testing for another language.

0:17:38	SPEAKER_08
 Perhaps the most important is to have neural networks trained on the target languages, with general database, general databases.

0:17:48	SPEAKER_08
 So that, well, the guy who has to develop an application with one language can use the net train on that language, or a generic net, but in other terms,

0:17:58	SPEAKER_05
 That's how you mean using the net. So if you're talking about for producing these discriminative features, you can't do that, because what they are asking for is a feature set.

0:18:10	SPEAKER_05
 Right, and so we're the ones who have been weird by doing this training.

0:18:16	SPEAKER_05
 But if we say, no, you have to have a different feature set for each language, I think this is very, very bad.

0:18:22	SPEAKER_05
 Oh, yeah.

0:18:24	SPEAKER_05
 Yeah, I mean, in principle, I mean, concessually, sort of like they want, well, they want a replacement for Melcafster.

0:18:31	SPEAKER_05
 So they say, okay, this is the year 2000.

0:18:33	SPEAKER_05
 We've got something much better than Melcafster, it's, you know, Gavaldi-Gook.

0:18:36	SPEAKER_05
 And so Gavaldi-Gook features, but these Gavaldi-Gook features are supposed to be good for any language.

0:18:42	SPEAKER_05
 Because you don't know who's going to call, and you know, I mean, so it's, it's, it's, how do you know what language?

0:18:49	SPEAKER_05
 So it picks up the phone.

0:18:51	SPEAKER_05
 So this is their English.

0:18:52	SPEAKER_05
 So it picks up the phone, right?

0:18:54	SPEAKER_08
 And you pick up the application, there is a target, the English one, the application.

0:18:59	SPEAKER_05
 So, yeah, you pick up the phone.

0:19:03	SPEAKER_05
 Yeah, you talk the phone and it sends features in.

0:19:06	SPEAKER_05
 Okay, so the phone doesn't know what your language is.

0:19:11	SPEAKER_08
 If it's the phone, but, but that's the image that they could be the, the server side.

0:19:16	SPEAKER_05
 It could be, but that's the image they have.

0:19:19	SPEAKER_05
 So that's, that's, I mean, one could argue all over the place about how things really will be in 10 years.

0:19:25	SPEAKER_05
 But the particular image that the cellular industry has right now is that it's distributed to be recognition where the probabilistic part and the semantics and so forth are all on the servers, and you compute features on the phone.

0:19:39	SPEAKER_05
 So that's, that's what we're involved.

0:19:41	SPEAKER_05
 You might, you might not agree that that's the way it will be in 10 years, but that's, that's what they're asking for.

0:19:47	SPEAKER_05
 So, so I think that the, it is an important issue whether it works cross-language.

0:19:53	SPEAKER_05
 Now it's the OGI folks perspective right now that probably that's not the biggest deal.

0:19:59	SPEAKER_05
 And that the biggest deal is the, you know, acoustic environment mismatch.

0:20:05	SPEAKER_05
 And they may very well be right, but I was hoping we could just do a task and determine if that was true.

0:20:11	SPEAKER_05
 That's true. We don't need to worry so much. Maybe, maybe we have a couple of languages in the training set, and that gives us enough breadth that, that, that the rest doesn't matter.

0:20:22	SPEAKER_05
 The other thing is this notion of training to, which I guess they're starting to look at up there, training to something more like articulate-toy features.

0:20:33	SPEAKER_05
 And if you have something that's just good for distinguishing different articulate-toy features, that should just be good across, you know, either into languages.

0:20:42	SPEAKER_05
 Yeah, so I don't, I don't, unfortunately, I don't, I see what you're coming, where you're coming from, I think, but I don't think we can, you know,

0:20:47	SPEAKER_08
 so we really have to do tests with real cross-language, I mean, for instance, training on English and testing in Italian, or we can train, or else, can we train a net on a range of languages, which can include the test, the target language.

0:21:10	SPEAKER_05
 Yeah, so there's, there's, this is complex. So, ultimately, is this saying, I think it doesn't fit within their image that you switch nets based on language.

0:21:23	SPEAKER_05
 Now, can you include the target language?

0:21:29	SPEAKER_05
 From a pure standpoint, it'd be nice not to, because then you can say, because surely someone is going to say at some point, okay, so you put in the German and the Finnish, now what do you do when somebody has Portuguese?

0:21:46	SPEAKER_05
 You know, and however, you aren't, it isn't actually a constraint in this evaluation.

0:21:56	SPEAKER_05
 So I would say, if it looks like there's a big difference to put it in, then we'd make note of it, and then we'd probably put in the other, because we have so many other problems in trying to get things to work well here, that it's not so bad as long as we know it, and we say, look, we did do this.

0:22:12	SPEAKER_02
 So ideally, what you'd want to do is you'd want to run it with and without the target language in a training set for a wide range of languages.

0:22:21	SPEAKER_02
 And that way you can say, well, you know, we're going to build it for what we think are the most common ones, but if that somebody is with a different language, you know, here's what, here's what's likely to happen.

0:22:33	SPEAKER_05
 And the truth is that it's not like there are, I mean, although there are thousands of languages, from the point of view of cellular companies, there aren't.

0:22:42	SPEAKER_05
 There's, you know, there's 50, there's something, you know, so, and they aren't, you know, the exception of Finnish, which I guess is pretty different from most things.

0:22:53	SPEAKER_05
 It's, most of them are like at least some of the others, I guess, that's why I guess this vanishes like the town.

0:23:03	SPEAKER_05
 I guess Finnish is a little bit like Hungarian, I suppose, right?

0:23:07	SPEAKER_05
 I don't know.

0:23:08	SPEAKER_05
 I don't know.

0:23:09	SPEAKER_05
 I know that, I mean, I'm not like this, but I guess Hungarian and Finnish, and one of the languages from the former Soviet Union is sort of the same family, but it's just this film.

0:23:20	SPEAKER_05
 The countries that are pretty far apart from one another, people are rode in on horses.

0:23:29	SPEAKER_07
 Okay.

0:23:34	SPEAKER_07
 Oh, my turn.

0:23:35	SPEAKER_07
 Okay.

0:23:36	SPEAKER_07
 Let's see, I spent the last week looking over Stefan Schulder, and understanding some of the data.

0:23:46	SPEAKER_07
 I reinstalled the HTK, the free version.

0:23:51	SPEAKER_07
 So, everybody's now using 3.0, which is the same version that OGI is using.

0:23:57	SPEAKER_07
 Oh, good.

0:23:58	SPEAKER_07
 Yeah, so, without any licensing big deals or anything like that.

0:24:03	SPEAKER_07
 And so, we've been talking about this cube thing, and it's beginning more and more looking like the, the Borg cube thing.

0:24:14	SPEAKER_07
 It's really gargantuan.

0:24:17	SPEAKER_07
 But, I...

0:24:19	SPEAKER_05
 I'm not going to be a similar one.

0:24:22	SPEAKER_07
 I'm not a resistant person.

0:24:24	SPEAKER_07
 Exactly.

0:24:27	SPEAKER_07
 Yeah, so, I've been looking at a Timmit stuff.

0:24:32	SPEAKER_07
 The stuff that we've been working on with Timmit, trying to get a, a labels file so we can train up a net on Timmit, and test the difference between this net train on Timmit and a net trained on digits alone, and seeing if it hurts or helps.

0:24:52	SPEAKER_05
 And again, just to clarify, when you're talking about training, have you been talking about training, have you been net for Tandem?

0:24:58	SPEAKER_05
 Yeah, yeah, I'm very...

0:25:00	SPEAKER_05
 And the inputs are POP and Delta.

0:25:03	SPEAKER_07
 Well, the inputs are one dimension of the cube, which we've talked about.

0:25:09	SPEAKER_07
 It being PLP, MFCCs, J-Rasta, LVA.

0:25:17	SPEAKER_05
 Yeah, but your initial things you're making one choice there.

0:25:20	SPEAKER_07
 Yeah, just PLP.

0:25:22	SPEAKER_07
 I haven't decided on an initial thing.

0:25:25	SPEAKER_07
 Probably something like PLP.

0:25:29	SPEAKER_05
 Yeah.

0:25:30	SPEAKER_05
 So, you take PLP and you...

0:25:35	SPEAKER_05
 You use HDK with it with the transform features using the neural net that's trained, and the training could either be from digits itself or from Timmit.

0:25:46	SPEAKER_05
 Right.

0:25:47	SPEAKER_05
 And then the testing would be these other things which might be foreign language.

0:25:52	SPEAKER_05
 Right.

0:25:53	SPEAKER_05
 I see.

0:25:54	SPEAKER_05
 I get in the picture about the cube.

0:25:56	SPEAKER_05
 Okay.

0:25:57	SPEAKER_05
 Okay.

0:25:58	SPEAKER_05
 I mean, those listening to this will not have a picture either, so I guess I'm not in worse off.

0:26:03	SPEAKER_05
 Somebody just told me the cube.

0:26:06	SPEAKER_05
 It sounds...

0:26:07	SPEAKER_05
 I get it.

0:26:08	SPEAKER_05
 I think I get it.

0:26:09	SPEAKER_02
 Yeah.

0:26:10	SPEAKER_02
 So, when you said that you're getting the labels for Timmit, what do you mean?

0:26:15	SPEAKER_07
 Oh, I'm just transforming them from the standard Timmit transcriptions to do a nice long, huge P-file.

0:26:24	SPEAKER_02
 Were the digits hand labeled for phones?

0:26:28	SPEAKER_02
 Or were they those labels automatically?

0:26:30	SPEAKER_07
 Oh, yeah. Those were automatically derived by Dan using embedded training in the alignment.

0:26:39	SPEAKER_05
 Oh, but which Dan?

0:26:41	SPEAKER_07
 Alice.

0:26:42	SPEAKER_07
 Okay.

0:26:43	None
 Yeah.

0:26:45	SPEAKER_06
 So.

0:26:46	SPEAKER_02
 I was just wondering because that test...

0:26:50	SPEAKER_02
 I think you're doing this test because you wanted to determine whether or not having general speech performs as well as having specific speech.

0:27:00	SPEAKER_02
 That's right.

0:27:01	SPEAKER_05
 Well, especially when you go over the different languages again, because you would...

0:27:05	SPEAKER_05
 The different languages have different words from the physics of it.

0:27:08	SPEAKER_02
 I was just wondering if the fact that Timmit, who is in a hand labeled stuff from Timmit, might be confused the results that you get.

0:27:18	SPEAKER_05
 I think it would, but on the other hand, it might be better.

0:27:23	SPEAKER_02
 But if it's better, it may be better because it was handed.

0:27:26	SPEAKER_05
 Yeah, so probably use it.

0:27:28	SPEAKER_05
 I mean, you know, I guess I'm Sonic Cavalier, but I mean, I think the point is you have a bunch of labels and hand-marked...

0:27:38	SPEAKER_05
 I guess actually Timmit was not entirely hand-marked.

0:27:41	SPEAKER_05
 It was automatically first and then corrected.

0:27:44	SPEAKER_05
 But it might be a better source.

0:27:52	SPEAKER_05
 So, you're right. It would be another interesting scientific question to ask.

0:27:56	SPEAKER_05
 Is it because it's a broad source or because it was, you know, carefully?

0:28:00	SPEAKER_05
 And that's something you could ask.

0:28:02	SPEAKER_05
 But given limited time, I think the main thing is if it's a better thing for you to run across languages on this training tandem system.

0:28:09	SPEAKER_02
 What about the differences in the phone sets?

0:28:12	SPEAKER_07
 Between languages?

0:28:14	SPEAKER_02
 No, between Timmit and the physics.

0:28:17	SPEAKER_07
 Oh, right.

0:28:19	SPEAKER_07
 Well, there's a mapping from the 61 phonemes in Timmit to 56, the XE56.

0:28:27	SPEAKER_07
 And then the digits phonemes, there's about 22 or 24 of them.

0:28:34	SPEAKER_07
 Out of that 56?

0:28:36	SPEAKER_07
 Out of that 56.

0:28:38	SPEAKER_07
 So, it's definitely broader.

0:28:45	SPEAKER_08
 But actually the issue of phonemes, phonemapics, will arise when we will do several languages.

0:28:54	SPEAKER_08
 Because some phonemes are not in every languages.

0:29:01	SPEAKER_08
 So, we plan to develop a subset of phonemes that includes all the phonemes of training languages.

0:29:11	SPEAKER_08
 Using the two words, kind of 100 of boots.

0:29:15	SPEAKER_08
 Yeah, super set.

0:29:17	SPEAKER_08
 Yeah, super set.

0:29:20	SPEAKER_00
 I look for some per form.

0:29:26	SPEAKER_00
 For English, American English.

0:29:29	SPEAKER_00
 And the language who have more phonemes are the English of the language.

0:29:39	SPEAKER_00
 But, for example, in Spain, in the Spanish half, several phonemes that does not appear in the English and we are too complete.

0:29:50	SPEAKER_00
 But for that, in this, we must do a lot of work because we need to generate the transcription for the database that we have.

0:30:03	SPEAKER_04
 Other than the languages, there are reasons not to use Timmit phonemes.

0:30:08	SPEAKER_04
 Because it's larger.

0:30:10	SPEAKER_04
 Is it supposed to be X-E?

0:30:13	SPEAKER_07
 Oh.

0:30:14	SPEAKER_07
 I mean, why map the 61 to 56?

0:30:17	SPEAKER_07
 I don't know.

0:30:18	SPEAKER_05
 I figured if that happened to start with you or was it, or was there, yeah, so it's what you did that.

0:30:24	SPEAKER_05
 But I think basically there were several phonemes that were just hardly over there.

0:30:29	SPEAKER_02
 Yeah, and I think some of them, they were making distinctions between silence at the end and silence at the beginning.

0:30:35	SPEAKER_02
 And really, most silence.

0:30:38	SPEAKER_02
 And it was things like that that got it mapped down.

0:30:40	SPEAKER_05
 Yeah, especially a system like ours, which is a discriminative system.

0:30:43	SPEAKER_05
 You know, you really ask him to map the learn.

0:30:46	SPEAKER_02
 There's not much different.

0:30:48	SPEAKER_02
 The ones that are gone, I think, or they also, they also, and Timmit had like a glottal stop, which basically a short period of silence.

0:30:56	SPEAKER_02
 Well, we have that now too.

0:30:58	SPEAKER_02
 I don't know.

0:30:59	SPEAKER_05
 It's actually pretty common that a lot of the recognition systems people use have things like, like say, 39, simple symbols, right?

0:31:07	SPEAKER_05
 And then they get the variety by by bringing in the context.

0:31:11	SPEAKER_05
 And that context.

0:31:13	SPEAKER_05
 So we actually have them usually large number.

0:31:16	SPEAKER_05
 We'll be 10 to use here.

0:31:20	SPEAKER_05
 So actually, maybe now you've got me sort of intrigued, but there's...

0:31:26	SPEAKER_05
 Can you describe what's on the cube?

0:31:29	SPEAKER_07
 I think that's a good idea to talk about the whole cube, and maybe we can cut out sections in the cube for people to work on.

0:31:38	SPEAKER_07
 Okay.

0:31:39	SPEAKER_05
 So even the one in the quarter doesn't, since we're not running a video camera, we'll get this.

0:31:44	SPEAKER_05
 If you use a board, it'll help us anyway.

0:31:47	SPEAKER_05
 Okay, point out one of the limitations of this.

0:31:50	SPEAKER_05
 You've got to worry less, right?

0:31:52	SPEAKER_07
 Yeah, worry less.

0:31:53	SPEAKER_07
 Can you walk around too? No.

0:31:56	SPEAKER_07
 Okay, well, basically the cube will have three dimensions.

0:32:02	SPEAKER_07
 First dimension is the features that we're going to use.

0:32:08	SPEAKER_07
 And the second dimension is the training corpus.

0:32:15	SPEAKER_07
 And that's the training on the discriminant neural net.

0:32:20	SPEAKER_07
 And the last dimension happens...

0:32:25	SPEAKER_05
 Yeah, so the training for HDK is always...

0:32:29	SPEAKER_05
 That's always set up for the individual test, right?

0:32:32	SPEAKER_05
 There's some training data and some test data.

0:32:34	SPEAKER_05
 Right, right.

0:32:35	SPEAKER_07
 This is for ANN only.

0:32:40	SPEAKER_07
 And the training for the HDK models is always fixed for whatever language you're testing on.

0:32:48	SPEAKER_07
 And then there's the testing corpus.

0:32:58	SPEAKER_07
 So then I think it's probably instructive to go and show you the features that we were talking about.

0:33:07	SPEAKER_07
 So let's see.

0:33:08	SPEAKER_07
 It's a healthy L.

0:33:09	SPEAKER_07
 Okay, what?

0:33:10	SPEAKER_07
 BLP.

0:33:11	SPEAKER_07
 FST.

0:33:14	SPEAKER_08
 MSG.

0:33:15	SPEAKER_07
 MSG.

0:33:17	SPEAKER_08
 J-Rasta.

0:33:18	SPEAKER_07
 J-Rasta.

0:33:19	SPEAKER_07
 And J-Rasta L.E.

0:33:23	SPEAKER_08
 J-Rasta L.E.

0:33:24	SPEAKER_08
 J-Rasta L.E.

0:33:26	SPEAKER_07
 Multiband.

0:33:27	SPEAKER_07
 Multiband.

0:33:28	SPEAKER_08
 So there would be multiband before...

0:33:39	SPEAKER_08
 Before the tour, I mean...

0:33:42	SPEAKER_07
 Yeah, just the multiband features, right?

0:33:44	SPEAKER_07
 Yeah.

0:33:47	SPEAKER_08
 So something like the CT with advanced.

0:33:54	SPEAKER_08
 And then multiband after networks, meaning that you would have...

0:33:59	SPEAKER_08
 You run networks, the script that you run, the script for each band.

0:34:04	SPEAKER_08
 And using the outputs of these networks or the linear outputs.

0:34:09	SPEAKER_08
 Something like that.

0:34:11	SPEAKER_02
 Yeah.

0:34:14	SPEAKER_02
 What about no?

0:34:15	SPEAKER_02
 Oh.

0:34:16	SPEAKER_02
 You don't include that because it's part of the base.

0:34:20	SPEAKER_05
 We do have a baseline system that's the smell capture, right?

0:34:26	SPEAKER_08
 But...

0:34:27	SPEAKER_08
 Not for the A and M.

0:34:30	SPEAKER_08
 Okay.

0:34:31	SPEAKER_08
 So yeah, we could add a CT.

0:34:34	SPEAKER_05
 Probably should.

0:34:35	SPEAKER_05
 At least conceptually, you know, it doesn't mean you actually have to do it.

0:34:39	SPEAKER_05
 Conceptually, it makes sense, so it's a baseline.

0:34:42	SPEAKER_02
 It'd be an interesting test just to have...

0:34:44	SPEAKER_02
 Just to do MFCC with the neural mat.

0:34:47	SPEAKER_02
 And everything else the same.

0:34:48	SPEAKER_02
 Compare that with just that MFCC without the net.

0:34:51	SPEAKER_02
 Yeah.

0:34:52	SPEAKER_07
 I think Dan did some of that in his previous Aurora experiments.

0:35:00	SPEAKER_07
 And with the net, it's wonderful.

0:35:03	SPEAKER_07
 And now the net's just baseline.

0:35:06	SPEAKER_05
 I think OGI folks have been doing it too.

0:35:09	SPEAKER_05
 Because I think they're for a bunch of their experiments.

0:35:11	SPEAKER_05
 They used a no-cam study.

0:35:12	SPEAKER_05
 Yeah, actually.

0:35:15	SPEAKER_05
 Of course, that's there.

0:35:17	SPEAKER_05
 It's here.

0:35:18	SPEAKER_07
 Okay.

0:35:19	SPEAKER_07
 Okay.

0:35:20	SPEAKER_07
 For the training corpus, we have the digits from the various languages.

0:35:32	SPEAKER_07
 English, Spanish, French, what else do we have?

0:35:46	SPEAKER_08
 The finish.

0:35:48	SPEAKER_08
 The finish.

0:35:49	SPEAKER_00
 Where did that come from?

0:35:53	SPEAKER_00
 Italy, no.

0:35:54	SPEAKER_00
 Italy, yes.

0:35:56	SPEAKER_00
 Italian.

0:35:57	SPEAKER_02
 Is that distributed with Aurora or Aurora?

0:36:03	SPEAKER_08
 So English, the finish and Italian are Aurora.

0:36:08	SPEAKER_08
 And Spanish and French is something that we can use in addition to Aurora.

0:36:14	SPEAKER_08
 What?

0:36:15	SPEAKER_05
 Yes, the German brother, Spanish and stuff on both the French.

0:36:29	SPEAKER_07
 Okay.

0:36:31	SPEAKER_07
 And...

0:36:33	SPEAKER_07
 Oh, yeah.

0:36:35	SPEAKER_07
 Is it French French or Belgian?

0:36:37	SPEAKER_07
 It's French French.

0:36:39	SPEAKER_00
 French French.

0:36:40	SPEAKER_00
 French French.

0:36:47	SPEAKER_00
 French French.

0:36:48	SPEAKER_05
 I think that is more important than French French.

0:36:53	SPEAKER_05
 Yeah, probably.

0:36:54	SPEAKER_05
 Yeah.

0:36:55	SPEAKER_05
 Yeah, everybody always insists the Belgium is absolutely pure French.

0:36:58	SPEAKER_05
 But he says those pre-easions talk about it.

0:37:04	SPEAKER_05
 He likes Belgian fries too.

0:37:10	SPEAKER_07
 And then we have a broader corpus like Timit.

0:37:22	SPEAKER_07
 Timit's so far.

0:37:23	SPEAKER_07
 Spanish.

0:37:24	SPEAKER_07
 Spanish stories.

0:37:27	SPEAKER_00
 I buy you tea.

0:37:29	SPEAKER_07
 What about TI digit?

0:37:32	SPEAKER_07
 All these Aurora data is derived from TI digits.

0:37:39	SPEAKER_07
 Basically, they corrupted it with different kinds of noises at different SNR levels.

0:37:49	SPEAKER_05
 And I think Stefan was saying there's some broader material in the French also.

0:37:53	SPEAKER_05
 Yeah, we could use it.

0:37:55	None
 Okay.

0:37:56	SPEAKER_00
 French data.

0:37:58	SPEAKER_00
 Spanish stories.

0:38:00	SPEAKER_07
 Spanish.

0:38:02	SPEAKER_07
 Spanish something.

0:38:04	SPEAKER_07
 Yeah.

0:38:05	SPEAKER_07
 Okay.

0:38:06	SPEAKER_03
 The Aurora people actually corrupted themselves or just specifies a signal on signal noise ratio.

0:38:14	SPEAKER_07
 They corrupted it themselves, but they also included the noise files for us, right?

0:38:21	SPEAKER_07
 Yeah.

0:38:22	SPEAKER_07
 So we can go ahead and corrupt other things.

0:38:25	SPEAKER_05
 I'm just curious to come here.

0:38:26	SPEAKER_05
 I mean, I couldn't tell if you were joking or is it Mexican Spanish?

0:38:29	SPEAKER_05
 No, no, no.

0:38:30	SPEAKER_00
 Oh, no, no.

0:38:31	SPEAKER_05
 Spanish is Spanish.

0:38:32	SPEAKER_05
 Spanish is Spanish.

0:38:33	SPEAKER_05
 Spanish is Spanish.

0:38:34	SPEAKER_07
 Okay.

0:38:35	None
 Spanish is Spanish.

0:38:36	SPEAKER_05
 Spanish is Spanish.

0:38:37	SPEAKER_05
 Spanish is Spanish.

0:38:38	SPEAKER_05
 Yeah, we're really covered.

0:38:39	SPEAKER_05
 Okay.

0:38:40	SPEAKER_05
 Yeah, no different.

0:38:42	SPEAKER_08
 Yes.

0:38:43	SPEAKER_08
 From Paris.

0:38:44	SPEAKER_08
 Oh, from Paris.

0:38:46	SPEAKER_08
 Yeah.

0:38:47	SPEAKER_07
 Timit's from lots of different places.

0:38:57	SPEAKER_05
 It's from Texas.

0:39:00	SPEAKER_04
 It's from Texas.

0:39:01	SPEAKER_05
 It's not really from the US either.

0:39:06	SPEAKER_07
 Okay.

0:39:07	SPEAKER_07
 And within the training corpus, we're thinking about training with noise.

0:39:15	SPEAKER_07
 So incorporating the same kinds of noises that Aurora is incorporating in their training corpus.

0:39:24	SPEAKER_07
 I don't think we were given the unseen noise conditions.

0:39:30	SPEAKER_05
 I think what they were saying was that for this next test, there's going to be some of the cases where they have the same type of noises you were given beforehand, and in some cases where you're not.

0:39:43	SPEAKER_07
 Okay.

0:39:44	SPEAKER_05
 So presumably that'll be part of the topic of analysis of the test results.

0:39:48	SPEAKER_05
 How will you do when it's matching noise?

0:39:50	SPEAKER_05
 How will you do when it's not?

0:39:52	SPEAKER_05
 Right.

0:39:53	SPEAKER_07
 I think that's right.

0:39:54	SPEAKER_07
 So I guess we can't train on the unseen noise conditions.

0:39:58	SPEAKER_07
 Well, it matters.

0:39:59	SPEAKER_05
 Not seen.

0:40:00	SPEAKER_07
 It matters, yeah.

0:40:01	SPEAKER_06
 Yeah.

0:40:02	SPEAKER_05
 I mean, it does seem to me that a lot of times when you train with something that's at least a little bit noisy, it can help you out in other kinds of noise, even if it's not matching, just because there's some more variance that you've built into things.

0:40:19	SPEAKER_05
 But exactly how well your work will, how near it is to what you have at the time.

0:40:28	SPEAKER_05
 Okay, so that's your training corpus and then your testing corpus?

0:40:33	SPEAKER_07
 The testing corpus are just the same one that's oral testing.

0:40:40	SPEAKER_07
 And that includes the English, Italian, Finnish.

0:40:58	SPEAKER_07
 We're going to get German, right?

0:41:04	SPEAKER_05
 Well, so yeah, the final test on the English.

0:41:10	SPEAKER_07
 The Spanish perhaps?

0:41:12	SPEAKER_07
 Oh, yeah, we can test on the Spanish.

0:41:14	SPEAKER_08
 But the oral test.

0:41:16	SPEAKER_05
 Oh, yeah.

0:41:18	SPEAKER_05
 Oh, there's Spanish testing in Europe.

0:41:21	SPEAKER_08
 Not yet, but...

0:41:23	SPEAKER_00
 Yeah, it's providing.

0:41:25	SPEAKER_08
 They're preparing it.

0:41:26	SPEAKER_08
 Well, according to Hineki, it would be good at this at the end of November.

0:41:35	SPEAKER_05
 Okay, so I think like seven things in each column, so that's 343 different systems that are going to be developed.

0:41:47	SPEAKER_05
 There's three of you.

0:41:49	SPEAKER_01
 100.

0:41:50	SPEAKER_01
 What about noise conditions?

0:41:53	SPEAKER_01
 What?

0:41:56	SPEAKER_05
 Don't we need to put in the column for noise conditions?

0:42:00	SPEAKER_07
 You're just trying to be difficult.

0:42:03	SPEAKER_07
 Well, when I put these testings on there, I'm assuming there's three tests.

0:42:10	SPEAKER_07
 Type A, type B and type C.

0:42:14	SPEAKER_07
 And they're all going to be tested with one training of the HTK system.

0:42:20	SPEAKER_07
 Test all three different types of noise conditions.

0:42:23	SPEAKER_07
 Test A is like a match, noise.

0:42:25	SPEAKER_07
 Test B is a slightly mismatched.

0:42:27	SPEAKER_07
 And test C is a mismatched channel.

0:42:31	SPEAKER_01
 And do we do all our training on clean data?

0:42:36	SPEAKER_07
 No, no, no.

0:42:37	SPEAKER_07
 We're going to be training on the noise files that we do have.

0:42:44	SPEAKER_05
 So, yeah, so I guess the question is how long does it take to do a training?

0:42:50	SPEAKER_05
 I mean, it's not totally crazy.

0:42:53	SPEAKER_05
 I mean, a lot of these are built-in things.

0:42:55	SPEAKER_05
 We know we have programs that compute POP, we have MSG, we have JVET.

0:42:58	SPEAKER_05
 You know, a lot of these things, which is kind of how people take in a huge amount of developments, just trying it out.

0:43:04	SPEAKER_05
 So we actually can't be quite a few experiments.

0:43:06	SPEAKER_05
 But how long does it take?

0:43:09	SPEAKER_05
 We think one of these trainings.

0:43:15	SPEAKER_02
 That's a good question.

0:43:16	SPEAKER_02
 What about combinations of them?

0:43:18	SPEAKER_05
 Oh, yeah, that's right.

0:43:20	SPEAKER_05
 I mean, because so, for instance, I think the major advantage of MSG, yeah.

0:43:24	SPEAKER_05
 What's the point?

0:43:25	SPEAKER_05
 The major advantage of MSG, I see that we've seen in the past, is combined with POP.

0:43:35	SPEAKER_07
 Now this is turning into a fourth dimensional queue.

0:43:40	SPEAKER_02
 Oh, you just select multiple things on the one dimension.

0:43:44	SPEAKER_07
 Oh, yeah, okay.

0:43:46	SPEAKER_05
 Yeah, so I mean, you don't want to see seven cheeses too.

0:43:58	SPEAKER_05
 POP is 21.

0:44:00	SPEAKER_05
 Different combinations.

0:44:03	SPEAKER_03
 It's not a complete set of combinations, don't we?

0:44:05	SPEAKER_03
 What?

0:44:06	SPEAKER_03
 It's not a complete set of combinations, though.

0:44:08	SPEAKER_06
 Yeah, I hope not.

0:44:09	SPEAKER_07
 Yeah, that would be...

0:44:11	SPEAKER_05
 Yeah, so POP and MSG, I think we definitely want to try, because we've had a lot of good experience with putting this together.

0:44:23	SPEAKER_02
 When you do that, you're increasing the size of the inputs to the net.

0:44:26	SPEAKER_02
 Do you have to...

0:44:27	SPEAKER_02
 Well, so it doesn't increase the number of trainings.

0:44:31	SPEAKER_02
 I'm just wondering about number of parameters in the net.

0:44:34	SPEAKER_02
 Do you have to worry about keeping that the same length?

0:44:37	SPEAKER_05
 I don't think so.

0:44:38	SPEAKER_04
 There's a computation limit, doesn't it?

0:44:40	SPEAKER_04
 Yeah, I mean, just more computation.

0:44:42	SPEAKER_04
 Excuse me?

0:44:43	SPEAKER_04
 Isn't there like a limit on the computation, or latency, or something like that, for a large amount?

0:44:47	SPEAKER_05
 Oh, yeah, we haven't talked about that in a new then, and all that.

0:44:50	SPEAKER_05
 Yeah.

0:44:51	SPEAKER_05
 So it's not really a limit.

0:44:52	SPEAKER_05
 What it is is that there's...

0:44:54	SPEAKER_05
 There's a...

0:44:55	SPEAKER_05
 Just...

0:44:56	SPEAKER_05
 If you're using a megabyte, then I'll say that's very nice, but of course, you will remember growing a cheap cell phone.

0:45:04	SPEAKER_05
 And I think the computation isn't so much a problem.

0:45:11	SPEAKER_05
 I think it's more than that, right?

0:45:13	SPEAKER_05
 And the expensive cell phones, expensive handhelds, and so forth, are going to have lots of memory.

0:45:19	SPEAKER_05
 So it's just that, as people see, the cheap cell phones is being the biggest market.

0:45:25	SPEAKER_05
 So...

0:45:26	SPEAKER_05
 But yeah, I was just realizing that actually it doesn't explode out.

0:45:32	SPEAKER_05
 It's not only through the cell, but it's...

0:45:39	SPEAKER_05
 But it doesn't really explode out the number of trainings, because these are all trained individually.

0:45:46	SPEAKER_05
 And so if you have all of these nets trained in some place, then you can combine their outputs and do the care transformation and so forth.

0:45:59	SPEAKER_05
 And so what it uploads out is the number of test things.

0:46:06	SPEAKER_05
 And the number of times you do that last part, that last part I think is so...

0:46:10	SPEAKER_05
 It's got to be pretty quick.

0:46:13	SPEAKER_05
 And it's just running the data through...

0:46:17	SPEAKER_02
 Well, you got to do the care transformation.

0:46:19	SPEAKER_02
 What about a net that's trained on multiple languages?

0:46:22	SPEAKER_02
 Is that just separate nets for each language, then combined, or is that actually one net trained?

0:46:27	SPEAKER_08
 Probably one net.

0:46:30	SPEAKER_08
 One would think one.

0:46:34	SPEAKER_05
 But I don't think we tested that.

0:46:39	SPEAKER_08
 So in the broader training course, we can use the three or combinations of two languages.

0:46:53	SPEAKER_02
 In one net.

0:46:57	SPEAKER_05
 Yeah, so I guess the first thing is, if we know how long a training takes, if we can train up all these combinations, then we can start working on testing of them individually and in combination.

0:47:15	SPEAKER_05
 And putting them in combination, I think is not as much a computationally as they're training with the net in the first place.

0:47:30	SPEAKER_08
 It's not too much.

0:47:33	SPEAKER_08
 But there is a testicle, so it's nice training.

0:47:39	SPEAKER_05
 It's a lot of things to learn.

0:47:46	SPEAKER_05
 How long does it take for an APK training?

0:47:49	SPEAKER_08
 It's around six or so, I think.

0:47:53	SPEAKER_00
 For training at this thing?

0:47:56	SPEAKER_00
 For the Italian, maybe one day.

0:48:02	SPEAKER_00
 Running on what?

0:48:06	SPEAKER_05
 I don't know what is the value.

0:48:11	SPEAKER_05
 I don't know what the value is.

0:48:16	SPEAKER_04
 I don't know.

0:48:20	SPEAKER_04
 I don't know.

0:48:24	SPEAKER_08
 It's not so long because the data is about 30 yards of speech.

0:48:40	SPEAKER_05
 There's no way we can begin to do any significant amount here unless we use multiple machines.

0:48:57	SPEAKER_05
 What machines are fast, what machines are used a lot, are we still using P-Make?

0:49:12	SPEAKER_05
 Once you get the basic thing set up, you have all these combinations.

0:49:30	SPEAKER_08
 I would say two days.

0:49:37	SPEAKER_00
 I think you folks are probably the only ones using it.

0:49:47	SPEAKER_05
 It's faster to do it on the spurred board.

0:49:55	SPEAKER_05
 It's still a little faster on the spurred board.

0:50:13	SPEAKER_07
 Adam did some testing.

0:50:21	SPEAKER_07
 You run on a spurred and then you can do other things on your computer.

0:50:27	SPEAKER_05
 You could set up 10 different jobs on spurred boards and have 10 other jobs running on different computers.

0:50:39	SPEAKER_05
 It's going to take that sort of thing.

0:50:47	SPEAKER_05
 I kind of like this because we have very limited time.

0:50:57	SPEAKER_05
 We have quite a bit of computational resource available.

0:51:05	SPEAKER_05
 We can look across the institute and now a little things are being used.

0:51:19	SPEAKER_05
 We've gotten before about voice-down voice silence, detection features, and I think it's a great thing to go to.

0:51:37	SPEAKER_05
 I like about this.

0:51:57	SPEAKER_05
 This is what you're thinking of doing in short terms.

0:52:07	SPEAKER_05
 Adam sort out about what's the best way to really attack this as a mass problem in terms of using many machines.

0:52:27	SPEAKER_05
 We can then present to them what it is that we're doing.

0:52:41	SPEAKER_05
 We can pull things out of this list that we think they are doing sufficiently.

0:53:01	SPEAKER_07
 How they go to the net trade region?

0:53:11	SPEAKER_07
 For the net trade on digits, we have been using 400 border hidden units.

0:53:21	SPEAKER_07
 The digit's nets will be correspond to about 20 phonemes.

0:53:33	SPEAKER_07
 We're actually broader classes, actually finer classes.

0:53:53	SPEAKER_00
 Carmen, did you have something else to add?

0:54:03	SPEAKER_00
 I tried to do a different thing with the HTG program.

0:54:23	SPEAKER_05
 I don't know what is better if you look at us or J. Rasta.

0:54:37	SPEAKER_05
 J. Rasta is more complicated.

0:54:57	SPEAKER_05
 J. Rasta is more complicated.

0:55:17	SPEAKER_05
 There are more ways that it can go wrong.

0:55:27	SPEAKER_00
 I think to recognize the Italian digit with the NetWorps and also to try another NetWorps with the Spanish digit.

0:55:53	SPEAKER_00
 The data base was at difficult work last week with the level of time that I have the difference with the level of time.

0:56:29	SPEAKER_05
 I'm sorry.

0:56:39	SPEAKER_00
 The Spanish level was in different formats for the program to train the NetWorps.

0:56:59	SPEAKER_00
 You just have to be properly converting the labels.

0:57:19	SPEAKER_00
 I don't know what I asked to do.

0:57:39	SPEAKER_00
 I think that with LiveCat I can transfer to ACS format.

0:57:53	SPEAKER_00
 I want to put ACS format to ACS format and then use LiveCat to do that.

0:58:13	SPEAKER_05
 It seems like there are some peculiarities of the dimensions that are getting sorted out.

0:58:37	SPEAKER_05
 We have a lot more computation.

0:58:55	SPEAKER_05
 I was thinking two things.

0:59:13	SPEAKER_07
 I thought of this as not in stages but more on the time axis.

0:59:33	SPEAKER_05
 I was thinking of how much you can realistically do.

0:59:53	SPEAKER_05
 I was thinking of how much you can do.

1:00:13	SPEAKER_05
 I still think we could do a lot of it.

1:00:33	SPEAKER_07
 The second thing was about scratch space.

1:00:51	SPEAKER_05
 I want to clarify my point about that.

1:01:11	SPEAKER_05
 I want to clarify my point about how much you can do.

1:01:31	SPEAKER_05
 I want to clarify my point about how much you can do.

1:01:51	SPEAKER_05
 I want to clarify my point about how much you can do.

1:02:11	SPEAKER_05
 I want to clarify my point about how much you can do.

1:02:31	SPEAKER_05
 I want to clarify my point about how much you can do.

1:02:51	SPEAKER_05
 I want to clarify my point about how much you can do.

1:03:11	SPEAKER_05
 I want to clarify my point about how much you can do.

1:03:31	SPEAKER_05
 I want to clarify my point about how much you can do.

1:03:51	SPEAKER_05
 I want to clarify my point about how much you can do.

1:04:11	SPEAKER_05
 I want to clarify my point about how much you can do.

1:04:31	SPEAKER_05
 I want to clarify my point about how much you can do.

1:04:51	SPEAKER_05
 I want to clarify my point about how much you can do.

1:05:11	SPEAKER_02
 I want to clarify my point about how much you can do.

1:05:31	SPEAKER_02
 I want to clarify my point about how much you can do.

1:05:51	SPEAKER_02
 I want to clarify my point about how much you can do.

1:06:11	SPEAKER_02
 I want to clarify my point about how much you can do.

1:06:31	SPEAKER_02
 I want to clarify my point about how much you can do.

1:06:51	SPEAKER_05
 I want to clarify my point about how much you can do.

1:07:11	SPEAKER_02
 I want to clarify my point about how much you can do.

1:07:31	SPEAKER_02
 I want to clarify my point about how much you can do.

1:07:51	SPEAKER_02
 I want to clarify my point about how much you can do.

1:08:11	SPEAKER_02
 I want to clarify my point about how much you can do.

1:08:31	SPEAKER_02
 I want to clarify my point about how much you can do.

1:08:51	SPEAKER_08
 I want to clarify my point about how much you can do.

1:09:11	SPEAKER_08
 I want to clarify my point about how much you can do.

1:09:31	SPEAKER_05
 I want to clarify my point about how much you can do.

1:09:51	SPEAKER_05
 I want to clarify my point about how much you can do.

1:10:11	SPEAKER_05
 I want to clarify my point about how much you can do.

1:10:31	SPEAKER_05
 I want to clarify my point about how much you can do.

1:10:51	SPEAKER_05
 I want to clarify my point about how much you can do.

1:11:11	SPEAKER_05
 The last topic I had here was Dave's fine offer to do something.

1:11:31	SPEAKER_05
 I was thinking perhaps if additionally to all this experiment with this not really research,

1:11:59	SPEAKER_08
 it's not really research, but it's running programs. The closer look at the speed noise detection or voice sound detection.

1:12:29	SPEAKER_02
 The thing that Sue Neill was talking about with the labels, the thing that was running through these models very quickly,

1:13:01	SPEAKER_05
 and maybe that's the only problem I have with it is the same reason why I thought it would be a good thing to do. Let's fall back to that.

1:13:21	SPEAKER_05
 That's good.

1:13:31	SPEAKER_05
 What an additional clever person could help with when we're really in crunch or time.

1:13:41	SPEAKER_05
 So over the years, if he's interested in voice and voice silencing, he could do a lot.

1:14:01	SPEAKER_05
 I think it's a good thing to do with the holidays and the middle of it to get a lot done.

1:14:11	SPEAKER_05
 The very fact that it is just work and it's running programs and so forth is exactly why it's possible that some piece of it could be handed to someone to do because it's not.

1:14:21	SPEAKER_05
 That's a question. We don't have to solve it right this second, but we can think of some piece that's well defined that he could help with.

1:14:33	SPEAKER_02
 What about training up a multilingual map?

1:14:41	SPEAKER_00
 It's good to have the label between them and Spanish or something like that.

1:14:51	SPEAKER_05
 So what we were just saying was that I was arguing for if possible coming up with something that really was development and wasn't research because we have a time crunch.

1:15:19	SPEAKER_05
 So if there's something that would save some time for someone else to do, then we should think of that first.

1:15:29	SPEAKER_05
 So I think that's the only way to do a core job is to do a core job.

1:15:49	SPEAKER_05
 So I think that's the only way to do a core job is to do a core job.

1:15:59	SPEAKER_05
 And the other tricky thing is that we are, even though we don't have a strict prohibition on memory size and computation and complexity, clearly there's a limitation to it.

1:16:21	SPEAKER_05
 So I think that's one of the very important things to do with the organization, at least some kind of harmonic, or something.

1:16:31	SPEAKER_05
 This is another whole thing, take a while to develop.

1:16:35	SPEAKER_05
 And then one of the things along with current speech recognition is that we really use the whole way of the harmonious information.

1:16:53	SPEAKER_05
 So I think the other suggestion just came up was what about having worked on the multi-lingual super-set and coming up with that and training and that on that.

1:17:21	SPEAKER_05
 Is that our multiple database? What would you think? What would this task consist of?

1:17:31	SPEAKER_08
 Yeah, it would consist in creating the super-set, modifying the labels for matching the super-set.

1:17:51	SPEAKER_05
 So you're creating the changing labels on timid or on multiple languages? Yeah, with the treated languages.

1:18:01	SPEAKER_07
 So you have to create a mapping from each language to the super-set.

1:18:21	SPEAKER_07
 So you have a machine readable IPA sort of thing. And they have a website that Stefan was showing us has all the English phonemes and their Sampa correspondent phonemes.

1:18:41	SPEAKER_07
 And then they have Spanish and German have all sorts of languages mapping to the Sampa phonemes.

1:18:51	SPEAKER_00
 You're comparing the Sampa's version to Albaist.

1:19:11	SPEAKER_02
 international. No, it's it's saying that you use a special die critical stuff which you can't you can't print out a husky. So the sample is just mapping. Got it. What does OGI

1:19:23	SPEAKER_05
 have done anything about this issue? Do they have do they have any kind of

1:19:28	SPEAKER_08
 superset they already have? I don't think so well. They they're going actually the other way defining funny blisters. So they just throw the speech from all

1:19:48	SPEAKER_02
 different languages together then clustered in the 60 or 50 or whatever. I think they've

1:19:54	SPEAKER_08
 not done it during multiple language yet but what they did is to training English nets with all the phonemes and then training English nets with kind of 17 I think it was 70 automatic root glasses. Yeah automatically dried but yeah I think so. And the result was that apparently when testing on cross language it was better but you didn't add didn't have all the results when you showed me that

1:20:28	SPEAKER_05
 but so that doesn't make an interesting question though. Is there some way that we should tie into that? Right I mean if if in fact that is a better thing to do should we have the training with our own categories and now we're saying well how do we have to cross language in one way is to come to the superset but they're trying to come up with clusters.

1:21:08	SPEAKER_05
 Do we think there's something wrong with that? I think it does something wrong with

1:21:12	SPEAKER_08
 okay well because well for a moment we're testing on digits perhaps you using broad phonemes classes it's okay for classifying your digits but as soon as you will have more words words can differ with only a single phonemes which could be the same class. So

1:21:35	SPEAKER_05
 right although you are not using this for the future generation. Yeah but you will ask the net to put

1:21:47	SPEAKER_02
 a one for the plus so you're saying there may not be enough information coming out of a net to

1:21:56	SPEAKER_03
 help discriminate the words. Like most confusions are with it from classes. I think

1:22:04	SPEAKER_04
 Larry was saying like obstetricians are only confused with obstetricians. So maybe we could look

1:22:14	SPEAKER_07
 at articulatory. Did they not do that? I don't think so. They were looking at both

1:22:23	SPEAKER_05
 phonemes but they were talking about it but that sort of a question of what they did because that's the other route to go. So suppose you don't really market it to really market your features you really want to look at the acoustics and see where we can get them. So the second class way of doing it is to look at the phones that are labeled and translate them into acoustic and articulatory features. It won't really be right you won't have these

1:22:57	SPEAKER_02
 old and laughing things. So the targets of the net are these articulatory features. Right. But that implies that you can have more than one honor at a time.

1:23:05	SPEAKER_05
 That's right. You either do that or you have an fulfillment.

1:23:07	SPEAKER_05
 I see. And I don't know if our software, if the version of the quick net that we're using allows for that.

1:23:18	SPEAKER_07
 Do you know? It allows for multiple targets being one.

1:23:22	SPEAKER_07
 We have gotten soft targets to work.

1:23:27	SPEAKER_05
 To work that way. Yeah. Okay. So that's another thing that could be that.

1:23:33	SPEAKER_05
 We could just translate instead of translating to a superset just translate to a articulatory feature. So the articulatory features are training that.

1:23:43	SPEAKER_05
 The fact even though it's a smaller number, it's still fine because you have the combinations.

1:23:50	SPEAKER_05
 So in fact it has every distinction in it.

1:23:54	SPEAKER_05
 But you should go across on your just.

1:23:58	SPEAKER_02
 We do an interesting thing. It's very not a fact too.

1:24:00	SPEAKER_02
 We could, if you had the phone labels you could replace them by their articulatory features and then feed in a vector with those things turned on based on what there's supposed to be for each phone.

1:24:18	SPEAKER_02
 Let's see if it's a big win. Do you know what I'm saying?

1:24:24	SPEAKER_02
 So I mean if your net is going to be outputting a vector of basically well it's going to have probabilities but let's say they were ones and zeros then you know for each.

1:24:36	SPEAKER_02
 I don't know if you know this for your testing data but if you know for your test data what the string of phones is and you have them aligned then you can just instead of going through the net just create the vector for each phone and feed that in to see if that data helps.

1:24:58	SPEAKER_02
 Let me think about this as I was talking with TNIC and he said that there was a guy at AT&T who spent 18 months working on a single feature and because they had done some cheating experiments.

1:25:08	SPEAKER_05
 This was the guy that we were just talking that we saw in campus who's the lawyer's fault.

1:25:14	SPEAKER_05
 He was on our hands.

1:25:15	SPEAKER_05
 Right okay.

1:25:16	SPEAKER_02
 So he was doing it.

1:25:17	SPEAKER_02
 And they had done a cheating experiment or something right?

1:25:19	SPEAKER_02
 He didn't mention that part.

1:25:21	SPEAKER_02
 But he said that I guess before they had him work on this they had done some experiment where if they could get that one feature right it dramatically improved the results I was thinking you know if you think about this that it would be interesting experiment just to see you know if you did get all of those right.

1:25:37	SPEAKER_05
 Should be because if you get all of them in there that defines all of the phones that's the squirtle I'm saying.

1:25:41	SPEAKER_05
 Right and you've got all the phones right so that doesn't help us.

1:25:46	SPEAKER_05
 Oh yeah it would be an interesting cheating experiment because we are using it in this funny way we're converting it into features.

1:25:52	SPEAKER_02
 And then you also don't know what error they've got on the HTK side you know it sort of gives you the best you could hope for.

1:26:02	SPEAKER_04
 The soft training of the nets still requires the vector to sum to one.

1:26:08	SPEAKER_04
 This one up to one.

1:26:09	SPEAKER_04
 So you can't really feed it like two articulatory features that are on at the same time with ones because it will kind of normalize them down to one half or something like that.

1:26:19	SPEAKER_08
 But the rest you have destroyed the binon.

1:26:22	SPEAKER_08
 Not many areas.

1:26:23	SPEAKER_08
 You know you can use it.

1:26:25	SPEAKER_07
 It's a six.

1:26:28	SPEAKER_07
 No it's actually sigmoid x.

1:26:30	SPEAKER_08
 So if you choose sigmoids.

1:26:32	SPEAKER_07
 I think apparently the linear outputs.

1:26:40	SPEAKER_04
 Linear outputs?

1:26:41	SPEAKER_04
 No what you want.

1:26:42	SPEAKER_04
 If you're going to do a KL transfer mod.

1:26:46	SPEAKER_07
 Right.

1:26:47	SPEAKER_07
 Right.

1:26:48	SPEAKER_07
 But during the training we're trained on sigmoid x and then at the end just chop off the final non-miniarity.

1:26:58	SPEAKER_05
 We're up there.

1:27:00	SPEAKER_05
 Oh no.

1:27:01	SPEAKER_05
 OK.

1:27:18	None
 Really fun.

1:27:22	None
 Yes.

