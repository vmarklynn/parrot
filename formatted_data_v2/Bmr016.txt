SPEAKER_06: And we already got the crash out of the way.
SPEAKER_06: It did crash, so I feel much better earlier.
SPEAKER_06: Interesting.
SPEAKER_07: Get the door.
SPEAKER_05: Hey.
SPEAKER_05: So I did collect an agenda.
SPEAKER_06: I did collect an agenda.
SPEAKER_06: So I'm going to go first.
SPEAKER_06: Well, it shouldn't take too long.
SPEAKER_06: So we're pretty much out of digits.
SPEAKER_06: We've gone once through the set.
SPEAKER_06: So the only thing I have to do, that's right.
SPEAKER_06: So I just have to go through them and pick out the ones that have problems and either correct them or have them reread.
SPEAKER_06: So we probably have like four or five more forms to be read to be once through the set.
SPEAKER_06: I've also extracted out about an hour's worth.
SPEAKER_06: We have about two hours worth.
SPEAKER_06: I extracted about an hour's worth, which are the digits for which whose speaker have speaker forms, have filled out speaker forms.
SPEAKER_06: Not everyone's filled out a speaker form.
SPEAKER_06: So I extracted one for speakers who have speaker forms and for meetings in which the key file and the transcript files are possible.
SPEAKER_06: Some of the early key files it looks like were done by hand.
SPEAKER_06: And so they're not automatically possible, and I have to go back and fix those.
SPEAKER_06: So what that means is we have about an hour of transcribed digits that we can play with.
SPEAKER_07: Liz.
SPEAKER_07: I think two hours is the total value.
SPEAKER_07: Yeah.
SPEAKER_07: Yeah.
SPEAKER_07: And you can go to the question, all these different things that are not quite right, but you can go to the other three or the other hour.
SPEAKER_06: Yes, absolutely.
SPEAKER_06: So that's just a question of a little hand editing of some files and then waiting for more people to turn in their speaker forms.
SPEAKER_06: I have this web-based speaker form, and I sent mail to everyone who hadn't filled out a speaker form, and they're slowly trickling in.
SPEAKER_07: So the relevance of the speaker form here
SPEAKER_06: is for labeling the extracted audio files. Oh, OK.
SPEAKER_06: By speaker ID and microphone type.
SPEAKER_07: Permission to use their digits.
SPEAKER_06: No, I spoke with Jane about that, and we sort of decided that it's probably not an issue, that we edit out any of the errors anyway.
SPEAKER_06: So there are no errors in the digits.
SPEAKER_06: You always read the string correctly.
SPEAKER_06: So I can't imagine why anyone would care.
SPEAKER_06: So the other topic with digits is, Liz would like to elicit different prosotics.
SPEAKER_06: And so we tried last week with them written out in English, and it just didn't work at all, because no one grouped them together.
SPEAKER_06: So it just sounded like many, many more lines instead of anything else.
SPEAKER_06: So in conversations with Liz and Jane, we decided that if you wrote them out as numbers instead of words, it would elicit more phone number, social security number, like readings.
SPEAKER_06: The problem with that is it becomes numbers instead of digits.
SPEAKER_06: When I look at this, that first line is 61, 62, 18, 86, 10.
SPEAKER_06: And so the question is, does anyone care?
SPEAKER_06: I've already spoken with Liz, and she feels that, correct me if I'm wrong, that for her connected numbers is fine, as opposed to connected digits.
SPEAKER_06: I think two hours is probably fine for test set, but it may be a little short if we actually want to do training and adaptation and all that other stuff.
SPEAKER_07: Yeah.
SPEAKER_07: Do you want different prasadics?
SPEAKER_07: So if you always had the same group, things you wouldn't like that, is that correct?
SPEAKER_04: Well, we actually figured out a way to do the groupings are randomly generated.
SPEAKER_07: No, but I was asking if that was something you really cared about, because if it wasn't, it seems to me if you made it really specifically telephone groupings that maybe people wouldn't bring with a number so much.
SPEAKER_07: You know, if it is a bit...
SPEAKER_04: They may still do it.
SPEAKER_04: Maybe some.
SPEAKER_08: What about putting a hyphen between the numbers in the group?
SPEAKER_07: So if you have six-1, you mean?
SPEAKER_07: If you go six-6-6-2931.
SPEAKER_04: Well, OK, it might help.
SPEAKER_04: I would like to get away from having only one specific grouping.
SPEAKER_04: So if that's your question.
SPEAKER_04: But it seems to me that at least for us, we can learn to read the math digits.
SPEAKER_04: If that's what people want.
SPEAKER_04: I don't think that'd be that hard.
SPEAKER_04: I agree.
SPEAKER_04: To read the math single digits.
SPEAKER_04: And it seems like that might be better for you guys, since then you'll have just more digit data.
SPEAKER_04: And that's always a good thing.
SPEAKER_04: It's a little bit better for me, too, because the digits are easier to recognize.
SPEAKER_04: They're better trained than the numbers.
SPEAKER_06: So we could just put in the instructions
SPEAKER_04: with the math digits. Right.
SPEAKER_04: Read the math single digits.
SPEAKER_04: So 61 was read as 6-1.
SPEAKER_04: And if people make a mistake, we...
SPEAKER_06: So versus 0.
SPEAKER_07: I mean, the other thing is we could just beg it because it's worrying about it.
SPEAKER_07: I mean, because we do have digits training data that we have from OGI.
SPEAKER_07: I'm sorry, it's numbers training data that we have from OGI.
SPEAKER_07: We've done lots and lots of studies with that.
SPEAKER_07: And...
SPEAKER_04: But it's nice to get it in this room with the...
None: Yeah.
SPEAKER_04:...accus.
SPEAKER_07: No, no, I guess what I'm saying is that...
SPEAKER_06: Just let them read it.
SPEAKER_07: How they read it.
SPEAKER_07: Just read it.
SPEAKER_07: How they read it.
SPEAKER_07: And just means we have to expand our vocabulary out to...
SPEAKER_04: Well, that's fine with me.
SPEAKER_04: As long as it's just that I didn't want to cause the people who would have been collecting digits the other way to not have the digits.
SPEAKER_04: So...
SPEAKER_07: We can go back to the other thing later.
SPEAKER_07: I mean, we...
SPEAKER_07: OK.
SPEAKER_07: We can do this for a while and then go back to digits for a while.
SPEAKER_07: OK.
SPEAKER_07: Yeah, I mean, you want this...
SPEAKER_07: Do you need training data or adaptation data or this?
SPEAKER_07: How much of this do you need with the...
SPEAKER_04: It's actually unclear right now.
SPEAKER_04: I just thought, well, if we're collecting digits and add them, it's that we were running out of the TI forms.
SPEAKER_04: I thought it'd be nice to have them in groups.
SPEAKER_04: And probably, all else being equal, it'd be better for me to just have single digits since...
SPEAKER_04: OK.
SPEAKER_04:...you know, I recognize it's going to do better on those anyway.
SPEAKER_04: And it's more predictable.
SPEAKER_04: So we can know from the transcript what the person said and the transcriber in general.
SPEAKER_04: But if they make mistakes, it's no big deal.
SPEAKER_04: If people say 100 instead of 100 and also maybe we can just let them choose 0 versus 0 as they like.
SPEAKER_04: Because even the same person sometimes says 0 and sometimes 0 in different contexts and that's sort of interesting.
SPEAKER_04: So I don't have a specific need because if I did, I'd probably try to collect it without bothering this group.
SPEAKER_04: But if we can try...
SPEAKER_06: So I can just add to the instructions to read it as digits.
SPEAKER_04: Right.
SPEAKER_04: And you can give an example like, you know, 6-61 would be read as 6-1.
SPEAKER_04: Right.
SPEAKER_00: And actually, it's no more artificial than what we've been doing with words.
SPEAKER_00: I'm sure people can adapt to this.
SPEAKER_00: Right.
SPEAKER_00: It's simple.
SPEAKER_00: The space is already biased toward being separated.
SPEAKER_00: Right.
SPEAKER_00: And I know I'm going to find this easier than words.
SPEAKER_06: Oh, yeah, absolutely.
SPEAKER_06: Cognitively, it's much easier.
SPEAKER_04: I also had a hard time with the words.
SPEAKER_04: But then we went back and forth.
SPEAKER_04: OK, so let's give that a try.
SPEAKER_06: And is the spacing all right or do you think there should be more space between digits and groups?
SPEAKER_04: I mean, what do other people think because you guys are reading them?
SPEAKER_00: I think that it's fine.
SPEAKER_00: To me, it looks like you've got the idea of grouping and you have the idea of separation.
SPEAKER_00: And it's just a matter of the instructions.
SPEAKER_00: That's right.
SPEAKER_00: OK.
SPEAKER_06: And I think there are about 10 different grouping patterns.
SPEAKER_06: Isn't that right, Liz?
SPEAKER_04: Right.
SPEAKER_04: Right.
SPEAKER_04: And you just, they're randomly generated and randomly assigned to digits.
SPEAKER_04: I did.
SPEAKER_07: So what?
SPEAKER_07: I was just going to say, so we have, we've sent you 40 hours of recordings now.
SPEAKER_07: And you're saying two hours is digits.
SPEAKER_07: That's for the ratio.
SPEAKER_07: Yeah.
SPEAKER_07: 20 to 1, which I guess makes sense.
SPEAKER_07: So if we did another 40 hours of recordings, then we could get a couple hours.
SPEAKER_07: Right.
SPEAKER_07: Yeah, like you say, I think a couple hours for a test sense, OK?
SPEAKER_07: It'd be nice to get more later because we're going to use this up in some sense.
SPEAKER_07: Right?
SPEAKER_00: I also would like to argue for that because it seems to me that there's a real strength in having the same test replicated a whole bunch of times and adding to that basic test bank.
SPEAKER_00: Right?
SPEAKER_00: Because then you have more and more chances to get away from random errors.
SPEAKER_00: And I think the other thing too is that right now, we have sort of a stratified sample with reference to dialect groups.
SPEAKER_00: And there might be an argument to be made for for replicating all of the digits that we've done, which were done by non-native speakers, so that we have a core that totally replicates the original data set, which is totally American speakers.
SPEAKER_00: And then we have these stratified additional language groups overlapping certain aspects of the database.
SPEAKER_06: I think that trying to duplicate, spending too much effort trying to duplicate the existing TI digits probably isn't too worthwhile because the recording situation is so different.
SPEAKER_06: It's going to be very hard to be comparable.
SPEAKER_00: Except that if you have a stimuli comparable, then it says something about the contribution of setting.
SPEAKER_07: But the other differences are so major.
SPEAKER_07: OK.
SPEAKER_07: Yeah.
SPEAKER_07: Red versus not.
SPEAKER_07: It's a very serious thing.
SPEAKER_00: What's an example of some of the other differences?
SPEAKER_00: Any other differences?
SPEAKER_07: Well, individual human glottis is going to be different.
SPEAKER_06: You know, there's so many things.
SPEAKER_06: And not just that.
SPEAKER_06: I mean, the corpus itself, we're collecting it in a red digit in a particular list.
SPEAKER_06: And I'm sure that they're doing more specific stuff.
SPEAKER_06: I remember correctly, it was like postman reading zip codes and things like that.
SPEAKER_06: The idea did it suppose?
SPEAKER_06: I thought so.
SPEAKER_07: Was it red?
SPEAKER_07: Yeah, I think the reading zip code stuff, you think you're going to do.
SPEAKER_07: Oh, I may well be.
SPEAKER_07: Yeah, not TI digits was red in red.
SPEAKER_06: I haven't ever listened to TI digits.
SPEAKER_06: So I don't really know how to compare this.
SPEAKER_06: But regardless, it's going to be hard to compare cross-corpus.
SPEAKER_07: It's different people.
SPEAKER_07: This is a different thing.
SPEAKER_07: And they're different circumstances, different recording, and so forth.
SPEAKER_07: So it's really pretty different.
SPEAKER_07: But I think the idea of using a set thing was just to give you some sort of framework so that even though you couldn't exact comparison, this is what these valid, something like group is doing.
SPEAKER_07: Some kind of reference.
SPEAKER_08: OK, what do the groupings represent?
SPEAKER_08: You said there's like 10 different groupings.
SPEAKER_04: Right, just groupings in terms of number of groups in a line and number of digits in a group and the pattern of groupings.
SPEAKER_08: Are the patterns based on anything there?
SPEAKER_04: I just roughly looked at what kinds of digit strings are out there, and they're usually grouped into either two, three, or four digits at a time.
SPEAKER_04: And they can have, I mean, actually, things are getting longer and longer.
SPEAKER_04: In the old days, you probably only had three sequences.
SPEAKER_04: Telephone numbers were less and so forth.
SPEAKER_04: So there's between, well, if you look at it, there are between like three and five groups.
SPEAKER_04: And each one has between two and four groupings.
SPEAKER_04: I purposely didn't want them to look like they're in any kind of pattern.
SPEAKER_06: And which group appears as picked randomly and what the numbers are are picked randomly?
SPEAKER_06: So unlike the previous one, which I simply replicated to T.I. digits, this is generated randomly.
SPEAKER_08: Oh, OK.
SPEAKER_04: But I think it would be great to be able to compare digits, whether it's these digits or T.I. digits to speakers, and compare that to their spontaneous speech.
SPEAKER_04: And then we do need a fair amount of digit data because you might be wearing a different microphone.
SPEAKER_04: So it's nice to have the digits replicated many times, especially for speakers that don't talk a lot.
SPEAKER_04: So for adaptation, no, I'm serious.
SPEAKER_04: So we have a problem with acoustic adaptation.
SPEAKER_04: And we're not using the digit data now, but not for adaptation.
SPEAKER_04: We were running adaptation only on the data that we ran recognition on.
SPEAKER_04: And as soon as someone started to read transcript number, that's red speech.
SPEAKER_04: And I thought, well, we're going to do better on that.
SPEAKER_04: It's not fair to use.
SPEAKER_04: Oh, yeah, that's true.
SPEAKER_04: Absolutely.
SPEAKER_04: It might be fair to use the data for adaptation.
SPEAKER_04: So those speakers who are very quiet shy.
SPEAKER_04: Right.
SPEAKER_04: Like adapting on, yeah.
SPEAKER_04: Well, I mean, it's the same microphone.
SPEAKER_04: See, the nice thing is we have that in the same meeting.
SPEAKER_04: Right.
SPEAKER_04: Same acoustic, same channels.
SPEAKER_04: Right.
SPEAKER_04: And so I still like the idea of having some kind of good.
SPEAKER_07: Yeah, I mean, for the acoustic research, for the signal processing, far field stuff, I see it as the place that we started.
SPEAKER_07: But I mean, it'd be nice to have 20 hours of digit data.
SPEAKER_07: But the truth is, I'm hoping that the stuff that you guys have been doing is continue that.
SPEAKER_07: We get the best we can do on this spontaneous stuff.
SPEAKER_07: And then we do a lot of the testing of the algorithms on the digits for the far field.
SPEAKER_07: At some point, when we feel it's mature, and we understand what's going on, we can then we have to do the spontaneous data with the far field.
SPEAKER_04: The only thing that we don't have, I know this sounds weird.
SPEAKER_04: And maybe it's completely stupid.
SPEAKER_04: But we don't have any overlapping digits.
SPEAKER_06: Yeah.
SPEAKER_06: We talked about that a couple of times.
SPEAKER_04: I know it's weird.
SPEAKER_06: But the problem I see with trying to do over lapping digits is the cognitive load.
SPEAKER_04: I know everybody's laughing.
SPEAKER_04: OK.
SPEAKER_04: No, it's stupid.
SPEAKER_04: It's just, I'm just talking for the stuff that I can't do.
SPEAKER_04: You try to do it.
SPEAKER_04: I mean, here, let's try it.
SPEAKER_06: You read the line.
SPEAKER_06: I'll read the first line.
SPEAKER_04: These are all the same forms.
SPEAKER_04: OK.
SPEAKER_06: So you read the last line.
SPEAKER_06: I'll read the first line.
SPEAKER_06: You plug your ears.
SPEAKER_06: Oh, I guess you plug your ears.
SPEAKER_06: You could do it.
SPEAKER_06: But then you don't get the same effects.
SPEAKER_04: Well, what I mean is actually not the overlaps that are well governed linguistically, but the actual fact that there's speech coming from two people, beam forming stuff, all the acoustic stuff that like Dan Ellison and company want to do, digits are nice and well-behaved.
SPEAKER_04: I mean, anyway, it's just a thought.
SPEAKER_04: I think so.
SPEAKER_04: It would go faster.
SPEAKER_04: We take one over and.
SPEAKER_08: It's the remake of DigitRea.
SPEAKER_04: Well, looking a little strife.
SPEAKER_04: I mean, I was sort of serious, but I really, I mean, I don't feel strongly enough that it's a good idea.
SPEAKER_06: I do the last line.
SPEAKER_06: I'll do the first line.
SPEAKER_06: 6-1-6-2-1-8-6-1-0.
SPEAKER_06: That's not bad.
SPEAKER_06: No, I can do it.
SPEAKER_04: And that probably was great.
SPEAKER_04: By the way, I think it was numbers, but I'm not sure.
SPEAKER_04: It just sort of sounded like a duet or something.
SPEAKER_08: Performance, OK.
SPEAKER_07: Let's try three.
SPEAKER_07: You'll pick one in the middle.
SPEAKER_04: OK.
SPEAKER_06: 6-1-6-2-1-8-6-1-0.
SPEAKER_06: I'm sorry.
SPEAKER_06: I mean, I think it was doable.
SPEAKER_06: Of course, transcribers.
SPEAKER_04: So we could have a round where you do two at a time, and then the next person picks up when the first hit is done
SPEAKER_00: or something.
SPEAKER_04: Like a, what do you call it? This pair was.
SPEAKER_04: Around.
SPEAKER_04: Like, yeah.
SPEAKER_04: Yeah, like that.
SPEAKER_04: Then it would go like twice as fast.
SPEAKER_04: A third is fast.
SPEAKER_04: Anyway, it's just a thought.
SPEAKER_04: I'm actually sort of serious if it would help people do that.
SPEAKER_04: But the people who want to work in it, we should talk to them.
SPEAKER_07: I don't think we're going to say the best amount of data that way.
SPEAKER_07: Having a little bit might at least be fun for somebody like Dan.
SPEAKER_06: I think maybe if we wanted to do that, we would do it as a separate session, something like that, rather than doing a real meeting and, you know, do two people at a time, then three people at a time, and things like that.
SPEAKER_04: Can try it out.
SPEAKER_04: If we have nothing.
SPEAKER_04: So you see what Dan says.
SPEAKER_04: We have no agenda.
SPEAKER_07: Do it some week.
SPEAKER_07: OK.
SPEAKER_07: It's been the whole time reading.
SPEAKER_07: I thought this was going to happen.
SPEAKER_00: Another question about this.
SPEAKER_00: So there are these digits, which are detached digits.
SPEAKER_00: But there are other words that contain the same general phoneme sequences, like wonderful, has one in it.
SPEAKER_00: And Victor Borja had a piece on this where he inflated the digits.
SPEAKER_00: Well, I wonder if there would be a value in having digits that are in essence embedded in real words to compare in terms of the articulation of one and wonderful versus one as a digit being read.
SPEAKER_07: That's too bad.
SPEAKER_04: I'm all for it.
SPEAKER_01: Not after I ate that.
SPEAKER_07: I don't know what work is well, isn't it?
SPEAKER_07: How does nine work in it?
SPEAKER_05: Nine.
SPEAKER_05: You scream in German?
SPEAKER_01: Yes, there's a German.
SPEAKER_01: Oh.
SPEAKER_03: Ellis, I was good when you scream at them.
SPEAKER_07: Everybody's a little punchy here.
SPEAKER_00: Well, I mean, I just wanted to offer that as a possible task because we were to each read this embedded numbers words in sentences because it's like an entire sketchy does.
SPEAKER_00: And I wouldn't take the inflated version.
SPEAKER_00: So he talks about the woman being two to fall in.
SPEAKER_00: But if it were to be deflated just the normal word, it would be like a little story that we could read.
SPEAKER_00: I don't know if that would be useful for comparison, but it's embedded numbers.
SPEAKER_06: I think for something like that would be better off doing like Tim, it.
SPEAKER_07: Well, I think the question is what the research is.
SPEAKER_07: So I mean, I presume that the reason that you wanted to have these digits this way is because you wanted to actually do some research looking at the exotic form here.
SPEAKER_04: Right.
SPEAKER_07: Yeah.
SPEAKER_07: So if somebody wanted to do that, if they wanted to look at the difference in the phones and the digits in the context or a non-digit word versus in the digit word, that would be a good thing to do.
SPEAKER_07: But I think someone would have to express interest in that.
SPEAKER_07: I see.
SPEAKER_07: OK.
SPEAKER_07: Maybe you were interested in doing that.
None: OK.
SPEAKER_00: Thank you.
SPEAKER_06: Yeah, we know digits.
SPEAKER_06: We have ASR results from Liz transcripts asked from Jane and disk space and storage formats from Don.
SPEAKER_06: Do we have any preference on which way we want to go?
SPEAKER_04: Well, I was actually going to skip the ASR results part in favor of getting the transcription stuff talked about since I think that's more important to moving forward.
SPEAKER_04: But I mean, Morgan has this paper copy.
SPEAKER_04: And if people have questions, it's pretty preliminary in terms of ASR results because we didn't do anything fancy.
SPEAKER_04: But I think just having the results there and pointing out some main conclusions, like it's not the speaking style that differs.
SPEAKER_04: It's the fact that there's overlap, that causes recognition errors.
SPEAKER_04: And the fact that it's almost all insertion errors, which you would expect.
SPEAKER_04: But you might also think that in the overlap regions, you would get substitutions and so forth, leads us to believe that doing a better segmentation like your channel-based segmentation or some kind of echo cancellation to get basically back down to the individual speaker utterances would be probably all that we would need to be able to do good recognition on the close documents.
SPEAKER_06: Why don't you have a hard copy?
SPEAKER_06: Why don't you email it to the list?
SPEAKER_04: But this is Morgan has this paper.
SPEAKER_04: Oh, it's in the paper.
SPEAKER_04: It's that paper.
SPEAKER_04: Yeah.
SPEAKER_04: Everybody's going to know.
SPEAKER_04: OK, then it's already done.
SPEAKER_04: Basically did a lot of work on that.
SPEAKER_04: And it's, let's see, I guess the other neat thing is it shows for sure that the lapel within speaker is bad.
SPEAKER_04: And it's bad because it picks up the overlapping speech.
SPEAKER_02: So your results were run on the channel synchronized?
SPEAKER_04: Yes, because that's all that had been transcribed at the time.
SPEAKER_04: But as we, I mean, I wanted to hear more about the transcription, if we can get the channel asynchronous or the closer to that would be very interesting for us.
SPEAKER_07: Because we use the part from which you had about the all over the channels or mixed channels.
SPEAKER_08: So if there was a segment of speech this long,
SPEAKER_06: someone said, oh, in the front. Oh, in the front.
SPEAKER_08: Oh, the whole thing was passed to the record.
SPEAKER_04: That's right.
SPEAKER_04: In fact, I pulled out a couple classic examples in case you want to use them in your talk of Chuck on the lapel.
SPEAKER_04: So Chuck wore the lapel three out of four times.
SPEAKER_04: I noticed that Chuck was early on.
SPEAKER_04: And I wore the lapel once.
SPEAKER_04: And for me, the lapel was OK.
SPEAKER_04: I mean, I still, and I don't know why.
SPEAKER_04: But for you, it was for who was next to me.
SPEAKER_03: And where you were sitting probably affected.
SPEAKER_04: Right.
SPEAKER_04: But when Chuck wore the lapel and Morgan was talking, there are a couple of really long utterances where Chuck is saying a few things inside.
SPEAKER_04: And it's picking up all of Morgan's words pretty well.
SPEAKER_04: And so the error rates, because of insertions aren't bounded.
SPEAKER_04: So with a one word utterance and 10 insertions, you've got huge error rate.
SPEAKER_04: And that's where the problems come in.
SPEAKER_04: So this is sort of what we expected.
SPEAKER_04: But it's nice to be able to show it.
SPEAKER_04: And also, I just wanted to mention briefly that Andreas, when I called up Dan, Alice, who's still stuck in Switzerland.
SPEAKER_04: We were going to ask him if there was out there in terms of echo cancellation and things like that, not that we were going to do it, but we wanted to know what would need to be done.
SPEAKER_04: And we've given him the data we have so far.
SPEAKER_04: So these synchronous cases where there are overlap.
SPEAKER_04: And he's going to look into trying to run some things that are out there and see how all it can do.
SPEAKER_04: Because right now, we're not able to actually report on recognition in a real paper, like a Euro-Speed Paper, because it would look sort of premature.
SPEAKER_08: So the idea is that you would take this big hunk where somebody's only speaking a small amount in it, and then try to figure out where they're speaking based on.
SPEAKER_04: Right.
SPEAKER_04: Or at any point in time, who's the foreground speaker, who's the background speaker?
SPEAKER_06: I thought we were just going to move the boundary.
SPEAKER_06: Well, that's sort of hand stuff.
SPEAKER_06: So there's, how would you do that automatically?
SPEAKER_02: Well, there's actually done some experiments with cross correlation.
SPEAKER_02: And it seems to work pretty well to get that sort of thing.
SPEAKER_04: Yeah, exactly.
SPEAKER_08: So why do you want to do echo cancellation?
SPEAKER_04: It would be techniques used from adaptive echo cancellation, which I don't know enough about to talk about.
SPEAKER_04: It's just a journey to remove cross-down.
SPEAKER_04: But right.
SPEAKER_04: And that would be similar to what you're also trying to do, but using more than energy.
SPEAKER_04: I don't know what exactly would go into it.
SPEAKER_04: So the idea is to basically run this on the whole meeting and get the locations, which gives you also the time of dreams.
SPEAKER_08: To do sort of what he's already, what he's trying to do.
SPEAKER_04: Right.
SPEAKER_04: Except that there are many techniques for the kinds of cues that you can use to do that.
SPEAKER_04: I see.
SPEAKER_04: Yeah.
SPEAKER_07: If Dave is also going to be using the foreground with echo cancellation for the near field, far field stuff.
SPEAKER_04: And I guess S-pen?
SPEAKER_04: Is he here too?
SPEAKER_04: May also be.
SPEAKER_04: So that's really the next step, because we can't do too much in terms of recognition results, knowing that this is a big problem, until we can do that kind of processing.
SPEAKER_04: And so once we have some reviewers and we'll move on.
SPEAKER_08: I think this also ties in to one of the things that Jane's going to talk about, too.
SPEAKER_06: I also want to say I have done all this chopping up of digits.
SPEAKER_06: So I have some naming conventions that we should try to agree on.
SPEAKER_04: Oh, right.
SPEAKER_04: So let's do that offline.
SPEAKER_04: We don't do it during the year.
SPEAKER_04: Right.
SPEAKER_04: Definitely.
SPEAKER_06: And I've scripts that will extract it out from key files and do all the naming automatically.
SPEAKER_06: So I'll do it by hand.
SPEAKER_03: Great.
SPEAKER_03: So let's do this.
SPEAKER_03: You compile the list of speaker names.
SPEAKER_03: And OK.
SPEAKER_03: Not names, but.
SPEAKER_03: Yep.
SPEAKER_06: Not names.
SPEAKER_06: Names to IDs.
SPEAKER_06: So you do it.
SPEAKER_06: And it does all sorts of matches, because the way people fill out names is different on every single file.
SPEAKER_04: So it does a very fuzzy sort of match.
SPEAKER_04: So at this point, we can sort of finalize the naming and so forth.
SPEAKER_04: And we're going to basically rewrite out these waveforms that we did, because as you notice in the paper, your MO4 and one meeting and MO2 and another meeting, and we just need to standardize the.
SPEAKER_04: That was my fault.
SPEAKER_04: No.
SPEAKER_04: No, I didn't notice that.
SPEAKER_04: That's why those comments are in there.
SPEAKER_06: So I now have a script that you could just say, basically, look up Morgan, and it will give you a second.
SPEAKER_04: Great.
SPEAKER_06: Great.
SPEAKER_06: Terrific.
SPEAKER_06: All right.
SPEAKER_06: Do we, Dawn, you had disk space and storage formats?
SPEAKER_06: Is that something we need to talk about at the meeting, or should you just talk with Chuck?
SPEAKER_03: At some other time.
SPEAKER_03: I had some general questions just about the compression algorithms of shortening waveforms.
SPEAKER_03: And I don't know exactly who to ask.
SPEAKER_03: I thought maybe you would be the person to talk to.
SPEAKER_03: So is it a lossless compression when you compress?
SPEAKER_03: So entropy coding.
SPEAKER_03: It just uses entropy coding.
SPEAKER_03: OK.
SPEAKER_03: So I mean, I guess my question would be, is I just got this new 18 gig drive installed.
SPEAKER_03: Yeah.
SPEAKER_06: And I think half of it is scratch and half of it is.
SPEAKER_03: I'm not exactly sure how they partitioned it.
SPEAKER_03: Yeah.
SPEAKER_03: Yeah.
SPEAKER_03: I don't know what's typical here, but it's local, though.
SPEAKER_06: So that doesn't matter.
SPEAKER_06: But you can access it from anywhere in XC.
SPEAKER_06: OK.
SPEAKER_03: In fact, how do you do that?
SPEAKER_04: Drive versus the thing is the 18 gig.
SPEAKER_04: It was a spare that day pattern.
SPEAKER_06: Slash N slash machine name slash XC.
SPEAKER_03: OK.
SPEAKER_03: All right.
SPEAKER_06: I didn't know.
SPEAKER_06: So the only question is how much of it, the distinction between scratch and non-scratch is whether it's backed up or not.
SPEAKER_06: Right.
SPEAKER_06: So what you want to do is use the scratch for stuff that you can regenerate.
SPEAKER_06: OK.
SPEAKER_06: So stuff that isn't backed up, it's not a big deal because disks don't crash very frequently as long as you can regenerate it.
SPEAKER_03: Right.
SPEAKER_03: I mean, all the stuff can be regenerated.
SPEAKER_06: It's just going to put it all on scratch because where XC is model next to buy back up.
SPEAKER_06: Yeah.
SPEAKER_04: So all the transcripts should be backed up.
SPEAKER_04: But all the waveforms should not be backed.
SPEAKER_03: Right.
SPEAKER_03: The ones that you write out.
SPEAKER_03: OK.
SPEAKER_03: So I mean, I guess the other question was, then, should we shorten them, downsample them, or keep them in their original form?
SPEAKER_06: It just depends on your tools.
SPEAKER_06: I mean, because it's not backed up and it's just on scratch, if your tools can't take short and format, I would leave them expanded.
SPEAKER_06: So you don't have to un-shorten them every single time you want to do anything.
SPEAKER_03: OK.
SPEAKER_04: We can downsample them.
SPEAKER_03: Do you think that would be OK?
SPEAKER_03: Yeah.
SPEAKER_04: To downsample them.
SPEAKER_04: Yeah, we get the same performance.
SPEAKER_04: OK.
SPEAKER_04: I mean, the front end on the SRI recognize are just downsamples them on the fly.
SPEAKER_03: Yeah.
SPEAKER_03: I guess the only argument against downsampling is to preserve just the original files in case we want to experiment with different filtering techniques.
SPEAKER_07: Yeah.
SPEAKER_07: I'm sorry.
SPEAKER_07: Yeah.
SPEAKER_07: Overall, our data, we want to not downsample.
SPEAKER_04: You want to not.
SPEAKER_04: OK.
SPEAKER_04: So what we're doing is we're writing out, I mean, this is just a question.
SPEAKER_04: We're writing out these individual segments that wherever there's a time boundary from T-load or James transcribers, we chop it there.
SPEAKER_04: And the reason is that we can feed it to the recognizer and throw out ones that we're not using and so forth.
SPEAKER_04: And those are the ones that we're storing.
SPEAKER_04: Yeah.
SPEAKER_06: So it's regenerateable.
SPEAKER_06: Yeah.
SPEAKER_06: What I would do is take downsample it and compress it.
SPEAKER_06: However, the SRI recognize or wants to take it in.
SPEAKER_06: So we can't shorten them.
SPEAKER_04: We can downsample them.
SPEAKER_04: Yeah, I mean, I'm sorry.
SPEAKER_07: As long as there is a form that we can come from again, that's not downsample.
SPEAKER_04: Yeah, that's why we need more dis space because we're basically duplicating the originals.
SPEAKER_04: That's fine.
SPEAKER_07: But for future research, we're doing different microphones.
SPEAKER_04: Oh, yeah.
SPEAKER_04: No, we always have the original long ones.
SPEAKER_08: So the SRI front end won't take a large audio file name and then a list of segments to chop out from that large audio file.
SPEAKER_08: They actually have to be chopped out already.
SPEAKER_04: It's better if they're chopped out and it will be, yeah.
SPEAKER_04: We could probably write something to do that, but it's actually convenient to have them chopped out.
SPEAKER_04: Because you can run them in different orders.
SPEAKER_04: You can actually move them around.
SPEAKER_04: And that's a whole point of opinion.
SPEAKER_04: You can get rid of vengeance.
SPEAKER_04: It's a lot faster.
SPEAKER_06: Yeah, it's a lot faster.
SPEAKER_06: English speaking.
SPEAKER_06: All the native speakers and all the non-adjustable.
SPEAKER_06: You can grab everything with the word that we're in.
SPEAKER_04: And it's a lot quicker than actually trying to access the way file each time, find the time boundaries.
SPEAKER_04: So in principle, yeah, you could do that.
SPEAKER_04: Well, that's really right.
SPEAKER_08: But it's not right now.
SPEAKER_05: It's just not right now.
SPEAKER_05: These are long.
SPEAKER_01: These are long.
SPEAKER_06: So for example, what if you wanted to run all the native speakers?
SPEAKER_06: So if you did it that way, you would have to generate a program that looks in a database somewhere or extracts out the language, finds the time marks for that particular one, do it that way.
SPEAKER_06: The way they're doing it, you have that already extracted and it's embedded in the file name.
SPEAKER_06: And so you know, you just say.
SPEAKER_06: Yeah, that's part of it.
SPEAKER_06: So that's part of it.
SPEAKER_06: You just say, you know, asterisk eAsterisk.wave and you get what you want.
SPEAKER_04: Right.
SPEAKER_04: And the other part is just that once they're written out, it is a lot faster to process it.
SPEAKER_04: Rather than doing seeks.
SPEAKER_06: So.
SPEAKER_06: Through the file.
SPEAKER_04: Otherwise, you're just accessing.
SPEAKER_06: This is all just temporary access.
SPEAKER_06: So I don't, I think it's all just, it's fine.
SPEAKER_06: You know.
SPEAKER_06: You're wanting to do it however is convenient.
SPEAKER_06: Right.
SPEAKER_07: I mean, it just depends up if the file is file-sits in memory you can do it.
SPEAKER_04: The other thing is that believe it or not, I mean, we have some, so we're also looking at these in waves like for the alignment.
SPEAKER_04: And so forth.
SPEAKER_04: You can't load an hour of speech into x-waves.
SPEAKER_04: Yeah.
SPEAKER_04: You need to have these small files.
SPEAKER_04: And in fact, even for the transcriber program.
SPEAKER_08: Yeah, you can give waves a start in an end time.
SPEAKER_04: Yeah, if you try to load really long waveform into x-waves, you'll be waiting there.
SPEAKER_04: No, I'm not suggesting you load a long waveform.
SPEAKER_08: I'm just saying you give it a start in an end time.
SPEAKER_08: And it'll just go and pull out that section.
SPEAKER_06: The transcribers didn't have any problem with that. Did they, Jane?
SPEAKER_00: What's the loading process? They load it to some problem.
SPEAKER_06: It takes a very long time.
SPEAKER_02: Yeah, just to load the transcription to a little bit.
SPEAKER_02: Right. It takes a long time.
SPEAKER_02: But not to the waveform.
SPEAKER_02: The waveform is there immediately.
SPEAKER_02: Yeah.
SPEAKER_06: Are you talking about transcriber or x-waves?
SPEAKER_06: Yeah.
SPEAKER_04: Actually, you're talking about transcriber, right?
SPEAKER_04: Yeah.
SPEAKER_04: There's also true of the digit test, which is what it's supposed to do.
SPEAKER_06: We need x-waves to do the digits.
SPEAKER_06: Yeah.
SPEAKER_06: And they were loading the form x-waves in and it didn't seem to be any problem.
SPEAKER_04: I agree.
SPEAKER_04: Well, we have a problem with that time wise.
SPEAKER_04: It's a lot slower to load in a long file.
SPEAKER_04: It seems really good.
SPEAKER_04: And also to check the file.
SPEAKER_04: So if you have a transcript.
SPEAKER_04: Well, regardless, it's still there.
SPEAKER_04: I mean, it's, I think overall you could get everything to work by accessing the same waveform and trying to find two, you know, the beginning and end times.
SPEAKER_04: But I think it's more efficient if we have the storage space.
SPEAKER_04: To have the small ones.
SPEAKER_06: And it's no problem, right?
SPEAKER_06: Because it's not backed up.
SPEAKER_04: Yeah.
SPEAKER_06: So it's just, if we don't have a spare to sitting around, we go out and we buy ourselves an 80 gigabyte drive and make it all scratch space.
SPEAKER_06: It's not a big deal.
SPEAKER_00: You're right about the backup being a bottleneck.
SPEAKER_00: It's good to.
SPEAKER_00: Yeah, so these won't be scratch.
SPEAKER_04: It's a first batch.
SPEAKER_04: Yeah.
SPEAKER_06: Right.
SPEAKER_06: So remind me afterward and I'll, and we'll look at your disk.
SPEAKER_06: You can see where to put stuff.
SPEAKER_06: All right.
SPEAKER_03: And I can just do it to a, do you on it?
SPEAKER_03: Right.
SPEAKER_03: And just see which, how much is on each?
SPEAKER_03: Yep.
SPEAKER_06: Each partition.
SPEAKER_06: And you want to use either XA or scratch.
SPEAKER_06: Okay.
SPEAKER_06: Well, X question mark.
SPEAKER_06: Anything starting with X is scratch.
SPEAKER_06: Okay.
SPEAKER_00: With two digits.
SPEAKER_00: Two digits, right?
SPEAKER_06: XA, XB, XC.
SPEAKER_06: Okay.
SPEAKER_00: Jane.
SPEAKER_00: Okay. So I got a little print on here.
SPEAKER_00: So three on this side.
SPEAKER_00: Three on this side.
SPEAKER_00: And I stable them.
None: Okay.
SPEAKER_00: All right.
SPEAKER_00: So first of all, there was an interest in the transcribe transcription checking procedures.
SPEAKER_00: And I can tell you first to go through the steps, although you've probably seen them.
SPEAKER_00: As you might imagine, when you're dealing with, really, a fair number of words and natural speech, which means self-repairs and all these other factors that there are lots of things to be standardized and streamlined and checked on.
SPEAKER_00: And so I did a bunch of checks.
SPEAKER_00: And the first thing I did was obviously a spell check.
SPEAKER_00: And at that point, I discovered certain things like accommodate with one M, that kind of thing.
SPEAKER_00: And then in addition to that, I did an exhaustive listing of the forums in the data file, which included detecting things like faulty punctuation and things.
SPEAKER_08: I'm sorry to interrupt you.
SPEAKER_08: Could I just back up a little bit?
SPEAKER_08: Sure, please, please, please.
SPEAKER_00: So you're doing these.
SPEAKER_08: So the whole process is that the transcribers get the conversation and they do their pass over it.
SPEAKER_08: Yes.
SPEAKER_08: And then when they're finished with it, it comes to you and you begin these.
SPEAKER_08: Exactly.
SPEAKER_00: I do these checks.
SPEAKER_00: These quality checks.
SPEAKER_00: Uh-huh, exactly.
SPEAKER_00: Yeah, thank you.
SPEAKER_00: And so do an exhaustive listing of the forums.
SPEAKER_00: Actually, I will go through this in order.
SPEAKER_00: So if we can maybe wait and stick.
SPEAKER_00: Keep that for a second because we're not ready for that.
SPEAKER_00: So on the fifth page.
SPEAKER_00: Exactly, exactly.
SPEAKER_00: All right.
SPEAKER_00: So, uh, spelling check first.
SPEAKER_00: Then an exhaustive listing of the, uh, all the forums in the data with punctuation attached.
SPEAKER_00: And at that point, I pick up things like, oh, you know, word followed by two commas.
SPEAKER_00: And then, uh, another check involves, uh, being sure that every utterance has an identifiable speaker.
SPEAKER_00: And if not, then that gets checked.
SPEAKER_00: Then there's this issue of glossing, so-called spoken forums.
SPEAKER_00: So they're- most for the most part, we're keeping it standard, we're-we're level transcription.
SPEAKER_00: But there's- and that's done with the assumption that pronunciation variants can be handled.
SPEAKER_00: So for things like, and the fact that someone doesn't say the D, uh, that's not important enough to capture in the transcription because of a good pronunciation, uh, you know, model, we'll be able to handle that.
SPEAKER_00: However, things like, because, where you're lacking an entire very prominent for a syllable.
SPEAKER_00: And furthermore, it's a forum that's specific to spoken language.
SPEAKER_00: Those are reasons- for- for those reasons, I-I kept that separate and used the convention of using C, U, Z for that forum.
SPEAKER_00: However, glossing it so that it's possible with a script to plug in the full orthographic forum for that one.
SPEAKER_00: And a couple of others, not many.
SPEAKER_00: So, Wana is another one going- Gona is another one with just the assumption again that this- these are things which it's not really fair to consider expected at pronunciation model to handle.
SPEAKER_00: And Chuck, you- you indicated that, and this is- is one of those that's handled in a different way also.
SPEAKER_00: Didn't you? Did I?
SPEAKER_00: I don't remember.
SPEAKER_00: Okay, so I was- it might not have been- it might not have been you.
SPEAKER_00: But someone told me that in fact, Cuzz has treated differently in, um, in this context because of that reason that, um, it's a little bit farther than a pronunciation variant.
SPEAKER_00: Okay, so after that, let's see.
SPEAKER_08: Um, so that was part of the spell check or is that- that was after the spell check?
SPEAKER_00: Well, so when I get the exhaust- so the spell check picks up those words because they're not in the dictionary, so it gets Cuzz and Wana and that-
SPEAKER_06: And then you gloss them.
SPEAKER_00: Yeah, I've run it through- I have a said, you know, so I do a side script saying whenever you see Gona- convert it to Gona, you know, gloss equals quote, going to quote, you know.
SPEAKER_00: And with all of these things being in curly brackets, so they're always distinctive.
SPEAKER_00: Okay, I also wrote a script which will, um, retrieve anything in curly brackets or anything which I've classified as an acronym and it pronounced acronym.
SPEAKER_00: And the way I take- pronounced acronyms is that I have underscores between the component.
SPEAKER_00: So if it's ACL, then it's A underscores C underscore L.
SPEAKER_00: And-
SPEAKER_06: So your list here are these ones that actually occurred in the meetings?
SPEAKER_00: Yes, huh? Yeah.
SPEAKER_00: Okay, so now- We are asking-
SPEAKER_04: Can I ask a question about the glossing of the before we go on? So for a word like, because is it that it's always predictably because- I mean, is CUSE always meaning because?
SPEAKER_00: Yes, but not the reverse.
SPEAKER_00: So sometimes people will say because in the meeting.
SPEAKER_00: And if they actually said because, then it's written as because with no- with cause doesn't even figure into the equation.
SPEAKER_00: Because-
SPEAKER_07: But not eating as people don't say, hey, cause. Right, right, right, right, yeah.
SPEAKER_04: Yeah.
SPEAKER_04: So, I guess so from the point of view of- The only problem is that with- For the recognition we map it to B cause.
SPEAKER_04: And so we know that CUSE is the- Well, don't have the gloss.
SPEAKER_06: But you have the gloss forms, you always replace it.
SPEAKER_06: Exactly.
SPEAKER_06: If that's what you want to do.
SPEAKER_00: Uh-huh.
SPEAKER_00: And Don knows this, and he's been- Yeah, I replaced the cause with B cause if it's lost.
SPEAKER_04: Right, but- If it's okay.
SPEAKER_04: But then there are other glosses that we don't replace, right?
SPEAKER_00: Because- Yes, and that's why there are different tags on the glosses.
SPEAKER_00: Okay, so- On the different types of comments, which we'll see in just a second.
SPEAKER_00: So the pronounceable acronyms get underscores.
SPEAKER_00: The things in curly brackets are viewed as comments.
SPEAKER_00: They're comments of four types.
SPEAKER_00: So it's a good time to introduce that.
SPEAKER_00: Four types.
SPEAKER_00: And maybe we'll expand that.
SPEAKER_00: But the- But the comments are- Four types, mainly right now.
SPEAKER_00: One of them is- Um, the gloss type we just mentioned.
SPEAKER_00: Another type is-
SPEAKER_06: So are we done with acronyms? Because I had a question on what- What does this mean?
SPEAKER_00: I'm still doing the overview.
SPEAKER_00: I haven't actually gotten here yet.
SPEAKER_00: Awesome.
SPEAKER_00: Okay, so glosses things like- glosses things like replacing the full form with the, um, more abbreviated one to the left.
SPEAKER_00: Then you have- If it's- There's a couple different types of elements that can happen that aren't really properly words.
SPEAKER_00: And some of them are laughs and breeze.
SPEAKER_00: So we have- That's prepended with a tag of VOC.
SPEAKER_00: And the non-vulgones are like door slams and tapping.
SPEAKER_00: And that's prepended with a- So the non-vulgination- So the end being curly braces or something else?
SPEAKER_00: Oh yeah, so this would- Let's just take one example.
SPEAKER_00: Oh, oh, oh.
SPEAKER_00: And then the non-vulgization would be something like a door slam.
SPEAKER_00: They always end.
SPEAKER_00: So it's like they're paired curly brackets.
SPEAKER_00: And then the third type right now is things that fall in the category of comments about what's happening.
SPEAKER_00: So it could be something like, you know, referring to so-and-so, talking about such and such, you know, looking at so-and-so.
SPEAKER_00: So on the middle-
SPEAKER_08: So in the first case that gloss applies to the word to the left. Yeah.
SPEAKER_00: And this gets so- In the middle two, it's not applying anything, right?
SPEAKER_00: No, they're events.
SPEAKER_00: They're actually- The quality-
SPEAKER_06: The quality is applying to the left.
SPEAKER_08: Right, I just meant the middle two.
SPEAKER_00: Yeah. Well, and actually it is true that with respect to laugh, there's another one which is while laughing.
SPEAKER_00: And that is- An argument could be made for this turning that into a qualitative statement because it's talking about the thing that preceded it.
SPEAKER_00: But at present we haven't been coding the exact scope of laughing, you know?
SPEAKER_00: And so to have while laughing, you know that it happened somewhere in there, which could well mean that it occurred separately and following, or, you know, including some of the utterances to the left.
SPEAKER_00: Haven't been awfully precise about that.
SPEAKER_00: But I have here- Now we're about to get to this now.
SPEAKER_00: I have frequencies.
SPEAKER_00: So you'll see how often these different things occur.
SPEAKER_00: But the very front page deals with this final aspect of standardization which has to do with the spoken forms like mm-hmm, and ha, and uh-uh, and all these different types.
SPEAKER_00: And someone pointed out to me- This might have been Chuck.
SPEAKER_00: About how a recognizer if it's looking for mm-hmm with 3M's, and it's transcribed with 2M's, that it might increase the air rate, which should really be a shame, because I personally would not be able to make a claim that those are dramatically different items.
SPEAKER_00: So right now I've standardized across all the existing data with these spoken forms.
SPEAKER_00: I should say all existing data except 30 minutes which got found today.
SPEAKER_00: So I know- I'm going to check.
SPEAKER_00: Yeah, actually, yeah, it was stored in a place I didn't expect.
SPEAKER_00: So, and we reconstructed how to happen.
SPEAKER_00: And this would be great.
SPEAKER_00: So I'll be able to get through that tonight and then actually later today, really.
SPEAKER_00: And so then we'll have everything following these conventions.
SPEAKER_00: But notice it's a really rather small set of these kinds of things.
SPEAKER_00: And I made it so that these are- With a couple exceptions, but things that you wouldn't find in the spell checker so that they'll show up really easily.
SPEAKER_03: And- Jane, can I ask you a question?
SPEAKER_00: Well, does that very last one correspond to?
SPEAKER_00: Yeah, yeah.
SPEAKER_00: That's only hers once and I'm thinking of changing that.
SPEAKER_00: Is that like someone's warning or something?
SPEAKER_00: I haven't heard it actually.
SPEAKER_00: I need to listen to that.
SPEAKER_04: Actually, we gave this to our pronunciation person.
SPEAKER_04: She's like, I don't know what that is either.
SPEAKER_04: Did she actually hear it?
SPEAKER_04: No, we had- We gave her a list of words that weren't in our dictionary.
SPEAKER_04: And so of course it picked up stuff like this.
SPEAKER_04: And she just didn't listen.
SPEAKER_04: So if she didn't know, we're just waiting on it.
SPEAKER_00: Yeah, I'm curious to see what it is, but I didn't want to change it to something else until- You can't hear it.
SPEAKER_00: Well, you know-
SPEAKER_03: But that's not really like- Yeah.
SPEAKER_03: No one really says arg.
SPEAKER_03: Right, no- It's a- I said the highest ass, that's right.
SPEAKER_05: That's a big problem when we talk about that.
SPEAKER_05: We're going to never recognize this meeting.
SPEAKER_05: Money ties on the art.
SPEAKER_06: Well, yeah.
SPEAKER_06: Well, or if you're a see programmer.
SPEAKER_04: Yeah, that's right.
SPEAKER_04: The art, see, and art, see, and art.
SPEAKER_04: That's right.
SPEAKER_04: It has a different pros.
SPEAKER_06: It has arch-max, arch-max.
SPEAKER_05: So Jane, what's-
SPEAKER_04: Maybe tie- So I have one question about the EH versus like the AH and U-A.
SPEAKER_00: That's partly a non-native native thing, but I have found EH and the native speakers too.
SPEAKER_00: But it's mostly non-native.
SPEAKER_00: Okay.
SPEAKER_00: That's A versus A.
SPEAKER_04: A.
SPEAKER_04: A, yeah, right, because there were some speakers that did definite A's, but right now we- There were the Canadians, right?
SPEAKER_04: So it's actually probably good for us to know the difference between the real A and the one that- Exactly.
SPEAKER_04: Because in switchboard you would see all of these forms, but they all were like, ah.
SPEAKER_06: I mean, just the single letter A has in the particles group.
SPEAKER_04: No, no, I mean like the UH or the UH EH were all the same.
SPEAKER_04: And then we have this additional non-native version of A.
SPEAKER_03: All the EHs I've seen have been like that.
SPEAKER_03: They've been like, eh, like that has been transcribed to EH.
SPEAKER_03: And sometimes it's stronger, like A, which is like closer to EH, but- Yeah.
SPEAKER_06: I'm just these poor transcribers, I know.
SPEAKER_00: Well, we're not doing- We're not doing- We're not doing clause for us.
SPEAKER_00: That's right.
SPEAKER_00: Who knows?
SPEAKER_04: And your native German speaker, so not a- Not an issue for-
SPEAKER_06: It's only- Thick-thick Canadians.
SPEAKER_04: Not only if you don't have black swallows, I guess, right?
SPEAKER_00: That's my sense.
SPEAKER_00: That's my sense.
SPEAKER_00: That's my sense.
SPEAKER_00: Yeah, and so, you know, I mean, I have- There are some Americans who are using this A2, and I haven't listened to it systematically.
SPEAKER_00: Maybe with some of them, they'd end up being us, but- My spot checking has made me think that we do have A and also American data represented here.
SPEAKER_00: But, in any case, this is reduced down from really quite a long- Much longer list.
SPEAKER_00: Yeah, this is great.
SPEAKER_00: This is really, really helpful.
SPEAKER_00: Functionally pretty, you know, also.
SPEAKER_00: It was fascinating.
SPEAKER_00: I was listening to some of these, I guess, two nights ago, and it's just hilarious to listen to- To do a search for the m-hms, and you get m-hm, and do- Everybody's doing it.
SPEAKER_00: Just doing it.
SPEAKER_00: I think it would be fun to make a montage of it, because- Performance are just a extract.
SPEAKER_00: Right.
SPEAKER_00: It's really, it's really fun to listen.
SPEAKER_00: All these different vocal tracks, you know, but it's the same item, it's very interesting.
SPEAKER_00: Okay.
SPEAKER_00: Then the acronyms, and the ones in parentheses are ones which the transgarber wasn't sure of, and I haven't been able to listen to it to clarify.
SPEAKER_00: But you can see that the parentheses convention makes it very easy to find them, because it's the only question mark for- The question mark is punctuation, so they said that- Oh.
SPEAKER_00: D-C?
SPEAKER_00: So it's PLP?
SPEAKER_00: Exactly.
SPEAKER_00: Exactly.
SPEAKER_00: Yeah, so the only- Well, and I do have a stress marker here.
SPEAKER_00: Sometimes the contrast to a stress is showing up, and-
SPEAKER_07: That's right, I got lost here. What's the difference between the parenthesis acronym and the non-prenthesis?
SPEAKER_00: The parenthesis is something that the transgarber thought was A and N, but wasn't entirely sure.
SPEAKER_00: So I'd need to go back, or someone needs to go back and say, you know, yes or no, and then get rid of the parentheses.
SPEAKER_00: But the parentheses are used only in that context in the transcripts of- I've noticed- Noticing that there's something uncertain.
SPEAKER_00: Yeah, I mean, they have no idea right.
SPEAKER_04: If you hear CTPD, I mean, they do pretty well, but it's- I don't recognize- You know, how are they gonna know?
SPEAKER_06: I think a lot of them are the networks we think.
SPEAKER_00: I think that's true.
SPEAKER_00: Yeah, absolutely.
SPEAKER_00: In fact, a lot of these are coming from them.
SPEAKER_00: I listen to some of that.
SPEAKER_03: Although we don't have that many acronyms, comparatively.
SPEAKER_03: In this meeting, it's not a bad thing.
SPEAKER_00: And robustness is a fair amount, but the NSA group is just very, very many.
SPEAKER_04: The recognizeer is funny.
SPEAKER_04: kept getting PTA for PTA.
SPEAKER_04: This is supposed to rain, and the PTA wasn't these topics about children.
SPEAKER_04: That's interesting.
SPEAKER_00: Is the PTA working?
SPEAKER_00: Sometimes I mean, you see a couple of these are actually OK's.
SPEAKER_00: So it's- it's- it's maybe that they got to the point where it was low enough, understandable- and stand-ability that they weren't entirely sure the person said OK.
SPEAKER_00: You know, so it isn't really necessarily a- an undes ifable acronym, but it just needs to be double-checked.
SPEAKER_00: Now we get to the comments.
SPEAKER_07: The number to the left is the number of consonants.
SPEAKER_00: The number of times out of the entire database, except for that last 30 minutes I haven't checked yet.
SPEAKER_06: So CTS is really good.
SPEAKER_02: Yeah, I wonder what it is.
SPEAKER_02: So what is- I just between papers, rostling and drostling papers.
SPEAKER_00: I'd have to listen.
SPEAKER_00: I- I agree.
SPEAKER_00: I'd like to standardize these down farther, but, um, to me that sounds equivalent.
SPEAKER_00: I'm a little hesitant to- to collapse cross categories unless I actually listen to them.
SPEAKER_06: Oh, I'm sure we've said XML more than five times.
SPEAKER_06: Well, at least now.
SPEAKER_01: That's a least six times.
SPEAKER_01: OK, well- I'm so preferential.
SPEAKER_04: Yes, it's very bad.
SPEAKER_04: Well, this is exactly how people will prove that these meetings do differ because we're recording, right?
SPEAKER_04: Yes.
SPEAKER_04: Normally you don't go around saying, now you've said it six times.
SPEAKER_00: But you notice that there were 785 instances of OK.
SPEAKER_00: Yeah.
SPEAKER_00: And that's not the right one.
SPEAKER_00: And that's not the right one.
SPEAKER_00: And that's not the right one.
SPEAKER_00: The first question.
SPEAKER_00: So, yeah.
SPEAKER_00: On the page two acronyms.
SPEAKER_03: Yeah.
SPEAKER_03: Is this after- Like, did you do some replacements for all the different forms of OK to this?
SPEAKER_00: OK.
SPEAKER_00: So, that's the single existing convention for OK.
SPEAKER_08: Wait a minute.
SPEAKER_08: It's not worth a 788.
SPEAKER_03: Although, there's one with a slash after it.
SPEAKER_03: Yeah.
SPEAKER_00: That's kind of disturbing.
SPEAKER_00: I look for that one.
SPEAKER_00: I actually explicitly look for that one.
SPEAKER_00: And I think that I'm not exactly sure about that.
SPEAKER_08: Is that somewhere where they were going to say new speakers or something?
SPEAKER_00: No, I look for that.
SPEAKER_00: That doesn't actually exist.
SPEAKER_00: And maybe- I can't explain that.
SPEAKER_03: That's all right.
SPEAKER_00: It's only- It's the only pattern that has a slash after it.
SPEAKER_00: And I think it's not-
SPEAKER_06: I was just looking at the bottom of page three. Is that 2B or not 2B?
SPEAKER_06: Yeah.
SPEAKER_01: There's no tilde in front of it.
SPEAKER_01: That's funny.
SPEAKER_00: Yeah.
SPEAKER_00: OK.
SPEAKER_00: Anyways.
SPEAKER_06: There is one.
SPEAKER_06: There is one.
SPEAKER_06: It's no on topic, Adam.
SPEAKER_00: Well, let's- Let's legitimate- So, now, comments you can see they're listed again.
SPEAKER_00: Same deal with the exhaustive listing of everything found and everything except for these final 30 minutes.
SPEAKER_06: OK. So, on some of these quals- Yeah.
SPEAKER_06: Are they really quals or are they glosses?
SPEAKER_06: So, like, there's a- Qual TCL?
SPEAKER_00: TCL. Where do you see that?
SPEAKER_00: Uh-oh.
SPEAKER_00: The reason is because it was said tickle.
SPEAKER_06: Oh, I see, I see.
SPEAKER_06: So, it's not gloss.
SPEAKER_06: OK, I see.
SPEAKER_06: Yeah.
SPEAKER_03: It wasn't said TCL.
SPEAKER_03: Should it be called TIC-KLE or something?
SPEAKER_03: Like, it's not-
SPEAKER_00: In the actual script, in the actual transcript, I-so this happens in the very first one.
SPEAKER_00: I actually wrote it as tickle.
SPEAKER_00: OK.
SPEAKER_00: Because they didn't say TCL, they said tickle.
SPEAKER_00: Yeah.
SPEAKER_00: And then, following that is, Qual TCL.
SPEAKER_00: Oh, I see.
SPEAKER_03: OK.
SPEAKER_03: I forget what's Qual.
SPEAKER_00: Qualifier.
SPEAKER_00: Just comment about what they said.
SPEAKER_08: Comment.
SPEAKER_00: Yeah.
SPEAKER_00: Comment, dirt, text, your comment.
SPEAKER_08: So, they didn't mean tickle as in, Elmo, then tickle.
SPEAKER_08: But at some point, we probably-
SPEAKER_04: We should add it to the dictionary. No, it's the pronunciation model.
SPEAKER_04: What did I say?
SPEAKER_06: Language.
SPEAKER_06: Well, both.
SPEAKER_06: We can add it to the language model.
SPEAKER_04: Yeah, but it's the pronunciation model.
SPEAKER_04: It has to have a pronunciation of TIC-KLE.
SPEAKER_06: Well, TIC-KLE was pronounced TIC-KLE.
SPEAKER_06: Right.
SPEAKER_06: What do you say?
SPEAKER_06: It's pronounced the same as the verb.
SPEAKER_06: So, I think it's the language model that makes it different.
SPEAKER_06: Oh, sorry.
SPEAKER_04: What I meant is that there should be a pronunciation tickle for TCL as a word.
SPEAKER_04: And that word stays in the language model wherever it was.
SPEAKER_04: Right.
SPEAKER_04: Yeah, you never would put tickle in the language model in that form.
SPEAKER_04: Right.
SPEAKER_04: There's actually a bunch of cases like this.
SPEAKER_08: So, how would there be a problem for doing the language model and then with our transcripts over here?
SPEAKER_04: Yes.
SPEAKER_04: Yeah.
SPEAKER_04: Yeah, so there's a few cases like that where the word needs to be spelled out in a consistent way as it would appear in the language, but there's not very many of these.
SPEAKER_04: Tickles one of them.
SPEAKER_04: And you'll have to do it synchronously.
SPEAKER_04: Right.
SPEAKER_06: So, whoever's creating the new models will have to also go through the transcripts and choose them synchronously.
SPEAKER_04: Right.
SPEAKER_04: We have this, there is this thing I was going to talk to you at some point about, you know, what do we do with the dictionary as we're updating the dictionary?
SPEAKER_04: These changes have to be consistent with what's in the, like spelling people's names and so forth.
SPEAKER_04: If we make a spelling correction to their name, like someone had Deborah Tannen's name, Miss Bell, and since we know who that is, you know, we can correct it, but we need to make sure we have the misspell, if it doesn't get corrected, we have to have a pronunciation as a misspell word in the dictionary, it's like that.
SPEAKER_00: Well, of course now the, the Tannen, the spelling, I pick those up in the frequency check.
SPEAKER_00: Right.
SPEAKER_04: Right.
SPEAKER_04: So, if there's things that get corrected before we get them, it's not an issue.
SPEAKER_04: If there's things that we change later, then we always have to keep our dictionary up to date.
SPEAKER_04: And then, yeah, in the case of tickle, I guess we would just have a word TCL, which, which normally would be an acronym, you know, TCL, but just has another pronunciation.
SPEAKER_04: Yeah.
SPEAKER_00: XCs is one of those that sometimes people pronounce and sometimes they say I see a sign.
SPEAKER_00: So, those that are listed in the acronyms, I actually know they were said as letters.
SPEAKER_00: The others, those really do need to be listened to, because I haven't been able to go to all the XC things.
SPEAKER_00: And until they've been listened to, they stay as ICSI.
SPEAKER_00: Right.
SPEAKER_07: Don and I were just noticing, love this one over on page three.
SPEAKER_07: Vocal gesture mimicking sound, obscuring something to head to whole blanket place.
SPEAKER_07: It's great.
SPEAKER_07: It was me.
SPEAKER_06: It was, in fact, it was.
SPEAKER_06: Yeah.
SPEAKER_06: A lot of these are me.
SPEAKER_06: He said he said he said with a high pitch and lengthening.
SPEAKER_06: That was the, I was imitating beeping out.
SPEAKER_06: Perfect.
SPEAKER_04: Oh, there is something.
SPEAKER_04: I spelled out BEEE.
SPEAKER_04: Yeah.
SPEAKER_04: That's been changed.
SPEAKER_04: Thank you.
SPEAKER_04: Because he was saying, how many of these do I have to allow?
SPEAKER_03: You need a lot of qualification in it.
SPEAKER_03: That's been changed.
SPEAKER_00: So, exactly.
SPEAKER_00: That's where the lengthening comment can't get in.
SPEAKER_00: Right.
SPEAKER_00: Right.
SPEAKER_00: So, the vocalization.
SPEAKER_00: And those, of course, get picked up on the frequency check, because CBE, and you know, I mean, it gets kicked out in the spelling and also gets kicked out in the frequency listing.
SPEAKER_00: And I have the various things like breathe versus breath versus inhale.
SPEAKER_00: And, you know, I don't know.
SPEAKER_00: I think they don't have any implications for anything else.
SPEAKER_00: So, it's like I'm tempted to leave them for now.
SPEAKER_00: And it's easy enough to find them when they're in curly brackets.
SPEAKER_00: We can always get an exhaustive listing of these things and find them and change them.
SPEAKER_07: Things finale types on.
SPEAKER_07: Yeah.
SPEAKER_00: This is the first meeting.
SPEAKER_00: Yeah, but I don't actually remember what it was, but that was Eric did that.
SPEAKER_06: Yeah.
SPEAKER_06: So, on.
SPEAKER_06: Ta-da.
SPEAKER_00: I think maybe something like that.
SPEAKER_00: Maybe, yeah.
SPEAKER_00: I'll take a call if I want.
SPEAKER_06: On the glosses for numbers.
SPEAKER_00: Yeah.
SPEAKER_06: It seems like they're lost in different ways it's being done.
SPEAKER_00: Yes.
SPEAKER_00: Okay.
SPEAKER_00: Now, first of all, very important.
SPEAKER_00: Check, check lead to refinement here, which is to add numbs if these are parts of the red numbers.
SPEAKER_00: Now, you already know that I had places where they hadn't transcribed numbers, put numbers in place of any kind of numbers, but there were places where they, this convention came later and at the very first digits task in some transcripts, they actually transcribed numbers and, um, check point out that this is a red speech and it's nice to have the option of ignoring it for certain other pop things.
SPEAKER_00: And that's why there's this other tag here which occurs a 105, or 305 times right now, which is just, well, and numbs by itself, which means this is part of the numbers task.
SPEAKER_00: I may change it to digits.
SPEAKER_00: I mean, with the said command, you can really just change it however you want because it's systematically encoded.
SPEAKER_00: Yeah.
SPEAKER_00: You have to think about what's the best for the overall purposes, but in any case, numbers and numbs are part of this digits task thing.
SPEAKER_00: Now, then I have these numbers that have quotation marks around them.
SPEAKER_00: I didn't want to put them in its gloss comments because then you get the substitution.
SPEAKER_00: And actually, the reason I did it this way was because I initially started out with the other version.
SPEAKER_00: You have the numbers and you have the full form and the print of these.
SPEAKER_00: However, sometimes people stumble over these numbers they're saying.
SPEAKER_00: So you say, 78.2 or whatever.
SPEAKER_00: And there's no way of capturing that if you're putting the numbers off to the side.
SPEAKER_00: So what's the whole left of these?
SPEAKER_00: The left is, so example, the very first one, it would be spelled out in words.
SPEAKER_06: Okay, that's what I was asking.
SPEAKER_00: 0.5.
SPEAKER_00: Right.
SPEAKER_00: Only it's spelled out in words.
SPEAKER_00: So this is also spelled out in words.
SPEAKER_00: 0.5.
SPEAKER_00: Good.
SPEAKER_00: And then in here, numbs.
SPEAKER_00: So it's not going to be mistaken as a gloss.
SPEAKER_00: It comes out as numbs.
SPEAKER_00: Quote.5.
SPEAKER_06: Okay, now the other example is in the glosses right there.
SPEAKER_06: Gloss.1.1-130.
SPEAKER_06: Oh, no.
SPEAKER_00: What's the left of that?
SPEAKER_00: In that case, it's people saying things like 1.1.1-so-so or they're saying 2.0, whatever.
SPEAKER_00: And in that case, it's part of the numbers task and it's not going to be included in the red digits anyway.
SPEAKER_00: So there will be a numbs tag on those lines?
SPEAKER_08: There is.
SPEAKER_00: Yeah.
SPEAKER_00: I've added that all now too.
SPEAKER_00: There's a numbers tag.
SPEAKER_03: I'm sorry, I didn't follow that last thing.
SPEAKER_00: So gloss.
SPEAKER_00: In the same line that would have a gloss, quote, 1.1.1-130.
SPEAKER_00: Right.
SPEAKER_00: You'd have a gloss at the end of the line saying curly bracket, numbs curly bracket.
SPEAKER_00: So if you did a graph minus V numbs, so you get rid of anything that was red.
SPEAKER_04: So there wouldn't be something like, if somebody said something like, boy, I'm really tired, OK, and then started reading.
SPEAKER_04: That would be in a separate line.
SPEAKER_04: Yes.
SPEAKER_04: OK, great.
SPEAKER_04: Because I was doing the graph minus V quick and dirty and looked like that was working OK.
SPEAKER_04: Good.
SPEAKER_04: Great.
SPEAKER_04: Now, why do we, what's the reason for having like the 0.5 have the numbs on it?
SPEAKER_04: Is it just like when they're talking about their data or something?
SPEAKER_00: This is more because, yeah, oh, these are all these, the numbs point side.
SPEAKER_00: These are all where they're saying point.
SPEAKER_00: It's something or other.
SPEAKER_00: And the other thing too is for readability, the transcript.
SPEAKER_00: I mean, if you're trying to follow this while you're reading it, it's really hard to read.
SPEAKER_00: So in the data column 5 has 1.5 compared to 79.6.
SPEAKER_00: It's like, when you see the words, it's really hard to follow the argument.
SPEAKER_00: And this is just really a way of someone who would handle the data in a more discoracy way to be able to follow what's being said.
SPEAKER_00: So this is where it chucks overall architecture comes in, where we're going to have a master file of the channelized data.
SPEAKER_00: There will be scripts that are written to convert it into these main two uses.
SPEAKER_00: And some scripts will take it down into a format that's usable for the recognizer.
SPEAKER_00: Other scripts will take it to a form that's usable for linguistics and discourse analysis.
SPEAKER_00: And the implication that I have is that the master copy will stay unchanged.
SPEAKER_00: These will just be things that are generated.
SPEAKER_00: And by using scripts, when things change, then the script will change.
SPEAKER_00: But there won't be stored copies in different versions of things.
SPEAKER_04: So I guess I'd have one request here, which is just maybe to make it more robust, that the tag, whatever you would choose for this type of numbs, where it's inside the spontaneous speech is different than the tag that you use for the red speech.
SPEAKER_04: That would argue for changing the other ones.
SPEAKER_04: That way, if we make a mistake parsing or something, we don't see the.5 or it's not there, then we just, and actually for things like 7-8th, or people do fractions too, I guess, maybe you want one overall tag for sort of that would be similar to that.
SPEAKER_04: Or as long as there's different strings that will make our processing more robust, because we really will get rid of everything that has the numbs string in it.
SPEAKER_08: I suppose what you could do is just make sure that you get rid of everything that has curly brace numbs, curly brace.
SPEAKER_08: Exactly. That would be that. That was my motivation.
SPEAKER_00: And these can be changed, like I said.
SPEAKER_00: As I said, I was considering changing it to digits.
SPEAKER_00: And it's just a matter of deciding on whatever it is and being sure the scripts know.
SPEAKER_04: It would probably be safer if you're willing to have a separate tag, just because then we know for sure, and we can also do counts on them without having to do the processing.
SPEAKER_04: But you're right, we could do it this way. It should work.
SPEAKER_04: Yeah, and it makes it, I guess, the thing about... Probably not hard for a person to tell the difference, because one's in the context of a transcribed word.
SPEAKER_00: The thing is you can get really so minute with these things and increase the size of the files and decrease of readability such an extent by simply something like percent.
SPEAKER_00: Now, I could have adopted a similar convention for percent, but somehow percent is not so hard.
SPEAKER_00: When you have these points and you're trying to figure out where the decimal voices are, percent's easy to detect. Point, however, is a word that has a couple different meanings, and you'll find both of those in one of these meetings.
SPEAKER_00: We're saying the first point I want to make is so-and-so on.
SPEAKER_00: Those two four points and also has all these decimals.
SPEAKER_08: So, Liz, what does the recognizer do?
SPEAKER_08: What is the SRA recognizer output for things like that?
SPEAKER_08: 7.5. Does it output the word...
SPEAKER_08: 7.5.
SPEAKER_04: Right, the word 7?
SPEAKER_08: The number 7?
SPEAKER_08: The word 7.
SPEAKER_08: Yeah.
SPEAKER_04: And actually, you know, the language...
SPEAKER_04: What we talk about point...
SPEAKER_04: The same point, actually, the word 2 and the word...
SPEAKER_04: They're going to and to go to.
SPEAKER_04: Those are two different twos.
SPEAKER_04: So, there's no distinction there.
SPEAKER_04: It's just the word point.
SPEAKER_04: Every word has only one version, even if it's...
SPEAKER_04: Actually, even like the word read and read, those are two different words.
SPEAKER_04: They're spelled the same way.
SPEAKER_04: They're still going to be transcribed as R-E-A-D.
SPEAKER_04: So, yeah, I like the idea of having this in there.
SPEAKER_04: I was a little bit worried that the tag for removing the red speech.
SPEAKER_04: Because what if we have like red letters or red...
SPEAKER_04: We might want to just send them to the tag.
SPEAKER_01: It says it's red.
SPEAKER_04: Yeah, basically.
SPEAKER_04: But other than that, it sounds great.
SPEAKER_06: Okay.
SPEAKER_06: Are we done?
SPEAKER_00: Well, I wanted to say also regarding the channelized data that...
SPEAKER_00: Yeah.
SPEAKER_00: We requested that we get some segments done by hand to reduce the size of the time.
SPEAKER_00: It's what was Chuck was mentioning earlier.
SPEAKER_00: That if you said, oh, and it was in part of a really long complex overlapping segment, that the same starting end times would be held for that one, for the longer utterances.
SPEAKER_06: We did that for one meeting, right?
SPEAKER_06: So, you have that data, don't you?
SPEAKER_06: Yeah, that's a training data.
SPEAKER_00: He requested that there be similar samples done for five minute stretches, involving a variety of speakers and overlapping sections.
SPEAKER_00: He gave me...
SPEAKER_00: He did the very nice...
SPEAKER_00: He did some shopping through the data and found segments that would be useful.
SPEAKER_00: And at this point, all four of the ones that he specified have been done in addition.
SPEAKER_00: I have the transcribers expanding the amount that they're doing, actually.
SPEAKER_00: So, right now, I know that as of today, we got an extra 15 minutes of that type.
SPEAKER_00: And I'm having them expand the realm on either side of these places where they've already started.
SPEAKER_02: Okay.
SPEAKER_00: But if...
SPEAKER_00: And he's going to give me some more sections that he thinks would be useful for this purpose.
SPEAKER_00: Because it's true.
SPEAKER_00: I mean, if we could do the more fine grain tuning of this using an algorithm, that would be so much more efficient.
SPEAKER_00: And so, this is going to be...
SPEAKER_02: I thought we should perhaps try to start with those channelized versions, just to try to give one transcriber the channelized version of my speech and on speech detection.
SPEAKER_02: And look, if that's helpful for them or just let them try, if that's better.
SPEAKER_02: You mean to start from scratch and brand new transcriber?
SPEAKER_00: That'd be excellent.
SPEAKER_00: Yeah, that'd be really great.
SPEAKER_00: As it stands, we're still on the phase of sort of cleaning up the existing data, getting things in more tightly, time in the line.
SPEAKER_00: I also want to tell...I also want to bring the issue that...
SPEAKER_00: Okay, so there's this idea we're going to have this master copy, the transcript, it's going to be modified by scripts into these two different functions.
SPEAKER_00: And actually the master...
SPEAKER_00: Two or more to...
SPEAKER_00:...to our more.
SPEAKER_00: And the master is going to be the channelized version.
SPEAKER_00: Right.
SPEAKER_00: So right now, we've taken this initial one.
SPEAKER_00: It was a single channel, basically, the way it was input.
SPEAKER_00: And now, thanks to the advances made in the interface, we can, from now on, use the channelized part and any changes that are made in the channelized version, anything.
SPEAKER_00: But I wanted to get all the finishes and the checks.
SPEAKER_00: So that has implications for your scripts.
SPEAKER_03: So have those...the 10 hours that have been transcribed already, have those been channelized?
SPEAKER_03: Yes they have.
SPEAKER_00: And I've seen they've been channelized.
SPEAKER_00: Except for the missing 30 minutes.
SPEAKER_03: Right.
SPEAKER_03: And they've been...has the time...have the time marking been adjusted?
SPEAKER_00: For a total of like 20...for the total of the C-Pros, total of about 30 minutes, that's been the case.
SPEAKER_00: And plus the training.
SPEAKER_03: I guess, I mean, I don't know if we should talk about this now or now.
SPEAKER_03: But that's just where...
SPEAKER_03: Missing T.
SPEAKER_03: Yeah, I know.
SPEAKER_03: No, but I mean my question is, like, should I wait until all of those are processed and channelized like the time markings are adjusted before I do all the processing?
SPEAKER_03: And we start like branching off into the...into our layer of...
SPEAKER_00: Well, you know, the problem is that some...
SPEAKER_00: some of the adjustments that they're making are to bring...
SPEAKER_00: are to combine bins that were...
SPEAKER_00: time bins, which were previously separate.
SPEAKER_00: And the reason they do that is sometimes there's a word that's cut off.
SPEAKER_00: Right.
SPEAKER_00: And so it's true that it's likely to be adjusted in the way that the words are more complete.
SPEAKER_00: And...
SPEAKER_00: Okay.
SPEAKER_00: No, I know that adjusting those things is going to make it better.
SPEAKER_03: Yeah.
SPEAKER_03: I'm going to be more reliable in that.
SPEAKER_03: I'm not sure.
SPEAKER_03: I'm not sure.
SPEAKER_03: Yeah.
SPEAKER_03: I'm going to be more reliable in that.
SPEAKER_00: It's going to make it better.
SPEAKER_00: Yeah.
SPEAKER_00: I'm going to be more reliable in that.
SPEAKER_00: Yeah.
SPEAKER_00: And it will be to apply an algorithm because...
SPEAKER_00: Yeah.
SPEAKER_00: This takes time.
SPEAKER_00: You know, it takes a couple hours to do 10 minutes.
SPEAKER_03: Yeah, I don't doubt it.
SPEAKER_03: So...
SPEAKER_08: So right now, what you're doing is you're taking the...
SPEAKER_08: the original version and you're sort of channelizing yourself.
SPEAKER_08: Yeah.
SPEAKER_03: I'm doing it myself.
SPEAKER_03: I mean, if the time markings aren't different across channels, like the channelized version really doesn't have any more information.
SPEAKER_03: So I was just... I mean, originally I had done it before, like the channelized versions were coming out.
SPEAKER_03: Right.
SPEAKER_03: And so it's a question of...
SPEAKER_08: I think probably the way it'll go is that, you know, when we make this first general version and then start working on the script, that script that will be made, you know, primarily come from what you've done, we'll need to work on a channelized version of those originals.
SPEAKER_08: And so it should be pretty much identical to what you have to...
SPEAKER_08: except for the one that they've already tightened the boundaries.
SPEAKER_08: Right.
SPEAKER_08: Yeah.
SPEAKER_08: So...
SPEAKER_08: And then probably what will happen is as the transcribers finish tightening more and more, you know, that original version will get updated and we'll rerun the script and produce better versions.
SPEAKER_08: Okay.
SPEAKER_08: But I guess the effect for you guys, because you're pulling out the little waveforms into separate ones, that would mean these boundaries are constantly changing.
SPEAKER_08: You definitely constantly rerun that.
SPEAKER_08: Right.
SPEAKER_04: But that's not hard.
SPEAKER_04: I think the harder part is making sure that the transcription...
SPEAKER_04: So if you merge two things, then you know that it's a sum of the transcripts.
SPEAKER_04: But if you split inside something, you don't know where the word, which words moved.
SPEAKER_04: And that's where it becomes a little bit having to rerun the processing.
SPEAKER_04: The cutting of the waveform is pretty trivial.
SPEAKER_03: I mean, as long as it can all be done automatically, I mean, then that's not a concern.
SPEAKER_03: So if I just have to run three scripts to extract it, I'll let it run on my computer for an hour and a half, or however long it takes to parse and create all the reference files, it's not a problem.
SPEAKER_03: So yeah, as long as we're at that point, and I know exactly what steps, what work, what's going on in the editing process.
SPEAKER_03: Okay.
SPEAKER_00: So that's...
SPEAKER_00: I mean, I could...
SPEAKER_00: There were other checks that I did, but it's...
SPEAKER_00: I think that we...
SPEAKER_00: Unless you think there's anything else I think that I covered it.
SPEAKER_00: I can't give...
SPEAKER_02: Any other...
None: Okay.
None: Okay.
None: Okay.
SPEAKER_01: Oh.
