{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing our summarization service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../summarizer_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"../summarizer_service.py\"\n",
    "\n",
    "from django.http import HttpResponse\n",
    "from mangorest.mango import webapi\n",
    "import whisper, hashlib, os, datetime, json, torch\n",
    "from transformers import pipeline\n",
    "import keybert\n",
    "import math\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Remove timelines and return the result in this format:\n",
    "    {SPEAKER}: {SENTENCES}\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    lines = text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        parts = line.split('|')\n",
    "        speaker = parts[1].strip().split(':')[0]\n",
    "        content = parts[1].strip().split(':')[1].strip()\n",
    "        result.append(f\"{speaker}: {content}\")\n",
    "    return '\\n'.join(result)   \n",
    "\n",
    "#-----------------------------models------------------------------------------------------------------------               \n",
    "summarizer = pipeline(\"summarization\", \"vmarklynn/bart-large-cnn-samsum-acsi-ami-v2\", truncation=True)\n",
    "kw_model = keybert.KeyBERT(model='all-mpnet-base-v2')\n",
    "#-----------------------------------------------------------------------------------------------------               \n",
    "\n",
    "@webapi(\"/parrot/summarize_text/\")\n",
    "def summarizeText(request, **kwargs):\n",
    "    post_data = request.POST.dict()\n",
    "    transcription = post_data.get('transcription')\n",
    "    text = post_data.get('text')\n",
    "    wordCount = post_data.get('wordCount')\n",
    "    \n",
    "    input_cleanned_text = preprocess(transcription)\n",
    "    print(\"\\n\\n\", input_cleanned_text, \"\\n\\n\")\n",
    "    # print( \"min: \", math.ceil(int(wordCount) * 0.1), \"max: \", math.ceil(int(wordCount) * 0.25))\n",
    "    print(\"\\n\\nSummarizing...\")\n",
    "    summary = summarizer(input_cleanned_text)[0]['summary_text']\n",
    "    print(\"\\n\", summary, \"\\n\")\n",
    "    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(1, 1), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_1= list(dict(keywords).keys())\n",
    "    print(keywords_list_1)\n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(2, 2), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)\n",
    "    keywords_list_2= list(dict(keywords).keys())\n",
    "    print(keywords_list_2)    \n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                     keyphrase_ngram_range=(3, 3), \n",
    "                                     stop_words='english', \n",
    "                                     highlight=False,\n",
    "                                     top_n=5)    \n",
    "    keywords_list_3 = list(dict(keywords).keys())\n",
    "    print(keywords_list_3)\n",
    "    \n",
    "    response = {'transcription': transcription, 'summary': summary, \n",
    "                'keywords_list_1': keywords_list_1, 'keywords_list_2': keywords_list_2,\n",
    "                'keywords_list_3': keywords_list_3,}\n",
    "    return HttpResponse(json.dumps(response), content_type='application/json')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------               \n",
    "@webapi(\"/parrot/summarize_summary/\")\n",
    "def summarizeSummary(request, **kwargs):\n",
    "    post_data = request.POST.dict()\n",
    "    summary_input = post_data.get('summary')\n",
    "    wordCount = post_data.get('wordCount-summ')\n",
    "    \n",
    "    print( \"min: \", math.ceil(int(wordCount) * 0.1), \"max: \", math.ceil(int(wordCount) * 0.25))\n",
    "    print(\"\\n\\nSummarizing again...\")\n",
    "    summary = summarizer(summary_input, min_length = math.ceil(int(wordCount) * 0.1), max_length = math.ceil(int(wordCount) * 0.25))[0]['summary_text']\n",
    "    print(\"\\n\", summary, \"\\n\")\n",
    "    \n",
    "    response = {'summary': summary}\n",
    "    return HttpResponse(json.dumps(response), content_type='application/json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5115531045578c37682d962fe3244651dfde701d1aa72ed856a496a3ac6ab995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
