0:00:00	SPEAKER_03
 So next week we'll have both Berger and Michael, Michael Clansmith, Berger Comire will join us.

0:00:17	SPEAKER_03
 And you're probably going to go up in a couple of three weeks or so, when are you thinking of going up to...

0:00:23	SPEAKER_05
 Yeah, like, not next week, but maybe the week after.

0:00:30	SPEAKER_03
 Good, so at least we'll have one meeting with you with your store and that's good.

0:00:37	SPEAKER_05
 Yeah, well, maybe you can start with this.

0:00:46	SPEAKER_05
 All today, huh?

0:00:52	SPEAKER_05
 Yeah, so there was this conference call this morning and the only topic on the agenda was just to discuss and to get a decision about this latency problem.

0:01:05	SPEAKER_05
 No, this is a conference call between different rural people or just the conference call between the overall group.

0:01:14	SPEAKER_05
 Yeah, there were like two hours of discussions and then suddenly people were tired, I guess, and they decided on a number of 220.

0:01:28	SPEAKER_05
 Including everything.

0:01:31	SPEAKER_05
 It means that it's like 80 milliseconds less than before.

0:01:38	SPEAKER_04
 And what are we sitting at currently?

0:01:40	SPEAKER_05
 So currently we have a system that has 230, so that's fine.

0:01:46	SPEAKER_05
 230?

0:01:47	SPEAKER_05
 Yeah, so that's the system that's described on the second point of...

0:01:50	SPEAKER_04
 So we have to reduce it by 10 milliseconds.

0:01:53	SPEAKER_05
 Yeah, but that's not a problem, I guess.

0:01:58	SPEAKER_04
 Okay, it's primarily determined by the VAD at this point, right?

0:02:03	SPEAKER_03
 Yeah, so we can make the VAD a little shorter.

0:02:06	SPEAKER_03
 We probably should do that pretty soon so that we don't get used to it being a certain way.

0:02:12	SPEAKER_03
 Was Harri on the phone?

0:02:14	SPEAKER_05
 Yeah, sure.

0:02:15	SPEAKER_05
 Well, it was mainly a discussion between Harri and David who was like...

0:02:21	SPEAKER_05
 Yeah.

0:02:28	SPEAKER_05
 So the second thing is the system that we have currently, we have like a system that gives 62% improvement, but if you want to stick to this latency...

0:02:41	SPEAKER_05
 Well, it has a latency of 230, but if you want also to stick to the number of features that's limited to 60, then we go a little bit down, but it's still 61%.

0:02:54	SPEAKER_05
 And if we drop the tandem network, then we have 57%.

0:03:00	SPEAKER_04
 But the 230 includes the tandem network?

0:03:04	SPEAKER_05
 Yeah.

0:03:05	SPEAKER_04
 Okay.

0:03:06	SPEAKER_03
 And is the tandem network small enough that it will fit on the terminal size?

0:03:12	SPEAKER_05
 No, I don't think so.

0:03:14	SPEAKER_04
 No.

0:03:17	SPEAKER_05
 Okay.

0:03:18	SPEAKER_05
 In terms of computation, if we use their way of computing the MIPS, I think it fits, but it's a linear problem of memory.

0:03:31	SPEAKER_05
 And I don't know how much this can be discussed or not, because it could be in ROM, so it's maybe not that expensive.

0:03:39	SPEAKER_05
 How much memory do you have?

0:03:41	SPEAKER_05
 I don't remember exactly, but yeah, to check that.

0:03:45	SPEAKER_03
 Yeah, I'd like to see that. Maybe I could think a little bit about it, because maybe we could make it a little smaller, or...

0:03:52	SPEAKER_03
 I mean, it'd be neat if we could fit it all.

0:03:54	SPEAKER_03
 I'd like to see how far off we are.

0:03:57	SPEAKER_03
 But I guess it's still within their rules to have it on the server side.

0:04:03	SPEAKER_03
 Yeah.

0:04:05	SPEAKER_04
 Okay.

0:04:08	SPEAKER_04
 This is still...

0:04:10	SPEAKER_03
 Oh, well, you're saying here.

0:04:12	SPEAKER_05
 Yeah, there were small tricks to make this tandem network work.

0:04:19	SPEAKER_05
 And one of the tricks was to use some kind of hierarchical structure, where the silence probability is not computed by the final tandem network, but by the VAD network.

0:04:39	SPEAKER_05
 So apparently, it looks better when we use the silence probability from the VAD network, and we rescale the other probabilities by 1 minus the silence probability.

0:04:54	SPEAKER_05
 So it's some kind of hierarchical thing that Sonilo also tried on Spine, and apparently it helps a little bit also.

0:05:13	SPEAKER_05
 Yeah, the reason why we did that with the silence probability was that...

0:05:19	SPEAKER_03
 I'm really sorry. Can you repeat what you're saying about the silence probability only in my mind?

0:05:24	SPEAKER_05
 So there is a tandem network that estimates the phone probabilities and the silence probabilities also.

0:05:31	SPEAKER_05
 And things get better when instead of using the silence probability computed by the tandem network, we use the silence probability given by the VAD network.

0:05:47	SPEAKER_05
 The VAD network is smaller, but maybe...

0:05:53	SPEAKER_05
 So we have a network for the VAD which has 100 in the unit, and the tandem network has 500.

0:06:01	SPEAKER_05
 So it's smaller, but the silence probability from this network seems better.

0:06:07	SPEAKER_04
 Okay.

0:06:15	SPEAKER_04
 Well, it looks strange, but...

0:06:19	SPEAKER_05
 Maybe it's something to do with the fact that we don't have infinite training data.

0:06:25	SPEAKER_05
 We don't.

0:06:28	SPEAKER_05
 So things are not optimal.

0:06:33	SPEAKER_02
 You were going to say why what made you...

0:06:37	SPEAKER_05
 Yeah, there was a problem that we observed...

0:06:45	SPEAKER_05
 That there were many insertions in the system.

0:06:52	SPEAKER_05
 Actually, plugging the tandem network was increasing, I think, the number of insertions.

0:06:59	SPEAKER_05
 And...

0:07:03	SPEAKER_05
 So it looked strange, and then just using the other silence probability apps.

0:07:11	SPEAKER_05
 So the next thing we will do is train the tandem on more data.

0:07:16	SPEAKER_03
 So you know in a way what it's a little bit like combining knowledge sources, right?

0:07:22	SPEAKER_03
 Because the fact that you have these two nets that are different sizes, means they behave a little differently, they find different things.

0:07:31	SPEAKER_03
 And if you have the distribution that you have from speech sounds, you have sort of one source of knowledge, and rather than just taking one minus that to get the other, which is essentially what's happening.

0:07:47	SPEAKER_03
 You have this other source of knowledge that you're putting in there.

0:07:50	SPEAKER_03
 So you make use of both of them, which you're ending up with.

0:07:54	SPEAKER_03
 Maybe it's better.

0:07:56	SPEAKER_03
 Anyway, probably justify anything.

0:07:58	SPEAKER_05
 And the features are different also.

0:08:00	SPEAKER_05
 Do you need a new, the same features there?

0:08:04	SPEAKER_03
 Oh, that might be a key, actually.

0:08:07	SPEAKER_03
 Thinking about speech versus non-speech.

0:08:12	SPEAKER_03
 That's a good point.

0:08:19	SPEAKER_05
 Well, there are other things that we should do, but...

0:08:24	SPEAKER_05
 to require time.

0:08:27	SPEAKER_05
 We have ideas like, so these things are like having a better VAD.

0:08:32	SPEAKER_05
 We have some ideas about that, to probably implies working a little bit on features that are more suited to a voice activity detection.

0:08:41	SPEAKER_05
 Working on the second stream, of course we have ideas on this or so, but we need to try different things.

0:08:48	SPEAKER_05
 Better noise estimation.

0:08:53	SPEAKER_03
 I mean, back on the second stream, and that's something you've talked about for a while.

0:08:57	SPEAKER_03
 I think that's certainly a high hope.

0:09:00	SPEAKER_03
 Yeah.

0:09:02	SPEAKER_03
 So we have this default idea about just using some sort of purely spectral thing.

0:09:08	SPEAKER_05
 For a second stream.

0:09:11	SPEAKER_05
 We need to first try with this, and it clearly hurts.

0:09:15	SPEAKER_03
 But how was the stream combined?

0:09:19	SPEAKER_05
 It was just combined by the acoustic model.

0:09:24	SPEAKER_05
 So there was no neural network.

0:09:26	SPEAKER_03
 Right, so I mean, if you just had a second stream, it was just Spectro and had another neural net.

0:09:31	SPEAKER_03
 Combined there.

0:09:34	SPEAKER_05
 Might be good.

0:09:38	SPEAKER_05
 Yeah.

0:09:41	SPEAKER_05
 Yeah, and the other thing about noise estimation.

0:09:45	SPEAKER_05
 Maybe try to train the training data for the tandem network right now.

0:09:51	SPEAKER_05
 It's using the noises from the Aurora task.

0:09:55	SPEAKER_05
 I think that people might try to argue about that because then in some cases we have the same noise for training the network.

0:10:06	SPEAKER_05
 The noise is that are used for testing.

0:10:09	SPEAKER_05
 We have to try to get rid of these.

0:10:14	SPEAKER_03
 Maybe just put in some other noise.

0:10:16	SPEAKER_03
 Something is different.

0:10:18	SPEAKER_03
 I mean, it's probably helpful to have a little noise there.

0:10:21	SPEAKER_03
 But maybe something else they could say it was, if it doesn't hurt too much, that's a good idea.

0:10:30	SPEAKER_05
 Yeah, the last thing is that I think we are getting close to human performance.

0:10:35	SPEAKER_05
 Well, that's something I would like to investigate further.

0:10:38	SPEAKER_05
 But I did like, I did listen to the most noisy attorneys of the speech that Caritalian tried to transcribe them.

0:10:52	SPEAKER_03
 So this is a particular human.

0:10:55	SPEAKER_05
 Yeah, so that's the experiment.

0:10:58	SPEAKER_05
 It's just one subject.

0:11:03	SPEAKER_05
 But still, what happens is that the digit error rate on this is around one person while our system is currently at seven person.

0:11:17	SPEAKER_05
 But what happens also is that if I listen to the recentized version of the speech, and I recentize this using a white noise that's filtered by LPC filter, well, you can argue that this is not speech, so the era is not trained to recognize this, but actually it sounds like whispering.

0:11:42	SPEAKER_03
 Well, there's two problems there.

0:11:47	SPEAKER_03
 The first is that by doing LPC 12 with synthesized speech, like you're saying, you're adding other degradation.

0:11:54	SPEAKER_03
 So it's not just the noise, but you're adding in fact some degradation because it's only an approximation.

0:12:00	SPEAKER_03
 The second thing is, which is maybe more interesting, is that if you do it with whispered speech, you get this number.

0:12:11	SPEAKER_03
 What if you had done analysis, recentesis, and taken the pitch as well?

0:12:17	SPEAKER_03
 So now you put the pitch in.

0:12:20	SPEAKER_03
 What would the percentage be then?

0:12:23	SPEAKER_03
 That's a question.

0:12:25	SPEAKER_03
 So you see if it's, let's say it's back down to 1% again.

0:12:30	SPEAKER_03
 That would say at least for people having the pitch is really, really important, which would be interesting in itself.

0:12:36	SPEAKER_03
 If it's stayed up near 5%, then I'd say, boy, LPC 12 is pretty crummy.

0:12:45	SPEAKER_03
 So I'm not sure how we can conclude from this anything about that our system is close to.

0:12:52	SPEAKER_05
 The point is that what I listen to when I recent this idea at BC 12 Spectrum is in a way what the system is hearing.

0:13:05	SPEAKER_05
 Because all the excitation is not taken into account.

0:13:14	SPEAKER_05
 That's what we do with our system.

0:13:17	SPEAKER_05
 Well, it's not doing LPC.

0:13:20	SPEAKER_03
 What if you did LPC 20?

0:13:25	SPEAKER_03
 20.

0:13:27	SPEAKER_03
 The thing is LPC is not a really great representation of speech.

0:13:36	SPEAKER_03
 All I'm saying is that you have in addition to the removal of pitch, you also are doing a particular parameterization, which...

0:13:48	SPEAKER_03
 So let's see, how would you do that?

0:13:51	SPEAKER_05
 That's what we do with our systems.

0:13:54	SPEAKER_03
 We don't, because we do Mel filter bank.

0:14:00	SPEAKER_05
 Is it that different?

0:14:04	SPEAKER_03
 I don't know what Mel based synthesis would sound like, but certainly the specter are quite different.

0:14:11	SPEAKER_01
 Could you test the human performance on the state of original?

0:14:18	SPEAKER_05
 This is the one person's number.

0:14:20	SPEAKER_03
 Yeah, it's 1%. He's trying to remove the pitch information and make it closer to what we're seeing as the feature vectors.

0:14:27	SPEAKER_01
 So your performance was 1%.

0:14:30	SPEAKER_01
 And then when you recent the size of LPC 12, it went to 5.

0:14:35	SPEAKER_03
 I mean, it's a little bit still in the apples and oranges because we are choosing these features in order to be the best for recognition.

0:14:46	SPEAKER_03
 And if you listen to them, they still might not be there, even if you made something closer to what we're going to do.

0:14:52	SPEAKER_03
 It might not sound very good.

0:14:55	SPEAKER_03
 And the degradation from that might actually make it even harder to understand than the LPC 12.

0:15:02	SPEAKER_03
 So all I'm saying is that the LPC 12 puts in synthesis, puts in some degradation.

0:15:08	SPEAKER_03
 That's not what we're used to hearing.

0:15:10	SPEAKER_03
 And it's not just the question of how much information is there as if you will always take maximum advantage of any information that's presented to you.

0:15:20	SPEAKER_03
 In fact, you hear something is better than others.

0:15:24	SPEAKER_03
 But I agree that it says that the kind of information that we're feeding it is probably a little bit minimal.

0:15:34	SPEAKER_03
 There's definitely some things that we've thrown away.

0:15:37	SPEAKER_03
 And that's why I was saying it might be interesting if you, an interesting test of this would be if you actually put the pitch back in.

0:15:44	SPEAKER_03
 So you just extract it from the actual speech and put it back in and see does that, does that make the difference?

0:15:51	SPEAKER_03
 If that takes it down to 1% again, then you say, okay, it's in fact having not just the spectral envelope but also the pitch that has information that people can use anyway.

0:16:06	SPEAKER_01
 From this it's pretty safe to say that the system is with either 2% to 7% away from the performance of a human.

0:16:16	SPEAKER_03
 Well, or it's, yeah, so it's 1.4 times 2% to 7 times the error for Stefan.

0:16:29	SPEAKER_03
 But I don't know, I don't want to take you away from other things, but that's the first thing that I would be curious about is, you know, when you...

0:16:39	SPEAKER_05
 But the signal itself is like a mix of a periodic sound and a voiced sound and the noise which is mostly noise, I mean not periodic.

0:16:54	SPEAKER_05
 So what do you mean exactly by putting back the pitch in the LPC?

0:17:02	SPEAKER_03
 Yeah, you did LPC recentuses.

0:17:05	SPEAKER_03
 LPC recentuses.

0:17:06	SPEAKER_03
 So, and you did it with a noise source rather than with periodic source.

0:17:13	SPEAKER_03
 So if you actually did real recentuses like you do in an LPC synthesizer where it's on voice, do you use noise, where it's voice, do you use periodic pulses?

0:17:23	SPEAKER_05
 Yeah, but it's neither purely voiced or purely unvoiced.

0:17:29	SPEAKER_05
 Especially because there is noise.

0:17:31	SPEAKER_03
 Well, it might be hard to do it, but the thing is that if you detect that there's a very strong periodic components, then you can use a voiced thing.

0:17:42	SPEAKER_03
 Yeah, I mean, it's probably not worth your time. It's a side thing and it's allowed to do.

0:17:46	SPEAKER_03
 But I'm saying, at least as a thought experiment, that's what I would want to test. I would want to drive it with a two-source system rather than a one-source system.

0:17:56	SPEAKER_03
 And then that would tell you whether in fact it's because we've talked about like the harmonic tunneling or other things that people have done based on pitch.

0:18:04	SPEAKER_03
 Maybe that's really a key element. Maybe without that it's not possible to do a whole lot better than we're doing.

0:18:12	SPEAKER_05
 That's what I was thinking by doing this experiment.

0:18:17	SPEAKER_03
 But I mean, other than that, I don't think it's, I mean, other than the pitch-steed information, it's hard to imagine that there's a whole lot more in the signal that we're throwing away.

0:18:29	SPEAKER_05
 Yeah, but yeah.

0:18:32	SPEAKER_03
 I mean, we're using per-number filters in the filter bank.

0:18:44	SPEAKER_04
 Yeah.

0:18:47	SPEAKER_03
 Yeah. That's, I mean, 1% is sort of what I would figure if somebody was paying really close attention, you might get.

0:18:55	SPEAKER_03
 I would actually think that if you look to people in various times of the day in different amounts of attention, you might actually get up to 3% or 4%.

0:19:07	SPEAKER_03
 So we're not incredibly far off. On the other hand, with any of these numbers except maybe the 1%, it's not actually usable.

0:19:18	SPEAKER_03
 A commercial system with a full telephone number or something. Yeah, but these numbers never see.

0:19:27	SPEAKER_05
 Yeah. Right. Well, yeah, these numbers I mean.

0:19:34	SPEAKER_03
 Good. While we're still on Aurora stuff, maybe we can talk a little about the status with the, well, three journal things for it.

0:19:45	SPEAKER_01
 So I've downloaded a couple of things from the SIP state. One is their software, their OVC SR system, downloaded latest version of that, got it compiled and everything.

0:20:00	SPEAKER_01
 Downloaded the scripts. They wrote some scripts that sort of make it easy to run the system on a one-street journal data.

0:20:10	SPEAKER_01
 So I haven't run the scripts yet. I'm waiting. There was one problem with part of it. I wrote it and I know to Joe asking him about it.

0:20:17	SPEAKER_01
 So I'm waiting to hear from him. But I did print something out just to give you an idea about where the system is.

0:20:25	SPEAKER_01
 They, on their website, they did this little table of where their system performs relative to other systems that have done this task.

0:20:34	SPEAKER_01
 And the SIP state system using a diagram grammar is about 8.2%.

0:20:41	SPEAKER_01
 They're comparable systems from, we're getting from like 6.9628%.

0:20:48	SPEAKER_01
 So they're, this is unclean. This is on Corsa.

0:20:51	SPEAKER_01
 Yeah, they've started the table where they're showing their results on various different noise conditions, but they don't have a lot of it filled in.

0:20:59	SPEAKER_01
 And I didn't notice until after I printed out that they don't say here what these different testing conditions are going to have to click on it on the website to see them.

0:21:08	SPEAKER_03
 So I don't know what those, what kind of numbers are they getting on these on them.

0:21:12	SPEAKER_01
 Well, I was a little confused because on this table, I'm, they're showing word error rate, but on this one, I don't know if these are word error rates because they're really big.

0:21:23	SPEAKER_01
 So under condition one here is 10%, and under three goes to 64.6%.

0:21:30	SPEAKER_01
 That's probably a error. I mean, so I guess maybe there are error rates, but they're really high.

0:21:36	SPEAKER_03
 I don't find that, but I mean, what's, what's some of the lower error rates on, on, on some of the higher error rates on some of these highly mismatched, difficult conditions?

0:21:48	SPEAKER_05
 Yeah, it's around 15 to 20%.

0:21:54	SPEAKER_05
 Right. And the baseline error rate.

0:21:57	SPEAKER_05
 Yeah. 20% error rate.

0:21:59	SPEAKER_03
 Yeah. So 20% error rate on digits.

0:22:02	SPEAKER_03
 So if you're doing, so if you're doing, you know, 60,000, yeah, if you're saying 60,000 word recognition, getting 60% error on some of these noise conditions, not at all surprising.

0:22:13	SPEAKER_05
 The baseline is 60% on some digits on the more mismatched conditions.

0:22:19	SPEAKER_01
 Okay.

0:22:20	SPEAKER_05
 Yeah.

0:22:21	SPEAKER_01
 So yeah, that's probably what it is then.

0:22:25	SPEAKER_01
 Yeah, so that's a lot of different conditions.

0:22:27	SPEAKER_03
 It's a bad sign when you're looking at the numbers you can't tell whether it's accuracy or error rate.

0:22:32	SPEAKER_01
 Yeah, it's going to be hard.

0:22:34	SPEAKER_01
 I'm still waiting for them to release the multi CPU version of their scripts, because right now their script only handles processing on a single CPU, which will take a really long time.

0:22:47	SPEAKER_01
 So this is for the training.

0:22:52	SPEAKER_01
 I believe yes, for the training also.

0:22:55	SPEAKER_01
 Okay.

0:22:56	SPEAKER_01
 Yeah, so it gets started because I'll go ahead and fill with just the single CPU one.

0:23:07	SPEAKER_01
 They released like a smaller data set that you can use.

0:23:11	SPEAKER_01
 It only takes like 16 hours to train.

0:23:13	SPEAKER_01
 Oh, good.

0:23:14	SPEAKER_01
 Just to make sure with that.

0:23:16	SPEAKER_01
 Yeah.

0:23:17	SPEAKER_03
 That's cool.

0:23:19	SPEAKER_03
 I guess the actual evaluation will be in six weeks or something.

0:23:23	SPEAKER_03
 So, so all right.

0:23:26	SPEAKER_05
 We don't know yet.

0:23:27	SPEAKER_04
 I think we don't know.

0:23:30	SPEAKER_01
 It wasn't on the conference call this one.

0:23:37	SPEAKER_01
 Did they say anything on the conference call about how the Wall Street Journal part of the test was going to be run?

0:23:48	SPEAKER_01
 I thought I remember hearing that some sites were saying that they didn't have the compute to be able to run the Wall Street Journal stuff at their place.

0:23:56	SPEAKER_01
 So there was some talk about having Mississippi State run assistance for them.

0:24:01	SPEAKER_01
 Did that come up at all?

0:24:03	SPEAKER_05
 I know.

0:24:04	SPEAKER_05
 Well, this first this was not the point at all of this meeting today.

0:24:09	SPEAKER_05
 And frankly, I don't know because I didn't read also the most recent maize about the large vocabulary task.

0:24:18	SPEAKER_05
 But did you still get the maize?

0:24:23	SPEAKER_05
 You're not in the main English.

0:24:26	SPEAKER_01
 Only male I get is from Mississippi State.

0:24:32	SPEAKER_01
 So we should about this.

0:24:35	SPEAKER_03
 I have to say there's something funny sounding about saying one of these big companies doesn't have enough compute power to do that.

0:24:41	SPEAKER_03
 So they're having to have it done by Mississippi State.

0:24:44	SPEAKER_03
 Yeah.

0:24:45	SPEAKER_03
 It just sounds funny.

0:24:49	SPEAKER_03
 Yeah.

0:24:50	SPEAKER_01
 Anyway.

0:24:52	SPEAKER_01
 Yeah, I'm wondering about that because there's this whole issue about, you know, simple tuning parameters like word insertion penalty.

0:25:01	SPEAKER_01
 And whether or not those are going to be tuned or not.

0:25:09	SPEAKER_01
 It makes a big difference.

0:25:11	SPEAKER_01
 You change your front end.

0:25:13	SPEAKER_01
 You know, the scale is completely empty completely different.

0:25:17	SPEAKER_01
 It seems reasonable that that at least should be quick to match the front end.

0:25:22	SPEAKER_05
 You didn't get any answer from Joe.

0:25:25	SPEAKER_01
 I did, but Joe said, you know, what you're saying makes sense.

0:25:29	SPEAKER_01
 I don't know.

0:25:31	SPEAKER_01
 He doesn't know what the answer is.

0:25:33	SPEAKER_01
 I mean, we have this back in front a little bit about, you know, our site is going to, are you going to run this data for different sites?

0:25:40	SPEAKER_01
 And well, if Mississippi State runs it, then maybe they'll do a little optimization on that parameter.

0:25:49	SPEAKER_01
 But then he wasn't asked to run it for anybody.

0:25:52	SPEAKER_01
 So it's just not clear yet what's going to happen.

0:25:56	SPEAKER_01
 He's been putting this stuff out on their website and the people to grab that.

0:26:01	SPEAKER_01
 I haven't heard too much about it.

0:26:03	SPEAKER_03
 So it could be, I mean Chuck and I had actually talked about this a couple of times in some lunches, I think, that one thing that we might want to do, there's this question about, you know, what do you want to scale?

0:26:15	SPEAKER_03
 Suppose you can't adjust these word insertion penalties and so forth.

0:26:19	SPEAKER_03
 So you have to do everything at the level of the features. What could you do?

0:26:22	SPEAKER_03
 And one thing I suggested earlier time was maybe some sort of scaling, some sort of root or something of the features.

0:26:32	SPEAKER_03
 But the problem with that is that isn't quite the same. It occurred to me later because what you really want to do is scale the range of the likelihoods rather than.

0:26:42	SPEAKER_03
 But what might get at something similar just occurred to me is kind of an intermediate thing is because we do this strange thing that we do with the tandem system, at least in that system, what you could do is take the values that come out of the net, which are something like log probabilities and scale those.

0:27:04	SPEAKER_03
 And then, then at least those things would have the right values or the right range. And then that goes into the rest of it and it's used as observation.

0:27:15	SPEAKER_03
 So it's not the way to do it.

0:27:24	SPEAKER_05
 But these values are not directly used as probabilities.

0:27:34	SPEAKER_03
 So what we're doing is pretty strange and complicated. We don't really know what the effect is. So my thought was maybe, I mean, they're not used as probabilities. But the log probabilities were taking advantage of the fact that something like log probabilities has more of a Gaussian shape than probabilities.

0:27:53	SPEAKER_03
 And so we can model them better. So in a way, we're taking advantage of the fact that there are probabilities because they're this quantity that looks kind of Gaussian when you take its log self. Maybe, maybe you would have a reasonable effect to do that. I don't know.

0:28:10	SPEAKER_03
 But I mean, I guess we still haven't had a ruling back on this. And we may end up being a situation where we just really can't change the written session penalty. But the other thing we could do is also be could, I mean, this may not help us in the evaluation, but it might help us in our understanding, at least.

0:28:28	SPEAKER_03
 And it might just run it with different insertion penalties and show that, well, okay, not changing it, playing the roles the way we wanted. We did this, but in fact, if we did that, it made a big difference.

0:28:41	SPEAKER_01
 I wonder if it might be possible to simulate the backend with some other system. So we get our front end features. And then as part of the process of figuring out the scaling of these features, and I hope we're going to take it to a reader to power or something.

0:29:04	SPEAKER_01
 We have some backend that we attach on our features that sort of simulates what would be happening.

0:29:13	SPEAKER_03
 And just adjusted until the best number.

0:29:15	SPEAKER_01
 And just adjusted until that our version of the backend and decides that.

0:29:20	SPEAKER_03
 Well, we can probably use the real thing, can't we? And then just just using out a reduced test set or something.

0:29:26	SPEAKER_03
 And then we just use that to determine some scaling factor. Yeah, so I mean, I think that that's a reasonable thing to do. And the only question is what's the actual knob that we use and the knob that we use should.

0:29:37	SPEAKER_03
 Unfortunately, I don't know the analytic solution to this because what we really want to do is change the scale of the likelihoods, not the scale of the observations, but.

0:29:47	SPEAKER_02
 Yeah. Yeah. I'm curious to hear what we kind of recognize or.

0:29:57	SPEAKER_02
 What do you mean we say what kind of.

0:30:02	SPEAKER_02
 Is it like I got to. Yeah.

0:30:05	SPEAKER_01
 It's the same system that they use when they participate in the hot five evals. It's sort of came out of.

0:30:16	SPEAKER_01
 Looking a lot like hdk. I mean, they started off with when they were building their system, they were always comparing to hdk and make sure they were getting similar results.

0:30:24	SPEAKER_01
 So it's a Gaussian mixture system.

0:30:26	SPEAKER_03
 They have the same sort of mixed down sort of procedure where they start off with a small number of.

0:30:31	SPEAKER_01
 Yeah, and then. Yeah.

0:30:33	SPEAKER_01
 I don't know.

0:30:34	SPEAKER_03
 Yeah.

0:30:35	SPEAKER_03
 Yeah.

0:30:36	SPEAKER_03
 Do you know what kind of tying they use or they sort of some sort of under Gaussian's they share across everything or.

0:30:46	SPEAKER_01
 I have a whole system description that describes exactly what their.

0:30:57	SPEAKER_01
 It's some kind of a mixture of the out.

0:31:09	SPEAKER_03
 So the other or I think maybe I don't know if any of this is going to come in in time to be relevant, but we had talked about.

0:31:17	SPEAKER_03
 Playing around over in Germany and and possibly coming up with something that would fit in later. I saw that other mail where he said that he wasn't going to work for him to do CBS.

0:31:32	SPEAKER_05
 Yeah. So no, he has a version of the software.

0:31:34	SPEAKER_03
 So he just has it all sitting there. Yeah.

0:31:39	SPEAKER_03
 So if he'll he might work on improving the noise estimate or on some histogram thing. Yeah.

0:31:47	SPEAKER_03
 Yeah, I just saw the USB. We didn't talk about it at our meeting, but I just saw the just read the paper.

0:31:53	SPEAKER_03
 Someone I forget the name and they about histogram equalization.

0:31:59	SPEAKER_03
 Did you see that one?

0:32:01	SPEAKER_05
 It was a poster.

0:32:03	SPEAKER_03
 Yeah, I mean, I just read the paper. I didn't see the poster.

0:32:07	SPEAKER_05
 It was something similar to online organization.

0:32:17	SPEAKER_03
 Yeah, but it's a little more it's a little finer, right?

0:32:20	SPEAKER_03
 So they had like 10 quantiles.

0:32:22	SPEAKER_03
 And they just the distribution. So you have the distributions from the training set.

0:32:26	SPEAKER_03
 And then this is just a histogram of the amplitudes, I guess, right?

0:32:33	SPEAKER_03
 And then people do this image processing some you have this kind of histogram of levels of brightness or whatever.

0:32:41	SPEAKER_03
 And then when you get a new thing that you want to adjust to be better in some way, you adjust it so that the histogram of the new data looks like the old data.

0:32:52	SPEAKER_03
 You just kind of piecewise linear or some kind of piecewise approximation. They did a one version of piecewise linear and another that had a power law thing between them between the points.

0:33:03	SPEAKER_03
 And they said they said they sort of see it in a way is for the speech case of being kind of a generalization of spectral subtraction in a way because you know, spectral subtraction, you're trying to get rid of this excess energy.

0:33:17	SPEAKER_03
 It's not supposed to be there. And this is sort of adjusting it for a lot of different levels. And then they have some kind of floor or something.

0:33:31	SPEAKER_03
 So if it gets too low, you don't do it. And they claim very nice results.

0:33:37	SPEAKER_01
 So is this a histogram across different frequency pins?

0:33:42	SPEAKER_03
 Or I think this, you know, I don't remember that. Do you remember?

0:33:47	SPEAKER_05
 I think they have different histograms.

0:33:51	SPEAKER_05
 So it's like one per frequency band.

0:33:54	SPEAKER_05
 So what I did to gram per frequency band. Yeah, I guess.

0:33:59	SPEAKER_05
 But I should read the paper. I just went to the question.

0:34:01	SPEAKER_03
 Yeah. And I don't remember whether it was filter bank things or whether it was FFT bands or.

0:34:08	SPEAKER_01
 That histogram represents the different energy levels that have been seen and have that frequency.

0:34:15	SPEAKER_03
 And how often they have seen them. Yeah.

0:34:18	SPEAKER_03
 Yeah. And they do it. They said that they could do it for the test.

0:34:21	SPEAKER_03
 So you don't have to change the training. You just do a measurement over the training.

0:34:25	SPEAKER_03
 And then for testing, you can do it for one per utterance.

0:34:29	SPEAKER_03
 Even relatively short utterances. They claim it works pretty well.

0:34:33	SPEAKER_01
 So they is the idea that you run test utterance through some histogram generation thing.

0:34:40	SPEAKER_01
 And then you compare the histograms and that tells you what to do to the utterance to make it more like.

0:34:47	SPEAKER_03
 In principle, I didn't read carefully how they actually implemented.

0:34:50	SPEAKER_03
 Yeah.

0:34:51	SPEAKER_03
 Or whether it was a second pass or what, but they know sort of the idea.

0:34:57	SPEAKER_03
 So that seemed different. We were sort of curious about what are some things that are conceptually quite different from what we've done.

0:35:06	SPEAKER_03
 Because one thing that Stephanie and Sennielson defined was they could actually make a unified piece of software that handled a range of different things that people were talking about.

0:35:15	SPEAKER_03
 And it was really just sort of setting a different constants.

0:35:18	SPEAKER_03
 And it would turn one thing into another, turn winner filtering into spectral subtraction or whatever.

0:35:24	SPEAKER_03
 But there's other things that we're not doing. So we're not making any use of pitch.

0:35:29	SPEAKER_03
 Which again might be important.

0:35:32	SPEAKER_03
 Because the stuff between the harmonics is probably schmutz and the transcribers will have fun with that.

0:35:41	SPEAKER_03
 And the stuff at the harmonics isn't so much.

0:35:48	SPEAKER_03
 And there's this overall idea of really sort of matching the distribution somehow.

0:35:54	SPEAKER_03
 Not just subtracting off your estimate of the noise.

0:36:03	SPEAKER_03
 So I guess Gunther is going to play around with some of these things now over this next.

0:36:10	SPEAKER_05
 I don't know. I don't have feedback from him.

0:36:13	SPEAKER_03
 I guess he's got it anyway. So potentially if he came up with something that was useful, like a better noise estimation module, something he could ship it to you guys up there.

0:36:25	SPEAKER_03
 And we could put it in.

0:36:30	SPEAKER_03
 Yeah. So that's good.

0:36:36	SPEAKER_03
 So we just, I think starting, starting a couple weeks now, especially if you're not going to be around for a while, we'll be shifting more over to some other territory.

0:36:47	SPEAKER_03
 But not so much in this meeting about Aurora.

0:36:52	SPEAKER_03
 But maybe just quickly today about maybe to say a little bit about what you've been talking about with Michael.

0:37:00	SPEAKER_03
 And then very can say something about what we're talking about.

0:37:04	SPEAKER_00
 So Michael Klanchman, who's a PhD student from Germany, showed up this week.

0:37:09	SPEAKER_00
 He'll be here for about six months. And he's done some work using an auditory model of human hearing and using that to generate speech recognition features.

0:37:23	SPEAKER_00
 And he did work back in Germany with a toy recognition system using isolated digit recognition as the task. It was actually just a single layer neural network that classified words, classified digits. In fact, and he tried that.

0:37:45	SPEAKER_00
 I think on some Aurora data and got results that he thought seemed respectable.

0:37:49	SPEAKER_00
 And he's coming here to use it on a real speech recognition system.

0:37:57	SPEAKER_00
 So I'll be working with him on that.

0:38:00	SPEAKER_00
 Maybe I should say a little more about these features.

0:38:03	SPEAKER_00
 Although I don't understand in that. Well, I think it's a two-stage idea.

0:38:08	SPEAKER_00
 And the first stage of these features correspond to what's called the peripheral auditory system.

0:38:16	SPEAKER_00
 And I guess that is like a filter bank with a compressive nonlinearity.

0:38:22	SPEAKER_00
 And I'm not sure what we have in there that isn't already modeled in something like PLP.

0:38:29	SPEAKER_00
 I should learn more about that. And then the second stage is the most different thing, I think, from what we usually do. It's, it computes features which are based on sort of like based on different different wavelet basis functions used to analyze the input.

0:38:53	SPEAKER_00
 But so the, he uses analysis functions called GABAR functions, which have a certain extent in time and in frequency.

0:39:06	SPEAKER_00
 And the idea is these are used to sample the signal represented as a time frequency representation.

0:39:15	SPEAKER_00
 So you're sampling some piece of this time frequency plane.

0:39:19	SPEAKER_00
 And that is, is interesting because for, for one thing, you could use it in a multi-scale way.

0:39:31	SPEAKER_00
 You could have these instead of having everything like we use a 25 millisecond or so analysis window typically.

0:39:38	SPEAKER_00
 And that's our time scale for features. But you could using this basis function idea.

0:39:43	SPEAKER_00
 You could have some basis functions which have a lot longer time scale and some which have a lot shorter.

0:39:50	SPEAKER_00
 And so it would be like a set of multi-scale features. So he's interested in that.

0:39:56	SPEAKER_00
 This is because it's, there are these different parameters for the shape of these basis functions.

0:40:03	SPEAKER_00
 There are a lot of different possible basis functions. And so he actually does an optimization procedure to choose an optimal set of basis functions out of all the possible ones.

0:40:14	SPEAKER_01
 So he do to choose this.

0:40:16	SPEAKER_00
 The method he uses is kind of funny is he starts with, he has a set of M of them.

0:40:25	SPEAKER_00
 And then he uses that to classify. And he tries using just M minus one of them.

0:40:33	SPEAKER_00
 So there are M possible subsets of this length M vector.

0:40:37	SPEAKER_00
 He tries classifying using each of the M possible sub-vectors.

0:40:42	SPEAKER_00
 Whichever sub-vector works the best I guess he says the feature that didn't use was the most useless feature.

0:40:56	SPEAKER_00
 So we'll throw it out and we're going to randomly select another feature from the set of possible basis functions.

0:41:06	SPEAKER_01
 So it's actually a little bit like a genetic algorithm.

0:41:09	SPEAKER_03
 It's much simpler. But it's, there's a number of things I like about, I'm going to say.

0:41:14	SPEAKER_03
 So first of all, you're absolutely right. I mean, in truth, both pieces of this have their analogies and stuff we already do.

0:41:23	SPEAKER_03
 But it's a different take at how to approach it and potentially one that's maybe a bit more systematic than what we've done.

0:41:30	SPEAKER_03
 And a bit more inspiration from auditory things. So it's, so I think it's a neat thing to try.

0:41:37	SPEAKER_03
 The primary features are in fact, yeah, essentially it's, you know, POP or Melkepster or something like that.

0:41:45	SPEAKER_03
 You've got some compression. We always have some compression. We always have some, you know, the, the kind of filter bank with kind of quasi log scaling.

0:41:56	SPEAKER_03
 And if you put in, if you also include the Rasta in it, Rasta, the filtering being done in the log domain has an A.G.C. like characteristic, which, you know, people typically put in these kind of auditory front-end.

0:42:11	SPEAKER_03
 So it's very, very similar, but it's not exactly the same.

0:42:15	SPEAKER_03
 I would agree that the second one is somewhat more different, but it's mainly different in that the things that we've been done like that have been had a different kind of motivation and have ended up with different kinds of constraints. So for instance, if you look at the LDA, Rasta stuff, you know, basically what they do is they, they look at the different Eigenvectors out of the LDA and they form filters out of it, right?

0:42:39	SPEAKER_03
 And those filters have different kinds of temporal extents and temporal characteristics. And so in fact, they're multi-scale.

0:42:47	SPEAKER_03
 But they're not sort of systematically multi-scale. Like let's start here and go to there and go to there and go to there and so forth. It's more like you run it on this, you do discriminate analysis and you find out what's helpful.

0:42:57	SPEAKER_03
 It's multi-scale because you use several of these in parallel, so I'm going to use several of them, yeah. I mean, I don't have to, but, but, but, but, but, but, but, but, but, but, but, but, but, but, he can't.

0:43:06	SPEAKER_03
 But it's also, even he and X had people do this kind of LDN analysis, they've done it on frequency direction and they've done it on the time direction.

0:43:18	SPEAKER_03
 I think he may have had some people sometimes doing it on both simultaneously, some 2D, and that would be the closest to these good board function kind of things.

0:43:26	SPEAKER_03
 But I don't think they've done that much of that.

0:43:29	SPEAKER_03
 And the other thing that's interesting, the feature selection thing, it's a simple method, but I kind of like it.

0:43:36	SPEAKER_03
 There's an old old method for feature selection.

0:43:39	SPEAKER_03
 I mean, I remember people referring to his old one I was flying with it 20 years ago, so I know it's pretty old, called stepwise linear discriminant analysis, in which I think it's used in social sciences a lot.

0:43:51	SPEAKER_03
 So you pick the best feature, and then you find the next feature that's the best in combination with it, and then so on and so on.

0:44:01	SPEAKER_03
 And what Michael's describing seems to me much, much better, because the problem with the stepwise discriminant analysis is that you don't know if you've picked the right set of features, just because something's a good feature doesn't mean you should be adding it.

0:44:16	SPEAKER_03
 Here, at least, you're starting off with all of them, and you're throwing out useless features.

0:44:20	SPEAKER_03
 I think that seems like a lot better idea.

0:44:25	SPEAKER_03
 You're always looking at things in combination with other features.

0:44:27	SPEAKER_03
 So the only thing is, of course, there's this artificial question of exactly how you assess it, and if your order had been different in throwing them out, I mean, it still isn't necessarily really optimal, but it seems like a pretty good heuristic.

0:44:44	SPEAKER_03
 So I think it's kind of neat stuff, and the thing that I wanted to add to it also was to have us use this in a multi-stream way. So that when you come up with these different things, these different functions, you don't necessarily just put them all into one huge factor, but perhaps you have some of them in one stream, some of another stream, and so forth.

0:45:23	SPEAKER_03
 And we've also talked a little bit about shia pshamas stuff, in which the way you look at it is that there's these different mappings and some of them emphasize upward moving energy and frequency and some are emphasizing downward and fast things and slow things and so forth. So this is a bunch of stuff to look at, but we're sort of going to start off with what he came here with and branch out from there.

0:45:53	SPEAKER_03
 And his advisor is here too, at the same time, so it'll be another interesting source of wisdom.

0:46:06	SPEAKER_02
 As we're talking about this, I was thinking whether there's a relationship between my close approach to some sort of optimal brain damage or optimal brain surgeon on the neural nets.

0:46:25	SPEAKER_02
 We have our roster features and presumably the neural nets are learning some sort of nonlinear mapping from the features to this probably posterior space. And each of the hidden units is learning some sort of some sort of pattern, it could be like these auditory patterns that Michael is looking at, and then when you're looking at the best features, you can take out the brain surgery by taking out

0:47:06	SPEAKER_03
 hidden units that don't really help at all. Or the four features, right? I mean, actually, you make me think of a very important point here is that if we again try to look at how is this different from what we're already doing, there's a nasty argument that we made that it's not different at all because if you ignore the selection part because we are going into a very powerful nonlinearity that in fact is combining over time and frequency and is coming up with its own better than good bore functions, its neural net functions, whatever it finds to be best.

0:47:47	SPEAKER_03
 So you could argue that in fact it, but I don't actually believe that argument because I know that you can computing features is useful even though in principle you haven't added anything, in fact, you subtracted something from the original waveform. If you've processed it in some way, you've typically lost something, some information. And so you've lost information and yet it does better with features than it does with the waveform. So I know that sometimes it's useful to constrain things. So that's why it really seems like the constraint in all this stuff, it's the constraints that actually matters because it wasn't the constraints that mattered then we would have completely solved this problem long ago because long ago we already knew how to put waveforms into

0:48:35	SPEAKER_05
 powerful statistical mechanisms. Yeah, well, if we had infinite processing power and data,

0:48:42	SPEAKER_03
 then it would work, yeah, I agree. Yeah, there's the problems. Yeah, that would work, but I mean, it's with finite those things. I mean, we have done experiments where we literally have put waveforms in and kept number parameters the same and so forth and used a lot of training data and it's not infinite, but in a lot compared to the number parameters and it just doesn't do it nearly as well. So the point is that you want to suppress, it's not just having the maximum information, you want to suppress the aspects of the input signal that are not helpful for for the discrimination they're trying to make. So I'm just briefly.

0:49:28	SPEAKER_02
 Well, that's sort of segues into what I'm doing. So the big picture is come up with a set of intermediate categories, the intermediate category class drives and the recognition and groups between recognition that way. So right now in the phase we're looking at and deciding on an initial set of intermediate categories and I'm looking for data-driven methods that can help me find a set of intermediate categories that must be each that helped me to discriminate better down the line. And one of the ideas that was to take a neural net, train an ordinary neural net to learn the posterior probabilities of phones. And so at the end of the day you have this neural net and it has hidden hidden units and each of these hidden units is is learning some sort of pattern. And so what are these patterns? I don't know.

0:50:36	SPEAKER_02
 And I'm going to try to look at those patterns to see from those patterns presumably those are important patterns for discriminating between phone glasses. Maybe some intermediate categories can come from just looking at the pattern of that neural net. Before we go on next part,

0:50:58	SPEAKER_03
 let me just point out that there's a pretty nice relationship between what you're talking about doing and what you're talking about doing there, right? So it seems to me that if you take away the the difference of this primary features and say you just as we talked about me be doing use Rasta PLP or something for the primary features then this feature discovery thing is just what he's talking about doing too except that he's talking about doing them in order to discover intermediate categories that correspond to these what these sub features are showing you. And the other difference is that he's doing this in a multi band setting which means that he's constraining himself to look across time in some relatively limited spectral extent, right? And whereas in this case you're saying let's do it on constraint. So they're really pretty related maybe they'll be at some point where we'll see the connections a little better and connect them.

0:52:11	SPEAKER_02
 Yes, so that's the first part one of the ideas to get at some pattern of intermediate categories. The other one was to come up with a model, a graphical model that treats the intermediate categories as hidden variables, like variables that we don't know anything about but that through statistical training and the EM algorithm at the end of the day we have we have learned something about these latent variables which happen of course farmed to intermediate categories. So those are the two directions. I'm looking at you now.

0:53:00	SPEAKER_03
 Okay, we do our digits and get our treats. Yeah, it's kind of like the little rats with the little thing dropping down to and we do the digits and then we get our treats.

0:53:23	SPEAKER_03
 Okay, transcript L-371. 205453359. 445104230. 3317556710. 8, 831172630. 5606049727.

0:53:52	SPEAKER_02
 8183298057. 9857208893. 6534585475. Transcript L-370. 9868763723. 492168110. 6171438.

0:54:22	SPEAKER_02
 4883. 4184. 731. 7191. 3165. 4378. 9669. 231. 7. 0. 564. 460. 411. 611. 741. 890. 750. 449.

0:54:52	SPEAKER_01
 Transcript L-369. 026. 0. 9985. 4494. 395. 756. 403. 496. 266. 7000. 942. 120. 767. 177. 468. 289. 2519. 37. 285. 2.

0:55:22	SPEAKER_05
 1. 73577. 2. 864. 788. 313. 576. Transcript L-368. 596. 9. 3. 890. 615. 609. 303. 0. 703. 880. 2. 0. 0. 0. 6.

0:55:47	SPEAKER_05
 413. 683. 743. 737. 966. 933. 666. 7. 845. 308. 576. 332. 021. 828. 960. 475. 2. 3.

0:56:11	SPEAKER_00
 Transcript L-367. 987. 231. 9211. 620. 722. 238. 9. 475. 738. 970. 393. 587. 540. 5. 765. 8. 542. 0.

0:56:41	SPEAKER_00
 1. 846. 515. 9. 680. 9. 3. 1. 2. 8. 5. 313. 639. 7. 480. 1. 6. 0. 4. 9. 460. 0.

