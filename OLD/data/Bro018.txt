0:00:00	SPEAKER_02
 Okay, so, it's not here, so.

0:00:05	SPEAKER_06
 Yeah, I would try to explain the thing that I did this week, during this week.

0:00:11	SPEAKER_06
 Well, you know that I began to work with a new feature to the text voice and voice, where I tried to MLP to with this new feature and the feature from the bass system.

0:00:35	SPEAKER_04
 The Melcapsum?

0:00:37	SPEAKER_06
 No, exactly the Melcapsum, the new bass system, the new bass system.

0:00:43	SPEAKER_06
 Oh, the Aurora system.

0:00:45	SPEAKER_06
 Yeah, the Aurora system with the new filter, the idea, something like that.

0:00:52	SPEAKER_06
 And I tried to MLP one that only have three outputs, voice and voice and silence, and other one that have 56 outputs, the probabilities that they have.

0:01:09	SPEAKER_06
 And I tried to do some experiment of recognition with that.

0:01:14	SPEAKER_06
 And I only have a result with the MLP with the three outputs.

0:01:19	SPEAKER_06
 And I put together the 15 features and the three MLP outputs.

0:01:23	SPEAKER_06
 And while the results are a little bit better but more or less similar.

0:01:30	SPEAKER_02
 I'm slightly confused.

0:01:31	SPEAKER_02
 What feeds the three output net?

0:01:35	SPEAKER_06
 Voice and voice and silence.

0:01:36	SPEAKER_02
 No, no, what feeds it? What features does it say?

0:01:39	SPEAKER_06
 The input, the input are the 15, the 15 basses feature with the new code.

0:01:50	SPEAKER_06
 And the other three features are the variance of the different between the two spectrum.

0:01:57	SPEAKER_06
 The variance of the autocorrelation function, I said that the first point, because half the height value is 0 and also R0.

0:02:09	SPEAKER_06
 The first coefficient of the autocorrelation function, that is like the energy with the three features.

0:02:17	SPEAKER_02
 You wouldn't do like R1 over R0 or something like that.

0:02:22	SPEAKER_02
 Usually for voice and voice, you do something, you do energy but then you have something like spectral slope, which is you get like R1 over R0 or something like that.

0:02:31	SPEAKER_06
 What are the R's?

0:02:34	SPEAKER_01
 Are the correlations?

0:02:35	SPEAKER_06
 No, autocorrelations, yes, the variance of the autocorrelation function that used that.

0:02:41	SPEAKER_06
 Well, that's the variance.

0:02:42	SPEAKER_02
 But if you just say what is, I mean, the first order, one of the different between voice and voice and silence is energy.

0:02:49	SPEAKER_02
 But the other one is the spectral shape.

0:02:51	SPEAKER_02
 The spectral shape, yes.

0:02:53	SPEAKER_02
 Yeah, and so R1 over R0 is what you typically use for that.

0:02:56	SPEAKER_06
 No, I don't use that. I can use.

0:02:58	SPEAKER_02
 No, I'm saying that's what people usually typically use.

0:03:01	SPEAKER_02
 See, because this is just like a single number to tell you does the spectrum look like that or does it look like that?

0:03:10	SPEAKER_02
 So if it's low energy but the spectrum looks like that or like that, it's probably silence.

0:03:23	SPEAKER_02
 But if it's low energy and the spectrum looks like that, it's probably unvoiced.

0:03:29	SPEAKER_02
 Yeah.

0:03:30	SPEAKER_02
 So if you just had to pick two features to determine voice and voice, you pick something about the spectrum like R1 over R0 and R0.

0:03:42	SPEAKER_02
 Or you know, you have some other energy measure and like in the old days people did like zero crossing counts.

0:03:50	SPEAKER_02
 Yeah, yeah.

0:03:51	SPEAKER_06
 Right.

0:03:53	SPEAKER_06
 I can also use this.

0:03:55	SPEAKER_01
 Yeah.

0:03:57	SPEAKER_06
 Because the result are really better but we have in a point that everything is more or less similar.

0:04:06	SPEAKER_06
 But not quite the...

0:04:09	SPEAKER_02
 Right, but it seemed to me that what you were getting at before was that there is something about the difference between the original signal or the original FFT.

0:04:20	SPEAKER_02
 And what the filter, which is what...

0:04:23	SPEAKER_02
 And the variance was one take on it.

0:04:25	SPEAKER_02
 Yeah, I used this too.

0:04:27	SPEAKER_02
 Right, but it could be something else because suppose you didn't have anything like that.

0:04:30	SPEAKER_02
 Then in that case, if you have two nets, and this one has three outputs, and this one has whatever, 56 or something.

0:04:44	SPEAKER_02
 If you were to sum up the probabilities for the voice and for the unvoiced and for the silence here, we found in the past you would do better at voice and voice silence than you do with this one.

0:04:55	SPEAKER_02
 So just having the three output thing doesn't really value anything.

0:04:59	SPEAKER_02
 Yeah.

0:05:00	SPEAKER_02
 The issue is what you feed it.

0:05:03	SPEAKER_06
 Yeah, I have...

0:05:05	SPEAKER_06
 Yeah.

0:05:06	SPEAKER_02
 So you're saying...

0:05:07	SPEAKER_04
 So you can take the features that go into the voice and voice silence net and feed those into the other one as additional inputs rather.

0:05:14	SPEAKER_02
 Yeah.

0:05:15	SPEAKER_02
 Well, that's another way.

0:05:16	SPEAKER_02
 That wasn't what I was saying, but yeah, that's certainly another thing to do.

0:05:19	SPEAKER_02
 Now, I was just trying to say, if you bring this into the picture over this, what more does it buy you?

0:05:25	SPEAKER_02
 And what I was saying is that the only thing that I think that it buys you is based on whether you feed it something different.

0:05:34	SPEAKER_02
 And something different and something fun to men away.

0:05:37	SPEAKER_02
 And so the kind of thing that she was talking about before was looking at something...

0:05:43	SPEAKER_02
 Something about the difference between the log FFT log power and the log magnitude FF spectrum in the filter bank.

0:06:01	SPEAKER_02
 Yeah.

0:06:04	SPEAKER_02
 And so the filter bank is chosen, in fact, to sort of integrate out the effects of pitch.

0:06:10	SPEAKER_02
 And she's saying, you know, trying...

0:06:11	SPEAKER_02
 So the particular measure that she chose was the variance of this difference.

0:06:16	SPEAKER_02
 But that might not be the right number.

0:06:19	SPEAKER_02
 Maybe.

0:06:20	SPEAKER_02
 I mean, maybe there's something about the variance that's not enough or maybe there's something else that one could use.

0:06:26	SPEAKER_02
 I think that for me, the thing that struck me was that you want to get something back here.

0:06:32	SPEAKER_02
 So here's an idea.

0:06:34	SPEAKER_02
 What about if you skip all the really clever things and just fed the log magnitude spectrum into this?

0:06:44	SPEAKER_06
 I'm sorry.

0:06:45	SPEAKER_02
 This is...

0:06:46	SPEAKER_02
 You have the log magnitude spectrum.

0:06:48	None
 Yeah.

0:06:48	SPEAKER_02
 And you were looking at that and the difference between the filter bank and computing the variance.

0:06:53	SPEAKER_02
 That's a clever thing to do. What if you stop being clever?

0:06:56	SPEAKER_02
 And you just took this thing in here because it's a neural net and neural nets are wonderful and figure out what they can...

0:07:03	SPEAKER_02
 What they most need from things.

0:07:05	SPEAKER_02
 Yeah.

0:07:06	SPEAKER_02
 I mean, that's a good act.

0:07:08	SPEAKER_02
 So I mean, you're trying to be clever and say, what's the statistic that we should get about this difference?

0:07:14	SPEAKER_02
 But in fact, you know, maybe just feeding this in or feeding both of them in.

0:07:21	SPEAKER_02
 In other words, saying let it figure out what is the interaction.

0:07:25	SPEAKER_02
 Especially if you do this over multiple frames, then you have this over time and both kinds of measures and you might get something better.

0:07:39	SPEAKER_04
 So don't do the division but let the net have everything.

0:07:45	SPEAKER_01
 That's another thing you could do, yeah.

0:07:47	SPEAKER_02
 I mean, it seems to me if you have exactly the right thing, then it's better to do it without the net because otherwise you're asking the net to learn this thing.

0:07:59	SPEAKER_02
 You know, say if you wanted to learn how to do multiplication.

0:08:02	SPEAKER_02
 I mean, you could feed two numbers that you wanted to multiply into a net and have a bunch of nonlinearities in the middle and train it to get the product at the output.

0:08:12	SPEAKER_02
 And it would work, but it's kind of crazy because we know how to multiply and you'd be much lower error usually if you just multiply it out.

0:08:21	SPEAKER_02
 But suppose you don't really know what the right thing is.

0:08:24	SPEAKER_02
 And that's what these sort of dumb machine learning methods are good at.

0:08:31	SPEAKER_04
 How long does it take Carmen to train up on a business?

0:08:35	SPEAKER_06
 Oh, not too much. One day useless.

0:08:39	SPEAKER_02
 Yeah, it's probably worth it.

0:08:42	SPEAKER_00
 What are your frame error rates?

0:08:46	SPEAKER_06
 56, the frame error rate 56 AC.

0:08:53	SPEAKER_06
 Is that maybe that's accuracy?

0:08:55	SPEAKER_00
 The accuracy.

0:08:56	SPEAKER_00
 56% accurate for voice unvoiced.

0:08:59	SPEAKER_06
 No, yes.

0:09:01	SPEAKER_06
 I don't remember for voice unvoiced.

0:09:03	SPEAKER_06
 Maybe for the other one.

0:09:04	SPEAKER_06
 Yeah, for voice unvoiced, hopefully it'd be a lot better.

0:09:06	SPEAKER_06
 Maybe for voice unvoiced.

0:09:09	SPEAKER_06
 This is for the other one.

0:09:11	SPEAKER_06
 I can't say that.

0:09:12	SPEAKER_06
 But I think that 55 was for the when the output are the 56 foam.

0:09:17	SPEAKER_06
 That I look in the with the other.

0:09:20	SPEAKER_06
 The other MLP that we have are more or less the same number.

0:09:23	SPEAKER_06
 It's a little bit better, but more or less the same.

0:09:26	SPEAKER_02
 I think at the frame level for 56, I was the kind of number we were getting for.

0:09:32	SPEAKER_02
 Reduce bandwidth.

0:09:35	SPEAKER_06
 I think that for the other one for the three output is 62, 63 more or less.

0:09:44	SPEAKER_06
 That's all?

0:09:45	SPEAKER_02
 Yeah.

0:09:46	SPEAKER_06
 That's pretty bad.

0:09:47	SPEAKER_06
 Yeah, because it's noise also.

0:09:49	SPEAKER_06
 And we have.

0:09:52	SPEAKER_02
 I know.

0:09:53	SPEAKER_02
 But even in training still.

0:09:58	SPEAKER_02
 Well, actually, so this is a test you should do then.

0:10:01	SPEAKER_02
 If you're getting 56% over here, that's a noise also, right?

0:10:04	SPEAKER_02
 Yeah, yeah, yeah.

0:10:05	SPEAKER_02
 Okay.

0:10:06	SPEAKER_02
 If you're getting 56 here, try adding together the probabilities of all of the voice phones here and all the unvoiced phones and see what you get then.

0:10:16	SPEAKER_02
 I bet you get better than 63.

0:10:19	SPEAKER_06
 Well, I don't know.

0:10:22	SPEAKER_06
 I think that we have to resolve more.

0:10:25	SPEAKER_06
 Maybe.

0:10:26	SPEAKER_06
 I don't know.

0:10:27	SPEAKER_06
 I'm not sure, but I remember it's a sensitivity.

0:10:30	SPEAKER_06
 I can't solve that.

0:10:31	SPEAKER_02
 Okay, but that's a good checkpoint.

0:10:34	SPEAKER_02
 You should do that anyway.

0:10:36	SPEAKER_02
 Given this regular old net that's just what you're using for other purposes, add up the probabilities of the different subclasses and see how well you do.

0:10:46	SPEAKER_02
 Anything that you do over here should be at least as good as that.

0:10:54	SPEAKER_06
 How do you do that?

0:10:55	SPEAKER_04
 The targets for the neural net.

0:10:57	SPEAKER_04
 They come from forced alignments.

0:11:02	SPEAKER_06
 Timmit.

0:11:03	SPEAKER_06
 Oh, this is trained on Timmit.

0:11:06	SPEAKER_02
 Yeah.

0:11:07	SPEAKER_06
 Okay.

0:11:08	SPEAKER_06
 Yeah, this is for Timmit.

0:11:09	SPEAKER_02
 But noisy Timmit?

0:11:10	SPEAKER_06
 No, it's a Timmit.

0:11:11	SPEAKER_06
 We have no system with the noise of the TID digits.

0:11:15	SPEAKER_06
 And now we have another noise system, it also with the noise of Italian database.

0:11:24	SPEAKER_01
 Yeah.

0:11:27	SPEAKER_02
 Well, there's going to be, it looks like there's going to be a noisy, some large vocabulary noisy stuff too.

0:11:34	SPEAKER_02
 Something that's preparing.

0:11:35	SPEAKER_02
 Yeah.

0:11:36	SPEAKER_02
 I forget it will be resource management, Wall Street Journal, something.

0:11:41	SPEAKER_02
 Some red task, actually, that there.

0:11:44	SPEAKER_02
 Or what?

0:11:45	SPEAKER_02
 Or a raw?

0:11:46	SPEAKER_02
 Yeah.

0:11:47	SPEAKER_02
 Yeah.

0:11:48	SPEAKER_02
 Yeah.

0:11:54	SPEAKER_02
 So the issue is whether people make a decision now based on what they've already seen or they make it later.

0:11:57	SPEAKER_02
 And one of the arguments for making it later is let's make sure that whatever techniques that we're using work for something more than connected digits.

0:12:07	SPEAKER_01
 So.

0:12:10	SPEAKER_04
 When are they planning, when would they do that?

0:12:13	SPEAKER_02
 I think in the summer sometime.

0:12:21	SPEAKER_01
 Okay.

0:12:22	SPEAKER_06
 This is the work that I did in this date.

0:12:26	SPEAKER_06
 And also, I, Henik, last week, say that if I have time, I can't begin to study seriously the French telecom proposal to look at the code and something like that.

0:12:42	SPEAKER_06
 To know, is that what they are doing?

0:12:45	SPEAKER_06
 Because maybe that we can have some ideas.

0:12:48	SPEAKER_06
 But not only to read the proposal, look carefully what they are doing with the program and something like that.

0:12:57	SPEAKER_06
 And I begin to work also in that.

0:13:01	SPEAKER_06
 But the first thing that they don't understand is that they are using the lock energy.

0:13:11	SPEAKER_06
 That this quiet, I don't know why they have some constant in the expression of the law energy.

0:13:20	SPEAKER_06
 I don't know what that means.

0:13:23	SPEAKER_04
 They have a constant.

0:13:26	None
 Yeah.

0:13:27	SPEAKER_02
 At the front, it says, a lock energy is equal to the rounded version of 16 over the log of two.

0:13:37	SPEAKER_01
 Well, this is natural log and maybe something to do with the fact that this is, I have no idea.

0:13:45	SPEAKER_01
 Yeah, that's what I was thinking.

0:13:47	SPEAKER_01
 But there's a 64.

0:13:55	SPEAKER_06
 Because maybe the threshold that they are using and the basis of this value, I don't know exactly.

0:14:03	SPEAKER_06
 Because I thought maybe they have a meaning, but I don't know what the meaning of this value is.

0:14:09	SPEAKER_06
 Yeah, that's pretty funny.

0:14:11	SPEAKER_04
 The number inside the log and raising it to 16 over log is two.

0:14:18	SPEAKER_01
 Yeah, I'm right.

0:14:24	SPEAKER_04
 I have to do with the 64.

0:14:28	SPEAKER_02
 For you know, the 16, the natural log of one over the natural log of two times the natural, I don't know.

0:14:41	SPEAKER_02
 Maybe something will think of something, but this is, it may just be that they want to have some very small energies.

0:14:49	SPEAKER_06
 They want to have some kind of a...

0:14:52	SPEAKER_06
 I can understand the effect of this, because it's too, to do something like that.

0:14:58	SPEAKER_02
 Well, since you're taking a natural log, it says that when you get down to essentially zero energy, this is going to be the natural log of one, which is zero.

0:15:07	SPEAKER_02
 So it'll go down to the natural log being the lowest value for this will be zero.

0:15:16	SPEAKER_02
 So it restricted to being positive and sort of smoothed it for very small energies.

0:15:22	SPEAKER_02
 Why they chose 64 and something else, that was probably just experimental.

0:15:26	SPEAKER_02
 Yeah.

0:15:27	SPEAKER_02
 And the constant in front of it, I have no idea.

0:15:30	SPEAKER_06
 What?

0:15:32	SPEAKER_06
 I will look to try if I move this parameter in the code, what happens?

0:15:37	SPEAKER_06
 Maybe the thresholds are in basis of this.

0:15:42	SPEAKER_02
 I agree.

0:15:45	SPEAKER_02
 They probably have some particular fixed pointer arithmetic that they're using.

0:15:49	SPEAKER_04
 They're just...

0:15:50	SPEAKER_04
 They do with hardware.

0:15:52	SPEAKER_02
 Yeah.

0:15:53	SPEAKER_02
 I mean, there's probably work with fixed pointer integer or something.

0:15:56	SPEAKER_02
 I think you're supposed to in this stuff anyway, and maybe that puts it in the right realm somewhere.

0:16:03	SPEAKER_02
 Yeah.

0:16:04	SPEAKER_02
 I think given that the level you're doing things in floating point on the computer, I don't think it matters to be my guess.

0:16:10	SPEAKER_06
 This is more of a thing.

0:16:14	SPEAKER_02
 Okay, and when did Stefan take off?

0:16:17	SPEAKER_06
 I think that the stuff I will arrive today, which tomorrow...

0:16:21	SPEAKER_02
 Well, he was gone this first few days, and then you see her for a couple days before he goes to Salt Lake City.

0:16:27	SPEAKER_06
 I think that he's in Las Vegas until then, that.

0:16:31	SPEAKER_02
 Yeah.

0:16:32	SPEAKER_02
 So he's going to a cast, which is good.

0:16:35	SPEAKER_02
 I don't know if there are many people here, but I cast, so make sure somebody go.

0:16:40	SPEAKER_04
 Have people sort of stopped going to a cast?

0:16:43	SPEAKER_02
 People are less consistent about going to a cast, but I think it's still a reasonable form for students to present things.

0:16:53	SPEAKER_02
 I think for engineering students of any kind, I think it's if you haven't been there much, it's good to go to, get a feel for things, a range of things, not just speech.

0:17:04	SPEAKER_02
 But I think for sort of diving the world's speech people, I think that ICSOP and Euro-Speech are much more targeted.

0:17:12	SPEAKER_02
 And then there are these other meetings like HLT and ASRU.

0:17:18	SPEAKER_02
 So there's actually plenty of meetings that are really relevant to computational speech processing, of one sort or another.

0:17:28	SPEAKER_02
 So, I mean, I mostly just ignored it because I was too busy, and didn't get to it.

0:17:34	SPEAKER_02
 So, I want to talk a little bit about what we're talking about this morning, just briefly.

0:17:40	SPEAKER_00
 Anything else?

0:17:42	SPEAKER_00
 Yeah, so I guess some of the progress I've been getting my committee members for the calls.

0:17:50	SPEAKER_00
 And so far, I'm more again, and being at Mike Jordan, and I asked John O'Halla to be agreed.

0:17:56	SPEAKER_00
 Cool? Yeah, yeah.

0:17:58	SPEAKER_00
 So, I just need to ask Malik.

0:18:02	SPEAKER_00
 And then I talked a little bit about continuing with these dynamic acoustic events.

0:18:12	SPEAKER_00
 And we're thinking about a way to test the completeness of a set of dynamic events, completeness in the sense that if we pick these X number of acoustic events, do they provide sufficient coverage for the phones that we're trying to recognize, or the words that we're going to try to recognize from Iran?

0:18:44	SPEAKER_00
 So, Morgan and I were discussing a form of a cheating experiment where we get a chosen set of features or acoustic events.

0:19:00	SPEAKER_00
 And we train up a hybrid system to do phone recognition on timet. So, the idea is if we get good phone recognition results using the set of acoustic events, then that says that these acoustic events are sufficient to cover a set of phones at least found a timet.

0:19:26	SPEAKER_00
 So, it would be a measure of, are we on the right track with the choices of our acoustic events.

0:19:34	SPEAKER_00
 So, that's going on. And also just working on my final project for Jordan's class, which is...

0:19:45	SPEAKER_02
 Actually, let me hold that thought. Let me back up while we're still on it. The other thing I was suggesting though is that given that you're talking about binary features, maybe the first thing to do is just to count and count co-occurrences and get probabilities for discrete HMM.

0:20:03	SPEAKER_02
 So, that would be pretty simple because it's just to say if you had 10 events that you were counting each frame would only have a thousand possible values for these 10 bits.

0:20:15	SPEAKER_02
 And so, you could make a table that would say if you had 39 phone categories, it would be a thousand by 39 and just count the co-occurrences and divide them by the...

0:20:27	SPEAKER_02
 count the co-occurrences between the event and the phone and divide them by the number of occurrences of the phone.

0:20:35	SPEAKER_02
 And that would give you the likelihood of the event given the phone. And then just use that in a very simple HMM.

0:20:44	SPEAKER_02
 And you could do phone recognition then and wouldn't have any of the issues of the training of the net or... I mean, it'd be on the simple side.

0:21:00	SPEAKER_02
 The example I was giving was if you had onset of voicing and end-of-voicing as being two kinds of events, then if you had those on mark correctly and you counted the co-occurrences, you should get it completely right.

0:21:19	SPEAKER_02
 So, but you'd get all the other distinctions randomly wrong. I mean, there'd be nothing to tell you that. So, if you just do this by counting, then you should be able to find out in a pretty straightforward way whether you have a sufficient set of events to do the kind of level of classification of phones that you'd like.

0:21:42	SPEAKER_02
 So, that was the idea. And the other thing that we were discussing was, okay, how do you get your training data?

0:21:51	SPEAKER_02
 Because the Switchboard Transcription Project was half-thousand people or so working off and on over a couple years.

0:22:02	SPEAKER_02
 Similar amount of data to what you're talking about with Dimit training. So, it seems to me that the only reasonable starting point is to automatically translate the current timid markings into the markings you want.

0:22:22	SPEAKER_02
 And it won't have the kind of characteristic that you'd like of catching funny kind of things that maybe aren't there from these automatic markings, but it's...

0:22:35	SPEAKER_04
 It's probably a good place to start.

0:22:37	SPEAKER_02
 Yeah, and a short amount of time. Just to, again, just to see if that information is sufficient to determine the phones.

0:22:48	SPEAKER_04
 And you could even then get an idea about how different it is you could maybe take some subset and go through a few sentences, mark them by hand and see how different it is from the clinical ones.

0:23:02	SPEAKER_04
 Just to get an idea of rough idea of if it really even makes a difference.

0:23:07	SPEAKER_02
 You get a little feeling for it that way. Yeah, that's probably right.

0:23:10	SPEAKER_02
 I guess it would be that this is since Dimit's red speech that this would be less of a big deal if you want to look at spontaneous speech before or after.

0:23:20	SPEAKER_02
 And the other thing would be say if you have these 10 events you want to see what if you took two events or four events or ten events.

0:23:27	SPEAKER_02
 And hopefully there should be some point in which having more information doesn't tell you really all that much more about what the phones are.

0:23:37	SPEAKER_04
 You could define other events as being sequences of these events.

0:23:44	SPEAKER_02
 You could, but the thing is what he's talking about here is a translation to a per frame feature vector.

0:23:57	SPEAKER_02
 So there's no sequence in that, I think.

0:24:01	SPEAKER_02
 Unless you've got your second pass over or something after you've got your...

0:24:08	SPEAKER_02
 Yeah, but we're just talking about something simple here.

0:24:13	SPEAKER_02
 Yeah, I'm adding complexity.

0:24:16	SPEAKER_02
 Yeah, with a very simple statistical structure, could you at least verify that you've chosen features that are sufficient?

0:24:30	SPEAKER_02
 Okay, and you were saying some of these are going to say something else about your class project?

0:24:34	SPEAKER_00
 Yeah, so for my class project, I'm tinkering with support vector machines, something that we learned in class, and basically just another method for doing classification.

0:24:48	SPEAKER_00
 So I'm going to apply that to compare it with the results by King and Taylor who did using recurrent known as they recognized a set of phonological features made of mapping from the MFCC's to phonological features.

0:25:09	SPEAKER_00
 So I'm going to do a similar thing with support vector machines.

0:25:14	SPEAKER_04
 So what's the advantage of support vector machines?

0:25:19	SPEAKER_00
 So support vector machines are good with dealing with less amount of data.

0:25:26	SPEAKER_00
 So if you give it less data, it still does a reasonable job in learning the patterns.

0:25:37	SPEAKER_02
 I guess they're sort of succinct.

0:25:43	SPEAKER_04
 Does there is some kind of a distance metric they use or what do they do for classification?

0:25:51	SPEAKER_00
 So the simple idea behind the support vector machine is you have this feature space, right? Then it finds the optimal separating play between these two different classes.

0:26:10	SPEAKER_00
 And so at the end of the day, what it actually does is it picks those examples of the features that are closest to the separating boundary and remembers those and uses them to recreate the boundary for the test set.

0:26:32	SPEAKER_00
 So given these features are these examples, critical examples, which they call support vectors, then given a new example, if the new example falls away from the boundary in one direction, then it's classified as being part of this particular class.

0:26:54	SPEAKER_04
 So why save the examples? Why not just save what the boundary itself is?

0:27:04	SPEAKER_00
 Yeah, that's a good question.

0:27:14	SPEAKER_02
 That's another way of doing it. So I guess it goes back to nearest neighbor sort of thing.

0:27:26	SPEAKER_02
 When is nearest neighbor good? Well nearest neighbor good is good if you have lots and lots of examples.

0:27:32	SPEAKER_02
 But of course if you have lots and lots of examples, then it can take a while to use nearest neighbor's lots of lookups. So a long time ago people talked about things where you would have condensed nearest neighbor where you would pick out some representative examples, which would be sufficient to represent to correctly classify everything that came in.

0:27:53	SPEAKER_04
 I think support vector stuff goes back to that kind of thing. So rather than doing nearest neighbor where you compare to every single one, you just pick a few critical ones.

0:28:07	SPEAKER_02
 And the neural net approach or gothse mixtures for that matter are sort of fairly brute force kinds of things where you sort of predefined that there's this big bunch of parameters and then you place them as your best can to define the boundaries.

0:28:25	SPEAKER_02
 And in fact, as you know, these things do take a lot of parameters. And if you have only a minus amount of data, you have trouble learning them.

0:28:35	SPEAKER_02
 So I guess the idea to this is that it is reputed to be somewhat better in that regard.

0:28:43	SPEAKER_00
 It can be a reduced parameterization of the model by just keeping certain selected examples.

0:28:55	SPEAKER_02
 But I don't know if people have done sort of careful comparisons of this on large tasks or anything. Maybe they have. I don't know.

0:29:05	SPEAKER_03
 So do you get some kind of number between zero and one at the output?

0:29:10	SPEAKER_00
 Actually, you don't get a nice number between zero and one. You get either a zero or one.

0:29:18	SPEAKER_00
 Basically, you get a distance measure at the end of the day. And then that distance measure is translated to a zero or one.

0:29:32	SPEAKER_04
 But that's looking at it for a binary classification. And you get that for each class. You get a zero or one.

0:29:40	SPEAKER_02
 But you have the distances to work with. Because actually, Mrs. Cippy stayed people to use support vector machines for speech recognition. And they were using it to make probabilities.

0:29:50	SPEAKER_00
 Yeah, they had a way to translate the distances into a probability with a simple sigmoidal function.

0:30:02	SPEAKER_02
 So they used sigmoidal or a softmax type thing. And didn't they like exponential or something and divide by the sum of them.

0:30:10	SPEAKER_02
 Oh, so it is a sigmoidal.

0:30:14	SPEAKER_04
 Did they get good results with that?

0:30:18	SPEAKER_02
 No, they were okay. I mean, I don't think they were earth shattering. But I think that this was a couple years ago.

0:30:25	SPEAKER_02
 I think people were very critical because it was interesting just to try this. And it was the first time they tried it.

0:30:34	SPEAKER_02
 So the numbers were not incredibly good. But there was reasonable.

0:30:40	SPEAKER_02
 I don't remember anymore. I don't remember what the task was. It was broadcast news or something.

0:30:48	SPEAKER_00
 So I'm not planning on doing speech recognition with it. Just doing detection of phonological features.

0:30:59	SPEAKER_00
 So for example, this feature set called the sound patterns of English is just a bunch of binary valued features.

0:31:11	SPEAKER_00
 So I don't know if you can say, is this voicing or is this not voicing this sound or is this.

0:31:18	SPEAKER_00
 Did you find any more mistakes in their table?

0:31:22	SPEAKER_00
 I haven't gone through my table. Yesterday I brought Chuck the table and I was like, is the mapping from N to this phonological feature called coronal?

0:31:35	SPEAKER_00
 Should it be a one? Or should it be a coronal instead of not coronal as it was labeled in the paper?

0:31:41	SPEAKER_00
 So I haven't hunted down all the mistakes yet.

0:31:46	SPEAKER_02
 But as I was saying, people do get probabilities of these things. And we were just trying to remember how they do.

0:31:52	SPEAKER_02
 But people have used it for speech recognition. They have gotten probabilities. So they have some conversion from these distances to probabilities.

0:32:00	SPEAKER_02
 You have the paper, right, the Mississippi State paper. Yeah, if you're interested.

0:32:05	SPEAKER_00
 I can show you.

0:32:08	SPEAKER_04
 So in a thing you're doing, you have a vector of ones and zeros for each phone.

0:32:16	SPEAKER_00
 Is this the class project?

0:32:18	SPEAKER_00
 Yeah.

0:32:19	SPEAKER_00
 Is that what you're right? Right, right. So for every phone there is a vector of ones and zeros.

0:32:31	SPEAKER_00
 Of course, whether it exhibits a particular phonological feature or not.

0:32:36	SPEAKER_04
 And so when you do your, what is the task for the class project to come up with the phones or to come up with these vectors to see how close they match the phone?

0:32:47	SPEAKER_00
 To come up with a mapping from MFCCs or some features that to whether there is an existence of a particular phonological feature.

0:33:03	SPEAKER_00
 And yeah, basically it's to learn a mapping from the MFCCs to phonological features.

0:33:15	SPEAKER_00
 Is it the answer to your question? I think so.

0:33:18	SPEAKER_04
 I guess, I mean, I'm not sure what you get out of your system.

0:33:25	SPEAKER_04
 Do you get out a vector of these ones and zeros and then try to find the closest matching phoneme to that vector?

0:33:32	SPEAKER_00
 Oh, no, no. I'm not planning to do any phoneme mapping yet.

0:33:38	SPEAKER_00
 It's basically, it's really simple, basically, a detection of phonological features.

0:33:46	SPEAKER_00
 I see.

0:33:48	SPEAKER_00
 Because the, so King and Taylor did this with recurrent neural nets.

0:33:55	SPEAKER_00
 And their idea was to first find a mapping from MFCCs to phonological features.

0:34:03	SPEAKER_00
 And then later on, once you have these phonological features, then map that to folks.

0:34:10	SPEAKER_00
 So I'm sort of reproducing phase one.

0:34:13	SPEAKER_04
 So they had one recurrent net for each particular feature.

0:34:20	SPEAKER_04
 Did they compare that? I mean, what if you just did phone recognition and did the reverse look up?

0:34:26	SPEAKER_04
 So you recognize a phone, then whichever phone was recognized, you spit out its vector of ones and zeros.

0:34:33	SPEAKER_02
 The spectrary could do that. That's probably not what you're going to do in this class, Brian.

0:34:39	SPEAKER_02
 So have you had a chance to do this thing we talked about yet with the search and penalty?

0:34:48	SPEAKER_02
 No, actually, I was going different, that's a good question too, but I was going to ask about the changes to the data in comparing PLP and no capstrom for the SRI system.

0:35:07	SPEAKER_04
 Well, what I've been changes to data, I'm not sure.

0:35:10	SPEAKER_02
 So we talked on the phone about this, that there was still a difference of a few percent, and you told me that there was a difference in how the normalization was done.

0:35:20	SPEAKER_02
 And I was asking if you were going to redo it for PLP with a normalization done as it had been done for the mock-up stream.

0:35:30	SPEAKER_04
 Right, no, I haven't had a chance to do that.

0:35:32	SPEAKER_04
 So what I've been doing is trying to figure out, it just seems to me like there's a, well, it seems like there's a bug because the difference in performance is, it's not gigantic, but it's big enough that it seems wrong.

0:35:50	SPEAKER_02
 I agree, but I thought that the normalization difference was one of the...

0:35:54	SPEAKER_04
 Yeah, but I don't know how this is.

0:35:55	SPEAKER_04
 Yeah, I guess I don't think that the normalization difference is going to count for everything. So what I was working on is just going through and checking the headers of the wave files to see if maybe there was a certain type of compression or something that was done that my script wasn't catching.

0:36:13	SPEAKER_04
 For some subset of the training data, the features I was computing were junk, which would cause it to perform okay, but the models would be all messed up.

0:36:25	SPEAKER_04
 So I was going through and just double checking that kind of thing first to see if there was just some kind of obvious bug in the way that I was computing the features.

0:36:33	SPEAKER_04
 Looking at all the sampling rates, make sure all the sampling rates were what HK, what I was assuming they were...

0:36:39	SPEAKER_04
 Yeah, that makes sense to check out. So I was doing that first before I did these other things just to make sure there wasn't something.

0:36:45	SPEAKER_02
 Although really, a couple of 3% difference in order or rate could easily come from some difference in normalization, I would think.

0:36:55	SPEAKER_04
 Yeah, and I think, I'm trying to remember, but I think I recall that Andreas was saying that he was going to run sort of the reverse experiment, which is to try to emulate the normalization that we did, but with ML Cup Straw features.

0:37:15	SPEAKER_04
 Sort of, you know, back up from the system that he had. I thought he said he was going to have to look back through my email from him.

0:37:23	SPEAKER_02
 Yeah, he's probably off at the end of his meeting there.

0:37:27	SPEAKER_02
 Yeah, but yeah, I just think they should be roughly equivalent. I mean, again, the Cambridge folk found the PLP actually be a little better.

0:37:39	SPEAKER_02
 I mean, the other thing I wondered about was whether there was something just in the bootstrapping of a system which was based on, but maybe not.

0:37:49	SPEAKER_04
 So one thing that's a little bit, I was looking, I've been studying and going through the logs for the system that Andreas created.

0:37:57	SPEAKER_04
 And his way that the SRI system looks like it works is that it reads the way files directly and does all of the capture computation stuff on the fly.

0:38:11	SPEAKER_04
 Right. And so there's no place where these, where the capture files are stored in, where that I can go look at and compare to the PLP one.

0:38:19	SPEAKER_04
 So, whereas with our features, he's actually storing the capture.

0:38:24	SPEAKER_04
 Yes, going to read those in, but it looked like he had to give it, even though the capture was already computed, he has to give it a front end parameter file, which talks about the kind of.

0:38:39	SPEAKER_04
 Computation that his malfunction thing does. So, I don't know if that probably doesn't mess it up, it probably just ignores it if it determines that it's already in the right format or something, but the two processes that happen are a little different.

0:38:55	SPEAKER_04
 So, anyway, there's stuff there to start with.

0:38:58	SPEAKER_02
 Yeah. So, okay, let's go back to what you thought I was asking you.

0:39:01	SPEAKER_04
 Yeah, no, and I didn't have a chance to do that.

0:39:04	SPEAKER_04
 Same answer anyway. Yeah, I've been working with Jeremy on his project, and then I've been trying to track down this bug.

0:39:16	SPEAKER_04
 That's a seed front end features.

0:39:18	SPEAKER_04
 So, one thing that I did notice yesterday, I was studying the, the, the Rostecode.

0:39:23	SPEAKER_04
 And it looks like we don't have any way to control the frequency range that we use in our analysis. We basically, it looks to me like we do the FFT, and then we just take all the bins, and we use everything.

0:39:39	SPEAKER_04
 We don't have any set of parameters where we can say, you know, only process from, you know, 110 hertz to 3750.

0:39:47	SPEAKER_02
 At least I couldn't see any kind of. Yeah, I don't think it's in there. I think it's in the, the filters.

0:39:55	SPEAKER_02
 So, the FFT is on everything, but the filters, for instance, ignore the lowest bins and the highest bins.

0:40:07	SPEAKER_02
 What it does is it copies the filter bank, which is created by integrating over FFT bins.

0:40:17	SPEAKER_02
 When you get the mail, when you get the mail scale. Yeah, it's a, it, it actually copies the, the second filters over to the first.

0:40:31	SPEAKER_02
 So, the first filters are always, and you can, you can specify a different number of features, different number of filters, I think.

0:40:39	SPEAKER_02
 So, you can specify a different number of filters, and whatever you specify, the last ones are going to be ignored.

0:40:49	SPEAKER_02
 So, that, that's a way that you sort of change what the, what the bandwidth is.

0:40:55	SPEAKER_02
 You, you can't do it without, I think, changing the number of filters.

0:40:59	SPEAKER_04
 I saw something about, it looked like it was doing something like that, but I didn't quite understand it.

0:41:05	SPEAKER_02
 So, maybe, yeah. So, the idea is that the very lowest frequencies, and typically the various highest frequencies are kind of junk.

0:41:11	SPEAKER_02
 Uh-huh. And so, you just, for continuity, you just approximate them by, by the second to highest and second to lowest.

0:41:19	SPEAKER_02
 It's just a simple thing we put in.

0:41:23	SPEAKER_02
 And so, but that's a fixed thing, there's nothing to put in. Yeah, I think that's a fixed thing, but see, see my point, if you had, if you had 10 filters, then you would be throwing away a lot at the two ends.

0:41:37	SPEAKER_02
 And if you had, if you had 50 filters, you'd be throwing away hardly anything.

0:41:43	SPEAKER_02
 I don't remember there being an independent way of saying we're just going to make you, just from here to here.

0:41:49	SPEAKER_04
 I don't know, it's actually been a while since I've looked at it. Yeah, I went through the FICAL code and then looked at, it was calling the RUST Alive and things like that.

0:41:56	SPEAKER_04
 And I didn't, I couldn't see any place where that kind of thing was done, but I didn't quite understand everything that I saw.

0:42:03	SPEAKER_02
 Yeah, see, I don't know FICAL at all, but it calls RUST with some options.

0:42:09	SPEAKER_02
 But I think, I don't know, I guess for some particular database, you might find that you could tune that and tweak that to get that a little better, but I think that in general, it's not that critical.

0:42:21	SPEAKER_02
 I mean, there's, you can, you can throw away stuff below 100 hertz or so and it's just not going to affect phonetic classification at all.

0:42:30	SPEAKER_04
 The other thing I was thinking about was, is there a, I was wondering if there's maybe certain settings of the parameters when you compute PLP, which would basically cause it to output Malcapsterum.

0:42:45	SPEAKER_04
 So that in effect, what I could do is use our code to produce Malcapsterum and compare that directly to...

0:42:52	SPEAKER_02
 Well, it's not precisely, yeah. I mean, what you can do is, you can definitely change the filter bank from being a trapezoidal integration to a triangular one, which is what the typical Malcapsteral filter bank does.

0:43:19	SPEAKER_02
 So some people have claimed that they got some better performance doing that, so you certainly can do that easily.

0:43:24	SPEAKER_02
 But the fundamental difference, I mean, there's other small differences.

0:43:28	SPEAKER_04
 There's a few groups that happen, sorry.

0:43:30	SPEAKER_02
 Yeah, but, you know, as opposed to the log and the other case.

0:43:33	SPEAKER_02
 I mean, the fundamental difference that we've seen any kind of difference from before, which is actually an advantage for PLP, I think is that the, the smoothing at the end is autoregressive instead of being kept from kept from location.

0:43:48	SPEAKER_02
 So it's a little more noise robust.

0:43:53	SPEAKER_02
 And that's why when people start getting databases that had a little more noise in it, like broadcast news and so on, that's why Cambridge Switched to PLP, I think.

0:44:03	SPEAKER_02
 So that's a difference that I don't think we put any way to get around since it was an advantage.

0:44:13	SPEAKER_02
 But we did, we did hear this comment from people at some point that it, they got some better results with the triangular filters rather than the trapezoidal, so that is an option in Rasta.

0:44:28	SPEAKER_02
 And you can certainly play with that.

0:44:31	SPEAKER_02
 But I think you're probably doing the right thing to look for books first.

0:44:34	SPEAKER_04
 Yeah, just, it just seems like this kind of behavior could be caused by, you know, some of the training data being messed up.

0:44:41	SPEAKER_04
 Yeah, you're sort of getting most of the way there, but there's a, so I started going through and looking, one of the things that I did notice was that the log likelihoods coming out of the recognizer from the PLP data were much lower, much smaller than for the male kept still stuff.

0:45:00	SPEAKER_04
 And that the average amount of pruning that was happening was therefore a little bit higher for the PLP features.

0:45:08	SPEAKER_04
 So since he used the same exact pruning thresholds for both, I was wondering if it could be that we're getting more pruning.

0:45:15	SPEAKER_02
 Oh, he, he, he used the identical pruning thresholds even though the, the range of the, the likelihood, oh well, that's, that's a pretty good point right there.

0:45:25	SPEAKER_02
 Yeah.

0:45:26	SPEAKER_02
 So I would think that you might want to do something like, you know, look at a few points to see where you were starting to get significant search errors.

0:45:33	SPEAKER_04
 Right. Well, what I was going to do is I was going to take a couple of the utterances that he had run through, then run them through again, but modify the pruning threshold and see if it, you know, affects the score.

0:45:46	SPEAKER_02
 Yeah.

0:45:47	SPEAKER_02
 But I mean, you could, if, if that looks promising, you could, you know, run the overall test set with a, with a few different pruning thresholds for both.

0:45:57	SPEAKER_02
 Yeah. And presumably he's running at some pruning threshold that's, that's, you know, gets very few search errors, but is, is relatively, right?

0:46:06	SPEAKER_04
 I mean, yeah, generally, these things, you, you turn back pruning really far. So I, I didn't think it would be that big a deal because I was freaking well, you'd have it turned back so far that, you know, but you may be in the wrong range for the PLP features for some reason.

0:46:18	SPEAKER_04
 And the, the, the runtime of the recognizer on the PLP features is longer, which sort of implies that the networks are busier.

0:46:25	SPEAKER_04
 You know, there's more things it's considering, which goes along with the fact that the matches aren't as good.

0:46:30	SPEAKER_04
 So, you know, it could be that we're just pruning too much.

0:46:35	SPEAKER_04
 So yeah.

0:46:37	SPEAKER_02
 Yeah, maybe just be different kind of distributions and, and yes, so that's, that's another possible thing.

0:46:43	SPEAKER_02
 They, they should really shouldn't, there's no particular reason why they would be exactly behave exactly the same.

0:46:49	SPEAKER_02
 Right.

0:46:51	SPEAKER_04
 So, so there's lots of little differences. Yeah. Yeah.

0:46:56	SPEAKER_02
 Trying to track it down. Yeah. I guess this was a little bit off topic. I guess I guess I was, I was thinking in terms of that this is being a core item that once we, once we had it going, we would use for a number of the front end things also.

0:47:11	SPEAKER_03
 That's as far as my stuff goes. Yeah. Well, I tried this mean subtraction method due to Avandano.

0:47:23	SPEAKER_03
 I'm taking six seconds of speech. I'm using two second FFT analysis frames, step by a half second. So it's a quarter length step.

0:47:34	SPEAKER_03
 And I take that frame and four, the four, I take the, sorry, I take the current frame and the four past frames and the four future frames.

0:47:42	SPEAKER_03
 And that, that adds up to six seconds of speech. And I calculate the spectral mean of the log magnitude spectrum over that.

0:47:51	SPEAKER_03
 And I use that to normalize the current center frame by mean subtraction. And then I move to the next frame and I do it again.

0:48:01	SPEAKER_03
 Well, actually I calculate all the means first and then I do the subtraction. And I tried that with HDK, the Aurora setup of HDK training on clean TI digits.

0:48:11	SPEAKER_03
 And it helped in a phony reverberation case where I just use a simulated impulse response. The error rate went from something like 80, from something like 18% to 4%.

0:48:27	SPEAKER_03
 And on meeting recorded for my digits, my channel F, it went from 41% error to 8% error.

0:48:39	SPEAKER_04
 On the real data, not with artificial reverb.

0:48:42	SPEAKER_03
 Right. And that that was trained on clean speech only, which I'm guessing is the reason why the baseline was so bad.

0:48:53	SPEAKER_02
 And that's actually a little side point is I think that's the first results that we have of any sort on the far field data for recorded in meetings.

0:49:07	SPEAKER_03
 Well, I'm actually Adam, around the SRI recognizer on the near field on the far field, also he did one 2CM channel and one PDH channel.

0:49:16	SPEAKER_02
 Oh, I didn't recall that. What kind of numbers was he getting with that?

0:49:20	SPEAKER_03
 I'm not sure. I think it was about 5% error for the PCM channel.

0:49:27	SPEAKER_02
 Five. Yeah. So why were you getting 41?

0:49:33	SPEAKER_03
 I'm guessing it was the training data. Clean TI digits is pretty pristine training data. And if they trained the SRI system on this TV broadcast type stuff, I think it's a much brighter range of channels.

0:49:46	SPEAKER_02
 No, but wait a minute. I think he was getting 1% error for the near field.

0:50:01	SPEAKER_02
 Yeah, I think it was getting around 1% for the close mic. So it's still this kind of ratio. It's just, yeah, it's a lot more training data.

0:50:16	SPEAKER_02
 So it probably should be something we should try then is to see if at some point is to transform the data and then use it for the SRI system.

0:50:28	SPEAKER_02
 So you have a system which for one reason or another is relatively poor.

0:50:36	SPEAKER_02
 Yeah. And you have something like 41% error and then you transform it to 8th by doing this work.

0:50:45	SPEAKER_02
 So here's this other system which is a lot better. There's still this kind of ratio. It's something like 5% error with the distant mic and 1% close mic.

0:50:56	SPEAKER_02
 So the question is how close to that one can you get if you transform the data using that system?

0:51:02	SPEAKER_03
 Right. So I guess the SRI system is trained on a lot of broadcast news or switchboard data.

0:51:08	SPEAKER_04
 Yeah. So do you know which one it is?

0:51:10	SPEAKER_04
 It's trained on a lot of different things. It's trained on a lot of switchboard, call home, a bunch of different sources, some digits or some digits training in there.

0:51:20	SPEAKER_03
 One thing I'm wondering about is what this mean subtraction method will do if it's faced with additive noise.

0:51:27	SPEAKER_03
 Because I don't know what log magnitude spectral subtraction is going to do to additive noise.

0:51:33	SPEAKER_02
 Yeah. Well, it's not exactly the right thing. But you were already seeing that because there is added noise here.

0:51:40	SPEAKER_03
 Yeah, that's true. Yeah. It's a good point.

0:51:44	SPEAKER_03
 So... Okay. So it's a reasonable to expect. It would be helpful if we used it with the SRI system.

0:51:53	SPEAKER_02
 Yeah. I mean, as helpful. So that's the question. We're often asked this when we work with a system that isn't sort of industry standard grade.

0:52:03	SPEAKER_02
 And we see some reduction error using some clever method, then we'll work on a good system.

0:52:09	SPEAKER_02
 So the other one is pretty good system. I think 1% would aerate on digits strings. It's not stellar.

0:52:21	SPEAKER_02
 But given that this is real digits is supposed to sort of laboratory.

0:52:26	SPEAKER_02
 And it wasn't trained on this task.

0:52:28	SPEAKER_02
 And it wasn't trained on this task. Actually, 1% sort of a reasonable range. People would say, I can imagine getting that.

0:52:34	SPEAKER_02
 And so the 4% or 5% or something is quite poor. If you're doing a 16-digit credit card number, you basically get it wrong almost all the time.

0:52:46	SPEAKER_02
 So significant reduction error for that would be great.

0:52:52	SPEAKER_02
 And then... Yeah. So... Yeah. Cool. Sounds good.

0:52:58	SPEAKER_02
 Right. I actually have to run. So I don't think I can do the digits. But I guess I'll leave my microphone on.

0:53:07	SPEAKER_04
 Yeah.

0:53:08	SPEAKER_02
 Yeah. Yeah. Yeah. Yeah. Yeah. You're actually kind of disinterprets. Yeah? Yeah. Yeah. I know. You're quickly.

0:53:28	SPEAKER_02
 Sorry. I just have to run for another appointment. Okay. Did I? Yeah, I left it.

0:53:32	SPEAKER_02
 Okay. Okay. Okay. This is transcript L-110. 902-573-266-166-674208. 5169-477-7950. 5602-2577.

0:53:56	SPEAKER_02
 6089-07264-5. 2192-89558. 3250-2934-4612-0885.

0:54:14	SPEAKER_00
 Transcript L-111-362-8332-207-16654268-9006-39334-754-7960-9903-7432-7542. 493-519-103-7452.

0:54:43	SPEAKER_00
 519-788-504-940753-758-441-701-8553253-8.

0:55:02	SPEAKER_04
 Transcript L-112-07876216-4653-1258-477-727-770. 636-34797-776. 529-327-329.

0:55:23	SPEAKER_04
 939-389-141-7604-400-867-458-066-359-777. 7355-3245-08.

0:55:41	SPEAKER_03
 I'm reading transcript L-113-65697-284-4087-5187-405702-425-037-681-779-217-909-0605-68859.

0:56:09	SPEAKER_03
 522-55662-911-7206-7162-023-019-023.

0:56:27	SPEAKER_06
 Transcript L-115-064-732-271-08758-673-755-119-14.

0:56:51	SPEAKER_06
 988-419-3102-410-368-219-628-969-186-565-0-3116-639-8-78-398-9.

0:57:20	None
 AS6-0810.

