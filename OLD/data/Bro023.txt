0:00:00	SPEAKER_04
 And Hans Gunter will be here with the mechanics x2's day or so.

0:00:20	SPEAKER_04
 Oh, okay.

0:00:21	SPEAKER_04
 So he's going to be here for about three weeks.

0:00:23	SPEAKER_03
 Oh, that's right.

0:00:24	SPEAKER_04
 Just for a visit.

0:00:25	SPEAKER_04
 We might end up with some longer collaboration or something.

0:00:30	SPEAKER_04
 So he's going to look in on everything we're doing and give us his thoughts.

0:00:35	SPEAKER_04
 And so be another good person looking at things.

0:00:39	SPEAKER_00
 Oh.

0:00:40	SPEAKER_00
 Yeah.

0:00:41	SPEAKER_00
 Oh, yeah.

0:00:42	SPEAKER_00
 Oh, yeah.

0:00:43	SPEAKER_00
 Yeah.

0:00:44	SPEAKER_00
 Oh, yeah.

0:00:45	SPEAKER_04
 Yeah.

0:00:46	SPEAKER_04
 Yeah.

0:00:47	SPEAKER_04
 Now, here we are after three weeks.

0:00:48	SPEAKER_04
 He's very, very easy going.

0:00:51	SPEAKER_04
 Easy to talk to.

0:00:52	SPEAKER_04
 Very interesting.

0:00:53	SPEAKER_04
 Everything.

0:00:54	SPEAKER_04
 Yeah.

0:00:55	SPEAKER_01
 Yeah, we're married and we're not sure.

0:00:57	SPEAKER_04
 Yeah.

0:00:58	SPEAKER_04
 Yeah, he's been here before.

0:00:59	SPEAKER_04
 I mean, he's been here.

0:01:00	SPEAKER_06
 He's been here.

0:01:01	SPEAKER_06
 He's been here for a year or six months, something like that.

0:01:05	SPEAKER_04
 Yeah.

0:01:06	SPEAKER_03
 Yeah.

0:01:07	SPEAKER_03
 He's done a couple of stays here.

0:01:10	SPEAKER_06
 So I guess we got lots to catch up on.

0:01:15	SPEAKER_06
 We haven't met for a couple of weeks.

0:01:17	SPEAKER_06
 We didn't meet last week morning.

0:01:18	SPEAKER_06
 I went around and walked everybody and seemed like they had some new results.

0:01:22	SPEAKER_06
 Other than them coming up and telling me I figured we should just wait a week and they can tell both, you know, all of us.

0:01:29	SPEAKER_06
 So why don't we start with you, Dave?

0:01:33	SPEAKER_06
 Oh, okay.

0:01:34	SPEAKER_06
 Then we can go on.

0:01:36	SPEAKER_00
 So since we're looking at putting this on, we need a lot of things.

0:01:42	SPEAKER_00
 I think you expect those factors to make this our conscious.

0:01:47	SPEAKER_00
 I just ask you if it would work using the path only and possibly present the pathway for me.

0:01:55	SPEAKER_00
 And so I just ask who I use 12 seconds from the path and the path and the path for me and 12 seconds.

0:02:03	SPEAKER_06
 12 seconds from the current.

0:02:07	SPEAKER_06
 12 seconds from the current.

0:02:09	SPEAKER_00
 12 seconds.

0:02:10	SPEAKER_00
 12 seconds from the end of the current frame.

0:02:20	SPEAKER_00
 So we've had to do it using a 12 seconds sandwich window.

0:02:24	SPEAKER_00
 I think it was a drop in performance, but it was slightly dropped.

0:02:28	SPEAKER_00
 Is that right?

0:02:29	SPEAKER_04
 Yeah, I mean, it was pretty tiny.

0:02:33	SPEAKER_00
 So that was encouraging.

0:02:40	SPEAKER_00
 That's encouraging for the idea of using it in hydroaccus.

0:02:43	SPEAKER_00
 It's like smart home.

0:02:44	SPEAKER_00
 And another issue I'm thinking about is in the smart home system.

0:02:48	SPEAKER_00
 So it's like 12 seconds in the early test.

0:02:50	SPEAKER_00
 It's like a good length of time.

0:02:52	SPEAKER_00
 But what happens if you have left in 12 seconds?

0:02:57	SPEAKER_00
 So I would be before back in May I did some experiments using say two seconds or four seconds or six seconds.

0:03:05	SPEAKER_00
 In those I trained the models using mean subtraction with the mean calculated over two seconds or four seconds or six seconds.

0:03:12	SPEAKER_00
 And here I was curious what if I trained the models using 12 seconds.

0:03:18	SPEAKER_00
 But I gave it a situation where the test I was subtracting using two seconds or four seconds or six seconds.

0:03:24	SPEAKER_00
 So I did that for about three different conditions.

0:03:31	SPEAKER_00
 And I think it was four seconds and six seconds and eight seconds, something like that.

0:03:44	SPEAKER_00
 And it seems like it hurts compared to if you actually trained the models using that same length of time.

0:03:50	SPEAKER_00
 But it doesn't hurt that much.

0:03:54	SPEAKER_00
 You usually left in 25% although I could see one where it was a point eight percent or so rise in word error rate.

0:04:01	SPEAKER_00
 But this is where even if I train on the model means subtractive with the same length of time as in the test, the word error rate is around 10% or 9%.

0:04:15	SPEAKER_00
 So it doesn't seem like that.

0:04:16	SPEAKER_04
 But looking at the other way, isn't it what you're saying that it didn't help you to have the longer time for training if you were going to have a short time for?

0:04:26	SPEAKER_00
 That's true.

0:04:29	SPEAKER_04
 I mean why would you do it if you knew that you were going to have short windows?

0:04:33	SPEAKER_06
 It seems like you're in normal situations you would never get 12 seconds of speech.

0:04:40	SPEAKER_01
 You need 12 seconds in the past two weeks to make it right?

0:04:46	SPEAKER_01
 Are you looking at six seconds in future and six in?

0:04:49	SPEAKER_06
 No, it's all 12 seconds in the past.

0:04:52	SPEAKER_06
 Is this 12 seconds of regardless of speech or sound?

0:04:56	SPEAKER_04
 The other thing which may be a little bit of something else we've talked about in terms of windowing and so on is that I wonder if you trained with 12 seconds.

0:05:07	SPEAKER_04
 And then when you were two seconds in, you used two seconds and when you were four seconds in, you used four seconds and you basically build up to the 12 seconds so that if you have very long utterances, you have the best.

0:05:18	SPEAKER_04
 But if you have shorter utterances, you use which you can.

0:05:21	SPEAKER_00
 Right. And that's actually what we're applying to do in small calm.

0:05:25	SPEAKER_00
 Yeah.

0:05:26	SPEAKER_00
 So the question I was trying to get out with those experiments is does it matter what models you use? Does it matter how much time you use to calculate the mean when you were doing the training data?

0:05:38	SPEAKER_04
 Right. But I mean the other thing is that that's, I mean the other way of looking at this going back to mean capital subtraction versus rasta kind of things is that you could look at mean capital subtraction, especially the way you're doing it as being kind of filter.

0:05:54	SPEAKER_04
 So the other thing is just to design a filter.

0:05:57	SPEAKER_04
 You know, basically you're doing a high pass filter or a band pass filter or some sort and just design a filter.

0:06:04	SPEAKER_04
 And then you know, filter will have certain behavior and you can look at the startup behavior.

0:06:09	SPEAKER_04
 You start up with nothing and you know, you will.

0:06:12	SPEAKER_04
 If you have an IAR filter, for instance, it will not behave in the study state way that you would like it to behave until you get along enough period.

0:06:25	SPEAKER_04
 But by just constraining yourself to have your filter be only subtraction of the mean, you're kind of tying your hands behind your back because there's filters have all sorts of temporal and spectral behaviors.

0:06:38	SPEAKER_04
 And the only thing consistent we know about is that you want to get rid of the very low frequency component.

0:06:47	SPEAKER_01
 But do you really want to calculate the mean you neglect all the silenced regions or you just use everything that's 12 seconds?

0:06:56	SPEAKER_00
 You mean in my task so far?

0:06:58	SPEAKER_00
 Yeah. Most of the silences have been cut out.

0:07:01	SPEAKER_00
 Okay.

0:07:02	SPEAKER_00
 Just just introwards silences.

0:07:04	SPEAKER_01
 And they are like pretty short.

0:07:11	SPEAKER_01
 So you really need a lot of speech to estimate the mean in it?

0:07:14	SPEAKER_00
 Well, if I only use six seconds, it still works pretty well.

0:07:18	SPEAKER_00
 I saw my task before.

0:07:20	SPEAKER_00
 I was trying 12 seconds, but that was the best.

0:07:22	SPEAKER_00
 Okay.

0:07:23	SPEAKER_00
 And increasing past 12 seconds didn't seem to help.

0:07:30	SPEAKER_00
 Yeah, I guess it's something I need to play with more of this.

0:07:34	SPEAKER_00
 I had to set that up for the smart answers that made maybe if I trained on six seconds, it would work better when I only had two seconds or four seconds.

0:07:44	SPEAKER_03
 Yeah.

0:07:46	SPEAKER_04
 And again, if you take this filtering perspective and if you essentially have it build up over time, I mean, if you computed means over two and then over four and over six, then it's literally getting into the kind of ramp up of a filter anyway.

0:08:03	SPEAKER_04
 And so you may just want to think of it as a filter.

0:08:05	SPEAKER_04
 But if you do that, then in practice, somebody using a smart-com system wouldn't think they're using it for a while.

0:08:14	SPEAKER_04
 It means that their first utterance instead of getting a 40% error rate reduction, they'll get what you get without this policy, you get 30%.

0:08:28	SPEAKER_04
 And then the second utterance that you give, they get the full benefit of it.

0:08:34	SPEAKER_06
 If it's ongoing, you catch the utterances.

0:08:37	SPEAKER_06
 That's how you get your...

0:08:39	SPEAKER_04
 Well, I'm saying in practice, yeah.

0:08:41	SPEAKER_04
 That's somebody's using a system to ask for directions or something.

0:08:45	SPEAKER_04
 Okay.

0:08:46	SPEAKER_04
 You know, they'll say something first and to begin with, if it doesn't get them quite right, maybe they'll come back and say, excuse me.

0:08:52	SPEAKER_04
 Or I mean, you should have some policy like that anyway. And in any event, they might ask a second question.

0:08:59	SPEAKER_04
 It's not like what he's doing doesn't improve things.

0:09:03	SPEAKER_04
 It does improve things just not as much as he would like.

0:09:07	SPEAKER_04
 And so there's a higher probability of making an error for a utterance.

0:09:14	SPEAKER_06
 It would be really cool if you could have...

0:09:16	SPEAKER_06
 There's probably users who would never like this, but if you had... could have a system where before they began to use it, they had to introduce themselves verbally.

0:09:24	SPEAKER_06
 Yeah.

0:09:25	SPEAKER_06
 I have my name and so on and so on.

0:09:27	SPEAKER_06
 And you could use that initial speech to do all these adaptations.

0:09:30	SPEAKER_04
 Right.

0:09:31	SPEAKER_04
 Well, the other thing, I guess, which I don't know much about... as much as I should about the rest of the system, but couldn't you...

0:09:41	SPEAKER_04
 If you sort of did a first pass, I don't know what kind of capability we have at the moment for doing second passes on some kind of little small lannister, a graph, or fusion network or something.

0:09:58	SPEAKER_04
 But if you did first pass with either without the means of subtraction or with a very short time one, and then once you actually had the whole utterance in, if you did the longer time version then based on everything that you had, and then at that point only used it to distinguish between, you know, top-end possible utterances or something.

0:10:31	SPEAKER_04
 You might... it might not take very much time. I mean, I know in the large vocabulary systems people were evaluating on the past, some people really pushed everything in to make it in one pass, but other people didn't and had multiple passes.

0:10:47	SPEAKER_04
 And the argument against multiple passes has often been, but we want this to be, you know, had nice interactive response.

0:10:58	SPEAKER_04
 And the counter argument to that, which is a BBN, I think, had was, yeah, but our second response is, second passes and third passes are really, really fast.

0:11:07	SPEAKER_04
 So if your second pass takes a millisecond, who cares?

0:11:15	SPEAKER_00
 So the idea of the second pass would be waiting until you have more recorded speech or...

0:11:23	SPEAKER_04
 If it turned out to be a problem, that you didn't have enough speech because you need longer window to do this processing, then one tactic is looking at the larger system, and not just at the front end stuff, is to take in the speech with some simpler mechanism or shorter time mechanism, do the best you can and come up with some possible alternates of what might have been said, and either in the form of an end-best list or in the form of a lattice or confusion network or whatever.

0:11:57	SPEAKER_04
 And then the decoding of that is much, much faster, or can be much, much faster if it isn't a big bushing network.

0:12:05	SPEAKER_04
 And you can decode that with speech that you've actually processed using this longer time subtraction.

0:12:14	SPEAKER_04
 So I mean, it's common that people do this sort of thing where they do more things that are more complex, a require looking over more time, whatever in some kind of second pass.

0:12:24	SPEAKER_04
 And again, if the second pass is really, really fast, another one I've heard of is in connected digit stuff, going back and through backtrace and finding regions that are considered to be a digit, but which have very low energy.

0:12:42	SPEAKER_04
 So, I mean, there's lots of things you can do in second pass, as in all sorts of levels.

0:12:49	SPEAKER_04
 Anyway, I'm throwing too many things out.

0:12:52	SPEAKER_06
 So is that...

0:13:00	SPEAKER_06
 Edit?

0:13:02	SPEAKER_06
 I got that.

0:13:05	SPEAKER_01
 Do you want to go for it?

0:13:12	SPEAKER_01
 So last two weeks, I've been working on that Vener filtering.

0:13:17	SPEAKER_01
 And I found that single, like I just do a normal Vener filtering, like the standard method of Vener filtering.

0:13:25	SPEAKER_01
 That doesn't actually give me any improvement over, like...

0:13:30	SPEAKER_01
 It actually improves over the baseline, but it doesn't need something like 50% or something.

0:13:35	SPEAKER_01
 So I've been playing with the baseline MFCC.

0:13:38	SPEAKER_06
 Yeah.

0:13:39	SPEAKER_01
 So that's the improvement is around 30% over the baseline.

0:13:44	SPEAKER_04
 Is that using contamination with something else?

0:13:47	SPEAKER_01
 No, just one stage Vener filter, which is a standard Vener filter.

0:13:50	SPEAKER_04
 No, but I mean, contamination with our non-normalization or with the...

0:13:54	SPEAKER_01
 Yeah, yeah, yeah.

0:13:55	SPEAKER_01
 So I just plug in the Vener filtering.

0:13:57	SPEAKER_01
 In our system where... Does it mean it gets worse?

0:14:01	SPEAKER_01
 No, it actually improves over the baseline of not having a Vener filter in the whole system.

0:14:05	SPEAKER_01
 Like I have an LDF plus online normalization.

0:14:08	SPEAKER_01
 And then I plug in the Vener filter in that.

0:14:10	SPEAKER_01
 So it improves over not having the Vener filter.

0:14:13	SPEAKER_01
 So it improves, but it doesn't take it like beyond like 30% over the baseline.

0:14:18	SPEAKER_01
 So...

0:14:19	SPEAKER_04
 That's when I'm confused about it, because I think I thought that our system was more like 40% without the Vener filter.

0:14:25	SPEAKER_04
 No, it's like...

0:14:27	SPEAKER_06
 What is the new VAD?

0:14:30	SPEAKER_01
 No, it's old VAD.

0:14:32	SPEAKER_01
 So my baseline was...

0:14:34	SPEAKER_01
 This is like the baseline is 95.68, 89.

0:14:43	SPEAKER_04
 So I mean, if you do all these word errors, it's a lot easier.

0:14:46	SPEAKER_04
 What is that?

0:14:47	SPEAKER_04
 If you do all these word error rates, it's a lot easier.

0:14:49	SPEAKER_01
 Oh, okay, okay, okay.

0:14:50	SPEAKER_01
 I don't have it. It's all accurate.

0:14:52	SPEAKER_02
 Yeah, the baseline is something similar to...

0:14:56	SPEAKER_02
 I mean, the baseline that you were talking about is the MFCC baseline.

0:15:01	SPEAKER_01
 Yeah, there are two baselines.

0:15:03	SPEAKER_01
 Okay, so the baseline...

0:15:04	SPEAKER_01
 One baseline is the MFCC baseline.

0:15:06	SPEAKER_01
 When I said 30% improvement is like the MFCC baseline.

0:15:08	SPEAKER_04
 So as a startup, the MFCC baseline is what?

0:15:12	SPEAKER_04
 Is it what level?

0:15:14	SPEAKER_01
 It's just the male frequency and...

0:15:17	SPEAKER_01
 That's what's the number.

0:15:19	SPEAKER_01
 So I don't have that number here.

0:15:22	SPEAKER_01
 Okay, okay, I have it here.

0:15:24	SPEAKER_01
 It's the VAD plus the baseline, actually.

0:15:27	SPEAKER_01
 I'm talking about the MFCC plus I do a frame dropping on it.

0:15:30	SPEAKER_01
 So that's like...

0:15:31	SPEAKER_01
 The word error rate is like 4.3.

0:15:33	SPEAKER_01
 4.3.

0:15:34	SPEAKER_01
 4.3.

0:15:35	SPEAKER_01
 And 0.7.

0:15:36	SPEAKER_01
 What's 10.7?

0:15:37	SPEAKER_01
 It's the medium mismatch.

0:15:38	SPEAKER_01
 Okay, sorry.

0:15:39	SPEAKER_01
 It's the well-match medium mismatch in the high mismatch.

0:15:41	SPEAKER_01
 So I don't have...

0:15:42	SPEAKER_01
 4.3, 10.7.

0:15:44	SPEAKER_01
 And 40.

0:15:46	SPEAKER_01
 40%.

0:15:47	SPEAKER_01
 It's the high mismatch.

0:15:50	SPEAKER_01
 Okay.

0:15:51	SPEAKER_01
 And it becomes like 4.3.

0:15:55	SPEAKER_04
 Not changed.

0:15:57	SPEAKER_01
 Yeah, it's like 10.1.

0:15:59	SPEAKER_01
 Still the same.

0:16:01	SPEAKER_01
 And the high mismatch is like 18.5.

0:16:07	SPEAKER_04
 18.5.

0:16:08	SPEAKER_04
 And what were you just describing?

0:16:10	SPEAKER_01
 The one is...

0:16:11	SPEAKER_01
 This one is just the baseline plus the...

0:16:14	SPEAKER_01
 We don't filter plugged into it.

0:16:17	SPEAKER_04
 But where's the online normalization and so on?

0:16:20	SPEAKER_01
 Okay, so...

0:16:21	SPEAKER_01
 Sorry.

0:16:22	SPEAKER_01
 So with the online normalization, the performance was...

0:16:27	SPEAKER_01
 And...

0:16:28	SPEAKER_01
 Okay, so it's like 4.3.

0:16:31	SPEAKER_01
 And again, that's the 10.4 and 20.1.

0:16:39	SPEAKER_01
 That was with online normalization and LDA.

0:16:43	SPEAKER_01
 So the well matched as like literally not changed by adding online or LDA on it.

0:16:49	SPEAKER_01
 But the...

0:16:51	SPEAKER_01
 I mean, even the medium mismatch is pretty much the same.

0:16:53	SPEAKER_01
 And the high mismatch is improved by 20% absolute.

0:16:57	SPEAKER_04
 Okay.

0:16:58	SPEAKER_04
 And what kind of number...

0:16:59	SPEAKER_04
 What are we talking about here?

0:17:00	SPEAKER_04
 Is this...

0:17:01	SPEAKER_01
 It's Italian.

0:17:02	SPEAKER_01
 Italian.

0:17:03	SPEAKER_04
 Yeah.

0:17:04	SPEAKER_04
 And what...

0:17:05	SPEAKER_04
 So what was the corresponding numbers say for the Alcatel system?

0:17:12	SPEAKER_04
 Yeah, 3.3.

0:17:14	SPEAKER_02
 Yeah, 3.3.

0:17:15	SPEAKER_02
 Yeah, 3.4.

0:17:18	SPEAKER_02
 Yeah, 3.4.

0:17:21	SPEAKER_02
 8.7.

0:17:24	SPEAKER_02
 And 13.7.

0:17:28	SPEAKER_02
 Okay.

0:17:29	SPEAKER_02
 Yeah.

0:17:30	SPEAKER_03
 Okay.

0:17:31	SPEAKER_03
 Thanks.

0:17:39	SPEAKER_05
 Okay.

0:17:40	SPEAKER_01
 So this is the single stage winner filter with the noise estimation was based on first 10 frames.

0:17:49	SPEAKER_01
 Actually, I started with using the VAD to estimate the noise.

0:17:52	SPEAKER_01
 And then I found that it works.

0:17:54	SPEAKER_01
 It doesn't work for finish and Spanish because the VAD endpoints are not good to estimate the noise.

0:18:00	SPEAKER_01
 Because it cuts into the speed sometime.

0:18:02	SPEAKER_01
 So I end up overestimating the noise and getting a worse result.

0:18:05	SPEAKER_01
 So it works only for Italian by using a VAD to estimate noise.

0:18:10	SPEAKER_01
 Work for Italian because the VAD was trained on Italian.

0:18:12	SPEAKER_01
 So this was giving...

0:18:18	SPEAKER_01
 This was not improving a lot on this baseline of not having the winner filter on it.

0:18:24	SPEAKER_01
 And so I ran this stuff with one more stage of winner filtering on it.

0:18:29	SPEAKER_01
 But the second time what I did was I estimated the new winner filter based on the cleaned up speech.

0:18:36	SPEAKER_01
 And did a smoothing in the frequency to reduce the variance.

0:18:43	SPEAKER_01
 I mean, I observed that a lot of bumps in the frequency when I do this winner filtering, which is more like a musical noise or something.

0:18:51	SPEAKER_01
 And so by adding another stage of winner filtering, the results on the speech that car was like...

0:18:58	SPEAKER_01
 So I still don't have the word error rate.

0:19:01	SPEAKER_01
 I'm sorry about it.

0:19:02	SPEAKER_01
 But the overall improvement was like 56.46.

0:19:05	SPEAKER_01
 This was again using 10 frames of noise estimate and two stage of winner filtering.

0:19:10	SPEAKER_01
 And rest is like the LDAP and online organization all remaining the same.

0:19:15	SPEAKER_01
 So this was like compared to 57 is what you got by using the French telecom system, right?

0:19:23	SPEAKER_02
 No, I don't think so. Is it on Italian?

0:19:26	SPEAKER_01
 No, this is all of the whole speech that car.

0:19:29	SPEAKER_01
 57 point.

0:19:30	SPEAKER_01
 Right.

0:19:31	SPEAKER_01
 Yeah, so the new new winner filtering scheme has like some 56.46, which is like one person still less than what you got using the French telecom system.

0:19:41	SPEAKER_04
 But it's a pretty similar number in any event.

0:19:44	SPEAKER_04
 It's very similar.

0:19:45	SPEAKER_04
 Yeah. But again, you're more or less doing what they were doing, right?

0:19:49	SPEAKER_01
 It's different in a sense.

0:19:51	SPEAKER_01
 Like, I'm actually cleaning up the cleaned up spectrum, which they're not doing.

0:19:55	SPEAKER_01
 They're what they're doing is they have two stage stages of estimating the winner filter.

0:20:00	SPEAKER_01
 Yeah, but the final filter what they do is they take it to the time domain by doing an inverse Fourier transform.

0:20:08	SPEAKER_01
 And they filter the original signal using that filter, which is like final filter is acting on the input noise speech rather than on the clean up.

0:20:17	SPEAKER_01
 So this is more like I'm doing winner filter twice, but the only thing is that the second time I'm actually smoothing the filter and then cleaning up the cleaned up spectrum first level.

0:20:29	SPEAKER_01
 Okay.

0:20:30	SPEAKER_01
 And so that that's that's what the difference is. And actually I tried it on the original clean.

0:20:35	SPEAKER_01
 I mean, the original spectrum where like I second time estimate the filter, but actually clean up the noisy speech rather than the first output of the first stage.

0:20:43	SPEAKER_01
 And that doesn't seem to be giving me that machine improvement.

0:20:48	SPEAKER_01
 I just didn't run it for the whole case.

0:20:50	SPEAKER_01
 And what I what I tried was by using the same thing, but so we actually found that the word is very like crucial.

0:21:01	SPEAKER_01
 I mean, just by changing the word itself gives you the lot of improvement by instead of using the current word, if you just take up the bad output from the channel zero.

0:21:10	SPEAKER_01
 When instead of using channel zero and channel one, because that was the reason why I was not getting a lot of improvement for estimating the noise.

0:21:18	SPEAKER_01
 So I just use the channel zero watt to estimate the noise so that it gives me some reliable markers for this noise estimation.

0:21:25	SPEAKER_04
 What's the channel zero then?

0:21:27	SPEAKER_03
 So it's like close talking.

0:21:29	SPEAKER_01
 Yeah, the close talking.

0:21:30	SPEAKER_01
 Because the channel zero and channel one are like the same speech only when the same endpoints, but only thing that the speech is very noisy for the channel one.

0:21:39	SPEAKER_01
 So can I also use the output of the channel zero for channel one for bad?

0:21:44	SPEAKER_01
 I mean, that's like a cheating.

0:21:46	SPEAKER_04
 Right. I mean, so what are they going to do?

0:21:49	SPEAKER_04
 Do we know yet about the source what the rules are going to be?

0:21:52	SPEAKER_02
 Yeah, so actually you received a new document.

0:21:54	SPEAKER_02
 Yeah, that's describing this.

0:21:57	SPEAKER_02
 And what they did find out is to not to align the utterances, but to perform recognition only on the close talking microphone.

0:22:08	SPEAKER_02
 Did you get the recognition to get the boundaries of speech?

0:22:15	SPEAKER_04
 So it's not like that's being done in one place or one time that's just a role and we you were permitted to do that.

0:22:22	SPEAKER_02
 I think they will send files, but we don't want.

0:22:28	SPEAKER_04
 Also, they will send files so everybody will have the same boundaries to work with.

0:22:31	SPEAKER_01
 Yeah, but actually the alignment actually is not seems to be improving in like on all cases.

0:22:36	SPEAKER_02
 Yeah, so what happened here is that the overall improvement that they have with this method.

0:22:44	SPEAKER_02
 Well, to be more precise what they have is they have these alignments and then they drop the beginning silence and the end silence, but they keep 200 milliseconds before speech and 200 after speech.

0:22:58	SPEAKER_02
 And they keep the speech post is also.

0:23:03	SPEAKER_02
 And the overall improvement over the MFCC baseline.

0:23:06	SPEAKER_02
 So when they just at this friend wrapping in addition is 40% right 14% I mean, which is which is the overall improvement.

0:23:22	SPEAKER_02
 But in some cases it doesn't improve at all like.

0:23:27	SPEAKER_01
 It gives like negative in some time like some Italian and the IDGETs right.

0:23:34	SPEAKER_01
 So by using the end point that speech actually it's worse than the baseline in some instances which could be due to the.

0:23:42	SPEAKER_02
 The other thing also is that 14% is less than what you obtain using a real VAD.

0:23:48	SPEAKER_02
 Yeah, our new result cheating like this. So I think this shows that there's still work.

0:23:58	SPEAKER_02
 But working on the VAD is still still important.

0:24:03	SPEAKER_06
 Can I ask just a high level question.

0:24:06	SPEAKER_06
 Can you just say like one or two sentences about Wiener filtering and why are people doing that? What's the deal?

0:24:14	SPEAKER_01
 So the Wiener filter it's like you try to minimize.

0:24:21	SPEAKER_01
 So the basic principle of Wiener filter is like you try to minimize the difference between the noise signal and the clean signal.

0:24:28	SPEAKER_01
 If you have two channels like let's say you have a clean signal and you have an additional channel where you know what is the noise signal.

0:24:35	SPEAKER_01
 And then you try to minimize error between these two.

0:24:37	SPEAKER_01
 So that's the basic principle and you can do that. If you have only a noise signal available with you, you try to estimate the noise from the assuming that the first few frames are noise or if you have voice activity director you estimate the noise spectrum.

0:24:52	SPEAKER_01
 And then you assume the noise is same.

0:24:55	SPEAKER_01
 Yeah, after the speech starts.

0:24:57	SPEAKER_01
 But that's not the case in many of our cases but it works reasonably well. And then you what you do is you.

0:25:07	SPEAKER_01
 So again, I can try down some of this.

0:25:11	SPEAKER_01
 Yeah, and then you do this. This is the transfer function of the Wiener filter.

0:25:15	SPEAKER_01
 So SF is the clean speech spectrum, and N is the noisy, and so this is the transfer function.

0:25:33	SPEAKER_01
 And then you multiply your noisy, and you get an estimate of the clean, and so.

0:25:43	SPEAKER_01
 But the thing is that you have to estimate the SF from the noisy spectrum what you have.

0:25:49	SPEAKER_01
 So you estimate the NF from the initial noise portions and then you subtract that from the current noise spectrum to get an estimate of the SF.

0:25:58	SPEAKER_01
 So sometimes that becomes zero because you don't have a true estimate of the noise.

0:26:03	SPEAKER_01
 So the filter will have like sometimes zeros, and it because some frequency values will be zeroed out because of that. And that creates a lot of discontinuities across the spectrum with the filter.

0:26:16	SPEAKER_01
 So that's what that was just the first stage of Wiener filtering that I tried.

0:26:22	SPEAKER_06
 So is this basically similar to just regular spectrocentrum?

0:26:28	SPEAKER_04
 It's all pretty related. There's a whole class of techniques where you try in some sense to minimize the noise.

0:26:37	SPEAKER_04
 And it's typically a mean square sense in some way.

0:26:42	SPEAKER_04
 And spectral subtraction is one approach to it.

0:26:49	SPEAKER_06
 So people use the Wiener filtering in combination with the spectral subtraction typically or are they sort of non-seating techniques?

0:26:58	SPEAKER_01
 They are very similar techniques. So it's like I've been seeing Wiener filter with spectral subtraction.

0:27:03	SPEAKER_04
 I mean in the long run you're doing the same thing, but you make different approximations. In spectral subtraction for instance there's an estimation factor.

0:27:12	SPEAKER_04
 So you figure out what the noise is and you multiply that noise spectrum times some constant and subtract that rather than sometimes people even though this really should be in the power domain, sometimes people work in the magnitude to mean because it works better.

0:27:30	SPEAKER_06
 So why did you choose Wiener filtering over some other one of these other techniques?

0:27:36	SPEAKER_01
 The reason was we had this choice of using spectral subtraction Wiener filtering and there was one more thing which I'm trying to do is the subspace approach.

0:27:45	SPEAKER_01
 Stefan is working on spectral subtraction. So I picked up a few sort of trying to be...

0:27:50	SPEAKER_01
 We just wanted to have a few noise production, composition techniques and then pick some from that.

0:27:56	SPEAKER_04
 Yeah, I mean there's comments working on other than the other activity series. So they were just trying to cover a bunch of different things with this task and see what are the issues for each of them.

0:28:08	SPEAKER_01
 So one of the things that I tried, like I said, was to remove those zeros in the filter by doing some smoothing of the filter.

0:28:16	SPEAKER_01
 Like you estimate the HF square and then you do smoothing across the frequency so that those zeros get like flattened out and that doesn't seem to be improving by trying it on the first time.

0:28:28	SPEAKER_01
 So what I did was like I did this and then you applied in the one more, the same thing but with the smooth filter, second time. That seems to be working.

0:28:38	SPEAKER_01
 So that's what I got like 56.5% improvement on speech.car with that. And so the other thing what I tried was I used till the 10 frames of noise estimate but I used this channel 0 watt to drop the frames.

0:28:52	SPEAKER_01
 So I'm not still on estimating and that has taken the performance to like 67% in speech.car which is which like sort of shows that by using a proper watt you can just take it to further better levels.

0:29:05	SPEAKER_01
 So that's sort of like you know, best case performance. Yeah so far I've seen 67% and I haven't seen like 67% and using the channel 0 watt to estimate the noise also seems to be improving but I don't have the results for all the cases with that.

0:29:22	SPEAKER_01
 So I used channel 0 watt to estimate noise as a lesser drop frame which is like everywhere I use the channel 0 watt and that seems to be the best combination than using a few frames to estimate and then drop channel.

0:29:35	SPEAKER_04
 So I'm still a little confused is that channel zero information going to be accessible.

0:29:42	SPEAKER_01
 Now this is just to test whether we can really improve by using a better watt. So I mean so this is like the noise compensation is fixed but you make a better decision on the end points that's like seems to be.

0:29:56	SPEAKER_01
 So which means which means like by using this technical it just the way we can just take the performance by another 10% of better.

0:30:06	SPEAKER_01
 So that that was just the reason for doing that experiment and yeah but this all these things have to still try it on the TI digits which is like I'm just running and that seems to be not improving a lot on the TI digits.

0:30:20	SPEAKER_01
 So the other thing is like I'm doing all this stuff on the power spectrum so try this stuff on the Mel as well Mel and the magnitude and Mel magnitude and all those things but seems to be the power spectrum seems to be giving the best result.

0:30:45	SPEAKER_01
 So one of the reasons I thought like do the averaging after the filtering using the Mel filter bank that seems to be maybe helping whether than trying it on the Mel filter filter out goods.

0:30:55	SPEAKER_01
 Yeah that's the only thing is I could think of why it's giving improvement on the Mel and yeah so that's about the subspace stuff.

0:31:15	SPEAKER_01
 So it's like going parallely but not much of improvement I'm just have some skeleton ready need some work.

0:31:33	SPEAKER_02
 Yeah so I've been working still on the spectral subtraction so to remind you a little bit of what I did before is just to apply some spectral subtraction with an over estimation factor also to get an estimate of the noise spectrum and subtract this estimation of the noise spectrum from the signal spectrum but subtracting more when the SNR is low which is the technique that it's subtracting from the meaning.

0:32:15	SPEAKER_02
 So you over estimate the noise spectrum you multiplied no spectrum by a factor which depends on the SNR so above 20 dB it's one so you just subtract the noise and then it's generally what I use actually on linear function of the SNR which is bounded to like two or three when the SNR is below zero dB.

0:32:42	SPEAKER_02
 Doing just this either on the FFT bands or on the Mel bands doesn't yield any improvement.

0:32:53	SPEAKER_03
 What are you doing with negative?

0:32:56	SPEAKER_02
 There is also threshold of course because after subtraction you can have negative energies and so what I just do is to add to put a threshold first and then to add a small amount of noise which right now is speed shaped.

0:33:21	SPEAKER_02
 So it has the overall energy as the overall power spectrum of speech so with a bump around one key.

0:33:31	SPEAKER_06
 When you talk about there being something less than zero after subtracting the noise is that a particular frequency band?

0:33:38	SPEAKER_06
 Yes there can be frequency bands with zero energy. So you're adding some it has overall.

0:33:51	SPEAKER_02
 For each frequency I'm adding some noise but the amount of noise I add is not the same for all the frequency bands.

0:34:02	SPEAKER_02
 Right now I don't think if it makes sense to add something that's speed shaped because then you have silence portion but if some spectra are similar to the overall speech spectrum.

0:34:14	SPEAKER_02
 So this is something I can still work on.

0:34:18	SPEAKER_06
 What does that mean? I'm trying to understand what it means when you do the spectral subtraction and you get a negative.

0:34:23	SPEAKER_06
 That means that that means that the frequency range you subtracted more energy than there was actually.

0:34:30	SPEAKER_02
 You have an estimation of the noise spectrum but sometimes of course as the noise is not perfectly stationary.

0:34:37	SPEAKER_02
 Sometimes this estimation can be too small so you don't subtract enough but sometimes it can be too large also.

0:34:45	SPEAKER_02
 If the noise energy in this particular frequency band drops for some reason.

0:34:54	SPEAKER_06
 So in an ideal world if the noise were always the same then when you subtracted it the worst that you would get would be a zero.

0:35:04	SPEAKER_06
 I mean the lowest you would get would be a zero because if there was no other energy there you're just subtracting exactly the noise.

0:35:10	SPEAKER_04
 There's all sorts of deviations from ideal here.

0:35:13	SPEAKER_04
 For instance you're talking about the signal noise at a particular point even if something is sort of stationary and it's third terms of statistics there's no guarantee that any particular instantiation or piece of it is exactly a particular number or bounded by a particular range.

0:35:32	SPEAKER_04
 So you're figuring out from some chunk of the signal what you think the noise is then you're subtracting that from another chunk.

0:35:42	SPEAKER_04
 And there's absolutely no reason to think that you'd know that it wouldn't be negative in some places.

0:35:47	SPEAKER_04
 On the other hand that just means that some sense you've made a mistake because you certainly have subtracted a bigger number than is due to the noise.

0:35:56	SPEAKER_04
 Also we speak where all this stuff comes from is from an assumption that signal noise are uncorrelated and that certainly makes sense in statistical interpretation that over all possible realizations that they're uncorrelated.

0:36:11	SPEAKER_04
 So that's why we're talking about the signal noise that is there uncorrelated or assuming your goodicity that across time it's uncorrelated.

0:36:21	SPEAKER_04
 But if you just look at quarter second and you cross multiply the two things you could very well end up with something that sums to something that's not zero.

0:36:33	SPEAKER_04
 So if you do signals could have some relation to one another and so there's all sorts of deviations from ideal in this and given all that you can definitely end up with something that's negative.

0:36:45	SPEAKER_04
 But if down the road you're making use of something as if it is a power spectrum then it can be bad to have something negative.

0:36:55	SPEAKER_04
 The other thing I wonder about actually is what if you left it negative what happens? I mean because the log are you taking the log before you add them up to the mill?

0:37:08	SPEAKER_04
 No, after.

0:37:09	SPEAKER_04
 Right. So the thing is I wonder how if you put your thresholds after that I wonder how often you would end up with negative values.

0:37:19	SPEAKER_01
 Would you end up reducing the neighboring frequency when the average right when you add the negative to the positive value which is the true estimate?

0:37:28	SPEAKER_04
 Yeah.

0:37:30	SPEAKER_04
 But nonetheless you know these are it's another kind of smoothing right that you're doing.

0:37:36	SPEAKER_04
 So you've done your best shot at figuring out what the noise should be and then you subtract it off and then after that instead of instead of leaving it as is and adding things adding up to neighbors you artificially push it up which is you know it's there's no particular reason that that's the right thing to do either right.

0:37:58	SPEAKER_04
 So in fact what you'd be doing is saying well we're we're we're going to definitely diminish the effect of this frequency and this is a little frequency bin in the in the overall mill summation.

0:38:15	SPEAKER_04
 This is the thought.

0:38:16	SPEAKER_06
 I don't know if you're going to get a negative number you don't do the subtraction for that.

0:38:22	SPEAKER_06
 Yeah, although almost the opposite right.

0:38:29	SPEAKER_06
 Instead of living it negative you don't do it.

0:38:32	SPEAKER_06
 If you're subtracting is going to result in a negative number you you don't do subtraction.

0:38:39	SPEAKER_04
 Yeah, but that means that in a situation where you thought that the bin was almost entirely noise you left it.

0:38:45	SPEAKER_03
 I'm just saying that's like yeah.

0:38:52	SPEAKER_02
 And some people also it's a negative value they recompute it using interpolation from the adjacent.

0:39:00	SPEAKER_02
 Yeah, from frequency bin things that you can do.

0:39:03	SPEAKER_04
 People can also reflect it back up and essentially do a four-way rectification instead of a set of half-wave.

0:39:10	SPEAKER_04
 But it's just a thought that that might be something to try.

0:39:15	SPEAKER_02
 Yeah, well actually I tried something else based on this.

0:39:21	SPEAKER_02
 It's to put some smoothing because it seems to help.

0:39:26	SPEAKER_02
 It seems to help the winner filtering.

0:39:31	SPEAKER_02
 So what I did is some kind of non-linear smoothing actually have a recursion that computes.

0:39:41	SPEAKER_02
 Yeah, let me go back a little bit.

0:39:43	SPEAKER_02
 Actually when you do spectral subtraction you can find this equivalent in the spectral domain you can compute.

0:39:54	SPEAKER_02
 You can see that the spectral subtraction is a filter and the gain of this filter is the signal energy minus what you subtract divided by the signal energy.

0:40:10	SPEAKER_02
 And this is the gain that varies over time.

0:40:13	SPEAKER_02
 Of course depending on the noise spectrum and on the speech spectrum.

0:40:20	SPEAKER_02
 And what happened actually is that during low SNR values the gain is close to zero but it varies a lot.

0:40:35	SPEAKER_02
 And this is the cause of musical noise and all these.

0:40:40	SPEAKER_02
 The fact that we go below zero on one frame and then you can have an energy that's above zero.

0:40:48	SPEAKER_02
 So the smoothing is, I did a smoothing actually on this gain trajectory but the smoothing is low in error in the sense that I try to not smooth if the gain is I.

0:41:02	SPEAKER_02
 Because in this case we know that the estimate of the gain is correct because we are not close to zero.

0:41:12	SPEAKER_02
 And to do more smoothing if the gain is low.

0:41:22	SPEAKER_02
 Yeah so basically that's this idea and it seems to give pretty good results.

0:41:29	SPEAKER_02
 Although I just tested on Italian and Finnish.

0:41:34	SPEAKER_02
 And on Italian it seems my result seems to be a little bit better than the winner filtering.

0:41:41	SPEAKER_01
 Yeah the one you showed yesterday.

0:41:44	SPEAKER_02
 I don't know if you have these improvements, the 10 improvements for Italian finishes.

0:41:49	SPEAKER_01
 No I don't have any.

0:41:50	SPEAKER_01
 I just just have the final number here.

0:41:52	SPEAKER_04
 So these numbers he was given before with the 4.3 and the 10 by 1.

0:41:55	SPEAKER_04
 And so those were Italian right?

0:41:58	SPEAKER_01
 Yeah so no I actually didn't give the number which is the final one which is after two stages of winner filtering.

0:42:04	SPEAKER_01
 I mean that was I just told like the overall improvement is like 56.5.

0:42:08	SPEAKER_01
 So his number is still better than what I got in that two stages of winner filtering.

0:42:13	None
 Right.

0:42:13	SPEAKER_02
 On Italian but on Finnish it's a little bit worse apparently.

0:42:19	SPEAKER_04
 But you have numbers in terms of water rates and...

0:42:21	SPEAKER_04
 Yeah.

0:42:22	SPEAKER_02
 That's what I just said.

0:42:23	SPEAKER_02
 You have some sensor reference.

0:42:24	SPEAKER_02
 3.8.

0:42:25	SPEAKER_03
 Oh okay.

0:42:28	SPEAKER_02
 And then 9.1.

0:42:34	SPEAKER_02
 And finally 16.5.

0:42:39	SPEAKER_04
 And this is spectrosotraction plus what?

0:42:43	SPEAKER_02
 Blood, plus non-linear smooth thing.

0:42:45	SPEAKER_02
 Well it's the system.

0:42:47	SPEAKER_02
 It's exactly the same system.

0:42:49	SPEAKER_02
 Oh my normalization.

0:42:50	SPEAKER_02
 But LDA.

0:42:51	SPEAKER_02
 Yeah.

0:42:52	SPEAKER_02
 But instead of the both stage winner filtering it's this smooth spectrosotraction.

0:43:00	SPEAKER_06
 Right. What is it? The France telecom system is it?

0:43:04	SPEAKER_06
 Did they use spectrosotraction or when you're filtering?

0:43:07	SPEAKER_02
 They use spectrosotraction right?

0:43:09	SPEAKER_02
 Forward?

0:43:10	SPEAKER_02
 French relic.

0:43:11	SPEAKER_02
 It's a winner filtering.

0:43:12	SPEAKER_01
 Oh it's a winner filtering.

0:43:13	SPEAKER_02
 Well it's some kind of winner filtering.

0:43:15	SPEAKER_01
 It's not exactly winner filtering but some variant of winner filtering.

0:43:19	SPEAKER_04
 Yeah.

0:43:20	SPEAKER_04
 Plus I guess they have some sort of capstone normalization.

0:43:23	SPEAKER_04
 They have like...

0:43:24	SPEAKER_01
 Yeah.

0:43:25	SPEAKER_01
 They're just noise compensation technique is a variant of winner filtering.

0:43:28	SPEAKER_01
 Because they do some smoothing techniques on the final filter.

0:43:33	SPEAKER_01
 They actually do the filtering in the time domain.

0:43:36	SPEAKER_01
 So they take this HF square back taking an inverse Fourier transform.

0:43:40	SPEAKER_01
 And they convolve that time domain signal with that.

0:43:43	SPEAKER_01
 And they do some smoothing on that final filter in pulse response.

0:43:48	SPEAKER_02
 But they also have to do different smoothing.

0:43:51	SPEAKER_02
 One in the time domain and one in the frequency domain by just taking the first.

0:43:57	SPEAKER_02
 Coefficient of the impulse response.

0:44:00	SPEAKER_02
 So basically similar...

0:44:03	SPEAKER_02
 I mean what you did it's...

0:44:05	SPEAKER_02
 It's similar in the smoothing.

0:44:06	SPEAKER_02
 They also have to kind of smoothing.

0:44:07	SPEAKER_02
 One in the time domain and...

0:44:09	SPEAKER_02
 Yeah.

0:44:10	SPEAKER_02
 One in the frequency domain.

0:44:12	SPEAKER_06
 Does the smoothing in the time domain help?

0:44:14	SPEAKER_06
 Well do you get this musical noise stuff with winner filtering?

0:44:17	SPEAKER_06
 Or is that only with the spectrosotraction?

0:44:19	SPEAKER_06
 Oh you get it with...

0:44:20	SPEAKER_01
 Yeah.

0:44:21	SPEAKER_06
 Winner filtering also?

0:44:22	SPEAKER_06
 Is the smoothing in the time domain help with that?

0:44:25	SPEAKER_01
 No, you still end up with zeros in the spectrum sometimes.

0:44:30	SPEAKER_04
 I mean it's not clear that these musical noises hurt us in recognition.

0:44:33	SPEAKER_04
 We don't know if they sound bad.

0:44:36	SPEAKER_03
 Yeah.

0:44:37	SPEAKER_03
 We're not listening to it usually.

0:44:39	SPEAKER_02
 Actually the smoothing that I did, the air reduced the music and noise.

0:44:46	SPEAKER_02
 Well I cannot...

0:44:48	SPEAKER_02
 You cannot hear...

0:44:49	SPEAKER_02
 Well actually what I did not say is that this is not in the FFT bands.

0:44:53	SPEAKER_02
 This is in the male frequency bands.

0:44:58	SPEAKER_02
 So it could be seen as a smoothing in the frequency domain because I use the male bands in addition.

0:45:05	SPEAKER_02
 And then the other phase of smoothing in the time domain.

0:45:10	SPEAKER_02
 But when you look at the spectrum, if you don't have any smoothing, you clearly see like in silence portions and at the beginning and end of speech, you see spots of high energy randomly distributed over the spectrum.

0:45:27	SPEAKER_02
 That's the musical.

0:45:29	SPEAKER_02
 Which is musical noisy.

0:45:31	SPEAKER_02
 If you listen to it, if you do this in the FFT bands, then you have spots of energy randomly distributed.

0:45:38	SPEAKER_02
 And if you recentize, this spot sounds as like sounds.

0:45:45	SPEAKER_04
 None of these systems, by the way, you both are working with our system that does not have the neural net.

0:45:56	SPEAKER_04
 Right?

0:45:57	SPEAKER_05
 Yeah.

0:45:58	SPEAKER_03
 So one would hope.

0:46:00	SPEAKER_03
 I assume we have the neural net part of it with improved things further as they did before.

0:46:06	SPEAKER_02
 Yeah.

0:46:09	SPEAKER_02
 Although if we look at the result from the proposals, one of the reasons the system with the neural net was more than well, around 5% better, is that it was much better on highly mismatched condition.

0:46:28	SPEAKER_02
 I'm thinking, for instance, on the TIDGITs trained on clean speech and tested noisy speech.

0:46:35	SPEAKER_02
 For this case, the system with the neural net was much better.

0:46:39	SPEAKER_02
 But not much on the other cases.

0:46:42	SPEAKER_02
 And if we have no spectral subtraction or winner filtering, the system is, we thought neural net work is much better than before, even in these cases of mismatch.

0:46:58	SPEAKER_02
 So maybe the neural net will help less.

0:47:02	SPEAKER_02
 Maybe.

0:47:03	SPEAKER_06
 Could you do any neural net with spectral subtraction?

0:47:07	SPEAKER_04
 Yeah, could do nonlinear spectral subtraction.

0:47:09	SPEAKER_04
 But I don't know if it, I mean, just figure out what your targets are.

0:47:14	SPEAKER_06
 Yeah, I was thinking if you had a clean version of the signal one.

0:47:18	SPEAKER_06
 A noisy version.

0:47:20	SPEAKER_06
 Right.

0:47:21	SPEAKER_06
 And targets for the, you know, whatever frequency.

0:47:24	SPEAKER_04
 Yeah, well, that's not so much spectral subtraction then.

0:47:27	SPEAKER_04
 But anyway, yeah, people, people do that.

0:47:30	SPEAKER_04
 Yeah, in fact, we had visitors here who did that, I think, when you were, a little bit way back when people had been lots of experimentation over the years with training neural nets.

0:47:41	SPEAKER_04
 It's not a bad thing to do.

0:47:42	SPEAKER_04
 It's another approach.

0:47:43	SPEAKER_04
 I mean, it's, it, the objection everyone always raises, which has some truth to it, is that it's good for mapping from a particular noise to clean, then you get a different noise.

0:47:54	SPEAKER_04
 And the experiments we saw that visitors did here showed that there was at least some gentleness to the degradation when you switched to different noises.

0:48:08	SPEAKER_04
 It did seem to help.

0:48:09	SPEAKER_04
 So that, you're right, that's another way to go.

0:48:11	SPEAKER_06
 You did care on, I mean, for good cases where it, it, it, it, stuff that it was trained on.

0:48:17	SPEAKER_06
 Did it do pretty well?

0:48:18	SPEAKER_06
 Oh, yeah, it did very well.

0:48:20	SPEAKER_04
 Yeah.

0:48:21	SPEAKER_04
 But to some extent, that's kind of what we're doing.

0:48:28	SPEAKER_04
 I mean, we're not doing exactly that.

0:48:30	SPEAKER_04
 We're not trying to generate good examples.

0:48:33	SPEAKER_04
 But by trying to do the best classifier you possibly can for these little fanatic categories.

0:48:39	SPEAKER_04
 It's sort of built in.

0:48:40	SPEAKER_04
 It's, yeah, it's kind of built into that.

0:48:42	SPEAKER_04
 And that's why we have found that it, it does help.

0:48:46	SPEAKER_04
 So, yeah, I mean, we'll just have to try it.

0:48:49	SPEAKER_04
 But I would, I would, I would imagine that it will help some.

0:48:52	SPEAKER_04
 I mean, it, this have to see whether it helps more or less the same.

0:48:57	SPEAKER_04
 But I would imagine it would help some.

0:48:59	SPEAKER_04
 So I didn't even, all of this, I was just confirming that all of this was with the simplices.

0:49:02	SPEAKER_04
 Yeah.

0:49:03	SPEAKER_04
 Yeah.

0:49:04	SPEAKER_02
 So this is the, well, actually this was kind of the first try with this spectral subtraction plus smoothing.

0:49:15	SPEAKER_02
 And I was kind of excited by the result.

0:49:19	SPEAKER_02
 Then I started to optimize the different parameters.

0:49:24	SPEAKER_02
 And the first thing I tried to optimize is the time constant of the smoothing.

0:49:32	SPEAKER_02
 And it seems that the one that I choose for the first experiment was the optimal one.

0:49:40	SPEAKER_02
 It's amazing how often I have.

0:49:45	SPEAKER_02
 This is the first thing.

0:49:48	SPEAKER_02
 Yeah, another thing that I, it's important to mention is that this as this as some additional latency, because when I do the smoothing, it's a recursion that estimates the means of the gain curve.

0:50:10	SPEAKER_02
 And this is a filter that has some latency.

0:50:15	SPEAKER_02
 And I noticed that it's better if we take into account this latency.

0:50:19	SPEAKER_02
 So instead of using the current estimated mean to subtract the current frame, it's better to use an estimate that's somewhere in the future.

0:50:32	SPEAKER_06
 And that's what causes the latency.

0:50:35	SPEAKER_01
 Yeah.

0:50:36	SPEAKER_01
 I mean, the mean is computed based on some frames in the future also.

0:50:40	SPEAKER_02
 No.

0:50:41	SPEAKER_02
 It's the recursion.

0:50:42	SPEAKER_02
 So it's the standard recursion, right?

0:50:46	SPEAKER_02
 And the latency of this recursion is around 50 milliseconds.

0:50:50	SPEAKER_03
 One five.

0:50:51	SPEAKER_03
 It's the one five five zero five zero.

0:50:53	SPEAKER_03
 Five zero.

0:50:54	SPEAKER_03
 Yeah.

0:50:55	SPEAKER_01
 Sorry, why is that delay coming like you estimate the mean?

0:51:01	SPEAKER_02
 Yeah, the mean estimation has some delay, right?

0:51:05	SPEAKER_02
 I mean, the filter has that estimated the mean as a distance.

0:51:08	SPEAKER_01
 Okay, so it's like it looks into the future also.

0:51:10	SPEAKER_04
 Yeah, okay.

0:51:11	SPEAKER_04
 What if you just look into the past?

0:51:13	SPEAKER_02
 It's not as good.

0:51:15	SPEAKER_02
 It's not bad.

0:51:16	SPEAKER_02
 How much?

0:51:17	SPEAKER_02
 It helps a lot of the baseline, but how much?

0:51:21	SPEAKER_02
 It's around 3% relative.

0:51:26	SPEAKER_02
 Where's?

0:51:29	SPEAKER_02
 Yeah.

0:51:30	SPEAKER_04
 So depending on all this stuff comes out, we may or may not be able to add any latency.

0:51:43	SPEAKER_02
 Yeah, but yeah.

0:51:46	SPEAKER_02
 Sorry, it's a different way.

0:51:47	SPEAKER_02
 Yeah, it's three percent.

0:51:54	SPEAKER_02
 Yeah, but I don't think we have to worry too much on that right now.

0:52:02	SPEAKER_04
 Yeah, I mean, I think the only thing is that I would worry about little, because if we completely ignore latency, then we discover that we really have to do something about it.

0:52:14	SPEAKER_04
 We're going to be finding ourselves in the bind.

0:52:17	SPEAKER_04
 So, you know, maybe you could make it 25.

0:52:21	SPEAKER_04
 Yeah, you know what I mean?

0:52:23	SPEAKER_04
 Yeah, just be a little conservative, because we may end up with this crunch where we have to cut the latency in half or something.

0:52:31	SPEAKER_02
 So, yeah, there are other things in the algorithm that I didn't play a lot yet,

0:52:39	SPEAKER_06
 which, sorry, quick question just about the latency thing. If there's another part of the system that causes the latency of 100 milliseconds, is this an additive thing or is yours hidden in that?

0:52:50	SPEAKER_06
 No, it's added.

0:52:52	SPEAKER_06
 It's added.

0:52:54	SPEAKER_01
 We can do something in parallel also.

0:52:57	SPEAKER_01
 In some cases, like if you wanted to do a voice activity detection, and you can do that in parallel with some other filtering you can do.

0:53:06	SPEAKER_01
 So, you can make a decision on that voice activity detection, and then you decide whether you want to filter or not.

0:53:11	SPEAKER_01
 But by then, you already have sufficient samples to do the filtering.

0:53:15	SPEAKER_06
 So, sometimes you can do it in.

0:53:18	SPEAKER_06
 I mean, couldn't you just also, I mean, if you know that the largest latency in the system is 200 milliseconds, couldn't you just buffer up that number of frames, and then everything uses that buffer in that way.

0:53:30	SPEAKER_06
 It's not additive.

0:53:32	SPEAKER_04
 One fact, everything is sent over and buffers kisses.

0:53:36	SPEAKER_04
 That TCP buffer, something.

0:53:39	SPEAKER_01
 I mean, the data, the superframe or something?

0:53:42	SPEAKER_01
 Yeah.

0:53:43	SPEAKER_01
 Yeah, but that has a variable latency, because the last frame doesn't have any latency, and the first frame has a 20 frame latency.

0:53:49	SPEAKER_01
 So, can't rely on that latency all the time.

0:53:52	SPEAKER_05
 Yeah.

0:53:54	SPEAKER_01
 Because, I mean, the transmission over the air interface is like a buffer, 20 frames.

0:53:59	SPEAKER_01
 Yeah.

0:54:00	SPEAKER_01
 So, but only thing is that the first frame in that 24 frame buffer has a 24 frame latency, and the last frame doesn't have any latency, because it just goes as...

0:54:09	SPEAKER_06
 Yeah, I wasn't thinking of that one in particular, but more of, you know, if there is some part of your system that has the buffer 20 frames, can't be other parts of the system draw out of that buffer and therefore not add to the latency.

0:54:21	SPEAKER_04
 Yeah, and that's sort of one of the, all of that sort of stuff is things they're debating in their standards committee.

0:54:29	SPEAKER_05
 Yeah, so, there is...

0:54:33	SPEAKER_02
 These parameters that I still have to look at, like, I played a little bit with this overestimation factor, but I still have to look more at this.

0:54:46	SPEAKER_02
 At the level of noise I add after, I know that adding noise at the system just using spellcrime subtraction without smoothing, but I don't know right now if it's still important or not, and if the level I choose before is still the right one, same thing for the shape of the noise.

0:55:11	SPEAKER_02
 And, you know, the level I choose before is still the right one, same thing for the shape of the noise.

0:55:20	SPEAKER_02
 Maybe it would be better to add just white noise instead of speech-shaped noise.

0:55:24	SPEAKER_04
 That'd be more like the aroust, I think, in a sense.

0:55:28	SPEAKER_02
 Yeah.

0:55:31	SPEAKER_02
 And another thing is to...

0:55:34	SPEAKER_02
 Yeah, for this I just use as noise estimates, the mean spectrum of the first 20 frames of each utterance.

0:55:43	SPEAKER_02
 I don't remember for this experiment, but did you use 10 frames?

0:55:46	SPEAKER_01
 I used 10 frames.

0:55:48	SPEAKER_01
 Yeah, because...

0:55:49	SPEAKER_01
 I mean, the reason was, like, in TI, did you say, I don't have a lot of 10 frames most of the time?

0:55:54	SPEAKER_02
 But so, what's this result you told me about the fact that if you use more than 10 frames, you can...

0:55:59	SPEAKER_01
 Oh, that's using the channel 0.

0:56:02	SPEAKER_01
 I use the channel 0 to estimate the noise.

0:56:05	SPEAKER_02
 But this is 10 frames plus...

0:56:07	SPEAKER_02
 channel 0.

0:56:09	SPEAKER_02
 Channel...

0:56:10	SPEAKER_02
 I know, these results...

0:56:12	SPEAKER_02
 Oh, two stage, winner filtering is 10 frames, but possibly more.

0:56:16	SPEAKER_02
 I mean, if channel 1, VAD gives you...

0:56:20	SPEAKER_02
 Yeah.

0:56:21	SPEAKER_02
 Okay.

0:56:22	SPEAKER_02
 Yeah, but in this experiment, I didn't use any VAD.

0:56:25	SPEAKER_02
 I just used the 21st frame to estimate the noise.

0:56:28	SPEAKER_02
 So I expected it to be a little bit better if I use more frames.

0:56:34	SPEAKER_02
 Okay, that's it for spectroscopy.

0:56:39	SPEAKER_02
 The second thing I was working on is to try to look at noise estimation and using some technique that doesn't need voice activity detection.

0:56:52	SPEAKER_02
 And for this, I simply used some code that I add from...

0:57:02	SPEAKER_02
 I add from Belgium, which is technique that takes a bunch of frames.

0:57:10	SPEAKER_02
 And for each frequency bands of this frame takes a look at the minima of the energy.

0:57:21	SPEAKER_02
 And then average this minima and take this as an energy estimate of the noise for this particular frequency band.

0:57:29	SPEAKER_02
 And there is something more to this, actually, what is done is that these minima are computed based on high-resolution spectra.

0:57:47	SPEAKER_02
 So I compute a FFT based on the long single frame, which is 64 milliseconds.

0:57:55	SPEAKER_06
 So you have one minimum per each frequency.

0:57:59	SPEAKER_02
 What I do actually is to take a bunch of...

0:58:03	SPEAKER_02
 to take a tile on the spectrogram and this tile is 500 milliseconds long and 200 hertz wide.

0:58:12	SPEAKER_02
 And this tile...

0:58:16	SPEAKER_02
 In this tile appears like the harmonics if you have a voice sound, because it's the FFT bands.

0:58:22	SPEAKER_02
 And when you take the minima of this tile, when you don't have speech, this minima will give you some noise level estimate.

0:58:33	SPEAKER_02
 If you have voiced speech, this minima will still give you some noise estimate because the minima are between the harmonics.

0:58:41	SPEAKER_02
 And if you have an or kind of speech sound, then it's not the case.

0:58:45	SPEAKER_02
 But if the time frame is long enough, like 500 milliseconds seems to be long enough, you still have portions which are very close...

0:58:56	SPEAKER_02
 which minima are very close to the noise energy.

0:58:59	SPEAKER_04
 I'm confused. You said 500 milliseconds, but you said 64 milliseconds.

0:59:03	SPEAKER_04
 What is that?

0:59:04	SPEAKER_02
 64 milliseconds is to compute the FFT bands, the FFT.

0:59:10	SPEAKER_02
 Actually, it's better to use 64 milliseconds because if you use 30 milliseconds, then because of this shortwindowing and at low pitch sounds, the harmonics are not correctly separated.

0:59:29	SPEAKER_02
 So if you take this minima, they will overestimate the noise a lot.

0:59:36	SPEAKER_04
 So you take 64 milliseconds and then you average them over 500 or what do you do over 500?

0:59:43	SPEAKER_02
 I take a bunch of these 64 milliseconds frame to cover 500 milliseconds.

0:59:50	SPEAKER_02
 And then I look for the minima on a bunch of 50 frames.

0:59:58	SPEAKER_02
 So the interest of this is that with this technique, you can estimate some reasonable noise spectra with only 500 milliseconds of signal.

1:00:10	SPEAKER_02
 So if the noise varies a lot, you can track better track the noise, which is not the case if you rely on a voice activity detector.

1:00:21	SPEAKER_02
 So even if there are no speech poses, you can track the noise level.

1:00:26	SPEAKER_02
 The only requirement is that you must have in this 500 milliseconds segment, you must have voiced sound at least.

1:00:33	SPEAKER_02
 Because this will add you to track the noise level.

1:00:42	SPEAKER_02
 So what I did is just to simply replace the VAD based noise estimate by this estimate.

1:00:52	SPEAKER_02
 First on speech.car. Well, only on speech.car actually.

1:00:56	SPEAKER_02
 And it's slightly worse, like one percent relative compared to the VAD based estimates.

1:01:10	SPEAKER_02
 I think the reason why it's not better is that speech.car. noise is our stationary.

1:01:19	SPEAKER_02
 There is no need to have something that's active. Well, there are mainly stationary.

1:01:27	SPEAKER_02
 But I expect maybe some improvement on TI digits because in this case the noises are sometimes very variable.

1:01:35	SPEAKER_02
 So I have to test it.

1:01:39	SPEAKER_04
 But are you comparing with something a little confused again?

1:01:45	SPEAKER_02
 When you compare it with the VAD based. It's the French Silicon based spectra winner filtering and VAD.

1:01:55	SPEAKER_02
 So it's their system, but just a replace. There are noise estimate by this one.

1:02:03	SPEAKER_02
 Oh, you're not doing this with our system. I'm not. No.

1:02:09	SPEAKER_02
 Yeah, it's our system, but we're just the winner filtering from their system.

1:02:19	SPEAKER_02
 Actually, the best system that we still have is our system, but with their noise compensation scheme.

1:02:28	SPEAKER_02
 So I'm trying to improve on this and by replacing their noise estimate by something that might be better.

1:02:37	SPEAKER_04
 Okay, but the spectra subtraction scheme that you reported on also requires a noise estimate.

1:02:45	SPEAKER_02
 But could I try this for that?

1:02:49	SPEAKER_02
 No, because I did this in parallel. I say what's working on it.

1:02:55	SPEAKER_02
 For sure, I can try it also.

1:02:59	SPEAKER_01
 I think that new noise estimate technique on this winner filtering what I'm trying. I have some experiments running around other results.

1:03:07	SPEAKER_01
 So I don't estimate the noise from the time France, but you see a system.

1:03:17	SPEAKER_02
 Yeah, I am also implemented spectral widening idea, which is in the Ericsson proposal.

1:03:29	SPEAKER_02
 The idea is just to flatten the log spectrum.

1:03:41	SPEAKER_02
 And to flatten it more if the probability of silence is higher.

1:03:49	SPEAKER_02
 So in this way, you can also reduce so much reduce the musical noise.

1:03:53	SPEAKER_02
 And you reduce the variability if you have different noise shapes.

1:03:59	SPEAKER_02
 Because the spectrum becomes more flat in the silence portions.

1:04:07	SPEAKER_02
 Yeah, with this no improvement, but there are a lot of parameters that we can play with.

1:04:17	SPEAKER_02
 And actually, this could be seen as a soft version of the frame dropping.

1:04:23	SPEAKER_02
 Because you could just put a threshold and say that below the threshold, I will flat on completely flat on the spectrum.

1:04:33	SPEAKER_02
 And above this threshold, give the same spectrum.

1:04:38	SPEAKER_02
 So it would be like frame dropping because during the silence portions, which are below the threshold of voice activity, probability, you would have some kind of dummy frame, which is perfectly flat spectrum.

1:04:53	SPEAKER_02
 And this widening is something that's more soft because you widen, you just have a function.

1:05:04	SPEAKER_02
 The widening is a function of the speech probability, so it's not a hard decision.

1:05:12	SPEAKER_02
 So I think maybe it can be used together with frame dropping and we are not sure about the speech of silence.

1:05:23	SPEAKER_04
 I mean, in J. Rasta, we were essentially adding in white noise, depending on our estimate of the noise, when the over-awesome in the noise.

1:05:37	SPEAKER_04
 I think it never occurred to us to use a probability in there.

1:05:43	SPEAKER_04
 You can imagine that that made use of where the amount that you added in was a function of the probability of it being speech or noise.

1:05:59	SPEAKER_02
 Yeah, we're at noise a constant that just depending on the noise spectrum.

1:06:14	SPEAKER_04
 Because that brings in sort of powers of classifiers that we don't really have in the other estimates, so it could be interesting.

1:06:26	SPEAKER_04
 What point does the system stop recording?

1:06:30	SPEAKER_04
 How much?

1:06:32	SPEAKER_06
 It'll be a little long.

1:06:35	SPEAKER_06
 I just ran out of this space.

1:06:40	SPEAKER_06
 I think we're okay.

1:06:46	SPEAKER_02
 Yeah, so with this technique, there are some... I just did something exactly the same as the direction proposal, but the probability of speech is not computed the same way.

1:07:03	SPEAKER_02
 I think for a lot of things, actually, a good speech probability is important. For frame-dropping, you can improve from 10%.

1:07:17	SPEAKER_02
 As a news show, if you use the channel zero speech probabilities, for this, it might add.

1:07:30	SPEAKER_02
 So, yeah, the next thing I started to do is to try to develop a better voice activity detector.

1:07:44	SPEAKER_02
 For this, I think we can maybe try to train the neural network for voice activity detection on all the data that we have, including all these speech data.

1:08:00	SPEAKER_02
 So, I'm starting to obtain alignment on these databases. The way I do that is that I just use the SDK system, but I train only on the closed-talking microphone, and then I obtain the Viterbi alignment of the training utterances.

1:08:21	SPEAKER_02
 It seems to be... Actually, what I observe is that for Italian, it doesn't seem... No, it doesn't seem to be a problem.

1:08:31	SPEAKER_01
 No, it doesn't seem to be a problem.

1:08:34	SPEAKER_02
 When you enter the frame-dropping, right? Yeah, but actually, the VAD was trained on Italians. The current VAD that we have was trained on the spine-right, the attitudes with noise and length.

1:08:55	SPEAKER_02
 It seems to work on Italian, but not on the Finnish and Spanish data. So, maybe one reason is that Finnish and Spanish noise are different.

1:09:07	SPEAKER_02
 Actually, we listen to some of the utterances, and sometimes for Finnish, there is music in the recordings and strange things.

1:09:18	SPEAKER_02
 Yeah, so the idea was to train on all the databases and obtain an alignment to train on these databases.

1:09:27	SPEAKER_02
 Also, to try different kinds of features, as input to the VAD network, we came up with a bunch of features that we want to try.

1:09:40	SPEAKER_02
 Like the spec-roslow, the degree of voicing with the features that we started to develop with Kermann, with the correlation between bands, and then different kinds of features.

1:10:01	SPEAKER_02
 Energy, of course.

1:10:13	SPEAKER_04
 Okay, well, Hans Kuntel will be here next week, so I think he'll be interested in all of these things.

1:10:31	SPEAKER_04
 Transcript L-209, 4-6-28-893020, 4-22-08-0952, 5-075-121056, 9-37105-2768, 316-727-5311, 7-329-7204-2, 7-646-7011, 5-8152-268.

1:11:08	SPEAKER_06
 Transcript L-294, 6-02459-228, 05827-34, scratch that, 3746, 7991-6007-1418, 6-6-717-449, 094019-6213, 6-059-78-2606, 8-613-296973, 8-556-71576, 6.

1:11:48	SPEAKER_00
 Transcript L-281-364-827-3611, 5-364-549408, 1-369-5408, 4-232-35773-8, 1-996-6488-2402, 153-393-89, 9-365-78060, 6-8915-07935.

1:12:26	SPEAKER_01
 Transcript L-285, 7269-4416, 07604-5454, 1-884-385-387-09, 0535-393-266, 4-909-909109, 6-713-05271-236, 7-832-96, 3-385, 4-757-1276-4975.

1:13:02	SPEAKER_02
 Transcript L-286, 3-7545146-9, 3-5453-5309, 6-85-2, 5-8-2143-44, 8-265-5808-1, 5-396-1055338, 5-01-195-910, 5-343-117-859, 6-050-1187-391.

