0:00:00	None
 Yes.

0:00:01	SPEAKER_05
 Okay, we're on.

0:00:10	SPEAKER_05
 So just make sure that the wireless mic is on.

0:00:12	SPEAKER_05
 If you're wearing wireless.

0:00:13	SPEAKER_05
 Check one.

0:00:15	SPEAKER_05
 And you should be able to see which one.

0:00:20	SPEAKER_05
 Which one you're on by watching the little bars change.

0:00:24	SPEAKER_03
 So which is my bar?

0:00:25	SPEAKER_03
 Number one.

0:00:26	SPEAKER_05
 So actually if you guys want to go ahead and read digits now as long as you've signed the consent form, that's all right.

0:00:35	SPEAKER_05
 Are we supposed to read digits at the same time?

0:00:37	SPEAKER_05
 No.

0:00:38	SPEAKER_05
 No, each individually.

0:00:39	SPEAKER_05
 We're talking about doing all at the same time, but I think cognitively that would be really difficult to try to read them while everyone else is.

0:00:48	SPEAKER_05
 So when you're reading the digits strings, the first thing to do is just say which transcript

0:00:52	SPEAKER_04
 you're on.

0:00:53	SPEAKER_05
 So you can see the transcript. There's two large number strings on the digits.

0:01:06	SPEAKER_05
 So you would just read that one.

0:01:08	SPEAKER_05
 And then you read each line with a small pause between lines.

0:01:12	SPEAKER_05
 And the pause is just so that the person transcribing it can tell where one line ends and the other begins.

0:01:16	SPEAKER_05
 And I'll read the digits strings first so you can see how that goes.

0:01:21	SPEAKER_05
 Again, I'm not sure how much I should talk about stuff before everyone's here.

0:01:33	SPEAKER_04
 Well, one more coming.

0:01:35	SPEAKER_05
 Well, I'm going to go ahead and read digit strings and then we can go on from there.

0:01:39	SPEAKER_05
 So this is transcript.

0:01:40	SPEAKER_05
 3, 6, 5, 1, 3, 6, 7, 0, 3, 3, 5, 1, 9, 0, 3, 6, 4, 0, 2, 3, 6, 5, 1, 3, 6, 5, 1, 0, 5, 1, 0, 5, 2, 1, 5, 6, 4, 7, 6, 7, 8, 0, 2, 1, 4, 0, 4, 6, 0, 5, 5, 8, 0, 2, 8, 1, 6, 8, 2, 8, 2, 9, 3.

0:02:18	SPEAKER_04
 Okay, well, we can start doing it.

0:02:26	SPEAKER_03
 Okay, this is transcript 3, 6, 3, 1, 3, 6, 5, 1, 3, 1, 5, 4, 2, 9, 7, 6, 3, 9, 5, 7, 1, 6, 7, 4, 4, 6, 7, 8, 9, 0, 4, 3, 0, 2, 4, 9, 2, 6, 9, 1, 7, 4, 1, 2, 3, 4, 0, 4, 7, 9, 3, 7, 4, 5, 6, 8, 3, 9, 9, 3, 4, 7, 0, 7, 0, 9, 1, 7, 1, 2,

0:03:15	SPEAKER_04
 3, 6, 7, 4, 7, 6, 7, 6, 6, 7, 8, 7, 8, 9, 1, 8, 7, 8, 8, 8, 8, 8, 9, 0-0-3129-889-0.

0:03:39	SPEAKER_04
 0-0-3129-8235-33049-490507-670531-80620-015-03832.

0:03:45	SPEAKER_04
 0-0-3309-490507-670531-80620-015-03832.

0:03:52	SPEAKER_04
 0-0-3309-015-030830620-015-03832.

0:04:22	SPEAKER_02
 0-0-3309-99900.

0:04:52	SPEAKER_01
 0-0-3351-357-0. 0-9-0802-0. 0-1-0-0-7-4-3401-4218-5478-8682-7.

0:04:59	SPEAKER_01
 0-0-7-4401-4218-5478-8682-7.

0:05:09	SPEAKER_01
 0-0-66450-901-152-6-0-3.

0:05:17	SPEAKER_01
 0-0-7-6-3100-5-6-827-92959-5904-2-3.

0:05:23	SPEAKER_01
 0-0-7-6-6-6-6-6-6-6-6-5708-5902-0.

0:05:30	SPEAKER_05
 0-0-7-6-6-6-7-6-7.

0:05:43	SPEAKER_05
 0-0-7-6-6-6-6-6-6.

0:05:48	SPEAKER_05
 only once the speaker form and the consent form.

0:05:52	SPEAKER_05
 And the short form, I mean you should read the consent form, but the thing to notice is that we will give you an opportunity to edit all the transcripts.

0:06:00	SPEAKER_05
 So if you say things and you don't want them to be released to the general public, which these will be available at some point to anyone who wants them, you'll be given an opportunity by email to bleep out any portions you don't like.

0:06:16	SPEAKER_05
 On the speaker form, just to tell you as much of the information as you can, if you're not exactly sure about the region, we're not exactly sure either.

0:06:22	SPEAKER_05
 So don't worry too much about it.

0:06:24	SPEAKER_05
 It's just self-rating.

0:06:28	SPEAKER_05
 I think that's about it.

0:06:30	SPEAKER_05
 And should I do want me to talk it all about why we're doing this and what this project is?

0:06:34	SPEAKER_04
 Yeah, no, let's see.

0:06:36	SPEAKER_02
 Does that see no, that we're meeting in here?

0:06:38	SPEAKER_04
 She's got an even, she was notified whether she knows.

0:06:43	SPEAKER_04
 It's another question.

0:06:47	SPEAKER_04
 So are the people going to be identified by name?

0:06:50	SPEAKER_05
 Well, we'll anonymize it in the transcript, but not in the audio.

0:06:55	SPEAKER_04
 OK, so in terms of people worrying about excising things from the transcript that's unlikely since it isn't attributed.

0:07:05	SPEAKER_04
 Oh, I see, but the other thing.

0:07:07	SPEAKER_05
 Right, so if I said, oh, hi, Jerry, how are you?

0:07:09	SPEAKER_05
 We're not going to go through and cancel out the jerry.

0:07:12	SPEAKER_05
 Sure.

0:07:13	SPEAKER_05
 So we will go through.

0:07:14	SPEAKER_05
 And in the speaker ID tags, there'll be M1 or 7M108.

0:07:19	SPEAKER_05
 But I don't know a good way of doing it on the audio and still have people who are doing discourse research to be able to use the data.

0:07:29	SPEAKER_04
 No, I was in the complain here.

0:07:30	SPEAKER_04
 I just wanted to understand.

0:07:31	SPEAKER_03
 Right, OK.

0:07:32	SPEAKER_03
 We can make up LES for each of us.

0:07:35	SPEAKER_05
 OK, I mean, whatever you want to do is fine.

0:07:36	SPEAKER_05
 But we find that we want the meeting to be as natural as possible.

0:07:40	SPEAKER_05
 We're trying to do real meetings.

0:07:42	SPEAKER_05
 And so we don't want to have to do aliases and we don't want people to be editing what they say.

0:07:46	SPEAKER_05
 So I think it's better just as a post-process to edit out every time you bash Microsoft.

0:07:54	SPEAKER_04
 OK, so why don't you tell us briefly?

0:07:56	SPEAKER_04
 Give your normal spiel.

0:07:59	SPEAKER_05
 So this is a project that's called Meeting Recorder.

0:08:01	SPEAKER_05
 And there are lots of different aspects of the project.

0:08:03	SPEAKER_05
 So my particular interest is in the pda of the future.

0:08:07	SPEAKER_05
 This is a mock-up of one.

0:08:08	SPEAKER_05
 Yes, we do believe the pda of the future will be made of wood.

0:08:12	SPEAKER_05
 The idea is that you'd be able to put a pda at the table at an impromptu meeting and record it and then be able to do querying and retrieval later on on the meeting.

0:08:20	SPEAKER_05
 So that's my particular interest is a portable device to do information retrieval on meetings.

0:08:25	SPEAKER_05
 Other people are interested in other aspects of meetings.

0:08:29	SPEAKER_05
 So the first step on that in any of these is to collect some data.

0:08:32	SPEAKER_05
 And so what we wanted is a room that's instrumented with both the tabletop microphones.

0:08:37	SPEAKER_05
 And these are very high quality pressure zone mics, as well as the close talking mics.

0:08:42	SPEAKER_05
 What the close talking mics gives us is some ground truth gives us high quality audio, especially for people who aren't interested in the acoustic parts of this corpus.

0:08:52	SPEAKER_05
 So for people who are more interested in language, we didn't want to penalize them by having only the far-field mics available.

0:08:58	SPEAKER_05
 And then also, it's a very, very hard task in terms of speech recognition.

0:09:04	SPEAKER_05
 And so on the far-field mics, we can expect very low recognition results.

0:09:08	SPEAKER_05
 So we wanted the near-field mics to at least isolate the difference between the two.

0:09:12	SPEAKER_05
 So that's why we're recording in parallel with the close talking and the far-field at the same time.

0:09:17	SPEAKER_05
 And then all these channels are recorded simultaneously and framed synchronously so that you can also do things like beam forming on all the microphones and degree search like that.

0:09:27	SPEAKER_05
 Our intention is to release this data to the public, probably through a body like the LDC, and just make it as a generally available corpus.

0:09:38	SPEAKER_05
 There's other work going on in meeting recordings.

0:09:40	SPEAKER_05
 So we're working with SRI with UW, NIST has started an effort, which will include video.

0:09:46	SPEAKER_05
 We're not including video, obviously.

0:09:50	SPEAKER_05
 And then also a small amount of assistance from IBM.

0:09:53	SPEAKER_05
 So I'm so involved.

0:09:56	SPEAKER_05
 Oh, and the digit strings.

0:09:57	SPEAKER_05
 This is just a more constrained task.

0:10:01	SPEAKER_05
 So because the general environment is so challenging, we decided to do at least one set of digit strings to give ourselves something easier.

0:10:10	SPEAKER_05
 And it's exactly the same digit strings as in TI digits, which is a common connected digits corpus.

0:10:16	SPEAKER_05
 So we'll have some comparison to be able to be made.

0:10:20	SPEAKER_05
 OK.

0:10:21	SPEAKER_05
 Anything else?

0:10:22	SPEAKER_05
 Nope.

0:10:23	SPEAKER_05
 OK, so when the last person comes in, just have him wear a wireless.

0:10:26	SPEAKER_05
 It should be on already, either one of those.

0:10:29	SPEAKER_05
 And read the digit strings and fill out the forms.

0:10:32	SPEAKER_05
 So the most important form is the consent form.

0:10:34	SPEAKER_05
 So be sure everyone signs that if they consent.

0:10:37	SPEAKER_05
 It should be usual for meeting certain people come late.

0:10:40	SPEAKER_05
 So you don't have to use these.

0:10:42	SPEAKER_05
 And just give me a call, which my team is up there when your meeting is over.

0:10:46	SPEAKER_05
 And I'm going to leave the mic here, but I'm not going to be on.

0:10:50	SPEAKER_05
 So don't have them use this one.

0:10:52	None
 Thank you.

0:10:54	None
 Thank you.

0:10:55	SPEAKER_03
 Any further?

0:10:57	SPEAKER_03
 Yeah, there we go.

0:10:59	SPEAKER_04
 Anyway, Adam, we will be using the screen as well.

0:11:04	SPEAKER_04
 So yeah.

0:11:08	SPEAKER_04
 Wow.

0:11:11	SPEAKER_04
 Organization.

0:11:12	SPEAKER_04
 So you guys got an email about this Friday or something about what we're up to.

0:11:17	SPEAKER_02
 No.

0:11:21	SPEAKER_03
 I got it.

0:11:24	SPEAKER_02
 What was the nature of the email?

0:11:25	SPEAKER_04
 Oh, this was about inferring intentions from pictures and context and the word.

0:11:33	SPEAKER_04
 It's like go to sea or visit.

0:11:36	SPEAKER_02
 Well, I didn't get it.

0:11:38	SPEAKER_02
 I don't think I did it.

0:11:40	SPEAKER_04
 I guess these have got better filters.

0:11:41	SPEAKER_04
 Because I said it to everybody.

0:11:43	SPEAKER_04
 You just blew it off.

0:11:44	SPEAKER_04
 OK.

0:11:46	SPEAKER_03
 It's only simple.

0:11:46	SPEAKER_03
 So this is the idea.

0:11:47	SPEAKER_03
 We could pursue, if we thought it's worth it, but I think we will agree on that, to come up with a very, very first crew prototype and do some implementation work and do some research and some modeling.

0:12:08	SPEAKER_03
 So the idea is if you want to go somewhere and focus on that object down, oh, I can actually walk around this.

0:12:19	SPEAKER_03
 Down here.

0:12:20	SPEAKER_03
 That's the powder tower.

0:12:22	SPEAKER_03
 Now, we found in our data experiments that there are three things you can do.

0:12:34	SPEAKER_03
 You can walk this way and come really, really close to it and touch it.

0:12:41	SPEAKER_03
 But you cannot enter or do anything else unless you're interested in rock climbing.

0:12:45	SPEAKER_03
 It would do you no good standing there.

0:12:47	SPEAKER_03
 It's just a dark alley.

0:12:48	SPEAKER_03
 You can touch it.

0:12:50	SPEAKER_03
 If you want to actually go up or into the tower, you have to go this way and then through some buildings and upstairs and so forth.

0:12:57	SPEAKER_03
 If you actually want to see the tower, and that's what actually most people want to do, is just have a good look of it, take a picture for the family.

0:13:07	SPEAKER_03
 You have to go this way and go up here.

0:13:10	SPEAKER_03
 There you have a really view.

0:13:11	SPEAKER_03
 It exploded during the 30 years war, really interesting side.

0:13:17	SPEAKER_03
 And these lines are paths, that's the street network of our geographic information system.

0:13:27	SPEAKER_03
 You can tell that we deliberately cut out this part, because otherwise we couldn't get our GIS system to lead people this way.

0:13:35	SPEAKER_03
 It would always use the closest point to the object and then the tourists would be faced in front of a wall, but would do them absolutely no good.

0:13:44	SPEAKER_03
 So what we found interesting is, first of all, intentions differ.

0:13:51	SPEAKER_03
 Maybe you want to enter a building.

0:13:54	SPEAKER_03
 Maybe you want to see it, take a picture of it.

0:13:58	SPEAKER_03
 Or maybe you actually want to come as close as possible to the building for whatever reason.

0:14:03	SPEAKER_02
 That's it.

0:14:03	SPEAKER_02
 What's it made out of?

0:14:06	SPEAKER_02
 Red limestone.

0:14:10	SPEAKER_02
 So maybe you would want to touch it.

0:14:11	SPEAKER_03
 Yeah, maybe you would want to.

0:14:15	SPEAKER_03
 OK, these intentions we could, if you want to call it the Vista mode, really just want to get the overview or look at it.

0:14:26	SPEAKER_03
 The Enter mode and the Tango mode, I always call it, with silly names.

0:14:32	SPEAKER_03
 So this Tango means literally translated to touch.

0:14:36	SPEAKER_03
 But sometimes the Tango mode is really relevant in the sense that if you don't have the intention of entering a building, but you know that something is really close to it, you just want to approach it or get to that building.

0:14:56	SPEAKER_03
 Consider, for example, the post office in Chicago building, so large that it has its own zip code.

0:15:01	SPEAKER_03
 So the entrance could be miles away from the closest point.

0:15:06	SPEAKER_03
 So sometimes it makes sense maybe to distinguish there.

0:15:10	SPEAKER_03
 So I've looked through 20 some.

0:15:16	SPEAKER_03
 I didn't look through all the data.

0:15:19	SPEAKER_03
 And there is a lot more different ways in people, the ways people phrase how to get, if they want to get to a certain place.

0:15:29	SPEAKER_03
 And sometimes here it's a little bit more obvious.

0:15:36	SPEAKER_03
 Maybe I should go back, come on steps.

0:15:38	SPEAKER_04
 OK, come in, sit down.

0:15:39	SPEAKER_04
 Grab yourself a microphone.

0:15:43	SPEAKER_03
 You need to sign some stuff.

0:15:44	SPEAKER_04
 Well, you can sign afterwards.

0:15:46	SPEAKER_02
 Yeah, absolutely.

0:15:47	SPEAKER_02
 I'll have to read some digits afterwards.

0:15:49	SPEAKER_03
 There are two.

0:16:03	SPEAKER_00
 Maybe small?

0:16:04	SPEAKER_00
 OK, I see.

0:16:05	SPEAKER_00
 OK.

0:16:08	SPEAKER_04
 Thank you.

0:16:10	SPEAKER_04
 OK, that was our idea.

0:16:11	SPEAKER_04
 It also has to be switched on.

0:16:13	SPEAKER_04
 No, it's already on it.

0:16:14	SPEAKER_04
 That's all.

0:16:14	SPEAKER_03
 OK, good.

0:16:15	SPEAKER_03
 Thank you.

0:16:16	SPEAKER_03
 OK.

0:16:17	SPEAKER_03
 There was a year people, when they want to go to a building, sometimes they just want to look at it.

0:16:23	SPEAKER_03
 Sometimes they want to enter it.

0:16:25	SPEAKER_03
 Sometimes they want to get really close to it.

0:16:28	SPEAKER_03
 That's something we found.

0:16:29	SPEAKER_03
 It's just a truth.

0:16:31	SPEAKER_03
 And the place is where you will lead them for these intentions is sometimes incredibly different.

0:16:37	SPEAKER_03
 I gave an example where at the point where you end up, if you want to look at it, it's completely different.

0:16:41	SPEAKER_03
 You want to enter it.

0:16:43	SPEAKER_03
 So this is sort of how people may phrase those requests to a mock-up system at least, the way they did it.

0:16:51	SPEAKER_03
 And we get tons of these.

0:16:54	SPEAKER_03
 How do I get to?

0:16:55	SPEAKER_03
 I want to go to.

0:16:56	SPEAKER_03
 But also, give me directions to, and I would like to see.

0:17:00	SPEAKER_03
 And what we can sort of do if we look closer at the data that was the wrong one, we can look at some factors that may make a difference.

0:17:12	SPEAKER_03
 First of all, very important.

0:17:14	SPEAKER_03
 And that I've completely forgot that when we talked.

0:17:17	SPEAKER_03
 This is of course a crucial factor.

0:17:19	SPEAKER_03
 What type of object is it?

0:17:21	SPEAKER_03
 So some buildings you just don't want to take pictures of or very rarely, but you usually want to enter them.

0:17:30	SPEAKER_03
 Some objects are more picturesque, more highly photographed.

0:17:35	SPEAKER_03
 Then of course, the actual phrases may give us some idea of what the person wants.

0:17:43	SPEAKER_03
 Sometimes I found in looking at the data in a superficial way I found some sort of modifiers that may also give us hint.

0:17:52	SPEAKER_03
 I'm trying to get to a need to get to sort of instead of the fact that you're not really sightseeing and just for their full pleasure and so forth.

0:18:02	SPEAKER_03
 And this is straight to the context, which ought to should be considered.

0:18:07	SPEAKER_03
 That whatever it is you're doing at the moment may also influence the interpretation of a phrase.

0:18:13	SPEAKER_03
 So this is really my suggestion.

0:18:17	SPEAKER_03
 It's really simple.

0:18:19	SPEAKER_03
 We start with now, let me say one more thing.

0:18:25	SPEAKER_03
 What we do know is that the parser reuse in the smart group system will never differentiate between any of these.

0:18:31	SPEAKER_03
 So basically all of these things will result in the same XMLM3L structure, sort of action go and then an object and a source.

0:18:43	SPEAKER_03
 So it's way to capture those differences in intentions.

0:18:49	SPEAKER_03
 So I thought maybe for a deep understanding task, that's a nice sort of playground or first little thing we can start it and sort of look, OK, we need, we're going to get those M3L structures, the crude, undifferentiated parsed, interpreted input.

0:19:06	SPEAKER_03
 We may need additional part of speech or maybe just some information on the verb and water fires, auxiliaries, we'll see.

0:19:17	SPEAKER_03
 And I will try to sort of come up with a list of factors that we need to get out of there.

0:19:21	SPEAKER_03
 And maybe we want to get a switch for the context.

0:19:25	SPEAKER_03
 So this is not something we can actually monitor now, but just something we can set.

0:19:33	SPEAKER_03
 And then you can all imagine sort of a constraint, satisfaction program, depending on what comes out.

0:19:41	SPEAKER_03
 We want to have a structure resulting if we repeat it through belief net or something along those lines.

0:19:49	SPEAKER_03
 We get an inferred intention.

0:19:50	SPEAKER_03
 We produce a structure that differentiates between the vista of the enter and the tango mode, which I think we may want to ignore, but that's my idea.

0:20:03	SPEAKER_03
 It's up for discussion.

0:20:04	SPEAKER_03
 We can change all of it any bit of it.

0:20:08	SPEAKER_03
 Go it all the way.

0:20:11	SPEAKER_01
 Now, this email you sent actually.

0:20:13	SPEAKER_01
 What?

0:20:13	SPEAKER_01
 Now I remember the email.

0:20:14	SPEAKER_02
 OK.

0:20:16	SPEAKER_02
 Huh.

0:20:16	SPEAKER_02
 Still, I have no recollection whatsoever of the email.

0:20:20	SPEAKER_02
 I'll have to go back and check.

0:20:21	SPEAKER_04
 Not important.

0:20:23	SPEAKER_04
 So what is important is that we understand what the proposed task is.

0:20:32	SPEAKER_04
 And Robert and I talked about this some on Friday.

0:20:36	SPEAKER_04
 And we think it's well-formed.

0:20:40	SPEAKER_04
 So we think it's a well-formed starter task for this deep understanding in the tourist domain.

0:20:52	SPEAKER_01
 So where exactly is the deep understanding being done?

0:20:55	SPEAKER_01
 Like, I mean, is it before the base net?

0:20:57	SPEAKER_01
 Is it?

0:20:57	SPEAKER_04
 Well, it's always all of it.

0:21:00	SPEAKER_04
 So in general, it's always going to be the answer is everywhere.

0:21:05	SPEAKER_04
 So the notion is that this isn't real deep, but it's deep enough that you can distinguish between these three quite different kinds of going to see some tourist thing.

0:21:17	SPEAKER_04
 And so that's the quote deep that we're trying to get at.

0:21:22	SPEAKER_04
 And Robert's point is that the current front end doesn't give you any way to not only doesn't it do it, but it also doesn't give you enough information to do it.

0:21:34	SPEAKER_04
 It isn't like if you just took what the front end gives you and use some clever inference algorithm on it, you would be able to figure out which of these is going on.

0:21:44	SPEAKER_04
 So in general, it's going to be true of any kind of deep understanding.

0:21:49	SPEAKER_04
 There's going to be contextual things.

0:21:50	SPEAKER_04
 There's going to be linguistic things.

0:21:51	SPEAKER_04
 There's going to be discourse things.

0:21:54	SPEAKER_04
 And they've got to be combined.

0:21:56	SPEAKER_04
 And my idea on how to combine them is with a belief net.

0:22:00	SPEAKER_04
 Well, it may turn out that some totally different thing is going to work better.

0:22:05	SPEAKER_04
 The idea would be that you take your editing your slide.

0:22:15	SPEAKER_03
 Yeah.

0:22:16	SPEAKER_03
 So as I get ideas, so discourse, I thought about the course that needs to be.

0:22:25	SPEAKER_03
 I'm sorry.

0:22:25	SPEAKER_04
 OK.

0:22:26	SPEAKER_04
 So this is taking minutes as we go in his own way.

0:22:31	SPEAKER_04
 But anyway, so the thing is, naively speaking, you've got for this little task a belief net, which is going to have as output the conditional probability of one of three things that the person wants to view it, to enter it, or to tango with it.

0:22:54	SPEAKER_04
 So the output of the belief net is pretty well formed.

0:22:58	SPEAKER_04
 And then the inputs are going to be these kinds of things.

0:23:05	SPEAKER_04
 And then the question is, the two questions is, one, where do you get this information from?

0:23:10	SPEAKER_04
 And two, what's the structure of the belief net?

0:23:12	SPEAKER_04
 So what are the conditional probabilities of this, that, and the other, given these things?

0:23:17	SPEAKER_04
 And you probably need intermediate notes.

0:23:19	SPEAKER_04
 I don't know what they are yet.

0:23:21	SPEAKER_04
 So it may well be that, for example, that knowing whether, oh, another thing you want is some information about the time of day.

0:23:37	SPEAKER_04
 Now, let me want to call that part of context.

0:23:39	SPEAKER_04
 But the time of day matters a lot.

0:23:42	SPEAKER_04
 And if things are obviously closed, then people don't want to enter them.

0:23:50	SPEAKER_04
 And if it's not obvious, you may want to actually point out the people that it's closed, what they're going to is closed, and they don't have the option of entering it.

0:24:02	SPEAKER_04
 So another thing that can come up, and will come up as soon as you get serious about this, is that another option, of course, is to have it more of a dialogue.

0:24:12	SPEAKER_04
 So if someone says something, you could ask them.

0:24:16	SPEAKER_04
 And now, one thing you could do is always ask them.

0:24:18	SPEAKER_04
 That's boring.

0:24:20	SPEAKER_04
 And it will also be a pain for the person using it.

0:24:23	SPEAKER_04
 So one thing you could do is build a little system that say, whenever you've got a question like that, I've got one of three answers, ask them which one you want.

0:24:31	SPEAKER_04
 But that's not what we're going to do.

0:24:34	SPEAKER_03
 But maybe that's a false state of the system that it's too close to call.

0:24:38	SPEAKER_04
 Oh, yeah.

0:24:39	SPEAKER_04
 You want the ability to ask.

0:24:40	SPEAKER_04
 You want the ability to ask.

0:24:42	SPEAKER_04
 But what you don't want to do is build a system that always asks every time.

0:24:46	SPEAKER_04
 And that's not getting out the scientific problem.

0:24:49	SPEAKER_04
 And in general, it's going to be much more complex than that.

0:24:55	SPEAKER_04
 This is purposely a really simple case.

0:24:58	SPEAKER_03
 So I have one more point to the bus question.

0:25:03	SPEAKER_03
 I think also the deep understanding part of it is going to be in the extent that we wanted in terms of modeling.

0:25:12	SPEAKER_03
 We can start, you know, basic from human beings, model that, it's motions, going, walking, seeing.

0:25:18	SPEAKER_03
 We can model all of that and then compose whatever inferences we make out of these really conceptual primitives.

0:25:25	SPEAKER_03
 That will be extremely deep in my understanding.

0:25:28	SPEAKER_04
 So the way that might come up, if you want to do that, you might say, as an intermediate step in your belief net, is there a source path goal schema involved?

0:25:41	SPEAKER_04
 And if so, is there a focus on the goal, or is there a focus on the path or something?

0:25:48	SPEAKER_04
 And that could be one of the condition, you know, in some piece of the belief net, that could be the appropriate thing to enter.

0:26:00	SPEAKER_01
 So when would we extract that information from the M3L?

0:26:03	SPEAKER_04
 No.

0:26:04	SPEAKER_04
 No.

0:26:04	SPEAKER_04
 So the M3L is not going to give you, what he was saying is, the M3L does not have any of that.

0:26:09	SPEAKER_04
 All it has is some really crude stuff saying, person wants to go to a place.

0:26:13	SPEAKER_02
 The M3L is the old smart com.

0:26:16	SPEAKER_04
 Right.

0:26:17	SPEAKER_04
 Well, M3L itself refers to modeling and markup language.

0:26:20	SPEAKER_04
 So we have to have a better way of referring to the parts that we're going to put.

0:26:27	SPEAKER_03
 Yeah, that implies speech.

0:26:29	SPEAKER_03
 I think you can actually actually intentionally let us, is what we're going to do.

0:26:35	SPEAKER_04
 But they call it intention lattice.

0:26:36	SPEAKER_03
 But in the intention lattice, they call it intention hypotheses.

0:26:42	SPEAKER_04
 So they're going to give us some, or we can assume that you get this crude information about intention.

0:26:51	SPEAKER_04
 And that's all they're going to provide.

0:26:54	SPEAKER_04
 And they don't give you the kind of object.

0:26:56	SPEAKER_04
 They don't give you any discourse history.

0:26:59	SPEAKER_04
 If you want to keep that, you have to keep it somewhere else.

0:27:04	SPEAKER_03
 Well, they keep it where they have to request it.

0:27:06	SPEAKER_03
 Right.

0:27:07	SPEAKER_04
 Well, they keep it by their lights.

0:27:10	SPEAKER_04
 It may or may not be what we want.

0:27:14	SPEAKER_02
 So if someone says I want to touch the side of the powder tower, that would basically we need to pop up Tango mode in the direction.

0:27:25	SPEAKER_02
 If you've got a simple as that, yeah.

0:27:28	SPEAKER_04
 But it wouldn't.

0:27:31	SPEAKER_02
 But that isn't necessarily, we have to infer a source path goal to something really good, touching the side, right?

0:27:40	SPEAKER_03
 There is a point there if I understand you correct me.

0:27:46	SPEAKER_03
 Because sometimes people just say, you find very often, where is the city hall?

0:27:56	SPEAKER_03
 And they don't want to see it on a map, or they don't want to know it's 500 yards away from you, or it's to the north.

0:28:03	SPEAKER_03
 They want to go there.

0:28:05	SPEAKER_03
 That's what they say is, where is it?

0:28:07	SPEAKER_02
 Where is it, that thing?

0:28:08	SPEAKER_02
 And the parser would output.

0:28:11	SPEAKER_03
 Well, that's a question mark.

0:28:13	SPEAKER_03
 A lot of parses just way beyond their scope of interpreting that.

0:28:22	SPEAKER_03
 But still outcome, the outcome will be some form of structure with a town hall, and maybe it's a WH focus on the town hall.

0:28:33	SPEAKER_03
 But to interpret it, somebody else has to do that later.

0:28:38	SPEAKER_02
 I'm trying to figure out what this mark is just without, but depending on these things.

0:28:43	SPEAKER_03
 It will probably tell you how far away it is.

0:28:46	SPEAKER_03
 That's even what you've mapped as it has to how far away it is and shows it to your own map.

0:28:53	SPEAKER_03
 Because we cannot differentiate at the moment between the intention of wanting to go there, or the intention of just knowing or wanting to know where it is.

0:29:02	SPEAKER_00
 People might not be able to infer that either.

0:29:06	SPEAKER_00
 I could imagine if someone came up to me and asked where is the city hall, are you trying to get there?

0:29:11	SPEAKER_00
 Because how I describe this location, probably kind of whether I should give them directions now or say whatever, a half a mile away or something like that.

0:29:21	SPEAKER_03
 granularity factor because we're people ask you.

0:29:24	SPEAKER_03
 Where is New York?

0:29:25	SPEAKER_03
 We'll tell them it's on the East Coast.

0:29:27	SPEAKER_03
 You won't tell them how to get there.

0:29:29	SPEAKER_03
 Check that bus to the airport in blah, blah, blah.

0:29:31	SPEAKER_03
 But if it's the post office, we will tell them how to get there.

0:29:35	SPEAKER_03
 So they have done some interesting experiments of that in Hamburg.

0:29:40	SPEAKER_04
 But go back to the, yeah,

0:29:45	SPEAKER_03
 this is on tour is knowledge about buildings, they're opening times, and then coupled with time of day.

0:29:55	SPEAKER_00
 So that context was like their presumed purpose context, like business or travel, as well as the utterance context.

0:30:03	SPEAKER_00
 Like I'm now standing at this point.

0:30:05	SPEAKER_04
 Yeah, I think we have all along.

0:30:08	SPEAKER_04
 We've been distinguishing between situational context, which is what you have is context in discourse context, which you have is D H, I don't know what the H means.

0:30:17	SPEAKER_04
 History, discourse.

0:30:19	SPEAKER_04
 OK, whatever.

0:30:20	SPEAKER_04
 So we can work out terminology later.

0:30:22	SPEAKER_04
 So they're quite distinct.

0:30:25	SPEAKER_04
 I mean, you need them both, but they're quite distinct.

0:30:27	SPEAKER_04
 And so what we're talking about doing as a first shot is not doing any of the linguistics except to find out what seems to be useful.

0:30:42	SPEAKER_04
 So the reason the belief net is in blue is the notion would be, this may be a bad idea, but the idea is to take us a first goal, see if we could actually build a belief net that would make this three-way distinction in a plausible way, given all these transcripts, and we're able to by hand extract the features to put into belief net, saying, here are the things which if you could get them out of the language and discourse and put them into the belief net, it would tell you which of these three intentions is most likely.

0:31:23	SPEAKER_04
 And to actually do that, build it, run it on the data where you hand transcribe the parameters and see how that goes.

0:31:34	SPEAKER_04
 If that goes well, then we can start worrying about how we would extract them.

0:31:41	SPEAKER_04
 So where would you get this information and expand it to other things like this?

0:31:47	SPEAKER_04
 But if we can't do that, then we're in trouble.

0:31:52	SPEAKER_03
 And if you can't do this task, we need a different engine.

0:31:59	SPEAKER_04
 Or something.

0:32:00	SPEAKER_04
 Well, if it's the belief net, we'll switch to logic or some terrible thing.

0:32:05	SPEAKER_04
 But I don't think that's going to be the case.

0:32:07	SPEAKER_04
 I think that if we can get the information, belief net is a perfectly good way of doing the inferential combination of it.

0:32:19	SPEAKER_04
 The real issue is what are the factors involved in determining this?

0:32:32	SPEAKER_04
 How many seconds?

0:32:33	SPEAKER_04
 So I know.

0:32:34	SPEAKER_04
 Is it clear what's going on here?

0:32:45	SPEAKER_00
 I missed the beginning.

0:32:46	SPEAKER_00
 But I guess could you go back to the slide, the previous one?

0:32:50	SPEAKER_00
 So is it that it's these are all factors that, these are the ones that you said that we are going to ignore now or that we want to take into account.

0:33:02	SPEAKER_00
 You're saying you're taking them into account.

0:33:04	SPEAKER_04
 But you don't worry about how to extract them.

0:33:07	SPEAKER_04
 So let's find out which ones we need first.

0:33:11	SPEAKER_00
 OK.

0:33:11	SPEAKER_00
 And it's clear from the data, the correct answer in each case.

0:33:16	SPEAKER_04
 But let's go back to the slide of data.

0:33:21	SPEAKER_00
 Like do we know from the data?

0:33:23	SPEAKER_00
 Which?

0:33:24	SPEAKER_00
 OK.

0:33:25	SPEAKER_00
 Not from that data.

0:33:26	SPEAKER_03
 But since we are designing, compared to this bigger data collection effort, we will definitely take care to put it in the form of the other.

0:33:39	SPEAKER_03
 To see whether we can get sort of empirically validated data.

0:33:44	SPEAKER_03
 From this, we can sometimes, you know, and that's not what we need for leave that anyhow.

0:33:49	SPEAKER_03
 It's sort of sometimes what people want to see.

0:33:52	SPEAKER_03
 They phrase it more like this.

0:33:54	SPEAKER_03
 But it doesn't exclude anybody from praising it.

0:33:56	SPEAKER_03
 Totally different.

0:33:57	SPEAKER_03
 They still.

0:33:58	SPEAKER_03
 But then other factors may come into play, the changey outcome of the leave that.

0:34:02	SPEAKER_03
 So this is exactly what, because you can never be sure.

0:34:06	SPEAKER_03
 And I'm sure even the most deliberate data collection experiment will never give you data that say, well, if it's phrased like that, be intentional.

0:34:16	SPEAKER_00
 I mean, the only way you could get those if you were to give the subjects a task, you're a current goal is to.

0:34:24	SPEAKER_00
 Yeah, that's what we're doing.

0:34:25	SPEAKER_03
 So that's what you want.

0:34:26	SPEAKER_03
 We will still get the phrasing all over the place.

0:34:28	SPEAKER_03
 I'm sure that.

0:34:31	SPEAKER_00
 No, that's fine.

0:34:31	SPEAKER_00
 I guess it's just knowing the intention from.

0:34:34	SPEAKER_00
 From that task, that's the experiment.

0:34:35	SPEAKER_04
 So I think you all know this.

0:34:37	SPEAKER_04
 But we are going to actually use this little room and start recording subjects, probably within a month or something.

0:34:44	SPEAKER_04
 So this is not any, and you guys worry, except that we may want to push that effort to get information we need.

0:34:55	SPEAKER_04
 So our job is to figure out how to solve these problems if it turns out that we need data of a certain sort, then the sort of data collection branch can be asked to do that.

0:35:08	SPEAKER_04
 And one of the reasons why we're recording the meeting for these guys is because we want their help when we start doing recording of subjects.

0:35:19	SPEAKER_04
 So yeah, you're absolutely right, though.

0:35:20	SPEAKER_04
 No, you will not have.

0:35:22	SPEAKER_04
 And there it is.

0:35:23	SPEAKER_04
 And the.

0:35:34	SPEAKER_00
 And I think the other concern that has come up before too is if it's, I don't know if this was collected.

0:35:41	SPEAKER_00
 What situation the data was collected?

0:35:42	SPEAKER_00
 It was the one that you showed in your talk.

0:35:45	SPEAKER_00
 Like people.

0:35:45	SPEAKER_00
 No, no.

0:35:46	SPEAKER_00
 OK.

0:35:46	SPEAKER_00
 So it was just like someone actually mobile, using a device?

0:35:54	SPEAKER_03
 No, not.

0:35:55	SPEAKER_03
 It was mobile, but not really with the system.

0:35:59	SPEAKER_00
 So there were never answers.

0:36:00	SPEAKER_00
 OK.

0:36:01	SPEAKER_00
 OK.

0:36:02	SPEAKER_00
 But it was it.

0:36:03	SPEAKER_00
 I guess I don't know the situation of collecting the data.

0:36:06	SPEAKER_00
 Here, you can imagine them being walking around the city.

0:36:09	SPEAKER_00
 It's like one situation.

0:36:10	SPEAKER_00
 And then you have all sorts of other situational context factors that would influence how to interpret, like you said, the scope and things like that.

0:36:18	SPEAKER_00
 If they're doing it in a, you know, I'm just going to hear with a map and ask questions.

0:36:22	SPEAKER_00
 I would imagine that the data would be really different.

0:36:25	SPEAKER_00
 So it's just.

0:36:27	SPEAKER_03
 Yeah.

0:36:28	SPEAKER_03
 But it was never the goal of that data connection to surface at a certain purpose.

0:36:34	SPEAKER_03
 So that's why, for example, the tasks were not differentiated by attentionality.

0:36:38	SPEAKER_03
 There was no label, you know, attention A, attention B, attention C, or task ABC.

0:36:46	SPEAKER_03
 I'm sure we can produce some if we needed that will help us in other times.

0:36:51	SPEAKER_03
 But you've got to leave something for other people to model, so to finding out what the context of the situation really is.

0:37:02	SPEAKER_03
 It's an interesting thing.

0:37:06	SPEAKER_03
 So if I'm at the moment curious and I want to approach it from the end where we can sort of start with this toy system that we can play around with.

0:37:15	SPEAKER_03
 So that we get a clearer notion of what input we need for that, what suffices and what doesn't.

0:37:21	SPEAKER_03
 And then we can start worrying about where to get this input.

0:37:25	SPEAKER_03
 What do we need, you know, ultimately, once we are all experts in changing that parser, for example, maybe there's just a couple of really things we need to do.

0:37:33	SPEAKER_03
 And then we get more whatever part of speech, more construction type, like stuff out of it.

0:37:42	SPEAKER_03
 Procmedic approach for the moment.

0:37:46	SPEAKER_02
 How exactly does the data collection, do they have a map?

0:37:49	SPEAKER_02
 And then you give them a scenario of some sort?

0:37:52	SPEAKER_03
 OK.

0:37:54	SPEAKER_03
 You mentioned you're the subject you're going to be in here.

0:37:56	SPEAKER_03
 And you see either the 3D model or a quick time animation of standing in a square in Heidelberg.

0:38:07	SPEAKER_03
 So you actually see that.

0:38:10	SPEAKER_03
 The first thing is you have to read a text about Heidelberg.

0:38:15	SPEAKER_03
 So just offer textbook, the tourist guide, to familiarize your sample, the sort of all-sound in German street names like Lerkerkasse.

0:38:24	SPEAKER_03
 So that's part one.

0:38:25	SPEAKER_03
 Part two is you're told that this new, wonderful computer system exists.

0:38:31	SPEAKER_03
 You can tell you everything you want to know.

0:38:33	SPEAKER_03
 And it understands your completely.

0:38:35	SPEAKER_03
 And so you're going to pick up that phone dialer number, and you get a certain amount of tasks that you have to solve.

0:38:40	SPEAKER_03
 First you have to know, find out how to get to that place.

0:38:43	SPEAKER_03
 Maybe with the intention of buying stamps in there.

0:38:46	SPEAKER_03
 Maybe some next task is to get to a certain place, take a picture for your grandchild.

0:38:51	SPEAKER_03
 The third one is to get inspiration

0:38:54	None
 of the history of the object. The fourth one, and then the system breaks down of crashes.

0:39:01	SPEAKER_03
 At the third, right then?

0:39:03	None
 After the third task.

0:39:05	SPEAKER_03
 And then, or after the fourth, sometimes you give that file.

0:39:08	SPEAKER_03
 And then a human operator comes on and apologizes that the system has crashed.

0:39:14	SPEAKER_03
 Or just you to continue.

0:39:16	SPEAKER_03
 Now we're the human operator.

0:39:18	SPEAKER_03
 And so you have basically the same tasks again, just with different objects.

0:39:23	SPEAKER_03
 And you go through it again, and that was it.

0:39:26	None
 Oh, and one little bit.

0:39:29	None
 The computer, you've been told the computer system knows exactly where you are via GPS.

0:39:34	SPEAKER_03
 When the human operator comes on, that person does not know.

0:39:39	SPEAKER_03
 So the GPS is crashed as well.

0:39:41	SPEAKER_03
 So the person first has to ask you, where are you?

0:39:44	SPEAKER_03
 And so you have to do some tell the person sort of where you are, depending on what you see there.

0:39:50	SPEAKER_03
 This is a bit that I don't think we did.

0:39:53	SPEAKER_03
 We discussed that bit.

0:39:55	None
 Just sort of squeezed that in now.

0:39:57	None
 But it's something that we would provide some very interesting data for some people, I know.

0:40:02	SPEAKER_03
 So.

0:40:03	SPEAKER_00
 So in the display, you said you might have a display that shows.

0:40:09	SPEAKER_03
 Yeah.

0:40:10	SPEAKER_03
 Additionally, you have a sort of a map type.

0:40:12	SPEAKER_00
 Your perspective.

0:40:13	SPEAKER_00
 And so as you, OK, so as you move through it, that's they just track it on the for themselves.

0:40:20	SPEAKER_03
 You don't, I don't know.

0:40:23	SPEAKER_03
 I don't think you really move.

0:40:25	SPEAKER_03
 OK.

0:40:26	SPEAKER_03
 Yeah.

0:40:26	SPEAKER_03
 I mean, that would be an enormous technical effort.

0:40:30	SPEAKER_03
 Unless we can show it, walks through, we can have movies of walking, you walking through Hydeburg and ultimately arriving there, maybe we want to do that.

0:40:42	SPEAKER_00
 I was just trying to figure out how.

0:40:44	SPEAKER_03
 The map was sort of a bit.

0:40:45	SPEAKER_03
 You want to go to that place.

0:40:48	SPEAKER_03
 And then sort of there, you see the label of the name.

0:40:52	SPEAKER_03
 So we get those labels, pronunciation stuff.

0:40:54	SPEAKER_03
 And so we can change that.

0:40:57	SPEAKER_00
 So your tasks don't require you to, I mean, you're told.

0:41:02	SPEAKER_00
 So when your task is, I don't know, a goodbye stamp or something like that.

0:41:05	SPEAKER_00
 So do you have to respond or do you, what are you supposed to be telling the system?

0:41:12	SPEAKER_00
 What you're doing now or?

0:41:15	SPEAKER_00
 Well, we'll see what people do.

0:41:16	SPEAKER_00
 OK.

0:41:16	SPEAKER_00
 So it's just like this figure out what they would say.

0:41:19	SPEAKER_00
 Yeah.

0:41:20	SPEAKER_03
 And we will record both sides.

0:41:21	SPEAKER_03
 I mean, we'll record the wizard.

0:41:23	SPEAKER_03
 In both cases, it's going to be human and the future and the operating limit.

0:41:28	SPEAKER_03
 And there will be some dialogue.

0:41:31	SPEAKER_03
 So you first have to listen to that and see what they say.

0:41:36	SPEAKER_03
 We can instruct the wizard and how expressive and talkative should be.

0:41:41	SPEAKER_03
 But maybe what you're suggesting is what you're suggesting is it might be to pour the data if you sort of limit it to this ping pong one.

0:41:51	SPEAKER_03
 Your task results in a question.

0:41:53	SPEAKER_03
 And then there's an answer.

0:41:54	SPEAKER_03
 And that's the end of the task.

0:41:55	SPEAKER_03
 Do you want to have it more steps sort of?

0:41:58	SPEAKER_00
 I don't know how much direction is given to the subject about what their interaction.

0:42:03	SPEAKER_00
 I mean, they're unfamiliar with interacting with the system.

0:42:06	SPEAKER_00
 All they know is it's this great system that can do stuff.

0:42:09	SPEAKER_04
 So some extent, this is a different discussion.

0:42:12	SPEAKER_04
 OK.

0:42:13	SPEAKER_04
 So we have to have this discussion of the experiment, the data collection, and all that sort of stuff.

0:42:18	SPEAKER_04
 And we do have a student who is a candidate for wizard.

0:42:27	SPEAKER_04
 She's going to get in touch with me.

0:42:28	SPEAKER_04
 It's a student of Eves, F-E-Y-F-A spelled F-E-Y-D.

0:42:33	SPEAKER_04
 Oh, PayPal.

0:42:34	SPEAKER_04
 You know her?

0:42:36	SPEAKER_04
 OK.

0:42:37	SPEAKER_00
 She started taking the class last year and then didn't continue.

0:42:42	SPEAKER_00
 She's a graduate.

0:42:43	SPEAKER_00
 She's a graduate.

0:42:44	SPEAKER_00
 Yeah, she is a good.

0:42:45	SPEAKER_00
 OK.

0:42:46	SPEAKER_00
 Yeah.

0:42:46	SPEAKER_00
 I know her very, very briefly.

0:42:48	SPEAKER_00
 I know she's interested in.

0:42:51	SPEAKER_00
 OK.

0:42:51	SPEAKER_04
 So anyways, she's looking for some more part-time work while she's waiting, actually, for graduate school.

0:42:57	SPEAKER_04
 And she'll be in touch.

0:42:58	SPEAKER_04
 So we may have someone to do this.

0:43:02	SPEAKER_04
 And she's got some background in all this stuff, and there's a linguist.

0:43:07	SPEAKER_04
 So Nancy, at some point, we'll have another discussion on exactly how that's going to go.

0:43:18	SPEAKER_04
 And Jane, but also Liz, have offered to help us do this data collection and design and stuff.

0:43:29	SPEAKER_04
 So when we get to that, we'll have some people doing it to know what to do.

0:43:34	SPEAKER_00
 I guess the reason I was asking the details of this kind of thing is that it's one thing to collect data for.

0:43:41	SPEAKER_00
 And speech recognition are various other tests that have pretty clear correct answers.

0:43:46	SPEAKER_00
 But with intention, obviously, as you point out, there's a lot of other factors.

0:43:50	SPEAKER_00
 I'm not really sure how the question of how to make it a appropriate toy version of that.

0:43:58	SPEAKER_00
 It's just hard.

0:43:59	SPEAKER_00
 So I mean, I guess that was my question.

0:44:01	SPEAKER_02
 Is the intention implicit in the scenario that's given?

0:44:07	SPEAKER_00
 It is if they have these tasks that they're supposed to.

0:44:09	SPEAKER_02
 Yeah, it just wasn't sure what level of detail it has.

0:44:12	SPEAKER_03
 Yeah.

0:44:13	SPEAKER_03
 Right.

0:44:14	SPEAKER_03
 No one is a moment.

0:44:16	SPEAKER_04
 So that's part of what we'll have to figure out.

0:44:19	SPEAKER_04
 But the problem that I was going to try to focus on today was let's suppose by magic you could collect dialogues in which one way or the other, you were able to figure out both the intention and set the context and know what language was used.

0:44:40	SPEAKER_04
 So let's suppose that we can get that kind of data.

0:44:45	SPEAKER_04
 The issue is can we find a way to basically featureize it so that we get some discrete number of features so that when we know the values to all those features or as many as possible, we can come up with the best estimate of which of the, in this case, three little intentions are most likely.

0:45:09	SPEAKER_00
 But where did the three intentions go there to see it?

0:45:12	SPEAKER_00
 And to come this close to?

0:45:14	SPEAKER_04
 The terminology we're using is to go back.

0:45:18	SPEAKER_04
 To view it, okay, to enter it.

0:45:23	SPEAKER_04
 Now, it seems to me, you have no trouble with those being distinct.

0:45:26	SPEAKER_04
 Take a picture of it.

0:45:28	SPEAKER_04
 You might well want to be really rather different place than entering it.

0:45:32	SPEAKER_04
 And for objects, it's at all big, sort of getting to the nearest part of it could be quite different than either of those.

0:45:41	SPEAKER_00
 Just sort of.

0:45:42	SPEAKER_00
 Okay, so now I understand the referendum.

0:45:44	SPEAKER_00
 Tango mode.

0:45:45	SPEAKER_02
 So I would have thought it was more of a wall.

0:45:48	SPEAKER_02
 To wall suits?

0:45:50	SPEAKER_00
 Yeah, like how close are you?

0:45:52	SPEAKER_00
 Yeah, because it's really close.

0:45:54	SPEAKER_00
 Yeah, well anyway.

0:45:56	SPEAKER_01
 So, so like what features do you want to try to extract from either parts or whatever?

0:46:02	SPEAKER_01
 Like the presence of a word or the presence of a certain stem?

0:46:05	SPEAKER_04
 Right.

0:46:07	SPEAKER_04
 Is there a construction or the kind of object or anything else that's in the, it's either in the discourse itself or in the context.

0:46:18	SPEAKER_04
 So if it turns out that whatever it is, you want to know whether the person's a tourist or not.

0:46:24	SPEAKER_04
 Okay, that becomes a feature.

0:46:25	SPEAKER_04
 And then how you determine that is another issue.

0:46:28	SPEAKER_04
 But for the current problem, it would just be, okay, if you can be sure that it's a tourist versus a businessman versus a native or something, that would give you a lot of discriminatory power and then you just have a little section in your belief net that said, though it's in the short run, you'd set them and see how it worked.

0:46:48	SPEAKER_04
 And then in the longer run, you would figure out how you could derive them from previous discourse or anything else you knew.

0:47:09	SPEAKER_01
 So what's the, like how should we go about it?

0:47:12	SPEAKER_04
 Okay, so first of all, do we, either of you guys, you got a favorite belief net that you've played with Java Bayes or something?

0:47:22	SPEAKER_01
 No, I'm getting it.

0:47:25	SPEAKER_04
 Okay, anyway, get one.

0:47:27	SPEAKER_04
 Okay, so one of the things we want to do is actually pick a package, it doesn't matter which one.

0:47:34	SPEAKER_04
 Presumably one that's got good interactive abilities because a lot of what we're gonna be, we don't need the one that'll solve massive belief nets quickly, these are not gonna get big in the foreseeable future.

0:47:47	SPEAKER_04
 And we do want one in which it's easy to interact with and modify it because a lot of what it's gonna be is playing with this.

0:48:01	SPEAKER_04
 And probably one in which it's easy to have what amounts to transcript files so that if we have all these cases, okay, so we make up cases that have these features, okay, and then you'd like to be able to say, okay, here's a bunch of cases.

0:48:16	SPEAKER_04
 There are even ones that you can do learning.

0:48:20	SPEAKER_04
 Okay, so you have all their cases and their results and you have algorithms go through and run around trying to accept the probabilities for you.

0:48:29	SPEAKER_04
 Probably that's not worth it.

0:48:32	SPEAKER_04
 I mean, my guess is we aren't gonna have enough data that's good enough to make these data fitting ones worth it that I don't know.

0:48:40	SPEAKER_04
 So I would say the first task for you too, guys, is to pick a package.

0:48:47	SPEAKER_04
 You know, the standard thing, you want it stable, you want it.

0:48:51	SPEAKER_04
 And as soon as we have one, we can start trying to make a first cut at what's going on.

0:49:02	SPEAKER_04
 But what I like about it is it's very concrete.

0:49:05	SPEAKER_04
 We know what the outcomes are gonna be and we have some data that's loose.

0:49:09	SPEAKER_04
 We can use our own intuition and see how hard it is.

0:49:14	SPEAKER_04
 And importantly, what intermediate notes we think we need.

0:49:18	SPEAKER_04
 So it turns out that just thinking about the problem you call things that you really need to, this is the kind of thing that is an intermediate little piece in your belief net.

0:49:28	SPEAKER_04
 That'd be really interesting.

0:49:33	SPEAKER_03
 It may self as a platform for a person, maybe me or whoever who is interested in doing some linguistic analysis, we have a whole frame that grew up here.

0:49:45	SPEAKER_03
 We can see what they have found out about those concepts already that are contained in the data.

0:49:52	SPEAKER_03
 You know, to come up with a nice set of features and maybe even means abstracting them.

0:49:59	SPEAKER_03
 And that altogether could also be a common nice paper that's going to be published somewhere between the two of them, right?

0:50:06	SPEAKER_03
 And when you said Java-based belief net, you were talking about when said Rano Coffee or that are in programming.

0:50:13	SPEAKER_04
 No, it turns out that there is a, the no end of Java libraries.

0:50:19	SPEAKER_04
 Okay, and it turns out one called Java-based, which is one that people around here use a fair amount.

0:50:25	SPEAKER_04
 I have no idea whether that's, the obvious advantage of that is that you can then relatively easily get all the other Java packages for GUIs or whatever else you might want to do.

0:50:37	SPEAKER_04
 So that's, I think, why a lot of people are doing research use that.

0:50:42	SPEAKER_04
 But that may not be, I have no idea whether that's the best choice.

0:50:45	SPEAKER_04
 And there are plenty of people around students in the department who live and breathe based nets.

0:50:53	SPEAKER_04
 So.

0:50:54	SPEAKER_00
 There's the toolkit that Kevin Murphy has developed which might be useful to you.

0:50:59	SPEAKER_04
 So yeah, Kevin would be a good person to start with.

0:51:03	SPEAKER_04
 Nancy knows them well.

0:51:04	SPEAKER_04
 I don't know whether you guys have met Kevin yet or not, but yeah, this is really probably a pretty sure

0:51:09	None
 that the, for example, this, the dialogue history is producing XML documents everywhere.

0:51:18	SPEAKER_03
 Of course, XML and the ontology that a student is constructing for me back in, in email is in oil.

0:51:28	SPEAKER_03
 It's also an XML.

0:51:29	SPEAKER_03
 So that's where a lot of knowledge about bakery is about what else about the process and stuff that is going to come from.

0:51:36	SPEAKER_03
 So it has, I owe capability and it's a job of engineering and then it'd be able to.

0:51:41	SPEAKER_04
 So yeah, we're sort of committed to XML as the kind of interchange, but that's not a big deal.

0:51:48	SPEAKER_04
 So in terms of interchange in and out of any module we build, it'll be XML.

0:51:55	SPEAKER_04
 And if you're going off the queries to the ontology, for example, you'll have to deal with its interface, but that's fine.

0:52:03	SPEAKER_04
 And all of these things have been built with much bigger projects than this in mind.

0:52:12	SPEAKER_04
 So they've worked very hard.

0:52:14	SPEAKER_04
 It's kind of blackboards and multi-way blackboards and ways of interchanging and registering.

0:52:19	SPEAKER_04
 And so that, I don't think, is even worth us worrying about just yet.

0:52:26	SPEAKER_04
 I mean, if we can get the core of the thing to work in a way that we're comparable with, and we can even out of it XML little descriptors.

0:52:40	SPEAKER_03
 I believe.

0:52:41	SPEAKER_03
 I don't see that.

0:52:42	SPEAKER_03
 I like for some of the, what you said about the getting input from just Pius about where you have the data is specified for features and so forth.

0:52:51	SPEAKER_03
 That's, of course, easy also to do with.

0:52:53	SPEAKER_04
 You can make an XML format for that, sure.

0:53:01	SPEAKER_04
 You know, feature value XML format.

0:53:03	SPEAKER_04
 Probably as good a way as any.

0:53:06	SPEAKER_04
 So it's all, yet, I guess it's also worth while you're proking around for XML packages, the things you'd like.

0:53:16	SPEAKER_03
 Does my comps just have set it back?

0:53:18	SPEAKER_03
 Yeah, sure.

0:53:19	SPEAKER_03
 The library does that.

0:53:21	SPEAKER_04
 And the question is, you have to look.

0:53:24	SPEAKER_04
 That should be, we should be able to look at that.

0:53:27	SPEAKER_04
 No, the, what I sort of came to my mind

0:53:32	SPEAKER_03
 was the notion of an idea that if there are nets that can actually try to set their own probability factors based on input, just in file format, if we get really wild on this, we may actually want to use some copper that other people made.

0:53:53	SPEAKER_03
 And for example, if they are in mate, then we get XML documents with discourse annotations.

0:54:00	SPEAKER_03
 From the discourse act down to the phonetic level, Michael has a project about recognizing discourse acts and he does it all in mate.

0:54:07	SPEAKER_03
 And so they're actually annotating data and data and data.

0:54:10	SPEAKER_03
 So if we think it's worth it one of these days, not with this first prototype, maybe with a second.

0:54:16	SPEAKER_03
 And we have the possibility of taking input that's generated elsewhere and learning from that.

0:54:22	SPEAKER_03
 That'd be nice.

0:54:23	SPEAKER_04
 It'd be nice, but I don't want to come.

0:54:26	SPEAKER_04
 I mean, you can't run your project based on the speculation that the data will come and you don't have to actually design the nets.

0:54:35	SPEAKER_04
 Just a backdoor.

0:54:37	SPEAKER_04
 It could happen.

0:54:38	SPEAKER_04
 So in terms of what the smart com gives us for M3L packages, it could be that they're fine.

0:54:45	SPEAKER_04
 It could be, you know, you don't really like it.

0:54:48	SPEAKER_04
 So we're not required to use their packages.

0:54:52	SPEAKER_04
 We are required at the end to give them stuff in their format, but hey, it doesn't control what you do in internal language.

0:55:15	None
 What was the time frame for this?

0:55:17	None
 Today?

0:55:19	None
 I have to do this.

0:55:20	SPEAKER_04
 Yeah, but this week, to have you guys pick a package and tell us what it is and give us a point where you can play with it or something.

0:55:34	SPEAKER_04
 And then as soon as we have it, I think we should start trying to populate it for this problem, make a first cut, you know, what's going on.

0:55:46	SPEAKER_04
 And probably the easiest way to do that is some online way.

0:55:54	SPEAKER_04
 I mean, you can figure out what you want.

0:55:55	SPEAKER_04
 I'm going to website her.

0:55:56	None
 You know, I was actually more joking with the 2.3.

0:56:01	None
 It was a usual joke.

0:56:05	None
 It was a big as long as you guys need for that.

0:56:10	SPEAKER_03
 Maybe it might be interesting if the two of you can agree on who's going to be the speaker next, but to tell us something about it, you should know what it does

0:56:19	None
 and how it does that. Well, or about the speaker.

0:56:21	SPEAKER_03
 Yeah, or you split it up.

0:56:23	SPEAKER_03
 So there will be sort of the assignment for that.

0:56:28	SPEAKER_03
 Force lines or whatever.

0:56:30	None
 I think that what it can do and how far you can.

0:56:33	SPEAKER_04
 Well, I like the also that I have a first cut at what the bleaching at looks like.

0:56:40	SPEAKER_04
 It's really crude.

0:56:43	SPEAKER_02
 So, you know, here are your features and whatnot.

0:56:46	None
 Right.

0:56:47	None
 Yeah.

0:56:50	SPEAKER_04
 And as I said, what I like to do is, I mean, what would be really great is bring it in if we could in the meeting, say, you know, here's the package here is the current one we have, what other ideas you have.

0:57:06	SPEAKER_04
 And then we can think about this idea of making up the data file of get a tentative format for it.

0:57:15	SPEAKER_04
 Let's say XML that says, you know, these are the various scenarios we can just add that.

0:57:23	SPEAKER_04
 And then we just this file of them.

0:57:24	SPEAKER_04
 And when you think you've got a better belief in it, use run it against this data file.

0:57:34	SPEAKER_01
 So we'd be like, yeah.

0:57:35	SPEAKER_01
 Oh, yeah.

0:57:40	None
 Until we know more.

0:57:42	SPEAKER_02
 And what's the relation to this with the table so that the system works out with English?

0:57:49	SPEAKER_03
 So this is why what you are doing this, I received two lovely emails, the full and T in the whole Linux version.

0:57:56	SPEAKER_03
 There, I uploaded the most.

0:58:01	SPEAKER_03
 And I started to unpack the Linux one, the int1 work kind of has a package.

0:58:06	SPEAKER_03
 Linux, when it told me that you can't really unpack it because it's a future date.

0:58:11	None
 So this is the tentative.

0:58:12	None
 It's between Germany.

0:58:12	None
 I had to wait until one of the talk was actually a new two package.

0:58:17	SPEAKER_03
 Now, then it will be my job to get this whole thing running both on suite and on this machine.

0:58:24	SPEAKER_03
 And so that we have it.

0:58:27	SPEAKER_03
 And then hopefully that hoping that my urgent message will now come through to Ralph and Tillman that it will send some more documentation along.

0:58:38	SPEAKER_03
 We, I can show, maybe that's what I will do next Monday.

0:58:41	SPEAKER_03
 I'll show the state of the system and show that.

0:58:45	SPEAKER_03
 Yeah.

0:58:45	SPEAKER_04
 So the answer to John O. is that these are at the moment separate.

0:58:52	SPEAKER_04
 One hopes is that when we understand how the analyzer works, we can both worry about converting it to English and worry about how it could extract the parameters we need for the belief net.

0:59:06	SPEAKER_02
 I guess my question is more about time frame.

0:59:08	SPEAKER_02
 So we're going to do belief nets this week.

0:59:10	SPEAKER_02
 And then, oh, yeah, I don't know.

0:59:11	SPEAKER_04
 None of this is neither of these projects has got a real tight timeline in the sense that over the next month, there's a deliverable.

0:59:22	SPEAKER_04
 So it's opportunity.

0:59:23	SPEAKER_04
 In that sense, it's opportunistic.

0:59:27	SPEAKER_04
 If we don't get any information for these guys for several weeks, then we aren't going to sit around wasting time trying to do the problem.

0:59:34	SPEAKER_04
 Or guess what?

0:59:35	SPEAKER_04
 I'm just going to do other things.

0:59:40	SPEAKER_03
 Yeah, but this point is really, I think, very valid.

0:59:45	SPEAKER_03
 But ultimately, we hope that both will merge into harmonious the wonderful state where we cannot only do very necessities, i.e. changing the tables.

1:00:00	SPEAKER_03
 That does exactly in English where it doesn't German.

1:00:02	SPEAKER_03
 But also that we can sort of have the system where we can say, OK, this is what it usually does.

1:00:07	SPEAKER_03
 And now we add this little thing to it, whatever John knows of musk or us, great belief net.

1:00:14	SPEAKER_03
 We plug it in.

1:00:15	SPEAKER_03
 And then for these certain tasks, and we know that navigational tasks are going to be core domain of the new system, it all of a sudden it does much better.

1:00:24	SPEAKER_03
 And because it produced better answers, tell the person, as I showed you on this map, produce either a red line that goes to the vista point or a red line that goes to the tango point or a red line that goes to the door, which would be great.

1:00:40	SPEAKER_03
 So you don't only can you show that you know something sensible, but ultimately, if you produce a system like this, it takes the person world points to go, rather than taking them always to the geometric center of a building, which is what they do now.

1:00:56	SPEAKER_03
 And we even had to take out, the men's you missed that part.

1:00:59	SPEAKER_03
 We had to take out a bit of the road work so that it doesn't take you to the wall.

1:01:05	SPEAKER_03
 Every time.

1:01:06	SPEAKER_03
 Yeah, really?

1:01:07	SPEAKER_03
 So this was actually an actual problem that we encountered, which is your way up.

1:01:15	SPEAKER_03
 Because current navigation systems don't really care.

1:01:18	SPEAKER_03
 They get you to the beginning of the street, somehow do the house number.

1:01:22	SPEAKER_03
 But even that is problematic.

1:01:24	SPEAKER_03
 If you go, if you want to drive to the SAP in Balthorff, I'm sure the same is true with Microsoft.

1:01:29	SPEAKER_03
 It takes you to the address, whatever street number blah, blah, blah.

1:01:34	SPEAKER_03
 I'm miles away from the entrance.

1:01:37	SPEAKER_03
 Because this postal address is maybe a mailbox somewhere.

1:01:42	SPEAKER_03
 But the entrance where you actually want to go is something completely different.

1:01:46	SPEAKER_03
 So unless you're a mail person, you really don't want to go.

1:01:51	SPEAKER_04
 Probably not then, because you probably can't drop the mail there anyway.

1:01:54	SPEAKER_04
 I don't even know if you're better.

1:02:03	None
 Clear?

1:02:04	None
 OK.

1:02:04	None
 That's it.

1:02:12	SPEAKER_02
 Outer towers made of red limestone.

1:02:16	SPEAKER_00
 I was wondering.

1:02:17	SPEAKER_00
 Do you want to see a picture?

1:02:18	SPEAKER_03
 Sure.

1:02:20	SPEAKER_03
 After we boot for that, though.

1:02:22	SPEAKER_00
 So you two who will be working on this, I mean, are you supposed to just do it by thinking about the station?

1:02:30	SPEAKER_00
 Can you use the sample data?

1:02:31	SPEAKER_00
 Of course, I do just sample data.

1:02:33	SPEAKER_00
 Is there more than there are lots of sample data that

1:02:36	SPEAKER_03
 is beyond what you have there? I think this is an in part my job to look at that and to see whether there are features in there that can be extracted.

1:02:48	SPEAKER_03
 And to come up with some features that are not purely based on a real experiment or on reality, but sort of on pure intuition of real, this is maybe a sign for that.

1:03:02	SPEAKER_03
 And this is maybe a sign for this.

1:03:11	SPEAKER_01
 So later this being should get together.

1:03:14	SPEAKER_01
 Talked down.

1:03:15	SPEAKER_01
 Let's see what we look at.

1:03:19	SPEAKER_04
 OK, we can end the meeting and call Adam.

1:03:25	SPEAKER_04
 And then we want to look at some filthy pictures of high version.

1:03:29	SPEAKER_04
 We can do that as well.

1:03:32	SPEAKER_03
 Is that OK?

1:03:32	SPEAKER_03
 They used the ammunition.

1:03:34	SPEAKER_03
 They start the ammunition in that tower.

1:03:36	SPEAKER_03
 And that's why when it was hit by a cannon ball.

1:03:45	SPEAKER_02
 That's what they call it, the powder tower.

1:03:49	SPEAKER_02
 At this time, I had so many material.

1:03:50	SPEAKER_02
 That's why I asked.

1:03:53	SPEAKER_00
 That's right.

1:03:53	SPEAKER_00
 OK.

1:03:55	None
 The little synthrodotter, powder metal.

1:03:58	None
 Hi there, we're done.

1:04:09	SPEAKER_00
 Is there a lot of ontological information available about the very time marks then?

1:04:14	SPEAKER_00
 And it's presumably what title we're doing.

1:04:18	SPEAKER_00
 I mean, we have this example of post office behind that.

1:04:22	SPEAKER_00
 Well, I think whether post office is going to be

1:04:25	None
 building a contextual feature, which we determined would have wanted to look at it.

1:04:29	SPEAKER_00
 Yep.

1:04:30	SPEAKER_00
 So going to those facts about that.

1:04:33	SPEAKER_00
 No.

1:04:34	SPEAKER_04
 Well, Robert knows exactly what they're going to be.

1:04:36	SPEAKER_04
 But the point is that, again, for our purposes, the probability to be six or seven buildings that will drop in the dialogue and we will put as much in as we need.

1:04:44	SPEAKER_04
 And if that turns out to be critical, that's part of what we learned is, OK, we're going to mean that kind of information in order to do this.

1:04:56	SPEAKER_04
 Oh, Nancy didn't read her numbers yet.

1:04:58	SPEAKER_04
 Sorry.

1:05:01	SPEAKER_04
 You love me, Andy?

1:05:02	SPEAKER_05
 Yeah.

1:05:03	SPEAKER_05
 So it's been a little bit more since then.

1:05:05	SPEAKER_05
 I think every time.

1:05:07	SPEAKER_04
 Let's see.

1:05:08	SPEAKER_04
 So you turned off your rhythm for two seconds.

1:05:10	SPEAKER_05
 I turned it off for two seconds.

1:05:11	SPEAKER_05
 So I have to turn off the strings like this.

1:05:13	SPEAKER_04
 Right.

1:05:13	SPEAKER_04
 OK.

