0:00:00	SPEAKER_04
 Yeah, we had a long discussion about how easy we want to make it for people to bleep things out.

0:00:06	SPEAKER_04
 So Morgan wants to make it hard.

0:00:10	SPEAKER_05
 It doesn't.

0:00:14	SPEAKER_05
 I didn't even check yesterday when I started it.

0:00:20	SPEAKER_04
 I don't know if it doesn't like low-three.

0:00:24	SPEAKER_05
 You know, I discovered something yesterday on these wireless ones.

0:00:29	SPEAKER_05
 You can tell if it's picking up breath noise and stuff.

0:00:33	SPEAKER_04
 Yeah, it has a little indicator on it on the AAF.

0:00:37	SPEAKER_05
 So if you breathe under, breathe and then you see AAF go off, then you know.

0:00:43	SPEAKER_05
 Pick up your mouth nice.

0:00:45	SPEAKER_03
 Oh, that's good.

0:00:47	SPEAKER_03
 Because we have a lot of tests.

0:00:49	SPEAKER_03
 In fact, if you listen to just the channels and people not talk, it's like...

0:00:53	SPEAKER_03
 What did you see, Hannibal, recently?

0:00:59	SPEAKER_03
 It's a very disconquering.

0:01:01	SPEAKER_03
 Okay.

0:01:03	SPEAKER_03
 So I was going to try to get out of here in half an hour.

0:01:07	SPEAKER_03
 Because I really appreciate people coming.

0:01:10	SPEAKER_03
 And the main thing that I was going to ask people to help with today is to give input on what kinds of database format we should use in starting to link up things like word transcripts and annotations of word transcripts.

0:01:29	SPEAKER_03
 So anything that transcribers or discourse coders would ever put in the signal with time marks for words and phone boundaries and all the stuff we get out of the forced alignments in the recognizer.

0:01:41	SPEAKER_03
 So we have this, I think a starting point is clearly the channelized output of Dave Galbert's program, which Don brought a copy of.

0:01:52	SPEAKER_03
 Yeah, I'm familiar with that.

0:01:53	SPEAKER_04
 I mean, I sort of already have developed an XML format for this sort of stuff.

0:01:58	SPEAKER_04
 And so the only question is it the sort of thing that you want to use or not.

0:02:03	SPEAKER_04
 Have you looked at that?

0:02:04	SPEAKER_04
 I mean, I had a web page up.

0:02:06	SPEAKER_03
 So I actually mostly need to be able to link up or.

0:02:11	SPEAKER_03
 I mean, it's a question both of what the represent.

0:02:14	SPEAKER_03
 I guess I am going to be standing up and drawing up.

0:02:16	SPEAKER_04
 Okay, yeah, so you should definitely.

0:02:17	SPEAKER_04
 So it definitely had that as a concept.

0:02:19	SPEAKER_04
 So that it has a single timeline.

0:02:21	SPEAKER_04
 And then you can have lots of different sections, each of which have IDs attached to it.

0:02:26	SPEAKER_04
 And then you can refer from other sections to those IDs if you want to.

0:02:30	SPEAKER_04
 So that you start with a timeline tag.

0:02:37	SPEAKER_04
 Timeline.

0:02:39	SPEAKER_04
 And then you have a bunch of times.

0:02:41	SPEAKER_04
 I don't remember exactly what my notation was.

0:02:44	SPEAKER_04
 Oh, I remember it.

0:02:45	SPEAKER_01
 Right.

0:02:46	SPEAKER_04
 Yeah, t equals one point three two.

0:02:51	SPEAKER_04
 And then I also had optional things like accuracy.

0:02:54	SPEAKER_04
 And then ID equals t one.

0:02:59	SPEAKER_04
 And then I also wanted to be able to not specify specifically what the time was and just have a stamp.

0:03:10	SPEAKER_04
 So these are arbitrary assigned by a program, not by users.

0:03:13	SPEAKER_04
 And you have a whole bunch of those.

0:03:15	SPEAKER_04
 And then somewhere further down you might have something like an utterance tag, which has start equals t 17 and equals t 18.

0:03:26	SPEAKER_04
 So what that's saying is we know it starts at this particular time.

0:03:29	SPEAKER_04
 We don't know when it ends.

0:03:31	SPEAKER_04
 Okay.

0:03:32	SPEAKER_04
 Right. But it ends at this t 18, which may be somewhere else.

0:03:35	SPEAKER_04
 We say there's another utterance.

0:03:38	SPEAKER_04
 We don't know what that time actually is, but we know that it's the same time as this end time.

0:03:46	SPEAKER_04
 Yeah, whatever you want.

0:03:48	SPEAKER_01
 So you're specifically defining a lot of us?

0:03:51	SPEAKER_04
 Yes, exactly.

0:03:53	SPEAKER_04
 And then these also have IDs.

0:04:00	SPEAKER_04
 Right.

0:04:01	SPEAKER_04
 So you could have some sort of other other tag later in the file.

0:04:05	SPEAKER_04
 That would be something like, no, I don't know.

0:04:13	SPEAKER_04
 Noise type equals dorslam.

0:04:20	SPEAKER_04
 And then you could either say time equals a particular time mark, or you could do other sorts of references.

0:04:27	SPEAKER_04
 So, or you might have a prosody.

0:04:30	SPEAKER_04
 Prosody, right?

0:04:32	SPEAKER_04
 D, t.

0:04:33	SPEAKER_03
 It's an O instead of an I, but the D is good.

0:04:36	SPEAKER_03
 You like the D?

0:04:37	SPEAKER_04
 It's a good D.

0:04:39	SPEAKER_04
 You know, so you could have some sort of type here.

0:04:42	SPEAKER_04
 And then you could have the utterance that it's referring to could be U 17 or something like that.

0:04:49	SPEAKER_03
 Okay, so I mean, that seems great for all of the encoding of things with time and...

0:05:02	SPEAKER_03
 I guess my question is more, what do you do with, say, a forced alignment?

0:05:08	SPEAKER_03
 We've got all these phone labels.

0:05:10	SPEAKER_03
 And what do you do if you just conceptually, if you get transcriptions where the words are staying, but the time boundaries are changing because you've got a new recognition output.

0:05:22	SPEAKER_03
 Sort of what's the sequence of going from the waveforms that stay the same, the transcripts that may or may not change.

0:05:32	SPEAKER_03
 And then the utterance, which are the time boundaries that may or may not change.

0:05:37	SPEAKER_01
 That's actually very nicely handled here because you could, you could, all you'd have to change is the time stamps in the timeline without...

0:05:46	SPEAKER_01
...to propagate all of them.

0:05:48	SPEAKER_04
 That's why you do that extra level of indirection, so that you can just change the timeline.

0:05:55	SPEAKER_01
 Except the timeline is going to be huge.

0:05:58	SPEAKER_01
 Yes.

0:05:59	SPEAKER_03
 Yeah, especially if you have a phone level.

0:06:01	SPEAKER_03
 We have a phone level back.

0:06:02	SPEAKER_04
 Yeah, I don't think I would do this for phone level.

0:06:04	SPEAKER_04
 I think for phone level, you want to use some sort of binary representation because it'll be too dense otherwise.

0:06:08	SPEAKER_03
 Okay, so if you were doing that and you had this sort of companion thing that gets called up for phone level, what would that look like?

0:06:15	SPEAKER_04
 I would use just an existing way of doing it.

0:06:19	SPEAKER_01
 It's just a matter of it being bigger.

0:06:23	SPEAKER_01
 But if you have, you know, barring memory limitations, I mean this is still...

0:06:28	SPEAKER_04
 It's parsing limitations.

0:06:29	SPEAKER_04
 I don't want to have this text file that you have to read in the whole thing to do something very simple for.

0:06:34	SPEAKER_01
 Oh no, you would use it only for purposes where you actually want the phone level information.

0:06:40	SPEAKER_03
 So you could have some file that configures how much information you want in your XML or something.

0:06:47	SPEAKER_04
 I mean, I imagine you'd have multiple versions of this depending on the information that you want.

0:06:53	SPEAKER_04
 What I'm wondering is whether I think for word level, this would be okay.

0:06:58	SPEAKER_04
 For word level, it's all right.

0:07:00	SPEAKER_04
 For lower than word level, you're talking about so much data that I don't know.

0:07:04	SPEAKER_04
 I don't know if that...

0:07:05	SPEAKER_03
 I mean, we actually have so one thing that Donna is doing...

0:07:11	SPEAKER_03
 Gladeth is a big key.

0:07:12	SPEAKER_03
 We're running for every frame you get a pitch value, not only one pitch value, but different kinds of pitch values.

0:07:15	SPEAKER_04
 Yeah, I mean, for something like that, I would use p-file or any frame level stuff I would use p-file.

0:07:20	SPEAKER_04
 Meaning?

0:07:21	SPEAKER_04
 Well, or something like it.

0:07:23	SPEAKER_04
 It's...it's...it's...it's...it's...it's...it's...it's a format for frame level representation of features.

0:07:28	SPEAKER_03
 Okay. That you could call...that you would tie into this representation with like an ID.

0:07:33	SPEAKER_04
 Or there's a particular way in XML to refer to external resources.

0:07:38	None
 Okay.

0:07:38	SPEAKER_04
 So you would say refer to this external file.

0:07:43	SPEAKER_04
 So that external file wouldn't be in...

0:07:45	SPEAKER_05
 But what's the advantage of doing that versus just putting it into this format?

0:07:49	SPEAKER_04
 More compact, which I think is better.

0:07:52	SPEAKER_04
 I mean, if you did it at this...

0:07:54	SPEAKER_03
 You don't want to do it with that.

0:07:55	SPEAKER_04
 Anything at frame level, you had better encode binary or it's going to be really painful.

0:07:59	SPEAKER_01
 Or you just...I mean, I like text formats. You can always...

0:08:03	SPEAKER_01
 Jesus, then.

0:08:04	SPEAKER_01
 And you know, decompress them on the fly if space is really...

0:08:08	SPEAKER_05
 I was thinking the advantage is that we can share that one frame.

0:08:12	SPEAKER_04
 You're talking about gigabyte-sized files.

0:08:14	SPEAKER_04
 You're going to actually run out of space in your file system for one file.

0:08:19	SPEAKER_04
 Right? Because you have two gigabyte limit on most OSs.

0:08:22	SPEAKER_01
 I would say, okay, so frame level is probably not a good idea.

0:08:25	SPEAKER_01
 But for phone level stuff, it's perfectly...like phones are syllables...

0:08:29	SPEAKER_01
 Well, they're every five frames though.

0:08:31	SPEAKER_03
 So something like that.

0:08:33	SPEAKER_01
 But most of the frames are actually not speech.

0:08:37	SPEAKER_01
 So, you know, people don't...

0:08:39	SPEAKER_01
 Yeah, but we actually...

0:08:40	SPEAKER_01
 Look at the average number of phones in an English word is...

0:08:44	SPEAKER_01
 I don't know, five maybe.

0:08:46	SPEAKER_01
 So look at it.

0:08:48	SPEAKER_01
 Number of words times five.

0:08:49	SPEAKER_03
 So you mean, pause phones take up a lot of the long pause phones?

0:08:52	SPEAKER_03
 Yeah, exactly.

0:08:53	SPEAKER_03
 Okay, that's true, but you do have to keep them in there.

0:08:56	SPEAKER_04
 So I think it's debatable whether you want to do phone level in the same thing.

0:08:59	SPEAKER_04
 Okay.

0:09:00	SPEAKER_04
 But I think anything at frame level, even P file is true for both.

0:09:03	SPEAKER_04
 So I would use something tighter than P files.

0:09:06	SPEAKER_04
 Are you familiar with it?

0:09:07	SPEAKER_03
 I haven't seen this particular format.

0:09:09	SPEAKER_01
 I've used them.

0:09:10	SPEAKER_01
 I don't know what their structure is.

0:09:12	SPEAKER_05
 But I mean, P file for each frame is storing a vector of capstroller, PLP values, right?

0:09:17	SPEAKER_05
 It's whatever you want, right?

0:09:18	SPEAKER_04
 Actually, so that...what's nice about the P file, it...built into it is the concept of frames, utterances, sentences, that sort of thing, that structure.

0:09:28	SPEAKER_04
 And then also attached to it is an arbitrary vector of values.

0:09:33	SPEAKER_04
 Oh.

0:09:34	SPEAKER_04
 And it can take different types.

0:09:35	SPEAKER_04
 So they don't all have to be floats.

0:09:37	SPEAKER_04
 You know, you can have integers and you can have doubles and all that sort of stuff.

0:09:40	SPEAKER_04
 That sounds about what I...

0:09:42	SPEAKER_04
 Right, and it has a header, it has a header format that describes it to some extent.

0:09:47	SPEAKER_04
 So the only problem with it is it's actually storing the utterance numbers and the frame numbers in the file, even though they're always sequential.

0:09:57	SPEAKER_04
 And so it does waste a lot of space.

0:09:59	SPEAKER_04
 But it's still a lot tighter than ASCII.

0:10:02	SPEAKER_04
 And we have a lot of tools already to deal with it.

0:10:04	SPEAKER_03
 Do you...okay.

0:10:05	SPEAKER_03
 Is there some documentation on the...

0:10:07	SPEAKER_03
 Yeah, there's a ton of it.

0:10:08	SPEAKER_04
 Man pages and source code and...

0:10:11	SPEAKER_04
 I mean, that sounds good.

0:10:12	SPEAKER_03
 I was just looking for something...

0:10:14	SPEAKER_03
 Not a database person, but something sort of standard enough that...

0:10:18	SPEAKER_03
 You know, if we start using this, we can give it out.

0:10:20	SPEAKER_03
 Other people can work on that.

0:10:21	SPEAKER_04
 Yeah, it's not standard.

0:10:22	SPEAKER_04
 I mean, it's something that we developed at ICSI.

0:10:25	SPEAKER_04
 But...

0:10:26	SPEAKER_03
 But it's been used here.

0:10:28	SPEAKER_04
 But it's been used here and...

0:10:29	SPEAKER_04
 And, you know, we have a well-configured system that you can distribute for free.

0:10:34	SPEAKER_05
 And...

0:10:35	SPEAKER_05
 I mean, it must be the equivalent of whatever you guys use to store your computed features in, right?

0:10:40	SPEAKER_01
 Yeah, we have... actually, we use a generalization of the sphere format.

0:10:48	SPEAKER_01
 But...

0:10:50	SPEAKER_01
 Yeah, so there's something like that.

0:10:52	SPEAKER_01
 But it's...

0:10:53	SPEAKER_04
 I think the word is HTK-DU for features.

0:10:55	SPEAKER_04
 Or does it even have a concept of features?

0:10:57	SPEAKER_01
 I mean, the topic has their own feature format.

0:11:00	SPEAKER_01
 It's called like SSD or something like that.

0:11:04	SPEAKER_04
 Yeah, I'm just wondering, would it be worthwhile to use that instead?

0:11:07	SPEAKER_03
 Yeah, this is exactly a good decision.

0:11:09	SPEAKER_05
 And people don't typically share this kind of stuff, right?

0:11:13	SPEAKER_05
 I mean, they generate their own.

0:11:15	SPEAKER_03
 Yeah.

0:11:16	SPEAKER_03
 Actually, I just...

0:11:17	SPEAKER_03
 You know, we've done this stuff on prasadix.

0:11:20	SPEAKER_03
 And three or four places I've asked for those prasadic files.

0:11:23	SPEAKER_03
 And we just have an ASCII.

0:11:24	SPEAKER_03
 All right.

0:11:25	SPEAKER_03
 But a frame by frame, which is fine.

0:11:27	SPEAKER_03
 But it gets unwieldy to go in and query these files with really huge files.

0:11:32	SPEAKER_03
 Right.

0:11:33	SPEAKER_03
 And we could do it.

0:11:34	SPEAKER_03
 I was just thinking if there's something...

0:11:35	SPEAKER_03
 And again, if you have a...

0:11:36	SPEAKER_03
 We're out of frame values, right?

0:11:37	SPEAKER_04
 If you have a two-hour long meeting, that's gonna...

0:11:41	SPEAKER_04
 They're quite large.

0:11:42	SPEAKER_03
 Yeah, I mean, they're very much enormous.

0:11:43	SPEAKER_03
 These are 10-minute switchboard conversations.

0:11:46	SPEAKER_03
 Right.

0:11:47	SPEAKER_03
 So it's doable.

0:11:48	SPEAKER_03
 It's just that you can only store a feature vector at frame by frame.

0:11:52	SPEAKER_03
 And it doesn't have any kind of...

0:11:55	SPEAKER_05
 Is the sharing part of this a pretty important consideration?

0:11:59	SPEAKER_05
 Or is that just sort of a nice thing to have?

0:12:03	SPEAKER_03
 I don't know enough about what we're gonna do with the data.

0:12:08	SPEAKER_03
 But I thought it would be good to get something that other people can use or adopt for their own kinds of encoding.

0:12:17	SPEAKER_03
 And we have to make some decision about what to do.

0:12:20	SPEAKER_03
 And especially for the prosody work, what it ends up being is you get features from the signal.

0:12:26	SPEAKER_03
 And of course, those change every time your alignments change.

0:12:28	SPEAKER_03
 So you rerun a recognize you want to recompute your features and then keep the database up to date.

0:12:35	SPEAKER_03
 You change a word or you change an utterance boundary segment, which is gonna happen a lot.

0:12:40	SPEAKER_03
 And so I wanted something where all of this can be done in an elegant way.

0:12:45	SPEAKER_03
 And that if somebody wants to try something or compute something else, that it can be done flexibly, doesn't have to be pretty.

0:12:53	SPEAKER_03
 It just has to be easy to use.

0:12:57	SPEAKER_04
 Yeah, the other thing we should look at Atlas, the NIST thing, and see if they have anything at that level.

0:13:05	SPEAKER_04
 I mean, I'm not sure what to do about this with Atlas because they chose a different route.

0:13:09	SPEAKER_04
 I chose something that there are sort of two choices.

0:13:12	SPEAKER_04
 Your file format can know about, know that you're talking about language and speech, which is what I chose in time, or your file format can just be a graph representation.

0:13:23	SPEAKER_04
 And then the application has to impose the structure on top.

0:13:26	SPEAKER_04
 So what it looked like Atlas chose is they chose the other way, which was their file format is just nodes and links.

0:13:33	SPEAKER_04
 And you have to interpret what they mean yourself.

0:13:36	SPEAKER_03
 And why did you not choose that?

0:13:38	SPEAKER_04
 Because I knew that we were doing speech and I thought it was better if you're looking at a raw file to be for the tags to say it's an utterance, as opposed to the tag to say it's a link.

0:13:49	SPEAKER_03
 But other than that, are they compatible?

0:13:51	SPEAKER_03
 I mean, you could sort of...

0:13:52	SPEAKER_04
 Yeah, they're reasonably compatible.

0:13:53	SPEAKER_04
 You could probably translate between them.

0:13:55	SPEAKER_04
 Yeah, that's...

0:13:56	SPEAKER_04
 So...

0:13:57	SPEAKER_04
 So, well, the other thing is, if we choose to use Atlas, which maybe we should just do, we should just throw this out before we invest a lot of time in it.

0:14:05	SPEAKER_03
 So this is what the meeting is about, just sort of, how to...

0:14:08	SPEAKER_03
 Because we need to come up with the database like this just to do our work.

0:14:11	SPEAKER_03
 And I actually don't care as long as it's something useful to other people what we choose.

0:14:15	SPEAKER_03
 So maybe it's...

0:14:17	SPEAKER_03
 You know, you have any idea of how to choose because I don't...

0:14:21	SPEAKER_03
 Yeah.

0:14:22	SPEAKER_04
 I mean, I chose this for a couple of reasons. One of them is that it's easy to parse.

0:14:26	SPEAKER_04
 You don't need a full XML parser.

0:14:28	SPEAKER_04
 It's very easy to just write a purl script to parse it.

0:14:32	SPEAKER_01
 As long as each tag is on one line...

0:14:34	SPEAKER_04
 Exactly, exactly, which I always do.

0:14:36	SPEAKER_03
 And you can have as much information in the tag as you want, right?

0:14:40	SPEAKER_04
 Well, I have it structured, right?

0:14:42	SPEAKER_04
 So each tag has only particular items that it can take.

0:14:46	SPEAKER_03
 But you can add to those structures if you...

0:14:50	SPEAKER_04
 If you have more information. So what this would say is that instead of doing this, you would say something like a link...

0:14:56	SPEAKER_04
 Start equals some node ID...

0:15:02	SPEAKER_04
 And equals some other node ID...

0:15:05	SPEAKER_04
 And then type would be utterance.

0:15:10	SPEAKER_04
 So it's very similar.

0:15:13	SPEAKER_03
 So why would it be a waste to do it this way if it's similar enough that we can always translate...

0:15:18	SPEAKER_05
 It probably wouldn't be a waste. It would mean that at some point if we wanted to switch, we'd just have to...

0:15:22	SPEAKER_05
 Write a translator, if that's...

0:15:24	SPEAKER_04
 Since they're developing a big...

0:15:26	SPEAKER_04
 I don't think that's a big deal.

0:15:28	SPEAKER_04
 They're developing a big infrastructure.

0:15:30	SPEAKER_04
 And so it seems to me that if we want to use that, we might as well go directly to what they're doing rather than...

0:15:36	SPEAKER_01
 Yeah, so that's the question.

0:15:38	SPEAKER_05
 I mean, how stable is there...

0:15:40	SPEAKER_05
 Are they ready to go?

0:15:42	SPEAKER_04
 I looked at it...

0:15:44	SPEAKER_04
 I looked at it a couple of years ago, probably a year ago, when we first started talking about this.

0:15:48	SPEAKER_04
 And at that time, at least, it was still not very complete.

0:15:52	SPEAKER_04
 And so specifically, they didn't have any external format representation at that time.

0:15:57	SPEAKER_04
 They just had sort of conceptual node annotated transcription graph, which I really liked.

0:16:04	SPEAKER_04
 And that's exactly what this stuff is based on.

0:16:06	SPEAKER_04
 Since then, they've developed their own external file format, which is this sort of thing.

0:16:12	SPEAKER_04
 And apparently, they've also developed a lot of tools, but I haven't looked at them.

0:16:16	SPEAKER_03
 Maybe I should...

0:16:18	SPEAKER_03
 Would the tools run on something like this?

0:16:20	SPEAKER_03
 And translate them anyway?

0:16:22	SPEAKER_04
 I mean, I guess that's the question.

0:16:24	SPEAKER_04
 What would worry me is that maybe we might miss a little detail.

0:16:28	SPEAKER_04
 That would make it very difficult to translate from one to the other.

0:16:31	SPEAKER_01
 They're conceptually close.

0:16:33	SPEAKER_01
 And they already have or will have tools that everybody else will be using.

0:16:38	SPEAKER_01
 I mean, we might as well.

0:16:42	SPEAKER_04
 So I'll take a closer look at it.

0:16:46	SPEAKER_03
 So that would really be the question is just what you feel is in the long run the best thing.

0:16:51	SPEAKER_03
 Because once we start sort of doing this, we don't actually have enough time to probably have to rehash it out again.

0:16:59	SPEAKER_04
 The other way that I sort of established this was as easy translation to and from the transcriber format.

0:17:06	SPEAKER_04
 Right.

0:17:08	SPEAKER_03
 But...

0:17:09	SPEAKER_03
 I mean, I like this.

0:17:10	SPEAKER_03
 This is sort of intuitively easy to actually read as easy as it could be.

0:17:14	SPEAKER_03
 But I suppose that as long as they have a type here that specifies...

0:17:20	SPEAKER_04
 It's almost the same.

0:17:22	SPEAKER_04
 The point is with this though is that you can't really add any supplementary information.

0:17:28	SPEAKER_04
 So if you suddenly decide that you want...

0:17:30	SPEAKER_04
 You have to make a different type.

0:17:32	SPEAKER_04
 Yeah, you'd have to make a different type.

0:17:34	SPEAKER_03
 Well, if you look at it, I guess in my mind, I don't know enough Jane would know better about the types of annotations.

0:17:43	SPEAKER_03
 But I imagine that those are things that...

0:17:45	SPEAKER_03
 Well, you guys mentioned this that could span any...

0:17:48	SPEAKER_03
 It could be in its own channel.

0:17:50	SPEAKER_03
 It could span time boundaries of any type.

0:17:53	SPEAKER_03
 It could be instantaneous.

0:17:54	SPEAKER_03
 Things like that.

0:17:56	SPEAKER_03
 And then from the recognition side, we have back traces at the phone level.

0:18:01	SPEAKER_03
 If you could handle that, it could handle states or whatever.

0:18:04	SPEAKER_03
 And then at the prosely level, we have frame sort of like actual feature files, like these P files or anything like that.

0:18:13	SPEAKER_03
 And that's sort of the world of things that I...

0:18:15	SPEAKER_03
 And then we have the align channels.

0:18:17	SPEAKER_03
 Very coarse.

0:18:18	SPEAKER_01
 It seems to me you want to keep the frame level stuff separate.

0:18:21	SPEAKER_01
 Yeah, I definitely agree.

0:18:23	SPEAKER_03
 And I wanted to find actually a nicer format or a maybe a more compact format that we used before.

0:18:28	SPEAKER_03
 Just because you've got 10 channels or whatever and two hours of a meeting.

0:18:33	SPEAKER_01
 How would you represent multiple speakers in this framework?

0:18:40	SPEAKER_01
 Were you just represent them as you would have like a speaker tag or something?

0:18:45	SPEAKER_04
 There's a speaker tag up at the top, which identifies them.

0:18:48	SPEAKER_04
 And then the way I had it is each turn or each other, and so I don't even remember now, had a speaker ID tag attached to it.

0:18:55	SPEAKER_04
 And in this format, you would have a different tag, which would be linked to the link.

0:19:03	SPEAKER_04
 So somewhere else you would have another thing that would be...

0:19:10	SPEAKER_04
 Let's see, would it be a node or a link?

0:19:13	SPEAKER_04
 And so this one would have an ID, is link 74 or something like that.

0:19:21	SPEAKER_04
 And then somewhere up here you would have a link that was referencing L74 and had speaker item.

0:19:33	SPEAKER_03
 Actually, the channel I think that...

0:19:36	SPEAKER_03
 Well, channel is what the channel lies out.

0:19:39	SPEAKER_04
 This isn't quite right. I have to look at it again.

0:19:41	SPEAKER_01
 But so how in the next format do we express a hierarchical relationship between, say, and utterance and the words within it?

0:19:50	SPEAKER_01
 So how do you tell that these are the words that belong to that utterance?

0:19:57	SPEAKER_04
 You would have another structure lower down than this, that would be saying they're all belonging to this ID.

0:20:05	SPEAKER_05
 So each thing refers to the utterance that it belongs to?

0:20:09	SPEAKER_05
 Right.

0:20:10	SPEAKER_04
 So each utterance could refer to a term, and each term could refer to something higher up.

0:20:16	SPEAKER_03
 And what if you actually have... Right now what you have as utterance, the closest thing that comes out of the channel lies is the stuff between the segment boundaries that the transcribers put in or that T-Lo put in.

0:20:29	SPEAKER_03
 Which may or may not actually be like... It's usually not the beginning and end of a sentence.

0:20:36	SPEAKER_03
 Well, that's why I didn't call it sentence.

0:20:37	SPEAKER_03
 Right. So it's like a segment or something.

0:20:40	SPEAKER_03
 So I mean, as soon as this is possible that if you have someone annotates the punctuation or whatever when they transcribe, you can say, you know, from the beginning of the sentence to the end of the sentence from the annotations, this is a unit.

0:20:55	SPEAKER_03
 Even though it never actually, it's only a unit by virtue of the annotations at the word level.

0:21:01	SPEAKER_03
 Sure. I mean, so you would have yet another tag.

0:21:03	SPEAKER_03
 That would be a tag somehow.

0:21:04	SPEAKER_04
 You'd have another tag which says, this is of type sentence.

0:21:07	SPEAKER_03
 But it's just not overtly in the... Because this is exactly the kind of...

0:21:13	SPEAKER_03
 I think that should be possible as long as the... But what I don't understand is where in this type of file that would be expressed.

0:21:24	SPEAKER_04
 Right. You would have another tag somewhere.

0:21:27	SPEAKER_04
 Well, there are two ways of doing it.

0:21:28	SPEAKER_04
 Two ways of doing it.

0:21:37	SPEAKER_04
 And ID is S whatever.

0:21:40	SPEAKER_04
 And then lower down, you could have an utterance.

0:21:44	SPEAKER_04
 So the type is utterance equals UT.

0:21:47	SPEAKER_04
 And you could either say that... No, I don't know.

0:21:51	SPEAKER_05
 I take this as a thing.

0:21:53	SPEAKER_04
 Can you say that this is... You would just have a part of this or do you say this is part of this?

0:21:59	SPEAKER_04
 I think... But you would refer up to the sentence.

0:22:01	SPEAKER_03
 They're actually overlapping each other.

0:22:03	SPEAKER_01
 The thing is that something may be a part of one thing for one purpose and another thing of another purpose.

0:22:10	SPEAKER_01
 You have to have another type then, I guess.

0:22:13	SPEAKER_04
 Well, I think I had better look at it again because I...

0:22:18	SPEAKER_01
 So, for instance... There's one more level of indirection that I'm forgetting.

0:22:21	SPEAKER_01
 So, once you have a word sequence and you have two different segmentations of that same word sequence, I say one segmentation is in terms of, you know, sentences.

0:22:34	SPEAKER_01
 And another segmentation is in terms of, I don't know, prosodic phrases.

0:22:40	SPEAKER_01
 And let's say that they don't nest.

0:22:43	SPEAKER_01
 So, you know, a prosodic phrase may cross two sentences or something.

0:22:48	SPEAKER_01
 I don't know if that's true or not.

0:22:49	SPEAKER_03
 Well, it's definitely true with the segment. That's what I exactly meant by the utterances versus the sentence.

0:22:54	SPEAKER_01
 So, you want to say this word is part of that sentence and this prosodic phrase.

0:23:01	SPEAKER_01
 But the phrase is not part of the sentence and neither is the sentence part of the phrase.

0:23:04	SPEAKER_04
 I'm pretty sure that you can do that, but I'm forgetting the exact level of nesting.

0:23:08	SPEAKER_01
 Two different pointers from the word up, one level up.

0:23:11	SPEAKER_04
 So, what you would end up having is a tag saying, here's a word and it starts here and it ends here.

0:23:16	SPEAKER_04
 And then lower down you would say, here's a prosodic boundary and it has these words in it.

0:23:21	SPEAKER_04
 And lower down you'd have, here's sentence and it has these words in it.

0:23:23	SPEAKER_03
 So, you would be able to go in and say, you know, give me all the words in the bound, in the prosodic phrase, and give me all the words in it.

0:23:29	SPEAKER_04
 So, I think that's the word. Let me look at it again.

0:23:32	SPEAKER_01
 Okay.

0:23:33	SPEAKER_01
 The other issue that you had was, how do you actually efficiently extract, find and extract information in a structural of this type?

0:23:44	SPEAKER_01
 So, you gave some examples, like.

0:23:47	SPEAKER_03
 Well, and I mean, you guys might, I don't know if this is premature because I suppose once you get the representation, you can do this, but the kinds of things I was worried about is...

0:23:58	SPEAKER_03
 No, that's not clear.

0:24:00	SPEAKER_01
 Well, okay, so you can do it, but can you do it?

0:24:03	SPEAKER_01
 I mean, I can't do it, but you know.

0:24:07	SPEAKER_01
 Well, you're going to do this, you're going to want to do this very quickly, or else you'll spend all your time sort of searching through various...

0:24:14	SPEAKER_03
 Right, you need to sort of a paradigm for how to do it, but an example would be, find all the cases in which Adam started to talk while Andreas was talking and his pitch was rising.

0:24:30	SPEAKER_03
 Andreas is a pitch.

0:24:32	SPEAKER_04
 Right, I mean, that's going to be, is the rising pitch a feature, or is it going to be in the same file?

0:24:39	SPEAKER_03
 Well, the rising pitch will never be hand annotated, so all the prasodic features are going to be automatically...

0:24:45	SPEAKER_04
 I mean, that's going to be hard, regardless, right, because you're going to have to write a program that goes through your feature file and looks for rising pitch.

0:24:52	SPEAKER_03
 So normally what we would do is we would say, what are we going to assign rising pitch to, are we going to assign it to words, are we going to just assign it to sort of... when it's rising, we have a begin and rise representation, but suppose we dump out this file, we say...

0:25:05	SPEAKER_03
 For every word, we just classify it as rise or fall or neither.

0:25:10	SPEAKER_04
 Okay, well, in that case, you would add that to this format.

0:25:13	SPEAKER_03
 We would basically be sort of taking the format and enriching it with things that we want to query in relation to the words that are already in the file, and then querying it.

0:25:24	SPEAKER_01
 You want sort of a graph that works at the structural...

0:25:28	SPEAKER_04
 You have that. There's a standard again in XML, specifically for searching XML documents, structured XML documents, where you can specify both the content and the structural position.

0:25:40	SPEAKER_01
 Yeah, but it's not clear that that's relative to the structure of the XML document.

0:25:45	SPEAKER_04
 You use it as a tool. You use it as a tool, not an end user. It's not an end user thing. You would use that to build your tool to do that sort of search.

0:25:54	SPEAKER_01
 Because here you're specifying a lattice, so the underlying data structure and the whole of the other search can have lattice.

0:26:03	SPEAKER_04
 That's different from searching. As long as the feature... Well, no, the whole point is that the text and the lattice are isomorphic. They represent each other completely.

0:26:14	SPEAKER_04
 So that...

0:26:17	SPEAKER_03
 That's true if the features from your acoustics or whatever that are not explicitly in this are at the level of these types.

0:26:26	SPEAKER_04
 If you can... Yeah, but that's going to be the trouble no matter what format you choose. You're going to have the difficulty of relating the frame level features.

0:26:36	SPEAKER_03
 I'm going to figure out what's the best format for this representation. It's still going to be not direct.

0:26:45	SPEAKER_03
 Another example was where in the language, where in the word sequence are people interrupting? I guess that one's actually easier.

0:26:58	SPEAKER_05
 What about the idea of using a relational database to store the information from the XML?

0:27:09	SPEAKER_05
 So you would have XML basically would... You could use the XML to put the data in. And then when you get data out, you put it back in XML.

0:27:19	SPEAKER_05
 So you use XML as sort of the transfer format. But then you store the data in the database, which allows you to do all kinds of good search things.

0:27:28	SPEAKER_04
 One of the things that Atlas is doing is they're trying to define an API, which is independent of the backstort.

0:27:34	SPEAKER_04
 So that you could define a single API and the storage could be flat XML files or a database.

0:27:41	SPEAKER_04
 My opinion on that is for the sort of stuff that we're doing, I suspect it's overkill to do a full relational database that...

0:27:49	SPEAKER_04
 Just a flat file and search tools I bet will be enough.

0:27:55	SPEAKER_04
 But that's the advantage of Atlas is that if we actually decide to go that route completely and we program to their API, then if we wanted to add a database later, it would be pretty easy.

0:28:07	SPEAKER_03
 It seems like the kind of thing you do if people start adding all kinds of files and whistles to the data.

0:28:15	SPEAKER_03
 And so that might be good for us to use a format where we know we can easily input that to some database of other people.

0:28:26	SPEAKER_04
 I guess I'm just a little hesitant to try to go whole hog on sort of the whole framework that that NIST is talking about with Atlas and the database and all that sort of stuff.

0:28:37	SPEAKER_04
 Because it's a big learning curve just to get going. Whereas if we just do a flat file format, sure it may not be as efficient, but everyone can program in Perl and use it.

0:28:49	SPEAKER_04
 Right, so as opposed to...

0:28:51	SPEAKER_01
 I'm still not convinced that you can do much at all on the text on the flat file that the text representation.

0:29:02	SPEAKER_01
 Because the text representation is going to be not reflecting the structure of your words and annotations.

0:29:12	SPEAKER_04
 So if it's not representing it, then how do you recover it? Of course it's representing it. That's the whole point.

0:29:17	SPEAKER_01
 You have to basically, yeah, you can use Perl to read it in and construct a internal representation that is essentially a lattice.

0:29:27	SPEAKER_04
 Okay, well that was a different point, right? So what I was saying is that...

0:29:32	SPEAKER_01
 But that's what you'll have to do.

0:29:34	SPEAKER_04
 Perl, if you want to just do Perl. If you wanted to use the structured XML query language, that's a different thing.

0:29:39	SPEAKER_04
 And it's a set of tools that let you specify given the DDDD of the document what sorts of structural searches you want to do.

0:29:50	SPEAKER_04
 So you want to say that you're looking for a tag within a tag within a particular tag that has this particular text in it and refers to a particular value.

0:30:02	SPEAKER_04
 And so the point isn't that an end user who is looking for a query like you specified wouldn't program it in this language.

0:30:10	SPEAKER_04
 What you would do is someone would build a tool that used that as a library so that you wouldn't have to construct the internal representations you're seeing.

0:30:19	SPEAKER_03
 So I think the kinds of questions, at least in the next to the end of this year, are there may be a lot of different ones, but they'll have a similar nature.

0:30:31	SPEAKER_03
 They'll be looking at either a word level, prosadik, a value, like a continuous value, like this slope of something.

0:30:41	SPEAKER_03
 But we'll do something where we, some kind of data reduction where the prosadik features are either at the word level or at the segment level or something like that.

0:30:52	SPEAKER_03
 They're not going to be at the phone level and they're not going to be at the frame level when we get done with sort of giving them simpler shapes and things.

0:31:00	SPEAKER_03
 And so the main thing is just being able, well, I guess the two goals, one that Chuck mentioned is starting out with something that we don't have to start over, that we don't have to throw away if other people want to extend it for other kinds of questions.

0:31:14	SPEAKER_03
 And being able to at least get enough information out on where we condition the location of features on information that's in the kind of file that you put up there.

0:31:27	SPEAKER_04
 And that would do it. I mean, I think that there are quick and dirty solutions and then there are long term big infrastructure solutions.

0:31:35	SPEAKER_04
 And so we want to try to pick something that lets us do a little bit of both.

0:31:38	SPEAKER_03
 And especially that the representation doesn't have to be thrown away, even if your tools change.

0:31:44	SPEAKER_04
 And so it seems to me that I have to look at it again to see whether it can really do what we want.

0:31:48	SPEAKER_04
 But if we use the Atlas external file representation, it seems like it's rich enough that you could do quick tools just as I said in Pearl.

0:31:59	SPEAKER_04
 And then later on, if we choose to go up the learning curve, we can use the whole Atlas infrastructure.

0:32:05	SPEAKER_04
 And that sounds good to me.

0:32:06	SPEAKER_03
 Which has all that built in.

0:32:07	SPEAKER_03
 So if you look at that and let us know what you think, I mean, I think we're sort of getting things because I want to get the property work done, but I don't want to waste time getting the, yeah?

0:32:19	SPEAKER_04
 Well, I wouldn't wait for the formats because anything you pick will be able to translate to another form.

0:32:25	SPEAKER_01
 Well, maybe you should actually look at it yourself to get a sense of what is your dealing with because Adam might have one opinion.

0:32:37	SPEAKER_01
 I definitely do.

0:32:38	SPEAKER_01
 Yeah, definitely.

0:32:39	SPEAKER_01
 I think the more eyes look at this, the better.

0:32:42	SPEAKER_03
 Especially if there's, you know, if someone can help with at least the setup of the right.

0:32:49	SPEAKER_03
 Okay.

0:32:50	SPEAKER_00
 The right representation.

0:32:54	SPEAKER_03
 Then, you know, I hope it won't, we don't actually need the whole full blown.

0:32:59	SPEAKER_03
 Thank you.

0:33:00	SPEAKER_03
 Thank you.

0:33:01	SPEAKER_03
 So maybe if you guys can look at it and see what I think we're, I think we're about to end.

0:33:13	SPEAKER_03
 Yeah, wrapping up.

0:33:14	SPEAKER_03
 But yeah, sorry, it's a short meeting, but why don't I, is there anything else like that helps me?

0:33:22	SPEAKER_04
 Well, I think the other thing we might want to look at is alternatives to P file.

0:33:26	SPEAKER_04
 I mean, the reason I like P files, I'm already familiar with it, we have expertise here.

0:33:30	SPEAKER_04
 And so if we pick something else, there's learning curve problem.

0:33:33	SPEAKER_04
 But I mean, it is just something we developed at XE.

0:33:36	SPEAKER_01
 And so, is there an API?

0:33:40	SPEAKER_04
 Yeah, there's an API for it.

0:33:42	SPEAKER_01
 And a bunch of libraries, P file utilities.

0:33:45	SPEAKER_01
 And so basically the file system,

0:33:49	SPEAKER_04
 Well, that's going to be a problem, no matter what. You have the two gigabyte limit on the file system size.

0:33:53	SPEAKER_04
 And we definitely hit that with broadcast news.

0:33:57	SPEAKER_01
 Maybe you could extend the API to a support, like, splitting up, you know, conceptually one file into smaller files on disk so that you can essentially.

0:34:07	SPEAKER_04
 Yeah, you know, most of the tools can handle that so that we didn't do it at the API level.

0:34:12	SPEAKER_04
 We did it at the tool level that most many of them can, you can specify several P files.

0:34:17	SPEAKER_04
 And they'll just be done sequentially.

0:34:21	SPEAKER_03
 So I guess, yeah, if you and Don can, if you can show them the P file stuff.

0:34:26	SPEAKER_03
 Sure.

0:34:27	SPEAKER_04
 So this may be like that.

0:34:28	SPEAKER_04
 So it's a file or apropos P file you see.

0:34:32	SPEAKER_00
 I've looked at it at least briefly.

0:34:35	SPEAKER_00
 I think what we were doing.

0:34:37	SPEAKER_04
 I have no idea.

0:34:41	SPEAKER_04
 I didn't do it.

0:34:42	SPEAKER_04
 I didn't develop it.

0:34:43	SPEAKER_04
 You know, I think it was Dave Johnson.

0:34:46	SPEAKER_01
 So it's all part of the quick net library has all the utilities for.

0:34:51	SPEAKER_01
 No, P files were around.

0:34:52	SPEAKER_01
 Maybe for quick net.

0:34:53	SPEAKER_01
 Oh, we're there.

0:34:54	SPEAKER_01
 Rapp, right?

0:34:58	SPEAKER_05
 You work with P files.

0:35:01	SPEAKER_05
 I don't remember what the P is.

0:35:05	SPEAKER_04
 But there are not there.

0:35:07	SPEAKER_04
 The quick net library has a bunch of things in it to handle P files so it works pretty well.

0:35:12	SPEAKER_03
 And that isn't really, I guess, as important as the main, I don't know what you call it.

0:35:17	SPEAKER_03
 The main third or third?

0:35:18	SPEAKER_05
 It only stands for fill.

0:35:21	SPEAKER_05
 Fill cone.

0:35:23	SPEAKER_05
 Is it fill file?

0:35:26	SPEAKER_03
 That's my guess.

0:35:29	SPEAKER_03
 Well, that's really useful.

0:35:30	SPEAKER_03
 I mean, this is exactly the kind of thing that wanted to set up.

0:35:36	SPEAKER_04
 You have been meaning to look at the whole stuff again anyway.

0:35:40	SPEAKER_03
 I guess it's also sort of a political decision.

0:35:43	SPEAKER_03
 I mean, if you feel like that's a community that would be good to tie into anyway, then it's something we're doing.

0:35:51	SPEAKER_04
 And as I said, what I did with this stuff, I based it on theirs.

0:35:55	SPEAKER_04
 It's just they hadn't actually come up with an external format yet.

0:35:58	SPEAKER_04
 So now that they have come up with a format, it seems pretty reasonable to use it.

0:36:04	SPEAKER_04
 But let me look at it again.

0:36:05	SPEAKER_03
 Okay, great.

0:36:06	SPEAKER_03
 As I said.

0:36:07	SPEAKER_04
 Because there's one level, there's one more level of indirection and I'm just blanking on exactly how it works.

0:36:12	SPEAKER_04
 I got to look at it again.

0:36:15	SPEAKER_03
 And we can start with, I guess, this input from Dave's, which you had printed out the channelized input.

0:36:22	SPEAKER_03
 Because he has all of the channels with the channels and the tag and stuff like that.

0:36:27	SPEAKER_03
 So that would be directly easy and wrapped.

0:36:31	SPEAKER_03
 And so then it would just be a matter of making sure to handle the annotations that are not at the word level.

0:36:39	SPEAKER_03
 Where are those annotations coming from?

0:36:41	SPEAKER_03
 Well, right, Jane would.

0:36:45	SPEAKER_02
 Are you talking about the overlap on annotations?

0:36:49	SPEAKER_03
 Yeah, any kind of annotation that isn't already there.

0:36:53	SPEAKER_03
 Anything in vision.

0:36:55	SPEAKER_02
 So what I was imagining was, so Dave says, we can have unlimited numbers of green ribbons.

0:37:00	SPEAKER_02
 And so put a green ribbon on for an overlap code.

0:37:04	SPEAKER_02
 And since we, I think it's important to remain flexible regarding the time bins for now.

0:37:11	SPEAKER_02
 And so it's nice to have, however, you know, you want to have it time located in the discourse.

0:37:20	SPEAKER_02
 So if we tie the overlap code to the first word in the overlap, then you'll have a time marking.

0:37:29	SPEAKER_02
 It'll be independent of the time bins, however, these evolve shranker, whatever increase, or also you could have different time bins for different purposes.

0:37:37	SPEAKER_02
 And having it tied to the first word in an overlap segment is unique, you know, anchored, clear.

0:37:45	SPEAKER_02
 And it would just end up on a separate ribbon.

0:37:47	SPEAKER_02
 So the overlap code is going to be easy with respect to that.

0:37:52	SPEAKER_02
 It'll be puzzled.

0:37:53	SPEAKER_05
 I just, I don't quite understand what these things are.

0:37:56	SPEAKER_05
 What codes themselves?

0:37:58	SPEAKER_05
 Overlap codes, I'm not sure what they're.

0:38:00	SPEAKER_05
 Well, I mean, it's that.

0:38:01	SPEAKER_05
 Well, it probably doesn't matter.

0:38:03	SPEAKER_02
 It doesn't mean it doesn't.

0:38:04	SPEAKER_02
 It's not for the topic of this meeting.

0:38:06	SPEAKER_02
 No, the idea is just to have a separate green ribbon, you know, and let's say this is the time bin.

0:38:11	SPEAKER_02
 There's a word here.

0:38:12	SPEAKER_02
 It's the first word of an overlap segment of any length overlapping with any other word, is segment of any length.

0:38:19	SPEAKER_02
 And then you can indicate that this here was perhaps a back channel, or you can say that it was usurping of the journey, or you could, you know, any number of categories.

0:38:29	SPEAKER_02
 But the fact is you have a time tag in a way this independent of the particular time bin that the word ends up in, if it's a large unit or a small unit, or we change the boundaries of the units.

0:38:40	SPEAKER_02
 Right?

0:38:41	SPEAKER_02
 It's still unique and fits with the format flexible on all that.

0:38:48	SPEAKER_01
 It would be nice.

0:38:50	SPEAKER_01
 This is sort of regarding, it's related but not correctly to the main topic of discussion.

0:38:56	SPEAKER_01
 But when it comes to annotations, you often find yourself in the situation where you have different annotations of the same word sequence.

0:39:10	SPEAKER_01
 Okay?

0:39:11	SPEAKER_01
 And sometimes the word sequence is even differ slightly because they were edited at one place, but not the other.

0:39:17	SPEAKER_01
 So once this data gets out there, some people might start annotating this for, I don't know, the ILR acts or, you know, topics or what the heck, you know, there's a zillion things that people might annotate this for.

0:39:35	SPEAKER_01
 And the only thing that is really sort of common among all the various versions of this data is the word sequence, or the proximity.

0:39:45	SPEAKER_01
 For the time.

0:39:46	SPEAKER_01
 Or the times, but see if you annotate dialogue acts, you don't necessarily want to, or topics, you don't really want to be dealing with time marks.

0:39:53	SPEAKER_01
 It's much more efficient for them to just see the word sequence, right?

0:39:57	SPEAKER_01
 I mean, most people aren't as sophisticated as we are here with, you know, time alignments and stuff.

0:40:04	SPEAKER_01
 So the point is...

0:40:07	SPEAKER_04
 Did you mention some names on the...

0:40:10	SPEAKER_01
 So my point is that you're going to end up with word sequences that are differently annotated.

0:40:19	SPEAKER_01
 And you want some tool that is able to sort of merge these different annotations back into a single version.

0:40:30	SPEAKER_01
 Okay.

0:40:32	SPEAKER_01
 And we had this problem very massively at SRI when we worked a while back on dialogue acts as well as, you know, what was it?

0:40:45	SPEAKER_01
 Well, all those things were...

0:40:46	SPEAKER_01
 At times, automatic punctuation and stuff like that.

0:40:51	SPEAKER_01
 Because we had one set of annotations that were based on one version of the transcripts with a particular segmentation.

0:41:00	SPEAKER_01
 And then we had another version that was based on a different slightly edited version of the transcripts with a different segmentation.

0:41:08	SPEAKER_01
 So we had these two different versions which were, you know, you could tell they were from the same source, but they weren't identical.

0:41:16	SPEAKER_01
 So it was extremely hard to reliably merge these two back together to correlate the information from the different annotations.

0:41:23	SPEAKER_04
 Yeah. I don't see any way that file formats are going to help us with that.

0:41:26	SPEAKER_04
 No.

0:41:27	SPEAKER_04
 It's all a question of semantic.

0:41:28	SPEAKER_01
 But once you have a file format, I can imagine writing, not personally, but someone writing a tool that is essentially an alignment tool that mediates between various versions.

0:41:41	SPEAKER_01
 And sort of like, you know, you have this thing in Unix where you have...

0:41:45	SPEAKER_01
 Diff.

0:41:46	SPEAKER_01
 W-diff, or...

0:41:47	SPEAKER_01
 There's the diff that actually tries to reconcile different...

0:41:52	SPEAKER_01
 And two diffs based on the same original. Something like that. But operating on these lattices that are really what's behind this...

0:42:02	SPEAKER_01
 Yeah.

0:42:03	SPEAKER_04
 There's actually a diff library you can use to do things like that.

0:42:08	SPEAKER_01
 So somewhere in the A4, you would like to have like a merge or some function that merges two...

0:42:15	SPEAKER_04
 Yeah, I think it's going to be very hard. Any sort of structured, anything when you try to merge is really, really hard because the hard part isn't the file format. The hard part is specifying what you mean by merge.

0:42:27	SPEAKER_03
 But the one that would work here actually for is that it's more reliable than the utterances is the speaker on and off.

0:42:36	SPEAKER_03
 So if you have a good...

0:42:38	SPEAKER_04
 But this is exactly what I mean. Is that the problem...

0:42:41	SPEAKER_04
 What to tie it to? Yeah, exactly. The problem is saying, what are the semantics? What do you mean by merge?

0:42:47	SPEAKER_01
 So just to let you know where we clued it by doing both were based on words.

0:42:56	SPEAKER_01
 So we had two versions of the same words, sprinkles with different tags.

0:43:02	SPEAKER_04
 And then you did diff.

0:43:03	SPEAKER_04
 And we did diff.

0:43:04	SPEAKER_04
 Yeah, that's just what I thought.

0:43:05	SPEAKER_04
 And that's just how I would have done it.

0:43:07	SPEAKER_01
 But you know, it had lots of errors and things would end up in the wrong order and so forth.

0:43:12	SPEAKER_01
 So if you had a more...

0:43:16	SPEAKER_01
 It was a clued because it was basically reducing everything to...

0:43:20	SPEAKER_01
 Textual.

0:43:21	SPEAKER_01
 To textual alignment.

0:43:23	SPEAKER_03
 But isn't that something where...

0:43:26	SPEAKER_03
 Whoever...

0:43:27	SPEAKER_03
 If the people who are making changes say in the transcripts, because this all happened when the transcripts were different.

0:43:34	SPEAKER_03
 If they tie it to something, like if they tied it to the acoustic segment.

0:43:39	SPEAKER_03
 If they...

0:43:40	SPEAKER_03
 You know what I mean?

0:43:41	SPEAKER_03
 If they tied it to an acoustic segment and we had the time march that would help.

0:43:45	SPEAKER_03
 But the problem is that exactly as Adam said that you get...

0:43:49	SPEAKER_03
 You know, you don't have that information or it's lost in the merge somehow.

0:43:52	SPEAKER_03
 Okay, one question.

0:43:53	SPEAKER_02
 It seems to me that we will have the official version of the corpus, which will be only one version in terms of the words.

0:44:00	SPEAKER_02
 We still have an emerging issue, maybe, of coding for done independently of the...

0:44:05	SPEAKER_01
 And you're going to get that because if the data gets out, people will do all kinds of things to it.

0:44:11	SPEAKER_01
 And you know, several years from now, you might want to look into the prosody of referring expressions.

0:44:22	SPEAKER_01
 And someone at the University of Who knows where has annotated the referring expressions.

0:44:28	SPEAKER_01
 So you want to get that annotation and bring it back in line with your data.

0:44:33	SPEAKER_04
 But unfortunately, they've also handed it to you.

0:44:35	SPEAKER_03
 But they've also handed it to you.

0:44:36	SPEAKER_03
 And so that's exactly what we should somehow when you distribute the data, say that you know, that have some way of knowing how to merge it back in and asking people to write it.

0:44:45	SPEAKER_05
 What's wrong with doing times?

0:44:48	SPEAKER_03
 Yeah, time is the...

0:44:50	SPEAKER_03
 Well...

0:44:51	SPEAKER_01
 Time is passage.

0:44:52	SPEAKER_01
 Time...

0:44:53	SPEAKER_01
 Times are...

0:44:55	SPEAKER_01
 What if they haven't notated?

0:44:56	SPEAKER_04
 He's a language-miling person.

0:44:58	SPEAKER_04
 So imagine, I think his example is a good one.

0:45:01	SPEAKER_04
 Imagine that this person who developed the corpus of the referring expressions didn't include time.

0:45:06	SPEAKER_04
 He included references to words.

0:45:08	SPEAKER_04
 He said that at this word is when it happened.

0:45:11	SPEAKER_03
 But still they...

0:45:13	SPEAKER_03
 They figure out the time.

0:45:14	SPEAKER_03
 Exactly.

0:45:15	SPEAKER_04
 Sure, but what if they change the words?

0:45:17	None
 Well, some angry point.

0:45:20	SPEAKER_05
 You couldn't change the word.

0:45:21	SPEAKER_04
 Sure, but they can't change the time of the word.

0:45:23	SPEAKER_04
 The point is that they may have annotated it off a word transcript.

0:45:28	SPEAKER_04
 That isn't the same as our word transcript.

0:45:30	SPEAKER_04
 So how do you merge it back?

0:45:31	SPEAKER_04
 And I understand what you're saying.

0:45:33	SPEAKER_04
 And I guess the answer is...

0:45:36	SPEAKER_04
 It's going to be different every time.

0:45:38	SPEAKER_04
 It's just going to be...

0:45:40	SPEAKER_03
 You only know the boundary.

0:45:41	SPEAKER_04
 Exactly what I said before, which is that what do you mean by merge?

0:45:44	SPEAKER_04
 So in this case where you have the words and you don't have the times, well, what do you mean by merge?

0:45:49	SPEAKER_04
 If you tell me what you mean, I can write a program to do it.

0:45:51	SPEAKER_03
 Right, you can merge at the level of the representation that the other person preserved.

0:45:55	SPEAKER_03
 Right, and that's not all you can do.

0:45:57	SPEAKER_03
 And that's not all you can do.

0:45:58	SPEAKER_03
 It's relative ordering and sometimes even that is wrong.

0:46:00	SPEAKER_04
 So in this one you would have to do a best match between the word sequences.

0:46:05	SPEAKER_04
 Extract the times from the best match of theirs to yours.

0:46:09	SPEAKER_04
 And use that.

0:46:10	SPEAKER_03
 And then it's for that their time marks are somewhere in between, yeah, exactly.

0:46:12	SPEAKER_02
 But it could be that they just...

0:46:14	SPEAKER_02
 It could be that they chumped.

0:46:16	SPEAKER_02
 They lost certain utterances.

0:46:17	SPEAKER_02
 Right, exactly.

0:46:18	SPEAKER_04
 So it could get very, very ugly.

0:46:19	SPEAKER_03
 Definitely.

0:46:20	SPEAKER_03
 All right, well, I guess I didn't want to keep people too long and Adam wanted people...

0:46:25	SPEAKER_03
 I'll read the digits if anyone else offers to that be great.

0:46:29	SPEAKER_03
 And it's not, I guess.

0:46:30	SPEAKER_03
 More digits the better.

0:46:31	SPEAKER_03
 Okay, this is.

0:46:32	SPEAKER_03
 I think a lot is really helpful.

0:46:33	SPEAKER_03
 I mean Adam and Don will sort of meet and I think that's great.

0:46:37	SPEAKER_03
 Great.

0:46:38	SPEAKER_04
 Transcript 2731-2750.

0:46:40	SPEAKER_04
 850-51950-61.

0:46:44	SPEAKER_04
 0780-1202.

0:46:48	SPEAKER_04
 443-4427-556-6600985-765-8849-00191-15614-2748560-34450567.

0:47:10	SPEAKER_05
 Transcript 1511-1530.

0:47:14	SPEAKER_05
 01066-739-310-5881-4538-59434-661134-87011-1243-80729.

0:47:34	SPEAKER_05
 Scratch that.

0:47:36	SPEAKER_05
 90729-01-1394-225-3867-4532-4734-875-2543.

0:47:43	SPEAKER_05
 3870-45722-810-951-0980.

0:47:56	SPEAKER_02
 Transcript 1451-1470-9520-0304-710-103-3307-3240104-6201-751-8512455-9571-0049-0401-103-5401-103-6201-103-6201-751-8512455.

0:48:22	SPEAKER_02
 9571-0049-01081790-33613-44580-5251-660-774-378.

0:48:40	SPEAKER_03
 Transcript 33313350-1377810-2595-384892-5605-9103-08583-1401-2303030-63030-7651-6601-10310-510.

0:49:05	SPEAKER_03
 836-406-130-765-837-993-0.

0:49:15	SPEAKER_01
 Transcripts 36-11-36-30-225-2590-342-456-708-9482-6-03-08801-234.

0:49:34	SPEAKER_01
 6-1-757-8453-791-9570-0025-010.

0:49:44	SPEAKER_00
 Transcript 2971-2990-7798990-902-0101-101-245-2-37445-67091460-7601-126.

0:50:03	SPEAKER_00
 460-039-4-01819-958-283-7590-340-7362626-7301.

