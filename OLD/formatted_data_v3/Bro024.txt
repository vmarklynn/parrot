Speaker I: And we're on.
Speaker G: Okay.
Speaker G: When I close the door, I get it, uh, stuff in it all.
Speaker I: Hey Dave, you go ahead and turn on that stuff on.
Speaker G: So that's the virtual stuff.
Speaker I: You see for recording or?
Speaker I: Yeah, learning spots.
Speaker I: It's got like 16 channels.
Speaker I: The quality is quite good though.
Speaker I: Yeah, it's up to 30, pretty good.
Speaker G: So, uh, yeah, it's just, I must have.
Speaker A: Let's go ahead and start.
Speaker A: Okay.
Speaker A: So, yeah, this past week I've been mainly occupied with, um, getting some results from the SRI system trained on this short Hub 5 training set for the mean subtraction method.
Speaker A: I've done some tests last night, but, um, the results are suspicious. Um, it's, um, because they're the baseline results are worse than, um, Andreas, the results Andreas got previously.
Speaker A: And it could have something to do with, um, that's on digits.
Speaker A: That's on digits.
Speaker A: It could, it could have something to do with, um, down sampling. That's, that's worth looking into.
Speaker A: Um, and, um, a part of that, I guess, the main thing I have to talk about is, um, where I'm planning to go over the next week.
Speaker A: So, I've been working on integrating this mean subtraction approach into the smart com system.
Speaker A: And there's this question of, well, so, um, in my test before with HDK, I found it worked, it worked the best with about 12 seconds of data used to estimate the mean, but we'll often have less in the smart com system.
Speaker A: Um, so I think we'll use as much data as we have at a particular time, and we'll, we'll concatenate utterances together, um, to get as much data as we possibly can from the user.
Speaker A: But, um, there's a question of how to set up the models. So, um, we could train the models.
Speaker A: If we think 12 seconds is ideal, we could train the models using 12 seconds to calculate the mean to mean subtract the training data, or we could, um, use some other amount.
Speaker A: So, like I did an experiment where I, um, was using six seconds in test.
Speaker A: Um, but I tried 12 seconds in train, and I tried, um, the same in train. I tried six seconds in train, and six seconds in train was about 0.3% better.
Speaker A: Um, and, um, it's not clear to me yet, whether that's something significant. So, I want to do some tests and, um, actually make some plots of, um, for a particular amount of data in test, what happens if you vary the amount of data in train.
Speaker G: I don't know if you'd follow the stuff, but this is, uh, a, uh, long term, long term window, FFTs. Yeah, we'll be talking about it.
Speaker A: So, I was, I actually ran the experiments mostly. You know, I was, I was hoping to have the plots with me today. I just didn't get to it. But, um, yeah, I would be curious about people's feedback on this, because I'm, I think there are some, I think this is kind of like a bit of a tricky engineering problem, trying to figure out what's the optimal way to set this up.
Speaker A: So, um, I'll try to make the plots and then put some post script up on my, on my web page, and I'll mention my status report if you want to take a look.
Speaker G: You can clarify something for me. You're saying 0.3%. You take a 0.3% hit when the training and testing links aren't don't match or something. Is that what it is?
Speaker A: Well, I don't think it's just for any mismatch. Yeah. Take a hit. In some cases, it might be better to have a mismatch. Yeah. Like, I think I saw something like, like if you only have two seconds in test, or, um, maybe it was something like four seconds, you actually do a little better if you, um, train on six seconds. And if you train on four seconds.
Speaker A: Um, but the case that with the 0.3% hit was using six seconds in test, um, comparing train on 12 seconds versus train on six seconds, which was worse, the train on 12 seconds.
Speaker G: Okay, but 0.3% from what to what? That's 0.3%.
Speaker A: Um, the, the accuracies went from it was something vaguely like 95.6 accuracy, um, improved to 95.9. What I, what I, 4.4 to 4.1.
Speaker G: Okay. So, yeah. So about about an 8%, uh, 78% relative. Okay.
Speaker G: Um, yeah. Well, I think, you know, if you're going for an evaluation system, you'd care, but if you were doing a live system of people are actually using nobody would notice, I think the thing is to get something that's practical.
Speaker A: That's interesting. I see a point. I guess I was thinking of it as, um, an interesting research problem. Yeah.
Speaker A: I was thinking for the ASRU paper, we could have a section saying for smart com, we, we tried this approach in an interactive system, which I don't think has been done before.
Speaker A: And, and then there was two research questions from that. And one is that does it still work if you just use the past history.
Speaker A: And the other was this question of, um, that was just talking about now. So I guess that's why I thought it was interesting.
Speaker G: So, um, the time FFT, short time Keptstrom calculation, uh, mean, mean calculation work that people have in commercial systems, they do this all the time.
Speaker G: They, they calculated from previous utterances. Yes. But, but, uh, as you say, there hasn't been that much of this long, long time, uh, specter work.
Speaker G: Oh, oh, okay. So that's, that's, that's standard. Uh, pretty common. Yeah. Okay. Um, but, uh, yeah. So it is interesting.
Speaker G: I mean, there's two sides to these really small, uh, gradations and performance. Um, I mean on the one hand, in a practical system, if something is, uh, 4.4% error, 4.1% error, people won't really tell it to be able to tell the difference.
Speaker G: On the other hand, when you're doing, uh, research, you may, you might find that the way that you build up a change from a 95% accurate system to a 98% accurate system is through 10 or 12 little things that you do that each are 0.3%.
Speaker G: So, so they, they, it's, I don't mean to say that they're, they're irrelevant. Uh, they are relevant. But, um, for a demo, you won't see it. Right. Okay.
Speaker A: And, um, let's, let's see. Um, okay. And then there's another thing I want to start looking at. Um, with is the choice of the analysis window length. So I've just been using two seconds just because that's what Carlos did before.
Speaker A: Um, I wrote to him asking about how he chose the two seconds and it seemed like he chose it a bit informally. So, um, with it with the HDK setup, I should be able to do some experiments. Um, just varying that length, say between one and three seconds in a few different reverberation conditions.
Speaker A: Say this room and also a few of the artificial impulse responses we have for reverberation, just making some plots and seeing how they look. And, um, so with the sampling rate I was using one second or two seconds or four seconds is a power of two.
Speaker G: Um, number of samples and, um, I'll, I'll do for the ones in between. I guess I'll just zero pad. I guess one thing that might also be an issue. Um, it's part of what you're doing is you're getting a spectrum over a bunch of different kinds of speech sounds.
Speaker G: Um, and so it might matter how fast someone was talking. Oh, you know, if, if, if there's a lot of phones in one second, maybe you'll get a really good sampling of all these different things. And, uh, on the other hand, someone's talking slowly, maybe you need more.
Speaker G: So I don't know if you have some samples of faster or slower speech, but it might make a difference. I don't know.
Speaker A: Yeah, I don't, I don't think the TI digits data that I have, um, it would be appropriate. Yeah.
Speaker A: What would you, what about if I fed it through some kind of, um, speech processing algorithm that changed the speech rate?
Speaker G: Yeah, but then you'll have the degradation of, uh, whatever you do, uh, add it onto that. But maybe, yeah, maybe if you get something that sounds that does a pretty good job at that.
Speaker A: Yeah. Well, I just, if you think it's worth looking into, I mean, it is getting a little away from reverberation.
Speaker G: Um, yeah. It's just that you're making a choice. I was thinking more from the system aspect. If you're making a choice for smart com, that, that, that it might be that it's.
Speaker G: The optimal number could be different. Right. Yeah. Could be.
Speaker A: And then the third thing, um, I was, um, very explained LDA filtering to me yesterday. And so, um, Mike, Sharon is thesis, um, did a series of experiments, um, training LDA filters in different conditions.
Speaker A: And you were interested in having me repeat this for, for this mean subtraction approach. Is that right? Or for these long analysis windows, I guess is the right way to put it?
Speaker G: I guess the, the issue I was, the general issue I was bringing up was that if you're, have a moving, moving window, uh, a set of weights, times things that, uh, move along, shift along in time, that you have, in fact, a linear time and varying filter.
Speaker G: And you just happen to have picked a particular one by setting all the weights to be equal. And so the issue is what are some other filters that you could use in that sense of filter.
Speaker G: And, um, as I was saying, I think the simplest thing to do is not to train anything, but just to do some sort of, uh, hamming or handing, right, kind of window, kind of thing, just sort of the, the emphasize the journey.
Speaker G: So I think that would sort of be the first thing to do. But then, yeah, the LDA is, is interesting because it would sort of say, well, suppose you actually trained this up to do the best you could by some criterion.
Speaker G: What would the filter look like that? And, um, that sort of we're doing in this, uh, Aurora stuff. And, uh, it's still not clear to me in the long run whether the best thing to do would be to do that or to have some stylized version of the filter that looks like these things you've trained up because, um, you always have the problem that it's trained up for one condition isn't quite right for another. So, uh, that's, that's why that's why the rest of the filter is actually ended up lasting a long time.
Speaker G: People still using it quite a bit because you don't change it. So it doesn't get any worse.
Speaker A: Okay, so, um, actually, I was just thinking about what I was asking about earlier, which is about having less than, say, 12 seconds in the smart calm system to do the mean subtraction.
Speaker A: You said in systems where you kept still mean subtraction, they can catenate utterances. And do you know how they address this issue of testing versus training?
Speaker D: I think what they do is they do it always online. I mean that you just take what you have from the past that you calculate some mean of this and subtract some mean.
Speaker D: Okay. Um, then you can, you can increase your window. Why do I get, why do I get more samples?
Speaker A: Okay. Um, and so, so in that, in that case, what, what do they do when they're performing the capital mean subtraction on the training data?
Speaker A: So because you'd have hours and hours of training data. So do they cut it off and start over at intervals or.
Speaker D: So do you have, you mean you have files which are hours of hours alone or?
Speaker D: Well, no, I guess not. I mean, usually you have in the training set, you have similar conditions. I mean, file links are, I guess, the same order or the same size for test data.
Speaker A: Okay. But it's okay. So if someone's interacting with the system, Morgan, Morgan said that you would tend to chain utterances together.
Speaker G: Well, I think what I was, I thought what I was saying was that, um, at any given point, you are going to start off with what you had from before.
Speaker G: From, and so if you're splitting things up into utterances, so for instance, in a dialogue system, where are you going to be asking for some information?
Speaker G: There's some initials of something. And, you know, the first time out, you might have some general average, but you don't have very much information yet.
Speaker G: But after they've given one utterance, you've got something. You can compute your mean capture from that and then can use it for the next thing that they say, so that, you know, the performance should be better that second time.
Speaker G: And I think the heuristics of exactly how people handle that and how they handle their training. I'm sure very from place to place, but I think the ideally it seems to me anyway that you, you would want to do the same thing in training as you do in test.
Speaker G: But that's just a prejudice. And I think anybody, and this with some particular task, we experiment.
Speaker A: I guess the question I had was, um, amount of data was the amount of data that you give it to, um, update this estimate because say you, if you have say 5,000 utterances in your training set, um, and you keep the mean from the last utterance by the time it gets to the 5,000 utterances.
Speaker G: So those are all different people with different, I mean, in, so for instance, in the telephone task, these are different phone calls, so you don't want to chain it together from a different phone call.
Speaker G: Okay, so, so they would, so it's within speaker, within phone call, if the dialogue system, it's within whatever this characteristic you're trying to get rid of is expected to be consistent over.
Speaker A: Right, and, right, okay, so you, in so in training, you would start over at every new phone call or at every new speaker. Yeah, okay.
Speaker G: Now, you know, maybe do something from the others just because at the beginning of a call, you don't know anything.
Speaker G: So you might have some kind of general thing that's your best guess to start with.
Speaker G: So I, you know, a lot of these things are proprietary, so we're doing a little bit of guesswork here.
Speaker G: And what do, what do people do who really face these problems in the field? Well, they have companies, they don't tell other people exactly what they do.
Speaker G: But, but I mean, when you, the hints that you get from what they, when they talk about it, or that they do, they all do something like this.
Speaker A: Right, okay, I see. Because I, so this smart contest, first of all, it's this TV and movie information system.
Speaker G: Yeah, but you might have somebody who's using it. And later you might have somebody else using it.
Speaker A: Right. I see. I was about to say. So if you ask it, what, what movies are on TV tonight?
Speaker A: If I look at my wristwatch, when I say that, it's about two seconds.
Speaker A: Yeah. So when I currently have the mean subtraction, set up the analysis windows two seconds.
Speaker A: So what you just said about what do you start with raises a question of what do I start with then? I guess it, because...
Speaker G: Well, okay. So in that situation, though, maybe it's a little different there is, I think you're talking about there's only one...
Speaker G: It, it, it also depends. We're getting a little off track here.
Speaker G: Oh, right. But, but, but, there's been some discussion about whether the work we're doing in that project is going to be for the kiosk or for the mobile or for both.
Speaker G: And I think for this kind of discussion, it matters. If it's in the kiosk, then the physical situation is the same.
Speaker G: It's going to, you know, the exact interaction with the microphones is going to differ depending on the person and so forth, but at least the basic acoustics are going to be the same.
Speaker G: So if it's really in one kiosk, then I think that you could just chain together and, you know, as much as much speech as possible to, because what you're really trying to get at is the reverberation characteristic.
None: Yeah.
Speaker G: But in, in the case of the mobile, presumably the acoustics is changing all over the place.
Speaker G: Right.
Speaker G: And in that case, you probably don't want to have it be endless, because you want to have some sort of, it's a question of how long do you think it's, you can get an approximation to a stationary something that it's not really stationary.
Speaker G: Right.
Speaker A: Right.
Speaker A: I, I guess I just started thinking of another question, which is for the very first frame, what, what do I do if I, if I take, if I use that frame to calculate the mean, then I'm just going to get nothing.
Speaker A: Right.
Speaker A: Right. So I should probably have some kind of default, mean for the first couple of frames.
Speaker A: Okay.
Speaker G: Or subtract nothing.
Speaker A: I mean, it's, or subtract nothing.
Speaker A: And that's, I guess that's something that's, people have figured out how to deal with in capstone mean subtraction as well.
Speaker G: Yeah, people do something.
Speaker G: They, they, they have some, in, in capstone mean subtraction for short term window analysis windows, as is usually done.
Speaker G: You're trying to get rid of some very general characteristic.
Speaker G: And so, if you have any other information about what a general kind of characteristic would be, then you can do it there.
Speaker I: You can also reflect the data.
Speaker I: So you take, you know, I'm not sure how many frames you need, but if you take that many from the front, flip it around.
Speaker I: Yeah, that's the negative values.
Speaker G: Yeah.
Speaker G: The other thing is that, and I remember BBN doing this is that if you have a multi pass system, if the first pass takes, it takes most of the computation.
Speaker G: The second and the third pass could be very, very quick, just looking at a relatively small, small space of hypotheses.
Speaker G: Then you can do your first pass without any subtraction at all.
Speaker G: And then your second pass eliminates those, most of those hypotheses by, by having improved, improved version of the analysis.
Speaker A: So, so that was all I had.
Speaker A: Yeah.
Speaker F: Okay, so for the past week or two, I've been just writing my formal thesis proposal. So I'm taking this qualifier exam, it's coming up in two weeks.
Speaker F: I finished writing a proposal and submit to the committee.
Speaker F: And should I explain more about what I'm proposing to do this?
Speaker F: Brief.
Speaker F: Okay. So briefly, I'm proposing to do a new approach to speech recognition, a combination of multi band ideas and ideas about the acoustic phonetic approach to speech recognition.
Speaker F: So I will be using these graphical models that implement the multi band approach to recognize a set of intermediate categories that might involve things like phonetic features or other feature things that are more closely related to the acoustic signal itself.
Speaker F: And the hope in all this is that by going multi band and by going into these intermediate classifications that we can get a system that's more robust to unseen noises and situations like that.
Speaker F: And so some of the research issues involved in this are what kind of intermediate categories do we need to classify? Another one is what other types of structures in these multi band graphical models should we consider in order to combine evidence from the sub-ant.
Speaker F: And the third one is how do we merge all the information from the individual multi band classifiers to come up with word recognition or from recognition.
Speaker F: So basically that's what I've been doing two weeks.
Speaker F: I got two weeks to brush up on presentation.
Speaker G: So I thought you were finishing your thesis in two weeks.
Speaker I: Oh that too.
Speaker F: Are you going to do any dry runs for your finger?
Speaker F: Yes.
Speaker F: I'm going to do some.
Speaker F: Would you be interested?
Speaker F: Sure.
Speaker F: I hope that.
Speaker I: Is that it?
Speaker I: That's it.
Speaker I: Okay.
Speaker I: Let's see.
Speaker I: So we've got 40 minutes left.
Speaker I: It seems like there's a lot of material.
Speaker I: Any suggestions about where we should go next?
None: Yeah.
Speaker C: Actually, most of these in our last meeting with Ginter.
Speaker C: But I'll just.
Speaker C: So the last week I showed some results with only speech thread curve, which was like some 56% and I didn't.
Speaker C: I mean, I found out the results.
Speaker C: I mean, I wasn't getting that results on the TI digit.
Speaker C: So I was like looking into what is wrong with the TI digits.
Speaker C: Why I was not getting it and I found that the noise estimation is the reason for the TI digits to perform worse than the baseline.
Speaker C: So I actually, I mean, the first thing I did was I just scaled the noise estimate by a factor which is less than 1 to see if that.
Speaker C: Because I found that a lot of zeros in the spectrogram for the TI digits when I use this approach.
Speaker C: So the first thing I did was I just scaled the noise estimate and I found.
Speaker C: So the results that I have shown here are the complete results using the new, the new technique is nothing but the noise estimate scale by a factor of 0.5.
Speaker C: So it's just an id hoc.
Speaker C: I mean, some intermediate result because it's not optimized for anything.
Speaker C: So the results that trend, the only trend I could see from those results was like the current noise estimation or the noise composition scheme is working good for like the car noise type of thing.
Speaker C: Because I've the only very good result in the TI digits is the noise car noise condition for the test A which is like the best I could see that for any non stationary noise like Babel or subway or any street some restaurant noise.
Speaker C: It's like it's not performing very well.
Speaker C: So that's the first thing I could make out from this stuff.
Speaker D: I think what is important to see is that there is a big difference between the training modes.
Speaker D: If you have clean training, you get also a 50% improvement.
Speaker D: But if you have multi-condition training, you get only 20%.
Speaker C: And in that 20% is very inconsistent across different noise conditions. So I have like a 45% for car noise and then there's a minus 5% for the Babel and there is a 33% for the station.
Speaker C: And so it's not actually very consistent across.
Speaker C: So the only correlation between the speech.car and this performance is the stationarity of the noise that is there in these conditions and the speech.car.
Speaker C: So the overall result is like in the last page which is like 47% which is still very imbalanced because I have like 56% on speech.car and 35% on the TI digits.
Speaker C: And the 56% is like comparable to what the French telecom gets but 35% is way off.
Speaker G: So I can fuse but looking at the second page and it says 50% looking in the lower right hand corner, 50% relative performance.
Speaker D: For the clean training.
Speaker G: Is that if you look at 50% improvement?
Speaker C: That's for the clean training and the noise you're testing for the i digits.
Speaker G: So it's improvement over the baseline melcapstrom. But the baseline melcapstrom under those training doesn't do as well.
Speaker G: I'm trying to understand why it's 80% that's an accuracy number I guess right?
Speaker G: So that's not as good as the one up above.
Speaker G: But the 50 is better than the one up above so I'm confused.
Speaker C: Actually the noise composition whatever we are putting in it works very well for the high mismatched condition.
Speaker C: When it's consistent in the speech.car and in the clean training also it gives but this 50% is that the high mismatched performance.
Speaker C: It's equivalent to the high mismatched performance.
Speaker I: So since the high mismatched performance is much worse to begin with, it's easier to get it.
Speaker C: So by putting this noise.
Speaker E: Yeah if we look at the figures on the right we see that the reference.
Speaker C: The reference drops like a way of life.
Speaker E: Oh my gosh.
Speaker G: This is T.I. digits?
Speaker C: Yeah.
Speaker C: Yeah it's not written anywhere.
Speaker C: It's the first spreadsheet is T.I. digits.
Speaker G: How does clean training do for the car?
Speaker C: The car still that's still consistent.
Speaker C: I mean I get the best performance in the case of car which is the third column in the A condition.
Speaker G: No I mean this is added noise.
Speaker G: I mean this is T.I. digits.
Speaker G: I'm sorry I'm in the multi-language.
Speaker C: That's the next spreadsheet.
Speaker C: So that is the performance for Italian, Finnish and Spanish.
Speaker G: Training condition.
Speaker G: Oh right so clean it corresponds to the high mismatched.
Speaker G: And increase.
Speaker C: That's increase improvement.
Speaker C: That percentage increases the percentage improvement of the baseline.
Speaker G: Which means decrease in order right?
Speaker G: Yep.
Speaker G: Okay so percentage increase means decrease.
Speaker D: Okay.
Speaker D: There was a very long discussion about this on the answer of meeting.
Speaker D: How to calculate it.
Speaker D: I guess you are using finally this key which is that in the spreadsheet.
Speaker C: I'm not changing anything in there.
Speaker C: So yeah so all the HM numbers are very good.
Speaker C: And the percentage they are better than what the French League of Cats.
Speaker C: But the only number that's still I mean which Stefan also guarding is resolved was that medium mismatch of the Finnish which is a very strange situation where we use the we change the proto for initializing the HM.
Speaker C: I mean this is basically because it gets stuck in some local minimum in the training.
Speaker C: That's 75.79 in the Finnish mismatch which is that the 11.96 what do you see?
Speaker G: We have to jiggle it somehow.
Speaker C: Yeah so we start with the difference proto and it becomes 88 which is like some 50% improvement.
Speaker C: We'll start with a different one.
Speaker C: Different prototype which is like a different initialization for the transition probabilities.
Speaker C: The right now the initialization is to stay more in the current state which is 0.4.6 right?
Speaker C: Yeah.
Speaker C: And if you change it to 0.5.5 which is equal the voltage for transition and self-loop when it becomes 88%.
Speaker I: So that involves mucking with the back end?
Speaker I: Yeah we can't do it.
Speaker H: Yeah.
Speaker D: It is well known this medium-match condition of the Finnish data is very strange effects.
Speaker C: It has a very few words also it's very very small said actually.
Speaker D: There is a lot of utterances with music and the background.
Speaker C: It has a musicals.
Speaker C: I mean very audible music like you can.
Speaker G: So maybe for that one you need a much smarter V80.
Speaker C: So that's about the results.
Speaker C: The summary is like okay so the other thing I tried was which I explained in the last meeting is using the channel 0 for both dropping and estimating the noise.
Speaker C: And that's like just to get a feel of how good it is.
Speaker C: And it gets the 56% improvement in the speech that car becomes like 67% like 10% better.
Speaker C: But that's not a cheating experiment.
Speaker D: But the 47.9% which you have now sets already a remarkable improvement in comparison to the first proposal.
Speaker C: Yeah so we had 44% in the first proposal.
Speaker C: So the major improvement that we got was in all the high mismatch cases because all those numbers were in 60s and 70s because we never had in our compensations.
Speaker C: So that's where the biggest improvement came up not much in the well match and the medium match and the iDG it's also right now.
Speaker C: So this is still a 3 or 4% improvement over the first proposal.
Speaker G: Yeah so that's good.
Speaker G: So we can improve the noise estimation.
Speaker D: Yeah I started thinking about that.
Speaker D: I mean I discovered the same problem when I started working on this Aurora task almost two years ago that you have the problem with this multi.
Speaker D: At the beginning we had only this multi-conditioned training of the t iDG and I found the same problem just taking what we were used to use.
Speaker D: I mean some type of spectral subtraction you get even worse results in the basis.
Speaker D: Yeah I tried to find an explanation for it.
Speaker C: Yes Stefan also has the same experience of using the spectral subtraction right.
Speaker C: So here I mean I found that if I change the noise estimate I could get an improvement.
Speaker C: So something which I can actually pursue is the noise estimate.
Speaker D: Yeah I think what you do is when you have this multi-conditioned training mode then you can train models for the speech, for the words as well as for the pauses where you really have all information about the noise available.
Speaker D: It was surprising at the beginning it was also surprising to me that you get really the best results when doing it this way.
Speaker D: I mean in comparison to any type of training on clean data and any type of processing but it was so it seems to be the best what we can do in this moment with this multi-conditioned training.
Speaker D: And when we now start introducing some noise reduction technique we introduce also some how artificial distortions.
Speaker D: And this artificial distortions I have the feeling that they are the reason why we have the problems in this multi-conditioned training.
Speaker D: I mean the HMM suit chains they are based on gossians and modeling gossians.
Speaker D: And can I move a little bit with this?
Speaker D: And if we introduce now this spectral subtraction or wiener filtering stuff.
Speaker D: So usually what you have is maybe I'm showing now an adulope, maybe first time.
Speaker D: So usually in clean condition you have something which looks like this and if it is noisy it is somewhere here.
Speaker D: And then you try to subtract it or wiener filter or whatever.
Speaker D: And what you get is you have always these problems that you have these zeros in there.
Speaker D: And you have to do something if you get these negative values.
Speaker D: I mean this is your noise estimate and you somehow subtract it or do whatever.
Speaker D: And then you have, and then I think what you do is you introduce some artificial distribution in this model.
Speaker D: I mean you train it also this way but somehow there is no longer a gossian distribution.
Speaker D: It is somehow a strange distribution which we introduce with this artificial distortions.
Speaker D: And I was thinking that that might be the reason why you get these problems especially in the multi-conditioned training.
Speaker C: The models are not complex enough to absorb that additional variability that you are introducing.
Speaker E: I also have the feeling that the reason why it doesn't work is that the models are not complex enough.
Speaker E: Because I actually always had a good experience with spectral subtraction, just a straight spectral subtraction algorithm when I was using neural networks, big neural networks which maybe are more able to model strange distributions.
Speaker E: But yeah, then I tried exactly the same spectral subtraction algorithm on these Aurora tasks and it simply doesn't work. It's even the Earths.
Speaker G: We probably should at some point here try the tandem, the system two kind of stuff with this with the spectral subtraction for that reason.
Speaker G: Because again it should do a transformation to the main war maybe. It looks more gossian.
Speaker D: Yeah, I was just yesterday when I was thinking about it.
Speaker D: What we could try to do about it, I mean if you get at this situation that you get this negative failure, you simply set it to zero or to a constant or whatever.
Speaker D: If we would use some random generator which has a certain distribution, not a certain special distribution, we have to think about it.
Speaker D: And so we introduce again some natural behavior in this trajectory.
Speaker C: Very different from speech.
Speaker D: I mean it shouldn't confuse them. Yeah, similar to what you see really in the real noisy situation or in the clean situation but somehow a natural distribution.
Speaker G: This nots again sort of the idea of the additive thing as we head in the J stuff. Basically if you have random data in the time domain, then we look at this spectrum, it's going to be pretty flat.
Speaker G: So just add something everywhere rather than just in those places. It's just a constant.
Speaker D: I think it's just especially in this segment I mean you introduce some very artificial behavior.
Speaker G: Yeah, we see if you add something everywhere it has almost no effect up on top. And it has significant effect down there.
Speaker C: That's true. Those regions are the cost for this big radiation. Those negative values and whatever you get.
Speaker D: We could think how what we could try. It was just an idea.
Speaker G: I think it was noisy. People should just speak up.
Speaker E: Look at the French Silicon proposal. They use some kind of noise addition. They have a random number generator right.
Speaker E: They add noise on the trajectory of the low energy.
Speaker E: Cicero low energy.
Speaker E: But I don't know much effect.
Speaker E: So it is similar to what I think because they have to log energy.
Speaker E: Yeah. And then just generate random number. They have some kind of mean and variance.
Speaker E: And they add this number to the log energy simply.
Speaker C: So the log energy after the cleaning up. So they add a random noise to it.
Speaker G: To the just the energy or to the to the male filter only to the log energy.
Speaker G: So because I mean I think this is most interesting for the male filters.
Speaker G: Right. Or FFTs one of the other.
Speaker D: But they do not apply filtering of the low energy or what?
Speaker D: Like like like spectral subtraction.
Speaker C: No, their filter is in the time domain.
Speaker C: Yeah. So they filter the time signal and then what are they calculate from this low energy?
Speaker C: Yeah. And after that it is almost the same as the baseline system.
Speaker C: And then the final log energy that they get that that they add some random noise.
Speaker G: Yeah, but again it's just log energy as opposed to.
Speaker C: Yeah. So it's not the right. Yeah. It's not the male filter bank output.
Speaker C: This is a log energy computed from the time domain signal or from the male filter banks.
Speaker E: So maybe it's just a way to decrease the importance of this particular parameter in the world feature vector.
Speaker E: If you add noise to one of the parameters you want to flat-ribute.
Speaker C: The variance here reduced.
Speaker G: So it could reduce the dependence on the amplitude.
Speaker D: So maybe.
Speaker C: So. So the other thing is just looking at a little bit on the delay issue where the delay of the system is like 180 millisecond.
Speaker C: So I just just tried another system.
Speaker C: I mean another filter which I've like shown at the end, which is very similar to the existing filter.
Speaker C: Only thing that the phase is like a totally non-linear phase because it's a it's not a symmetric filter anymore.
Speaker C: This is for the all the A. Yeah. So this like so this makes the delay like zero for the LDA because it's completely causal.
Speaker C: So I got actually just the results for the Italian for that and that's like so the 51.09 has become 48.06 which is like 3% relative degradation.
Speaker C: So I have like the 51.09.
Speaker C: So I know how it fares for the other conditions. So it's like a 3% relative degradation.
Speaker D: But is there a problem of the 180 millisecond?
Speaker D: Well, I mean I talked about it with Ufine.
Speaker G: So basically our position is that we shouldn't be unduly constraining the latency at this point because we're all still experimenting with trying to make the performance better in the presence of noise.
Speaker G: There is a minority in that group who is arguing who are arguing for having a further constraining of the latency.
Speaker G: So we're just continuing to keep aware of what the tradeoffs are and you know what do we gain from having longer or shorter latencies.
Speaker G: But since we always seem to at least get something out of longer latencies not being so constrained we're tending to go with that if we're not told we can't do it.
Speaker I: Where was the smallest latency of all the systems last time?
Speaker G: Well, the French Telecom was was was very short latency and they had very good result of 35.
Speaker G: So it's possible to get very short latency but again we're the the approaches that we're using are ones that.
Speaker G: I was just curious about where we are.
Speaker D: But I think this 30 milliseconds it did not include the CDLT calculation.
Speaker C: This is included now.
Speaker C: If they include the Delta it will be additional 40 milliseconds.
Speaker C: Yeah, they're using a nine point window which is like a four on either side which is like.
Speaker C: They didn't include that.
Speaker E: Where does the compression compression and decoding delay comes from?
Speaker C: That's the way the the frames are packed like you have to wait for one more frame to pack because it's the CRC is computed for two frames all years.
Speaker G: Well, they would need that 40 milliseconds also.
Speaker C: No, they actually change the compression scheme altogether.
Speaker C: So they have their own compression and decoding scheme and they know what they have but they have coded zero delay for that because they know they changed it.
Speaker C: The compression they have their own CRC their own error correction mechanism so they don't have to wait more one more frame to know whether the current frame is in error.
Speaker C: So they change the whole thing so that there's no delay for that compression and part also.
Speaker C: Even you have reported actually zero delay for the compression.
Speaker C: I don't know maybe you also have some different.
Speaker D: I think I used this scheme as it was before.
Speaker I: Okay, we've got 20 minutes.
Speaker E: Did you want to go next?
Speaker E: I can go next, you have.
Speaker E: Oh, it's a moment.
Speaker E: Yeah, you have to take my man.
Speaker G: I think I'm confused.
Speaker E: All right, so you have one sheet.
Speaker E: This one is you don't need it.
Speaker E: So you have to take the wall.
Speaker E: The five there should be five sheets.
Speaker G: Okay, I have four now because I left one with Dave because I thought I was dropping one off and passing the other's on.
Speaker G: So no, we're not.
None: Thanks.
Speaker E: Maybe there's not enough.
Speaker E: Yeah, this.
Speaker E: So yeah, there are two figures showing actually the performance of the current VAD.
Speaker E: So it's a new run network based on VLP parameters, which estimate seconds probabilities.
Speaker E: And then I just put a median filtering on this to smooth the probabilities.
Speaker E: I didn't use the scheme that's currently in the proposal because I don't want to.
Speaker E: In the proposal, well, in the system, we want to add like speech frame before every word and a little bit of a couple of frames after also.
Speaker E: But to estimate the performance of the VAD, we don't want to do that because it would artificially increase the.
Speaker E: The false alarm rate of speech detection.
Speaker E: So there is normally a figure for a finish and one for Italian.
Speaker E: And maybe someone has two for Italian because I'm missing one figure there.
Speaker E: No.
Speaker E: Yeah, so one surprising thing that we can notice first is that apparently the speech miss rate is higher than the false alarm rate.
Speaker D: So what is the little curve?
Speaker E: Yeah, there are two curves.
Speaker E: One curves for the close talking microphone, which is the lower curve.
Speaker E: One is for the distant microphone, which has more noise.
Speaker E: It's logical that the performance works.
Speaker E: So as I was saying, the miss rate is quite important, which means that we tend to label speech as silence.
Speaker E: I didn't analyze further yet, but I think it's maybe due to the fricative sounds, which maybe in noisy condition, maybe labeled silence.
Speaker E: And it may also be due to the alignment because, well, the reference alignment, because right now I just used an alignment obtained from a system train on channel zero.
Speaker E: I checked it a little bit, but there might be alignment errors.
Speaker E: Like the fact that the model tend to align their first state on silence and their last state on silence also.
Speaker E: So the reference alignment would label as speech some silence frame before speech and after speech.
Speaker E: This is something that we already noticed before.
Speaker E: So this code also explained the miss rate, maybe.
Speaker D: And this curve has the average over low database law?
Speaker E: Yeah, right.
Speaker E: And the different points of the curve are for five thresholds on the probability from point three to point seven.
Speaker E: So the threshold is the first threshold on the probability that puts the values to zero or one.
Speaker E: And then the median filtering.
Speaker C: So the median filtering is fixed.
Speaker E: You just change the threshold.
Speaker E: So going from channel zero to channel one, almost double the error rate.
Speaker E: Well, so it's a reference performance that we can know if you want to work on the VAD.
Speaker E: We can work on this basis.
Speaker F: Okay.
Speaker F: Is this VAD MLP?
Speaker F: Yeah.
Speaker E: How big?
Speaker E: It's a very big one.
Speaker E: I don't remember.
Speaker C: 350 inputs, 1000 hidden inputs and 12 outputs.
Speaker G: Middle sized one.
Speaker H: Yeah.
Speaker E: I don't know if you have questions about that or suggestions.
Speaker E: It seems the performance seems worse on finish.
Speaker E: Well, it's not trained on finish.
Speaker G: What's the train?
Speaker C: I mean, the MLP is not trained on finish.
Speaker C: Right.
Speaker C: What's the train?
Speaker C: Oh, sorry.
Speaker C: It's Italian.
Speaker C: Yeah, it is.
Speaker C: Oh, it's train.
Speaker C: Yeah.
Speaker H: Okay.
Speaker E: And also there are funny noises on finish more than on Italian.
Speaker E: I mean, like music.
Speaker E: Yeah, that's true.
Speaker E: So, yeah, we were looking at this.
Speaker E: But for most of the noises, noises are...
Speaker E: I don't know if we want to talk about that.
Speaker E: But the car noises are below like 500 Earth.
Speaker E: And we were looking at the music utterances.
Speaker E: And in this case, the noise is smaller about 2000 Earths.
Speaker E: Well, music energy is very low, apparently, from 0 to 2000 Earths.
Speaker E: So maybe just looking at this frequency range from 500 to 2000 would improve somewhat the VD.
Speaker H: Yeah.
Speaker C: So the world...
Speaker C: Some parameters we wanted to use or something.
Speaker D: So is the training based on this label files which you take as reference?
Speaker D: Well, it trains the neural net.
Speaker E: No, it's not.
Speaker E: It was trained on some alignment obtained.
Speaker E: For the Italian data, I think we trained a neural network with embedded training.
Speaker E: So, re-estimation of the alignment using the neural net work, yes.
Speaker E: Right?
Speaker C: Yeah, we actually trained on the Italian training part when we had another system.
Speaker E: So it was a phonetic classification system for the Italian or data.
Speaker E: For the other data that it was trained on, it was different. Like for the IDGETs, you used a word, a previous system that you added.
Speaker E: Yeah, yeah, that's true.
Speaker E: So the alignments from the different database that are used for training came from different system.
Speaker E: So system, then we put them together and you put them together and train the real.
Speaker E: Yeah.
None: Yeah.
Speaker E: But did you use channel...
Speaker E: Did you align channel one or so?
Speaker C: I just took the entire Italian training part.
Speaker C: So it was both channel zero plus channel one.
Speaker E: So the alignments might be wrong on channel one.
Speaker E: You know what?
Speaker E: It's possible.
Speaker E: So we might...
Speaker E: Yeah.
Speaker E: We can do a real alignments.
Speaker E: So these ones to retrain these alignments, which should be better because they come from close to working.
Speaker E: Yeah, that was my idea.
Speaker D: I mean, if it's not the same labeling, which is taking the spaces.
Speaker D: Yeah, possible.
Speaker D: Yeah.
Speaker C: I mean, so the system, so the VAD was trained on maybe different set of labels for channel zero and channel one.
Speaker C: Because the alignments were different for...
Speaker C: Certainly different because they were independently trained.
Speaker C: We didn't copy the channel zero alignments to channel one.
Speaker H: Yeah.
Speaker C: But for the new alignments, what you generated, you just copy the channel zero to channel one, right?
None: Yeah.
Speaker E: And actually, when we look at the VAD for some utterance, it's almost perfect.
Speaker E: I mean, just drop one frame, the first frame of speech.
Speaker E: So there are some utterances where it's almost 100% VAD performance.
Speaker E: But...
Speaker E: Yeah.
Speaker E: So the next thing is...
Speaker E: I have the spreadsheet for three different systems.
Speaker E: But for this, you only have to look right now on the speech data curve performance because I didn't test...
Speaker E: So I didn't test the spectrosuppraction on the IDGT yet.
Speaker E: So you have three sheets.
Speaker E: One is the proposal one system.
Speaker E: Actually, it's not exactly a proposal one.
Speaker E: It's the system that Sony will just describe.
Speaker E: But with Wiener filtering from...
Speaker E: Franz Telekom included.
Speaker E: So this gives like 57.7% error rate reduction on the speech data curve data.
Speaker E: And then I have two sheets where it's for a system where...
Speaker E: So it's again the same system, but in this case, we have spectrosuppraction with a maximum overestimation factor of 2.5.
Speaker E: There is smoothing of the gain trajectory with some kind of low pass filter, which has 40 milliseconds latency.
Speaker E: And then after subtraction, I add a constant to the energies.
Speaker E: And I have two cases where the first cases were the constant is 25 dB below the mean speech energy and the other is 30 dB below.
Speaker E: For this two system, we have like 55.5% improvement and 58.1.
Speaker E: So again, it's around 56.57.
Speaker G: As I know, the TI digit number is exactly the same for this last two.
Speaker E: Yeah, because I didn't...
Speaker E: For the Franz Telekom spectrosuppraction, including in our system, the TI digit number are the right one, but not for the other system.
Speaker E: Because I didn't test it yet, this system, including with spectrosuppraction on the TI digit data.
Speaker E: I just tested it on speech.com.
Speaker E: Ah, so that means the only thing...
Speaker E: You have two.
Speaker G: You just should look at that 58.09%.
Speaker G: Okay, good.
Speaker C: So by reducing the noise, the addition threshold to like minus 30 dB is like...
Speaker C: You are like reducing the flow of the...
Speaker C: No, it's a region.
Speaker E: It's a flow of slower.
Speaker G: I'm sorry, so when you say minus 25 or minus 30 dB with respect to what?
Speaker E: To the average speech energy, which is estimated on...
Speaker G: Okay, so basically you're creating a signal-to-noise ratio of 25 or 30 dB.
Speaker D: But I think what you do is...
Speaker D: When you have this...
Speaker D: After you subtract it, I mean, then you get something with this...
Speaker D: When you set the values to zero, and then you simply add an additive constant again.
Speaker D: So you shifted somehow, this whole curve is shifted again.
Speaker G: But did you do that before the thresholding to zero?
Speaker E: It's after the thresholding.
Speaker E: Oh, so you really want to do it before, right?
Speaker E: Maybe you might do it before.
Speaker G: Yeah, because then you would have lots of that phenomenon.
Speaker G: Yeah.
Speaker G: I think.
Speaker E: But still, when you do this and you take the log after that, it reduces the variance.
Speaker E: Right.
Speaker G: Yeah, that will reduce the variance that will help.
Speaker G: But maybe if you did it before, you get lots of these funny looking things.
Speaker G: He's trying.
Speaker C: But before, it's like adding this...
Speaker G: Right at the point where you're doing the subtraction.
Speaker G: Okay.
Speaker G: Essentially, you're adding a constant into everything.
Speaker D: But the way you step on it, it is exactly the way I've implemented it, the fold.
Speaker G: Oh, you better do it different than that.
Speaker G: Just use a set it for a particular signal-to-noise ratio.
Speaker G: Set you what?
None: Yeah.
Speaker D: I made a similar investigation lecture for that year.
Speaker D: Just adding this constant and looking...
Speaker D: How do you paint it?
Speaker D: Is it on the value of the constant?
Speaker D: Yeah.
Speaker D: So, those students are more to give on average the best result.
Speaker D: Yeah.
Speaker D: So, a range of a signal-to-noise ratio.
Speaker H: Oh, it's clear.
Speaker E: I should have given other results.
Speaker E: So, it's clear.
Speaker E: When you don't unknow, it's much worse, like around 5% worse, I guess.
Speaker E: And if you add too much noise, it gets worse.
Speaker E: So, and it seems that right now, this is a constant that does not depend on anything that you can learn from the utterance.
Speaker E: It's just a constant noise addition.
Speaker E: And I think...
Speaker G: I'm confused.
Speaker G: I thought you were saying it doesn't depend on the utterance, but I thought you were adding an amount that was 25 dB down from the signal-to-noise ratio.
Speaker E: Yeah. So, the way I did that, I just measured the average speech energy of the old Italian data.
Speaker E: Oh!
Speaker E: Then I use this as mean speech energy.
Speaker G: Oh, it's just a constant amount.
Speaker E: And overall...
Speaker E: I observe that for Italian and Spanish, when you go to 30 and 25 dB, it's good.
Speaker E: It stays in this range.
Speaker E: Well, the performance of this algorithm is quite good.
Speaker E: But for Finnish, you have a degradation already when you go from 35 to 30 and then from 30 to 25.
Speaker E: And I have the feeling that maybe it's because just Finnish has a mean energy that's lower than the other databases.
Speaker E: And due to this, the threshold should be...
Speaker E: Yeah.
Speaker E: The noise addition should be lower.
Speaker G: But in the real thing, you're not going to be able to measure what people are doing over half an hour or an hour or anything, right?
Speaker G: So, you have to come up with this number from something else.
Speaker D: But you're not doing it now, like, which dependent or?
Speaker E: It's not. It's just something that's fixed.
Speaker G: What he is doing language dependent is measuring what that number references that he comes down to 25.
Speaker E: No, because I did it.
Speaker E: I started working on Italian.
Speaker E: I obtained this average energy.
Speaker E: Yeah.
Speaker E: Then I used this one.
Speaker C: For all languages.
Speaker C: Yeah.
Speaker G: Okay.
Speaker G: So it's sort of arbitrary.
Speaker E: I mean, so if you think, yeah.
Speaker E: Yeah. So the next thing is to use this as maybe initialization and then use something online.
Speaker E: It's an important thing.
Speaker E: I expect improvement at least on Finnish because the way...
Speaker E: Well, for Italian and Spanish, this value works good, but not necessarily for Finnish.
Speaker E: But unfortunately, there is, like, this 40 millisecond latency.
Speaker E: Yeah. So I will try to so much reduce this.
Speaker E: I already know that if I completely remove this latency, so it...
Speaker E: There is 3% hit on Italian.
Speaker C: This latency...
Speaker C: Sorry.
Speaker D: Yeah.
Speaker D: Yes, moving was over this, sort of, say, the factor of the Wiener.
Speaker D: What was it?
Speaker D: This moving, it was over the subtraction factor, sort of, say.
Speaker E: It's the smoothing over the gain of the subtraction aggregate.
Speaker D: And you are looking into the future, into the past.
Speaker E: Right.
Speaker E: So you do smoot this.
Speaker D: Did you try simply to smooth...
Speaker D: To smooth through is the envelope?
Speaker E: No, I did not.
Speaker D: Because it means you should have a similar effect if you...
Speaker D: I mean, you have now several stages of moving, so to say you start up.
Speaker D: As far as I remember, you smooth somehow.
Speaker D: The envelope, you smooth somehow the noise estimate and later on, you smooth also this subtraction factor.
Speaker E: No, it's...
Speaker E: It's just a gain that smooth actually.
Speaker E: Actually, I do all the smoothing.
Speaker E: Oh, it was...
Speaker E: Yeah.
Speaker E: Yeah.
Speaker E: No, in this case, it's just a gain.
Speaker E: But the way it's done is that...
Speaker E: For low gain, there is this non-linear smoothing actually.
Speaker E: For low gains, I use the smoothed version.
Speaker E: But for high gain, I don't smooth.
Speaker D: It just...
Speaker D: The experience shows if you do the...
Speaker D: The best is to do the smooths moving as early as possible.
Speaker D: So when you start up, I mean, you start up with the...
Speaker D: Somehow with the noisy envelope.
Speaker D: But the best is to smooth this somehow.
Speaker E: Yeah, I could try this.
Speaker C: So before estimating the SNR itself smoothed envelope.
Speaker E: But yeah.
Speaker E: Then we need to find a way to smooth less also when there is high energy.
Speaker E: Because I noticed that it helps a little bit to smooth more during low energy portions.
Speaker E: Yes.
Speaker E: Less during speech because if you smooth...
Speaker E: Then you kind of distort the speech.
Speaker E: Right.
Speaker D: Yeah, I think when...
Speaker D: You could do it in this way that you say if you...
Speaker D: You have somehow a noise estimate.
Speaker D: If you say with my envelope, I'm close to this noise estimate.
Speaker D: Then you have a bad signal to a noise ratio and then you would like to have a stronger smoothing.
Speaker D: So you could...
Speaker D: Yeah, you could base it on your estimation of the signal to noise ratio on your actual.
Speaker C: Or some...
Speaker E: Yeah, but I don't trust the current value.
Speaker C: Yeah, not right now.
Speaker H: The value later will much better.
Speaker E: I think that's it.
Speaker D: To summarize the performance of the speech that car results as similar to the euros.
Speaker C: Yeah, so the 5080 is like the bad...
Speaker C: You have 56,000 euros.
Speaker C: Yeah, that's true.
Speaker D: And depending on this, the additive constant is slightly better.
Speaker E: It's better, of course.
Speaker E: And yeah, the condition where it's better than your approach, it's just because maybe it's better on well matched and that the weight on well matched is...
Speaker C: Yeah, you got a...
Speaker E: If you don't weigh differently, the different condition, you can see that your...
Speaker E: Well, the two-stage winner filtering is maybe better or...
Speaker E: It's better for I miss match, right?
Speaker C: Yeah, it's better for I miss match.
Speaker C: So the overall... Yeah, it was for the well matched condition.
Speaker I: So we need to combine these two.
Speaker C: That's the best thing is like the French telecom system is optimized for the well matched condition.
Speaker C: So they know that the weighting is good for the well matched.
Speaker C: So everywhere the well matched performance is very good for French telecom.
Speaker C: We also have to do something similar.
Speaker G: Our tradition here has always been to focus on this match.
Speaker D: Is this more interesting?
Speaker D: My body was a tool.
Speaker D: For a started working on this world.
Speaker I: Yeah.
Speaker I: Okay, Carmen, do you...
Speaker B: I only say that this is the summary of all the BTS' experiment.
Speaker B: And say that the result in the last...
Speaker B: For Italian, the last experiment for Italian are bad.
Speaker B: I make a mistake when I write.
Speaker B: Obviously, I copy one of the bad results.
Speaker B: So you...
Speaker B: Yeah.
Speaker B: You know, this...
Speaker B: Well, if we put everything, we improve a lot, compared to usual BTS, but the final result are not still good like the winner filter, for example.
Speaker B: I don't know, maybe it's possible.
Speaker B: It's somewhere to have the same result, I don't know exactly.
Speaker B: Because I have...
Speaker C: You have a better result.
Speaker B: What result in medium mismatch?
Speaker C: I have some results that are good for the high mismatch.
Speaker B: Yeah.
Speaker B: Or something.
Speaker B: I'm more or less similar, but are worse.
Speaker B: And still, I don't have the result for K-digit.
Speaker B: The program is running.
Speaker B: Maybe for this weekend, I will have the result for K-digit.
Speaker B: And I can't complete it.
Speaker B: I'm going to write this.
Speaker H: Right.
Speaker B: One thing that I know are not here in this result, but I spoke in before with Sunil, I improved my result using clean LDA filter.
Speaker B: If I use the LDA filter that are running with the noise speed, that hurts my result.
Speaker G: So what are these numbers here?
Speaker G: Are these with the cleaner with the noise?
Speaker B: With the noise, I have worse results that I didn't use.
Speaker B: Maybe because with this technique, we are using really clean speed.
Speaker B: The speed representation that goes to the HTK is really clean speed because it's from the dictionary.
Speaker B: They could move.
Speaker B: Maybe for that.
Speaker B: Because I think that it's an experiment using the two LDA filter, a noisard, doesn't matter to me.
Speaker E: Yeah, I did that, but it doesn't matter on the speech that car, but it matters a lot on the idgis.
Speaker E: It's better when you use the clean filter.
Speaker E: Yeah, it's much better when you use the clean derived LDA filter.
Speaker H: Yeah, it's clean.
Speaker C: But yeah, Sunil, you know, my result is with the noisard.
Speaker C: No, it's with the noisard.
Speaker C: It's not the clean LDA.
Speaker C: In the front sheet, I have like the summary.
Speaker E: And your result is with the clean LDA.
Speaker C: And in your case, it's all noisy.
Speaker E: All noisy.
Speaker E: Yeah, but I'm serving my case.
Speaker E: At least on speech that car, it doesn't matter, but the idgis is matters.
Speaker E: Like two or three percent absolute.
Speaker E: Absolutely.
Speaker E: So you really might want to.
Speaker G: Yeah, I would like to look at it.
Speaker G: Yeah, I could be sizable right there.
Speaker D: Maybe you're living in about two weeks.
Speaker D: Yeah.
Speaker D: So I mean, if I would put on the head of a project manager, I would say I'm the most.
Speaker D: So much time left.
Speaker D: So I mean, what I would do is I would pick the best consolation which you think.
Speaker D: And create all the results for the whole database that you get to the final number.
Speaker D: So no, didn't.
Speaker D: Maybe also to write somehow a document where you describe your approach.
Speaker B: I was thinking to do that this week.
Speaker G: I'll borrow the head back and agree.
Speaker B: Yeah, that's the.
Speaker G: Right.
Speaker G: In fact, actually, I guess the Spanish government requires that anyway they want some kind of report from everybody who's in the program.
Speaker G: So of course, we'd like to see it too.
Speaker I: So what do you think we should do the digits or skip it or?
Speaker G: We have them now.
Speaker G: Why don't we do it?
Speaker G: Just take a minute.
Speaker G: Oh, sorry.
Speaker I: So I guess I'll go ahead.
Speaker I: It was mentioned.
Speaker I: 5458895
Speaker B: Transcript L-303-136084-010 10401-238-4673-7474-3061536-3911-321472417-818018387 7358894381 083734619
Speaker C: Transcript L-3041164800349 821974208 6291-26456761-482749558 6101109292 6703449512 0380274696 819163804
Speaker D: This is transcript L-3052421872083 310896351060801834788 91917067026595916040016661048985 29053636371425227031
Speaker E: Transcript L-3025785876209 7251630444 8930628276 87297015 83840504347 892661195736 623105907
Speaker A: 71444222 8344222
None: 8344222 8344237404222 6571 54608550658916864078 012947781 94223648053
Speaker G: Transcript L-301 76698879925 2149198989 9411349543 8395604911 176734304 6469388185 12938212 175820090255
Speaker F: Transcript L-282 6730000596 733051850 0457555828 291515814306692073 8638177386 158196395967 5642534762
Speaker I: 8396 476
None: 642R 71 7
None: Thank you.
