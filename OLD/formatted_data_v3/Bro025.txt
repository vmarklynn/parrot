Speaker D: Alright, we're off.
Speaker E: Just test test.
Speaker E: Just test me.
Speaker E: Yeah.
None: Okay.
None: Okay.
Speaker E: So, there's two sheets of paper in front of us.
Speaker E: This is the arm wrestling?
Speaker A: Yeah, we formed a coalition, actually.
Speaker A: We already made it into one.
Speaker E: Oh, good.
Speaker E: Excellent.
Speaker E: That's the best thing.
Speaker E: Tell me about it.
Speaker C: So, it's well, it's spectacular.
Speaker C: Subjection or we know, finish, right?
Speaker C: We're bending on the big bits.
Speaker C: Each risk where I'm at.
Speaker C: Perfect.
Speaker C: And then with the estimation of knowing depending on the arm, it's moving at a time.
Speaker C: It's working out frequency.
Speaker C: It's very simple.
Speaker C: And the best result.
Speaker C: But you see, you're on 50 bands with a winner filter.
Speaker C: And there is no noise addition.
Speaker C: It's good because it's difficult.
Speaker C: Are you looking at one or two?
Speaker C: Yeah, so the sheet that gives you 3.66.
Speaker C: The second sheet, this, about the same, it's the same idea, but it's working on med bands.
Speaker C: And it's a spectrosupportion instead of a winner filter.
Speaker C: And there is also a noise addition after cleaning up the main bits.
None: Well, there is a term.
None: Let me know.
Speaker E: Yeah, I mean, it's actually very similar.
Speaker E: I mean, if you look at databases, the one that has the smallest.
Speaker E: The one that has the smallest overall number is actually better on the finish and Spanish.
Speaker E: But it is worse on the, I mean, on the DI digits.
Speaker E: So, probably doesn't matter that much either way.
Speaker E: But when you say unified, do you mean it's one piece of software now?
Speaker C: So no, we are already setting up software.
None: Should be ready.
Speaker D: So, what's happened?
Speaker D: I think I've missed something.
Speaker E: Okay, so a week ago, maybe you weren't around when you and I came close here.
Speaker E: You didn't go to your night?
Speaker E: Yeah, I didn't.
Speaker E: Oh, okay, so yeah, let's summarize.
Speaker E: And then if I summarize, somebody can tell me if I'm wrong, which will also be possibly helpful.
Speaker E: What if I just press here?
Speaker E: I hope this is still working.
Speaker E: We looked at, after coming back from Qualcomm, we had very strong feedback.
Speaker E: And I think it was, he didn't think there was an eye opinion also that, you know, we sort of spread out to look at a number of different ways of doing the only suppression.
Speaker E: But given limited time, it was sort of time to choose one.
Speaker E: And so the retail series hadn't really worked out that much.
Speaker E: The subspace stuff had not worked with so much.
Speaker E: So it sort of came down to spectral subtraction versus refiltoring.
Speaker E: We had a long discussion about how they were the same and how they were completely different.
Speaker E: And I mean, fundamentally, the same sort of thing, but the math is a little different so that there's an exponent difference in, you know, what's the ideal filtering, depending on how you construct the problem.
Speaker E: And I guess after that meaning sort of made more sense to me, because if you're dealing with powerspectra, then how are you going to choose your error?
Speaker E: And typically you'll do something like variance. So that means there'll be something like the square of the power spectra.
Speaker E: Whereas when you're doing the, looking at it the other way, you're going to be dealing with signals and you're going to have to looking at power of noise power that you're trying to reduce.
Speaker E: So there should be a difference of, you know, conceptually of a factor too in the exponent.
Speaker E: But there's so many different little factors that you adjust in terms of over subtraction and so forth that arguably you're, and the choice of do you operate on the Melvans or the operate on the FFT beforehand.
Speaker E: There are so many other choices to make that are almost, well, if not independent, certainly in addition to the choice of whether you, the respective subtraction or when you're filtering, that again, we sort of felt the gang should just sort of figure out which it is they want to do.
Speaker E: And then let's pick it, go forward with it. So that was, that was last week. And we said, take a week, go arm wrestle, you know, figure it out.
Speaker E: And the joke there was that each of them had specialized in one of them. And so they, so instead they went to your somebody and bonded and they can go to a single, single piece of software.
Speaker E: Another, another victory for international collaboration.
Speaker D: So so you guys have combined for you're going to be combining software.
Speaker A: Well, the piece of software has like plenty of options like you can pass command line arguments. So depending on that, it becomes either spec with subtraction or we're filtering.
Speaker E: Well, that's fine. But the important thing is that there is a piece of software that you that we all will be using. Yeah.
Speaker C: Yeah, it's just one piece of software.
Speaker C: And we want to make optimize parameters. Sure. But still so there is a piece of software.
Speaker C: How is how good is that? I don't have a sense of. It's just one person. Best system. It's between. We are single.
Speaker E: Yeah. But compared to the last evaluation, which we started for before, but we were considerably far behind. And the thing is this doesn't have neural net in yet, for instance.
Speaker E: So it's so it's it's not using our full ballot bag of tricks, if you will. And it is very close performance to the best thing that was there before.
Speaker E: But you know, looking at another way, maybe more importantly, we didn't have any explicit noise handling stationary dealing with we didn't explicitly have anything to deal with stationary noise. And now we do.
Speaker D: So will the neural net operate on the output from either the weener filtering or the spectral subtraction? Well, so so arguably what we should do, I mean, I gather you have, it sounds like you have a few more days of nailing things down with the software and so on.
Speaker E: But but arguably what we should do is, even though the software can do many things, we should for now pick a set of things, these things, I would guess.
Speaker E: And not change that and then focus on everything that's left. And I think you know that our goal should be by next week when he comes back to really just to have a firm path for the for the time he's gone of what things will be attacked.
Speaker E: So I would I would think that what we would want to do is not thoughts with this stuff for a while because what will happen is we'll change many other things in the system. And then we'll probably want to come back to this and possibly make some other choices.
Speaker E: But just conceptually where does the neural net do you want to run it on the output of the spectrally subtraction? Well, depending on its size, well, one question is, is it on the server side or is it on the terminal side?
Speaker E: If it's on the server side, we probably don't have to worry too much about size. So that's kind of an argument for that. We do still, however, have to consider its latency. So the issue is is, for instance, could we have a neural net that only looked at the past?
Speaker E: What we've done in the past is to use the neural net to transform all of the features that we use. So this is done early on. This is essentially, I guess it's more or less like a speech enhancement technique here.
Speaker E: We're just kind of creating new, if not new speech, at least new FFTs that have, which could be turned into speech, that have some of the noise removed. After that, we still do a mess of other things to produce a bunch of features.
Speaker E: And then those features are not now currently transformed by the neural net. And then the way that we had it in our proposal 2 before, we had the neural net transform features and we had the untransformed features, which, I guess you actually did linearly transform with KLT, but to our Thogon-Lyzen, but they were not processed through a neural net.
Speaker E: And Stefan's idea with that, as a recall, was that you'd have one part of the feature vector that was very discriminant in another part that wasn't, which would smooth things a bit for those occasions when the testing set was quite different than what you'd change your discriminant features for.
Speaker E: So all of that is still seems like a good idea. The thing is now we know some other constraints. We can't have unlimited amounts of latency. That's still being debated by people in Europe, but no matter how they end up there, it's not going to be unlimited amounts, so we have to be a little conscious of that.
Speaker E: So there's the neural net issue, there's the VAD issue, and there's the second stream thing. And I think last time we agreed that those are the three things that have to get focused on.
Speaker D: What was the issue with the VAD?
Speaker E: Well, better ones are good.
Speaker D: And so the default boundaries that they provide are okay, that's not all that great. I guess they still allow 200 milliseconds on either side, is that for the audience?
Speaker C: So the VAD issue is outside the beginning.
Speaker C: The speech pose, which is sometimes the speech that comes, you have to pose this down to 1.25 seconds. Wow! More than one second.
Speaker C: And it seems to us that this way of dropping the beginning and end is not... we can do better. Because this way of dropping the frames that improve for the baseline by 14% and so we already showed that we are currently in the end improve by 100% percent.
Speaker D: And so the top of the VAD that they provide? No.
Speaker C: Oh, okay. There's this 14.
Speaker C: And if we just take only the VAD probably is computed on the clean signal and apply them on the far-being test, the nuances, then results are much better.
Speaker C: In some case, the VAD is zero already, right?
Speaker D: So it means that there is still... how much latency does the VAD and... is the signal again?
Speaker C: Right now it's under undeniable, so it's 14 milliseconds plus the rank ordering should be...
Speaker A: So we have another 10 frames. The rank ordering, I'm sorry.
Speaker A: There's this... there's this... the... the filtering of the probabilities on the...
Speaker E: We don't think the media is going to be... we have 11.
Speaker E: So, yeah, I was just noticing on this that it makes reference to delay. So what's the... if you ignore... the VAD is sort of in parallel, isn't it?
Speaker E: With the... I mean, the additive with the... the LDA in the winter filtering.
Speaker A: Yeah, so what happened right now we removed that delay of the LDA.
Speaker A: So we're... I mean, if... so which is like... if we reduce the delay of VAD... so the... the final delay is now... is determined by the delay of the VAD.
Speaker A: Because the LDA doesn't have any delay. So if you reduce the delay of the VAD, I mean, it's like... effectively reducing the delay.
Speaker D: How much delay was on the LDA?
Speaker A: So the LDA and the VAD both had 100 milliseconds delay. So when they were in parallel, so which means you pick either one of them.
Speaker A: The biggest order. So right now that LDA delay is removed.
Speaker E: And there didn't seem to be any penalty for that. But there didn't seem to be any penalty for making it causal.
Speaker A: Oh no, it actually made it like 0.1% better or something.
Speaker E: It's just winter filter is 40 milliseconds today.
Speaker A: Yeah, so that's the one which Stefan was discussing like...
Speaker A: The smoothing. Yeah, you smoothed it and then delay the decision by.
Speaker E: Right, okay. So that's really not bad. So we may in fact... we'll see what they decide.
Speaker E: We may in fact have the latency time available to have an Earl Madden. I mean, it sounds like we probably will.
Speaker E: So that'd be good. Because it certainly always helped us before.
Speaker D: What amount of latency are you thinking about when you see it?
Speaker E: Well, they're disputing it. They're saying one group is saying 130 milliseconds and another group is saying 250 milliseconds.
Speaker E: 250 is what it was before actually. So some people are lobbying to make it shorter.
Speaker D: Were you thinking of the 250 or the 137th position?
Speaker E: Well, when we find that out, it might change exactly how we do it is all.
Speaker E: I mean, how much effort do we put into making it causal?
Speaker E: I mean, I think the neural net will probably do better if it looks a little bit of the future.
Speaker E: But it will probably work to some extent to look only at the past.
Speaker E: And we limited machine and human time and effort and how much time should we put into that.
Speaker E: So it'll be helpful if we find out from the standards, folks, whether they're going to restrict that or not.
Speaker E: But I think at this point our major concern is making the performance better.
Speaker E: And if something has to take a little longer in latency in order to do it, that's a secondary issue.
Speaker E: But if we get told otherwise, then we may have to clamp down a bit more.
Speaker A: So one difference that was there is like we tried computing the delta and then doing the frame dropping.
Speaker A: The earlier system was do the frame dropping and then compute the delta on the.
Speaker E: So this kind of an adult. Yeah. Oh, so that's fixed in us.
Speaker A: Yeah. So we have now delta and then so the frame dropping is the last thing that we do.
Speaker A: So we have what we do is we compute the silence probability converted into that binary flag.
Speaker A: And then in the end you up sampled it to match the final features number of.
Speaker A: It seems to be helping on the well match condition.
Speaker A: So that's why this improvement I got from the last result.
Speaker A: So and it actually reduced a little bit on the high mismatch.
Speaker A: So the final weight is it's better because the well match is the weighted more than.
Speaker E: So I mean you were doing a lot of changes. Did you happen to notice how much.
Speaker E: Change was due to just this frame dropping problem.
Speaker A: You had something on it, right?
Speaker C: Just a friend dropping problem. Yeah, but it's difficult. Sometimes we would change to things together.
Speaker E: But it's around maybe it's less than my personal.
Speaker E: Yeah.
Speaker E: But like we're saying there's four or five things like that.
Speaker E: Pretty shocked. Soon you're talking real improvement.
Speaker C: And the proposal that you're not that flexible. So working on that.
Speaker B: Mm hmm.
Speaker E: Oh, that's a real good point.
Speaker C: You can be the same guy.
Speaker E: Might be hard if it's at the server side, right?
Speaker C: Well, we can be the friend dropping server side or we can't just be careful.
Speaker D: Okay.
Speaker D: You have.
Speaker D: So when you maybe I don't quite understand how this works, but.
Speaker D: Couldn't you just send all of the frames, but mark the ones that are supposed to be dropped.
Speaker D: Because you have a bunch more bandwidth, right?
Speaker E: Well, you could, you know, I mean, it it always seemed to us that it would be kind of nice to in addition to reducing insertions actually use a plus bandwidth.
Speaker E: But nobody seems to care about that in this evaluation.
Speaker D: That's why the net use.
Speaker D: If the net's on the server side, then it could use all of the frames.
Speaker A: Yes, it could be like, you mean you just transmit everything and then finally drop the frames after the neuralite, right?
Speaker A: Yeah, that's one thing which you could even mark them.
Speaker A: Yeah, right now we have the server.
Speaker A: Right now what we did is like we just we just have this additional bit which goes along the features saying it's currently it's a speech or a non speech.
Speaker A: So there is no frame dropping till the final features like including the deltas are computed.
Speaker A: And after the delta's are computed, you just pick up the ones that are marked silence and then drop them.
Speaker E: So be more or less the same thing with the neural net, I guess.
Speaker A: So that's what that's what this is doing right now.
Speaker A: Yeah.
Speaker E: Okay.
Speaker E: So what's that's good set of work that.
Speaker A: Just one more thing like should we do something more for the noise estimation because we still.
Speaker E: Yeah, I was wondering about that. I hate to written that down there.
None: Actually, I did this experiment.
Speaker C: We just used in frames.
Speaker C: We take the first 15 frames of the judgements.
Speaker E: Yeah.
Speaker C: I tried just breaking the.
Speaker C: But of course, I didn't play.
Speaker E: Yeah, well, it's not surprising to be worse first time, but it does seem like some compromise between always depending on the first 15 frames and always depending on pause is a good idea.
Speaker E: Maybe you have to wait the estimate from the first 15 frames more heavily than was done in your first attempt.
Speaker E: Yeah.
Speaker E: I mean, do you have any way of assessing how well or how poorly the noise estimation is currently doing?
Speaker C: Yeah.
Speaker A: Was there any experiment with because I did the only experiment what I tried was I used the channel zero-watt for the noise estimation and frame dropping.
Speaker A: So I don't have a split like which one help more.
Speaker A: So it was the best result I could get.
Speaker E: So that's something you could do with this final system, right?
Speaker E: Just do this everything that is in this final system except use the channel zero for the noise estimation.
Speaker A: Yeah.
Speaker E: And then see how much better it gets.
Speaker E: If it's essentially not better than it's probably not worth.
Speaker A: Yeah, but the Ginters argument is slightly different. It's like even if I use a channel zero-watt, I'm just averaging the the pause spectrum.
Speaker A: But the Ginters argument is like if it is a non stationary segment, then he doesn't update the noise spectrum.
Speaker A: So it's like it tries to capture only the stationary part.
Speaker A: And so the averaging is like different from updating the noise spectrum only during stationary segments.
Speaker A: So the Ginters was arguing that even if you have a very good VAD averaging it like over the whole thing is not a good idea.
Speaker A: Because you are averaging the stationary and the non stationary and finally you end up getting something which is not really the same.
Speaker A: Anyway, you can't remove the stationary part from the signal.
Speaker A: No, using these messages.
Speaker A: Yeah, so you just update only the stationary components.
Speaker A: So that's still slight difference from what Ginters is trying in.
Speaker E: Well, yeah. And also there's just the fact that although we're trying to do very well in this evaluation, we actually would like to have something that worked well in general.
Speaker E: And relying on having 15 frames at the front or something is pretty.
Speaker E: I mean, you might not.
Speaker E: So it certainly be more robust to different kinds of input if you had at least some updates.
Speaker E: Well, what do you guys see as being what you would be doing in the next week given what's happened?
Speaker A: We have the VAD.
Speaker A: Was that VAD?
Speaker C: Okay.
Speaker C: Yeah.
Speaker D: So I don't remember what you said.
Speaker D: The answer to my question earlier.
Speaker D: Were you trained in that on after you've done the spectral subtraction or the different net?
Speaker A: Yeah, which is a new that's a neural it is some of the VAD net.
Speaker A: So that VAD was trained on the noisy features.
Speaker A: So right now we have like we have the cleaned up features so we can have a better VAD by turning the net on the clean up speeches.
Speaker A: But we need a VAD for noise estimation also.
Speaker D: Can you use the same net to do both for can use the same net that you that I was talking about to the VAD?
Speaker A: It actually comes at the very end.
Speaker A: So the net the final net I mean which is the feature net so that actually comes after a chain of like LDA plus everything.
Speaker A: So it's like it takes a long time to get a decision out of it and I can actually do it for final frame dropping but not for the V.
Speaker A: Noise estimation.
Speaker E: See the idea is that the initial decision to that that you're in silence or speech happens pretty quickly.
Speaker D: Is that used by somebody's own?
Speaker D: Yeah, that sort of fed forward and you say well flush everything it's not speech anymore.
Speaker D: I thought it would be used for doing frame dropping.
Speaker E: It is used.
Speaker E: Yeah, it's only used well it's used for frame dropping.
Speaker E: It's used for end of utterance because you know there's if you have more than 500 milliseconds of of non speech then you figure it's end of utterance.
Speaker E: Something like that so.
Speaker C: I see.
Speaker E: Yeah, so probably the VAD and maybe testing out the noise estimation a little bit and keeping the same method but seeing if the noise estimation could be improved.
Speaker E: It's sort of related issues.
Speaker E: It probably makes sense to move from there and then later on in the month I think we want to start including the neural at the end.
Speaker E: Okay, anything else?
Speaker E: Didn't fall.
Speaker E: Our effort would have been devastated.
Speaker E: So Henik is coming back next week?
Speaker E: No, no, he's dropped into the US.
Speaker E: So the idea was that we'd sort out where we're going next with this work before he left on his next trip.
Speaker E: Good.
Speaker E: Very you just got through your qual so I don't know if you have much to say but...
Speaker B: Now just looking into some of the things that John O'Halla and Henik gave as feedback as a starting point for the project.
Speaker B: In my proposal I was thinking about starting from a set of phonological features or a subset of them.
Speaker B: It might not be necessarily a good idea according to John. He said these phonological features are sort of figments of imagination also.
Speaker E: In conversational speech in particular, I think you can put them in pretty reliably in synthetic speech but we don't have too much trouble recognizing synthetic speech since we created in the first place.
Speaker B: Yeah, so a better way would be something more data-driven just looking at the data and seeing what's similar and what's not similar.
Speaker B: I'm taking a look at some of Sun Gita's work on traps.
Speaker B: She did something where the traps she clustered the temporal patterns of certain phonemes in average over many, many contexts and some things tended to cluster.
Speaker B: So I've stopped Constance clustered very well. Silence was by its own self and Vocalic was clustered.
Speaker B: So those are interesting things.
Speaker D: Now you're sort of looking to try to gather a set of these types of features.
Speaker B: Right, yeah. See where I could start off from a set of small features and continue to iterate and find a better set.
Speaker E: Okay, well, short meeting. It's okay. So next we hopefully will get Tina Kear to join us.
Speaker D: Digits.
Speaker E: Digits.
Speaker E: Okay, let me go ahead and get my glasses on so I can see. Okay. Transcript L-327.
Speaker E: 821-067-40000-800-4176-1281-630-224-1912-650-869-4624-8919-1485-6845-388-383-80-4518-829135-0-234-44812-2
Speaker C: Transcript L-328-9060-3955-984-3165-343-114-4166-4194-33-7655-505-754-075-666-8 630-5487-701-812-831-5734-8703-68
Speaker D: Transcript L-329-9977-30368-7627-1700-0996-9388-96987-099 2126-937206-672-308-949-8032-631-489-6444-2669-318-791-3247
Speaker A: Transcript L-330-2360-9593-546-348-6675-3704-3844-866-3675-8705-5739-150-9016-22-409-277-701-9206-8967-896701-9206-8967-660 164-191-2428
Speaker B: Transcript L-331-3788-9100-623-270-385-138-183795-096-6610-197-541-2420-41-11-11-197-142-121-107-8 096-761-0197-541-240-41-5612-309-1-6057-6528-341-164739-4528-8507 Okay, and we're up.
