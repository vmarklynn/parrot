Speaker E: And Hans Gunter will be here with the mechanics x2's day or so.
Speaker E: Oh, okay.
Speaker E: So he's going to be here for about three weeks.
Speaker D: Oh, that's right.
Speaker E: Just for a visit.
Speaker E: We might end up with some longer collaboration or something.
Speaker E: So he's going to look in on everything we're doing and give us his thoughts.
Speaker E: And so be another good person looking at things.
Speaker A: Oh.
Speaker A: Yeah.
Speaker A: Oh, yeah.
Speaker A: Oh, yeah.
Speaker A: Yeah.
Speaker A: Oh, yeah.
Speaker E: Yeah.
Speaker E: Yeah.
Speaker E: Now, here we are after three weeks.
Speaker E: He's very, very easy going.
Speaker E: Easy to talk to.
Speaker E: Very interesting.
Speaker E: Everything.
Speaker E: Yeah.
Speaker B: Yeah, we're married and we're not sure.
Speaker E: Yeah.
Speaker E: Yeah, he's been here before.
Speaker E: I mean, he's been here.
Speaker G: He's been here.
Speaker G: He's been here for a year or six months, something like that.
Speaker E: Yeah.
Speaker D: Yeah.
Speaker D: He's done a couple of stays here.
Speaker G: So I guess we got lots to catch up on.
Speaker G: We haven't met for a couple of weeks.
Speaker G: We didn't meet last week morning.
Speaker G: I went around and walked everybody and seemed like they had some new results.
Speaker G: Other than them coming up and telling me I figured we should just wait a week and they can tell both, you know, all of us.
Speaker G: So why don't we start with you, Dave?
Speaker G: Oh, okay.
Speaker G: Then we can go on.
Speaker A: So since we're looking at putting this on, we need a lot of things.
Speaker A: I think you expect those factors to make this our conscious.
Speaker A: I just ask you if it would work using the path only and possibly present the pathway for me.
Speaker A: And so I just ask who I use 12 seconds from the path and the path and the path for me and 12 seconds.
Speaker G: 12 seconds from the current.
Speaker G: 12 seconds from the current.
Speaker A: 12 seconds.
Speaker A: 12 seconds from the end of the current frame.
Speaker A: So we've had to do it using a 12 seconds sandwich window.
Speaker A: I think it was a drop in performance, but it was slightly dropped.
Speaker A: Is that right?
Speaker E: Yeah, I mean, it was pretty tiny.
Speaker A: So that was encouraging.
Speaker A: That's encouraging for the idea of using it in hydroaccus.
Speaker A: It's like smart home.
Speaker A: And another issue I'm thinking about is in the smart home system.
Speaker A: So it's like 12 seconds in the early test.
Speaker A: It's like a good length of time.
Speaker A: But what happens if you have left in 12 seconds?
Speaker A: So I would be before back in May I did some experiments using say two seconds or four seconds or six seconds.
Speaker A: In those I trained the models using mean subtraction with the mean calculated over two seconds or four seconds or six seconds.
Speaker A: And here I was curious what if I trained the models using 12 seconds.
Speaker A: But I gave it a situation where the test I was subtracting using two seconds or four seconds or six seconds.
Speaker A: So I did that for about three different conditions.
Speaker A: And I think it was four seconds and six seconds and eight seconds, something like that.
Speaker A: And it seems like it hurts compared to if you actually trained the models using that same length of time.
Speaker A: But it doesn't hurt that much.
Speaker A: You usually left in 25% although I could see one where it was a point eight percent or so rise in word error rate.
Speaker A: But this is where even if I train on the model means subtractive with the same length of time as in the test, the word error rate is around 10% or 9%.
Speaker A: So it doesn't seem like that.
Speaker E: But looking at the other way, isn't it what you're saying that it didn't help you to have the longer time for training if you were going to have a short time for?
Speaker A: That's true.
Speaker E: I mean why would you do it if you knew that you were going to have short windows?
Speaker G: It seems like you're in normal situations you would never get 12 seconds of speech.
Speaker B: You need 12 seconds in the past two weeks to make it right?
Speaker B: Are you looking at six seconds in future and six in?
Speaker G: No, it's all 12 seconds in the past.
Speaker G: Is this 12 seconds of regardless of speech or sound?
Speaker E: The other thing which may be a little bit of something else we've talked about in terms of windowing and so on is that I wonder if you trained with 12 seconds.
Speaker E: And then when you were two seconds in, you used two seconds and when you were four seconds in, you used four seconds and you basically build up to the 12 seconds so that if you have very long utterances, you have the best.
Speaker E: But if you have shorter utterances, you use which you can.
Speaker A: Right. And that's actually what we're applying to do in small calm.
Speaker A: Yeah.
Speaker A: So the question I was trying to get out with those experiments is does it matter what models you use? Does it matter how much time you use to calculate the mean when you were doing the training data?
Speaker E: Right. But I mean the other thing is that that's, I mean the other way of looking at this going back to mean capital subtraction versus rasta kind of things is that you could look at mean capital subtraction, especially the way you're doing it as being kind of filter.
Speaker E: So the other thing is just to design a filter.
Speaker E: You know, basically you're doing a high pass filter or a band pass filter or some sort and just design a filter.
Speaker E: And then you know, filter will have certain behavior and you can look at the startup behavior.
Speaker E: You start up with nothing and you know, you will.
Speaker E: If you have an IAR filter, for instance, it will not behave in the study state way that you would like it to behave until you get along enough period.
Speaker E: But by just constraining yourself to have your filter be only subtraction of the mean, you're kind of tying your hands behind your back because there's filters have all sorts of temporal and spectral behaviors.
Speaker E: And the only thing consistent we know about is that you want to get rid of the very low frequency component.
Speaker B: But do you really want to calculate the mean you neglect all the silenced regions or you just use everything that's 12 seconds?
Speaker A: You mean in my task so far?
Speaker A: Yeah. Most of the silences have been cut out.
Speaker A: Okay.
Speaker A: Just just introwards silences.
Speaker B: And they are like pretty short.
Speaker B: So you really need a lot of speech to estimate the mean in it?
Speaker A: Well, if I only use six seconds, it still works pretty well.
Speaker A: I saw my task before.
Speaker A: I was trying 12 seconds, but that was the best.
Speaker A: Okay.
Speaker A: And increasing past 12 seconds didn't seem to help.
Speaker A: Yeah, I guess it's something I need to play with more of this.
Speaker A: I had to set that up for the smart answers that made maybe if I trained on six seconds, it would work better when I only had two seconds or four seconds.
Speaker D: Yeah.
Speaker E: And again, if you take this filtering perspective and if you essentially have it build up over time, I mean, if you computed means over two and then over four and over six, then it's literally getting into the kind of ramp up of a filter anyway.
Speaker E: And so you may just want to think of it as a filter.
Speaker E: But if you do that, then in practice, somebody using a smart-com system wouldn't think they're using it for a while.
Speaker E: It means that their first utterance instead of getting a 40% error rate reduction, they'll get what you get without this policy, you get 30%.
Speaker E: And then the second utterance that you give, they get the full benefit of it.
Speaker G: If it's ongoing, you catch the utterances.
Speaker G: That's how you get your...
Speaker E: Well, I'm saying in practice, yeah.
Speaker E: That's somebody's using a system to ask for directions or something.
Speaker E: Okay.
Speaker E: You know, they'll say something first and to begin with, if it doesn't get them quite right, maybe they'll come back and say, excuse me.
Speaker E: Or I mean, you should have some policy like that anyway. And in any event, they might ask a second question.
Speaker E: It's not like what he's doing doesn't improve things.
Speaker E: It does improve things just not as much as he would like.
Speaker E: And so there's a higher probability of making an error for a utterance.
Speaker G: It would be really cool if you could have...
Speaker G: There's probably users who would never like this, but if you had... could have a system where before they began to use it, they had to introduce themselves verbally.
Speaker G: Yeah.
Speaker G: I have my name and so on and so on.
Speaker G: And you could use that initial speech to do all these adaptations.
Speaker E: Right.
Speaker E: Well, the other thing, I guess, which I don't know much about... as much as I should about the rest of the system, but couldn't you...
Speaker E: If you sort of did a first pass, I don't know what kind of capability we have at the moment for doing second passes on some kind of little small lannister, a graph, or fusion network or something.
Speaker E: But if you did first pass with either without the means of subtraction or with a very short time one, and then once you actually had the whole utterance in, if you did the longer time version then based on everything that you had, and then at that point only used it to distinguish between, you know, top-end possible utterances or something.
Speaker E: You might... it might not take very much time. I mean, I know in the large vocabulary systems people were evaluating on the past, some people really pushed everything in to make it in one pass, but other people didn't and had multiple passes.
Speaker E: And the argument against multiple passes has often been, but we want this to be, you know, had nice interactive response.
Speaker E: And the counter argument to that, which is a BBN, I think, had was, yeah, but our second response is, second passes and third passes are really, really fast.
Speaker E: So if your second pass takes a millisecond, who cares?
Speaker A: So the idea of the second pass would be waiting until you have more recorded speech or...
Speaker E: If it turned out to be a problem, that you didn't have enough speech because you need longer window to do this processing, then one tactic is looking at the larger system, and not just at the front end stuff, is to take in the speech with some simpler mechanism or shorter time mechanism, do the best you can and come up with some possible alternates of what might have been said, and either in the form of an end-best list or in the form of a lattice or confusion network or whatever.
Speaker E: And then the decoding of that is much, much faster, or can be much, much faster if it isn't a big bushing network.
Speaker E: And you can decode that with speech that you've actually processed using this longer time subtraction.
Speaker E: So I mean, it's common that people do this sort of thing where they do more things that are more complex, a require looking over more time, whatever in some kind of second pass.
Speaker E: And again, if the second pass is really, really fast, another one I've heard of is in connected digit stuff, going back and through backtrace and finding regions that are considered to be a digit, but which have very low energy.
Speaker E: So, I mean, there's lots of things you can do in second pass, as in all sorts of levels.
Speaker E: Anyway, I'm throwing too many things out.
Speaker G: So is that...
Speaker G: Edit?
Speaker G: I got that.
Speaker B: Do you want to go for it?
Speaker B: So last two weeks, I've been working on that Vener filtering.
Speaker B: And I found that single, like I just do a normal Vener filtering, like the standard method of Vener filtering.
Speaker B: That doesn't actually give me any improvement over, like...
Speaker B: It actually improves over the baseline, but it doesn't need something like 50% or something.
Speaker B: So I've been playing with the baseline MFCC.
Speaker G: Yeah.
Speaker B: So that's the improvement is around 30% over the baseline.
Speaker E: Is that using contamination with something else?
Speaker B: No, just one stage Vener filter, which is a standard Vener filter.
Speaker E: No, but I mean, contamination with our non-normalization or with the...
Speaker B: Yeah, yeah, yeah.
Speaker B: So I just plug in the Vener filtering.
Speaker B: In our system where... Does it mean it gets worse?
Speaker B: No, it actually improves over the baseline of not having a Vener filter in the whole system.
Speaker B: Like I have an LDF plus online normalization.
Speaker B: And then I plug in the Vener filter in that.
Speaker B: So it improves over not having the Vener filter.
Speaker B: So it improves, but it doesn't take it like beyond like 30% over the baseline.
Speaker B: So...
Speaker E: That's when I'm confused about it, because I think I thought that our system was more like 40% without the Vener filter.
Speaker E: No, it's like...
Speaker G: What is the new VAD?
Speaker B: No, it's old VAD.
Speaker B: So my baseline was...
Speaker B: This is like the baseline is 95.68, 89.
Speaker E: So I mean, if you do all these word errors, it's a lot easier.
Speaker E: What is that?
Speaker E: If you do all these word error rates, it's a lot easier.
Speaker B: Oh, okay, okay, okay.
Speaker B: I don't have it. It's all accurate.
Speaker C: Yeah, the baseline is something similar to...
Speaker C: I mean, the baseline that you were talking about is the MFCC baseline.
Speaker B: Yeah, there are two baselines.
Speaker B: Okay, so the baseline...
Speaker B: One baseline is the MFCC baseline.
Speaker B: When I said 30% improvement is like the MFCC baseline.
Speaker E: So as a startup, the MFCC baseline is what?
Speaker E: Is it what level?
Speaker B: It's just the male frequency and...
Speaker B: That's what's the number.
Speaker B: So I don't have that number here.
Speaker B: Okay, okay, I have it here.
Speaker B: It's the VAD plus the baseline, actually.
Speaker B: I'm talking about the MFCC plus I do a frame dropping on it.
Speaker B: So that's like...
Speaker B: The word error rate is like 4.3.
Speaker B: 4.3.
Speaker B: 4.3.
Speaker B: And 0.7.
Speaker B: What's 10.7?
Speaker B: It's the medium mismatch.
Speaker B: Okay, sorry.
Speaker B: It's the well-match medium mismatch in the high mismatch.
Speaker B: So I don't have...
Speaker B: 4.3, 10.7.
Speaker B: And 40.
Speaker B: 40%.
Speaker B: It's the high mismatch.
Speaker B: Okay.
Speaker B: And it becomes like 4.3.
Speaker E: Not changed.
Speaker B: Yeah, it's like 10.1.
Speaker B: Still the same.
Speaker B: And the high mismatch is like 18.5.
Speaker E: 18.5.
Speaker E: And what were you just describing?
Speaker B: The one is...
Speaker B: This one is just the baseline plus the...
Speaker B: We don't filter plugged into it.
Speaker E: But where's the online normalization and so on?
Speaker B: Okay, so...
Speaker B: Sorry.
Speaker B: So with the online normalization, the performance was...
Speaker B: And...
Speaker B: Okay, so it's like 4.3.
Speaker B: And again, that's the 10.4 and 20.1.
Speaker B: That was with online normalization and LDA.
Speaker B: So the well matched as like literally not changed by adding online or LDA on it.
Speaker B: But the...
Speaker B: I mean, even the medium mismatch is pretty much the same.
Speaker B: And the high mismatch is improved by 20% absolute.
Speaker E: Okay.
Speaker E: And what kind of number...
Speaker E: What are we talking about here?
Speaker E: Is this...
Speaker B: It's Italian.
Speaker B: Italian.
Speaker E: Yeah.
Speaker E: And what...
Speaker E: So what was the corresponding numbers say for the Alcatel system?
Speaker E: Yeah, 3.3.
Speaker C: Yeah, 3.3.
Speaker C: Yeah, 3.4.
Speaker C: Yeah, 3.4.
Speaker C: 8.7.
Speaker C: And 13.7.
Speaker C: Okay.
Speaker C: Yeah.
Speaker D: Okay.
Speaker D: Thanks.
Speaker F: Okay.
Speaker B: So this is the single stage winner filter with the noise estimation was based on first 10 frames.
Speaker B: Actually, I started with using the VAD to estimate the noise.
Speaker B: And then I found that it works.
Speaker B: It doesn't work for finish and Spanish because the VAD endpoints are not good to estimate the noise.
Speaker B: Because it cuts into the speed sometime.
Speaker B: So I end up overestimating the noise and getting a worse result.
Speaker B: So it works only for Italian by using a VAD to estimate noise.
Speaker B: Work for Italian because the VAD was trained on Italian.
Speaker B: So this was giving...
Speaker B: This was not improving a lot on this baseline of not having the winner filter on it.
Speaker B: And so I ran this stuff with one more stage of winner filtering on it.
Speaker B: But the second time what I did was I estimated the new winner filter based on the cleaned up speech.
Speaker B: And did a smoothing in the frequency to reduce the variance.
Speaker B: I mean, I observed that a lot of bumps in the frequency when I do this winner filtering, which is more like a musical noise or something.
Speaker B: And so by adding another stage of winner filtering, the results on the speech that car was like...
Speaker B: So I still don't have the word error rate.
Speaker B: I'm sorry about it.
Speaker B: But the overall improvement was like 56.46.
Speaker B: This was again using 10 frames of noise estimate and two stage of winner filtering.
Speaker B: And rest is like the LDAP and online organization all remaining the same.
Speaker B: So this was like compared to 57 is what you got by using the French telecom system, right?
Speaker C: No, I don't think so. Is it on Italian?
Speaker B: No, this is all of the whole speech that car.
Speaker B: 57 point.
Speaker B: Right.
Speaker B: Yeah, so the new new winner filtering scheme has like some 56.46, which is like one person still less than what you got using the French telecom system.
Speaker E: But it's a pretty similar number in any event.
Speaker E: It's very similar.
Speaker E: Yeah. But again, you're more or less doing what they were doing, right?
Speaker B: It's different in a sense.
Speaker B: Like, I'm actually cleaning up the cleaned up spectrum, which they're not doing.
Speaker B: They're what they're doing is they have two stage stages of estimating the winner filter.
Speaker B: Yeah, but the final filter what they do is they take it to the time domain by doing an inverse Fourier transform.
Speaker B: And they filter the original signal using that filter, which is like final filter is acting on the input noise speech rather than on the clean up.
Speaker B: So this is more like I'm doing winner filter twice, but the only thing is that the second time I'm actually smoothing the filter and then cleaning up the cleaned up spectrum first level.
Speaker B: Okay.
Speaker B: And so that that's that's what the difference is. And actually I tried it on the original clean.
Speaker B: I mean, the original spectrum where like I second time estimate the filter, but actually clean up the noisy speech rather than the first output of the first stage.
Speaker B: And that doesn't seem to be giving me that machine improvement.
Speaker B: I just didn't run it for the whole case.
Speaker B: And what I what I tried was by using the same thing, but so we actually found that the word is very like crucial.
Speaker B: I mean, just by changing the word itself gives you the lot of improvement by instead of using the current word, if you just take up the bad output from the channel zero.
Speaker B: When instead of using channel zero and channel one, because that was the reason why I was not getting a lot of improvement for estimating the noise.
Speaker B: So I just use the channel zero watt to estimate the noise so that it gives me some reliable markers for this noise estimation.
Speaker E: What's the channel zero then?
Speaker D: So it's like close talking.
Speaker B: Yeah, the close talking.
Speaker B: Because the channel zero and channel one are like the same speech only when the same endpoints, but only thing that the speech is very noisy for the channel one.
Speaker B: So can I also use the output of the channel zero for channel one for bad?
Speaker B: I mean, that's like a cheating.
Speaker E: Right. I mean, so what are they going to do?
Speaker E: Do we know yet about the source what the rules are going to be?
Speaker C: Yeah, so actually you received a new document.
Speaker C: Yeah, that's describing this.
Speaker C: And what they did find out is to not to align the utterances, but to perform recognition only on the close talking microphone.
Speaker C: Did you get the recognition to get the boundaries of speech?
Speaker E: So it's not like that's being done in one place or one time that's just a role and we you were permitted to do that.
Speaker C: I think they will send files, but we don't want.
Speaker E: Also, they will send files so everybody will have the same boundaries to work with.
Speaker B: Yeah, but actually the alignment actually is not seems to be improving in like on all cases.
Speaker C: Yeah, so what happened here is that the overall improvement that they have with this method.
Speaker C: Well, to be more precise what they have is they have these alignments and then they drop the beginning silence and the end silence, but they keep 200 milliseconds before speech and 200 after speech.
Speaker C: And they keep the speech post is also.
Speaker C: And the overall improvement over the MFCC baseline.
Speaker C: So when they just at this friend wrapping in addition is 40% right 14% I mean, which is which is the overall improvement.
Speaker C: But in some cases it doesn't improve at all like.
Speaker B: It gives like negative in some time like some Italian and the IDGETs right.
Speaker B: So by using the end point that speech actually it's worse than the baseline in some instances which could be due to the.
Speaker C: The other thing also is that 14% is less than what you obtain using a real VAD.
Speaker C: Yeah, our new result cheating like this. So I think this shows that there's still work.
Speaker C: But working on the VAD is still still important.
Speaker G: Can I ask just a high level question.
Speaker G: Can you just say like one or two sentences about Wiener filtering and why are people doing that? What's the deal?
Speaker B: So the Wiener filter it's like you try to minimize.
Speaker B: So the basic principle of Wiener filter is like you try to minimize the difference between the noise signal and the clean signal.
Speaker B: If you have two channels like let's say you have a clean signal and you have an additional channel where you know what is the noise signal.
Speaker B: And then you try to minimize error between these two.
Speaker B: So that's the basic principle and you can do that. If you have only a noise signal available with you, you try to estimate the noise from the assuming that the first few frames are noise or if you have voice activity director you estimate the noise spectrum.
Speaker B: And then you assume the noise is same.
Speaker B: Yeah, after the speech starts.
Speaker B: But that's not the case in many of our cases but it works reasonably well. And then you what you do is you.
Speaker B: So again, I can try down some of this.
Speaker B: Yeah, and then you do this. This is the transfer function of the Wiener filter.
Speaker B: So SF is the clean speech spectrum, and N is the noisy, and so this is the transfer function.
Speaker B: And then you multiply your noisy, and you get an estimate of the clean, and so.
Speaker B: But the thing is that you have to estimate the SF from the noisy spectrum what you have.
Speaker B: So you estimate the NF from the initial noise portions and then you subtract that from the current noise spectrum to get an estimate of the SF.
Speaker B: So sometimes that becomes zero because you don't have a true estimate of the noise.
Speaker B: So the filter will have like sometimes zeros, and it because some frequency values will be zeroed out because of that. And that creates a lot of discontinuities across the spectrum with the filter.
Speaker B: So that's what that was just the first stage of Wiener filtering that I tried.
Speaker G: So is this basically similar to just regular spectrocentrum?
Speaker E: It's all pretty related. There's a whole class of techniques where you try in some sense to minimize the noise.
Speaker E: And it's typically a mean square sense in some way.
Speaker E: And spectral subtraction is one approach to it.
Speaker G: So people use the Wiener filtering in combination with the spectral subtraction typically or are they sort of non-seating techniques?
Speaker B: They are very similar techniques. So it's like I've been seeing Wiener filter with spectral subtraction.
Speaker E: I mean in the long run you're doing the same thing, but you make different approximations. In spectral subtraction for instance there's an estimation factor.
Speaker E: So you figure out what the noise is and you multiply that noise spectrum times some constant and subtract that rather than sometimes people even though this really should be in the power domain, sometimes people work in the magnitude to mean because it works better.
Speaker G: So why did you choose Wiener filtering over some other one of these other techniques?
Speaker B: The reason was we had this choice of using spectral subtraction Wiener filtering and there was one more thing which I'm trying to do is the subspace approach.
Speaker B: Stefan is working on spectral subtraction. So I picked up a few sort of trying to be...
Speaker B: We just wanted to have a few noise production, composition techniques and then pick some from that.
Speaker E: Yeah, I mean there's comments working on other than the other activity series. So they were just trying to cover a bunch of different things with this task and see what are the issues for each of them.
Speaker B: So one of the things that I tried, like I said, was to remove those zeros in the filter by doing some smoothing of the filter.
Speaker B: Like you estimate the HF square and then you do smoothing across the frequency so that those zeros get like flattened out and that doesn't seem to be improving by trying it on the first time.
Speaker B: So what I did was like I did this and then you applied in the one more, the same thing but with the smooth filter, second time. That seems to be working.
Speaker B: So that's what I got like 56.5% improvement on speech.car with that. And so the other thing what I tried was I used till the 10 frames of noise estimate but I used this channel 0 watt to drop the frames.
Speaker B: So I'm not still on estimating and that has taken the performance to like 67% in speech.car which is which like sort of shows that by using a proper watt you can just take it to further better levels.
Speaker B: So that's sort of like you know, best case performance. Yeah so far I've seen 67% and I haven't seen like 67% and using the channel 0 watt to estimate the noise also seems to be improving but I don't have the results for all the cases with that.
Speaker B: So I used channel 0 watt to estimate noise as a lesser drop frame which is like everywhere I use the channel 0 watt and that seems to be the best combination than using a few frames to estimate and then drop channel.
Speaker E: So I'm still a little confused is that channel zero information going to be accessible.
Speaker B: Now this is just to test whether we can really improve by using a better watt. So I mean so this is like the noise compensation is fixed but you make a better decision on the end points that's like seems to be.
Speaker B: So which means which means like by using this technical it just the way we can just take the performance by another 10% of better.
Speaker B: So that that was just the reason for doing that experiment and yeah but this all these things have to still try it on the TI digits which is like I'm just running and that seems to be not improving a lot on the TI digits.
Speaker B: So the other thing is like I'm doing all this stuff on the power spectrum so try this stuff on the Mel as well Mel and the magnitude and Mel magnitude and all those things but seems to be the power spectrum seems to be giving the best result.
Speaker B: So one of the reasons I thought like do the averaging after the filtering using the Mel filter bank that seems to be maybe helping whether than trying it on the Mel filter filter out goods.
Speaker B: Yeah that's the only thing is I could think of why it's giving improvement on the Mel and yeah so that's about the subspace stuff.
Speaker B: So it's like going parallely but not much of improvement I'm just have some skeleton ready need some work.
Speaker C: Yeah so I've been working still on the spectral subtraction so to remind you a little bit of what I did before is just to apply some spectral subtraction with an over estimation factor also to get an estimate of the noise spectrum and subtract this estimation of the noise spectrum from the signal spectrum but subtracting more when the SNR is low which is the technique that it's subtracting from the meaning.
Speaker C: So you over estimate the noise spectrum you multiplied no spectrum by a factor which depends on the SNR so above 20 dB it's one so you just subtract the noise and then it's generally what I use actually on linear function of the SNR which is bounded to like two or three when the SNR is below zero dB.
Speaker C: Doing just this either on the FFT bands or on the Mel bands doesn't yield any improvement.
Speaker D: What are you doing with negative?
Speaker C: There is also threshold of course because after subtraction you can have negative energies and so what I just do is to add to put a threshold first and then to add a small amount of noise which right now is speed shaped.
Speaker C: So it has the overall energy as the overall power spectrum of speech so with a bump around one key.
Speaker G: When you talk about there being something less than zero after subtracting the noise is that a particular frequency band?
Speaker G: Yes there can be frequency bands with zero energy. So you're adding some it has overall.
Speaker C: For each frequency I'm adding some noise but the amount of noise I add is not the same for all the frequency bands.
Speaker C: Right now I don't think if it makes sense to add something that's speed shaped because then you have silence portion but if some spectra are similar to the overall speech spectrum.
Speaker C: So this is something I can still work on.
Speaker G: What does that mean? I'm trying to understand what it means when you do the spectral subtraction and you get a negative.
Speaker G: That means that that means that the frequency range you subtracted more energy than there was actually.
Speaker C: You have an estimation of the noise spectrum but sometimes of course as the noise is not perfectly stationary.
Speaker C: Sometimes this estimation can be too small so you don't subtract enough but sometimes it can be too large also.
Speaker C: If the noise energy in this particular frequency band drops for some reason.
Speaker G: So in an ideal world if the noise were always the same then when you subtracted it the worst that you would get would be a zero.
Speaker G: I mean the lowest you would get would be a zero because if there was no other energy there you're just subtracting exactly the noise.
Speaker E: There's all sorts of deviations from ideal here.
Speaker E: For instance you're talking about the signal noise at a particular point even if something is sort of stationary and it's third terms of statistics there's no guarantee that any particular instantiation or piece of it is exactly a particular number or bounded by a particular range.
Speaker E: So you're figuring out from some chunk of the signal what you think the noise is then you're subtracting that from another chunk.
Speaker E: And there's absolutely no reason to think that you'd know that it wouldn't be negative in some places.
Speaker E: On the other hand that just means that some sense you've made a mistake because you certainly have subtracted a bigger number than is due to the noise.
Speaker E: Also we speak where all this stuff comes from is from an assumption that signal noise are uncorrelated and that certainly makes sense in statistical interpretation that over all possible realizations that they're uncorrelated.
Speaker E: So that's why we're talking about the signal noise that is there uncorrelated or assuming your goodicity that across time it's uncorrelated.
Speaker E: But if you just look at quarter second and you cross multiply the two things you could very well end up with something that sums to something that's not zero.
Speaker E: So if you do signals could have some relation to one another and so there's all sorts of deviations from ideal in this and given all that you can definitely end up with something that's negative.
Speaker E: But if down the road you're making use of something as if it is a power spectrum then it can be bad to have something negative.
Speaker E: The other thing I wonder about actually is what if you left it negative what happens? I mean because the log are you taking the log before you add them up to the mill?
Speaker E: No, after.
Speaker E: Right. So the thing is I wonder how if you put your thresholds after that I wonder how often you would end up with negative values.
Speaker B: Would you end up reducing the neighboring frequency when the average right when you add the negative to the positive value which is the true estimate?
Speaker E: Yeah.
Speaker E: But nonetheless you know these are it's another kind of smoothing right that you're doing.
Speaker E: So you've done your best shot at figuring out what the noise should be and then you subtract it off and then after that instead of instead of leaving it as is and adding things adding up to neighbors you artificially push it up which is you know it's there's no particular reason that that's the right thing to do either right.
Speaker E: So in fact what you'd be doing is saying well we're we're we're going to definitely diminish the effect of this frequency and this is a little frequency bin in the in the overall mill summation.
Speaker E: This is the thought.
Speaker G: I don't know if you're going to get a negative number you don't do the subtraction for that.
Speaker G: Yeah, although almost the opposite right.
Speaker G: Instead of living it negative you don't do it.
Speaker G: If you're subtracting is going to result in a negative number you you don't do subtraction.
Speaker E: Yeah, but that means that in a situation where you thought that the bin was almost entirely noise you left it.
Speaker D: I'm just saying that's like yeah.
Speaker C: And some people also it's a negative value they recompute it using interpolation from the adjacent.
Speaker C: Yeah, from frequency bin things that you can do.
Speaker E: People can also reflect it back up and essentially do a four-way rectification instead of a set of half-wave.
Speaker E: But it's just a thought that that might be something to try.
Speaker C: Yeah, well actually I tried something else based on this.
Speaker C: It's to put some smoothing because it seems to help.
Speaker C: It seems to help the winner filtering.
Speaker C: So what I did is some kind of non-linear smoothing actually have a recursion that computes.
Speaker C: Yeah, let me go back a little bit.
Speaker C: Actually when you do spectral subtraction you can find this equivalent in the spectral domain you can compute.
Speaker C: You can see that the spectral subtraction is a filter and the gain of this filter is the signal energy minus what you subtract divided by the signal energy.
Speaker C: And this is the gain that varies over time.
Speaker C: Of course depending on the noise spectrum and on the speech spectrum.
Speaker C: And what happened actually is that during low SNR values the gain is close to zero but it varies a lot.
Speaker C: And this is the cause of musical noise and all these.
Speaker C: The fact that we go below zero on one frame and then you can have an energy that's above zero.
Speaker C: So the smoothing is, I did a smoothing actually on this gain trajectory but the smoothing is low in error in the sense that I try to not smooth if the gain is I.
Speaker C: Because in this case we know that the estimate of the gain is correct because we are not close to zero.
Speaker C: And to do more smoothing if the gain is low.
Speaker C: Yeah so basically that's this idea and it seems to give pretty good results.
Speaker C: Although I just tested on Italian and Finnish.
Speaker C: And on Italian it seems my result seems to be a little bit better than the winner filtering.
Speaker B: Yeah the one you showed yesterday.
Speaker C: I don't know if you have these improvements, the 10 improvements for Italian finishes.
Speaker B: No I don't have any.
Speaker B: I just just have the final number here.
Speaker E: So these numbers he was given before with the 4.3 and the 10 by 1.
Speaker E: And so those were Italian right?
Speaker B: Yeah so no I actually didn't give the number which is the final one which is after two stages of winner filtering.
Speaker B: I mean that was I just told like the overall improvement is like 56.5.
Speaker B: So his number is still better than what I got in that two stages of winner filtering.
None: Right.
Speaker C: On Italian but on Finnish it's a little bit worse apparently.
Speaker E: But you have numbers in terms of water rates and...
Speaker E: Yeah.
Speaker C: That's what I just said.
Speaker C: You have some sensor reference.
Speaker C: 3.8.
Speaker D: Oh okay.
Speaker C: And then 9.1.
Speaker C: And finally 16.5.
Speaker E: And this is spectrosotraction plus what?
Speaker C: Blood, plus non-linear smooth thing.
Speaker C: Well it's the system.
Speaker C: It's exactly the same system.
Speaker C: Oh my normalization.
Speaker C: But LDA.
Speaker C: Yeah.
Speaker C: But instead of the both stage winner filtering it's this smooth spectrosotraction.
Speaker G: Right. What is it? The France telecom system is it?
Speaker G: Did they use spectrosotraction or when you're filtering?
Speaker C: They use spectrosotraction right?
Speaker C: Forward?
Speaker C: French relic.
Speaker C: It's a winner filtering.
Speaker B: Oh it's a winner filtering.
Speaker C: Well it's some kind of winner filtering.
Speaker B: It's not exactly winner filtering but some variant of winner filtering.
Speaker E: Yeah.
Speaker E: Plus I guess they have some sort of capstone normalization.
Speaker E: They have like...
Speaker B: Yeah.
Speaker B: They're just noise compensation technique is a variant of winner filtering.
Speaker B: Because they do some smoothing techniques on the final filter.
Speaker B: They actually do the filtering in the time domain.
Speaker B: So they take this HF square back taking an inverse Fourier transform.
Speaker B: And they convolve that time domain signal with that.
Speaker B: And they do some smoothing on that final filter in pulse response.
Speaker C: But they also have to do different smoothing.
Speaker C: One in the time domain and one in the frequency domain by just taking the first.
Speaker C: Coefficient of the impulse response.
Speaker C: So basically similar...
Speaker C: I mean what you did it's...
Speaker C: It's similar in the smoothing.
Speaker C: They also have to kind of smoothing.
Speaker C: One in the time domain and...
Speaker C: Yeah.
Speaker C: One in the frequency domain.
Speaker G: Does the smoothing in the time domain help?
Speaker G: Well do you get this musical noise stuff with winner filtering?
Speaker G: Or is that only with the spectrosotraction?
Speaker G: Oh you get it with...
Speaker B: Yeah.
Speaker G: Winner filtering also?
Speaker G: Is the smoothing in the time domain help with that?
Speaker B: No, you still end up with zeros in the spectrum sometimes.
Speaker E: I mean it's not clear that these musical noises hurt us in recognition.
Speaker E: We don't know if they sound bad.
Speaker D: Yeah.
Speaker D: We're not listening to it usually.
Speaker C: Actually the smoothing that I did, the air reduced the music and noise.
Speaker C: Well I cannot...
Speaker C: You cannot hear...
Speaker C: Well actually what I did not say is that this is not in the FFT bands.
Speaker C: This is in the male frequency bands.
Speaker C: So it could be seen as a smoothing in the frequency domain because I use the male bands in addition.
Speaker C: And then the other phase of smoothing in the time domain.
Speaker C: But when you look at the spectrum, if you don't have any smoothing, you clearly see like in silence portions and at the beginning and end of speech, you see spots of high energy randomly distributed over the spectrum.
Speaker C: That's the musical.
Speaker C: Which is musical noisy.
Speaker C: If you listen to it, if you do this in the FFT bands, then you have spots of energy randomly distributed.
Speaker C: And if you recentize, this spot sounds as like sounds.
Speaker E: None of these systems, by the way, you both are working with our system that does not have the neural net.
Speaker E: Right?
Speaker F: Yeah.
Speaker D: So one would hope.
Speaker D: I assume we have the neural net part of it with improved things further as they did before.
Speaker C: Yeah.
Speaker C: Although if we look at the result from the proposals, one of the reasons the system with the neural net was more than well, around 5% better, is that it was much better on highly mismatched condition.
Speaker C: I'm thinking, for instance, on the TIDGITs trained on clean speech and tested noisy speech.
Speaker C: For this case, the system with the neural net was much better.
Speaker C: But not much on the other cases.
Speaker C: And if we have no spectral subtraction or winner filtering, the system is, we thought neural net work is much better than before, even in these cases of mismatch.
Speaker C: So maybe the neural net will help less.
Speaker C: Maybe.
Speaker G: Could you do any neural net with spectral subtraction?
Speaker E: Yeah, could do nonlinear spectral subtraction.
Speaker E: But I don't know if it, I mean, just figure out what your targets are.
Speaker G: Yeah, I was thinking if you had a clean version of the signal one.
Speaker G: A noisy version.
Speaker G: Right.
Speaker G: And targets for the, you know, whatever frequency.
Speaker E: Yeah, well, that's not so much spectral subtraction then.
Speaker E: But anyway, yeah, people, people do that.
Speaker E: Yeah, in fact, we had visitors here who did that, I think, when you were, a little bit way back when people had been lots of experimentation over the years with training neural nets.
Speaker E: It's not a bad thing to do.
Speaker E: It's another approach.
Speaker E: I mean, it's, it, the objection everyone always raises, which has some truth to it, is that it's good for mapping from a particular noise to clean, then you get a different noise.
Speaker E: And the experiments we saw that visitors did here showed that there was at least some gentleness to the degradation when you switched to different noises.
Speaker E: It did seem to help.
Speaker E: So that, you're right, that's another way to go.
Speaker G: You did care on, I mean, for good cases where it, it, it, it, stuff that it was trained on.
Speaker G: Did it do pretty well?
Speaker G: Oh, yeah, it did very well.
Speaker E: Yeah.
Speaker E: But to some extent, that's kind of what we're doing.
Speaker E: I mean, we're not doing exactly that.
Speaker E: We're not trying to generate good examples.
Speaker E: But by trying to do the best classifier you possibly can for these little fanatic categories.
Speaker E: It's sort of built in.
Speaker E: It's, yeah, it's kind of built into that.
Speaker E: And that's why we have found that it, it does help.
Speaker E: So, yeah, I mean, we'll just have to try it.
Speaker E: But I would, I would, I would imagine that it will help some.
Speaker E: I mean, it, this have to see whether it helps more or less the same.
Speaker E: But I would imagine it would help some.
Speaker E: So I didn't even, all of this, I was just confirming that all of this was with the simplices.
Speaker E: Yeah.
Speaker E: Yeah.
Speaker C: So this is the, well, actually this was kind of the first try with this spectral subtraction plus smoothing.
Speaker C: And I was kind of excited by the result.
Speaker C: Then I started to optimize the different parameters.
Speaker C: And the first thing I tried to optimize is the time constant of the smoothing.
Speaker C: And it seems that the one that I choose for the first experiment was the optimal one.
Speaker C: It's amazing how often I have.
Speaker C: This is the first thing.
Speaker C: Yeah, another thing that I, it's important to mention is that this as this as some additional latency, because when I do the smoothing, it's a recursion that estimates the means of the gain curve.
Speaker C: And this is a filter that has some latency.
Speaker C: And I noticed that it's better if we take into account this latency.
Speaker C: So instead of using the current estimated mean to subtract the current frame, it's better to use an estimate that's somewhere in the future.
Speaker G: And that's what causes the latency.
Speaker B: Yeah.
Speaker B: I mean, the mean is computed based on some frames in the future also.
Speaker C: No.
Speaker C: It's the recursion.
Speaker C: So it's the standard recursion, right?
Speaker C: And the latency of this recursion is around 50 milliseconds.
Speaker D: One five.
Speaker D: It's the one five five zero five zero.
Speaker D: Five zero.
Speaker D: Yeah.
Speaker B: Sorry, why is that delay coming like you estimate the mean?
Speaker C: Yeah, the mean estimation has some delay, right?
Speaker C: I mean, the filter has that estimated the mean as a distance.
Speaker B: Okay, so it's like it looks into the future also.
Speaker E: Yeah, okay.
Speaker E: What if you just look into the past?
Speaker C: It's not as good.
Speaker C: It's not bad.
Speaker C: How much?
Speaker C: It helps a lot of the baseline, but how much?
Speaker C: It's around 3% relative.
Speaker C: Where's?
Speaker C: Yeah.
Speaker E: So depending on all this stuff comes out, we may or may not be able to add any latency.
Speaker C: Yeah, but yeah.
Speaker C: Sorry, it's a different way.
Speaker C: Yeah, it's three percent.
Speaker C: Yeah, but I don't think we have to worry too much on that right now.
Speaker E: Yeah, I mean, I think the only thing is that I would worry about little, because if we completely ignore latency, then we discover that we really have to do something about it.
Speaker E: We're going to be finding ourselves in the bind.
Speaker E: So, you know, maybe you could make it 25.
Speaker E: Yeah, you know what I mean?
Speaker E: Yeah, just be a little conservative, because we may end up with this crunch where we have to cut the latency in half or something.
Speaker C: So, yeah, there are other things in the algorithm that I didn't play a lot yet,
Speaker G: which, sorry, quick question just about the latency thing. If there's another part of the system that causes the latency of 100 milliseconds, is this an additive thing or is yours hidden in that?
Speaker G: No, it's added.
Speaker G: It's added.
Speaker B: We can do something in parallel also.
Speaker B: In some cases, like if you wanted to do a voice activity detection, and you can do that in parallel with some other filtering you can do.
Speaker B: So, you can make a decision on that voice activity detection, and then you decide whether you want to filter or not.
Speaker B: But by then, you already have sufficient samples to do the filtering.
Speaker G: So, sometimes you can do it in.
Speaker G: I mean, couldn't you just also, I mean, if you know that the largest latency in the system is 200 milliseconds, couldn't you just buffer up that number of frames, and then everything uses that buffer in that way.
Speaker G: It's not additive.
Speaker E: One fact, everything is sent over and buffers kisses.
Speaker E: That TCP buffer, something.
Speaker B: I mean, the data, the superframe or something?
Speaker B: Yeah.
Speaker B: Yeah, but that has a variable latency, because the last frame doesn't have any latency, and the first frame has a 20 frame latency.
Speaker B: So, can't rely on that latency all the time.
Speaker F: Yeah.
Speaker B: Because, I mean, the transmission over the air interface is like a buffer, 20 frames.
Speaker B: Yeah.
Speaker B: So, but only thing is that the first frame in that 24 frame buffer has a 24 frame latency, and the last frame doesn't have any latency, because it just goes as...
Speaker G: Yeah, I wasn't thinking of that one in particular, but more of, you know, if there is some part of your system that has the buffer 20 frames, can't be other parts of the system draw out of that buffer and therefore not add to the latency.
Speaker E: Yeah, and that's sort of one of the, all of that sort of stuff is things they're debating in their standards committee.
Speaker F: Yeah, so, there is...
Speaker C: These parameters that I still have to look at, like, I played a little bit with this overestimation factor, but I still have to look more at this.
Speaker C: At the level of noise I add after, I know that adding noise at the system just using spellcrime subtraction without smoothing, but I don't know right now if it's still important or not, and if the level I choose before is still the right one, same thing for the shape of the noise.
Speaker C: And, you know, the level I choose before is still the right one, same thing for the shape of the noise.
Speaker C: Maybe it would be better to add just white noise instead of speech-shaped noise.
Speaker E: That'd be more like the aroust, I think, in a sense.
Speaker C: Yeah.
Speaker C: And another thing is to...
Speaker C: Yeah, for this I just use as noise estimates, the mean spectrum of the first 20 frames of each utterance.
Speaker C: I don't remember for this experiment, but did you use 10 frames?
Speaker B: I used 10 frames.
Speaker B: Yeah, because...
Speaker B: I mean, the reason was, like, in TI, did you say, I don't have a lot of 10 frames most of the time?
Speaker C: But so, what's this result you told me about the fact that if you use more than 10 frames, you can...
Speaker B: Oh, that's using the channel 0.
Speaker B: I use the channel 0 to estimate the noise.
Speaker C: But this is 10 frames plus...
Speaker C: channel 0.
Speaker C: Channel...
Speaker C: I know, these results...
Speaker C: Oh, two stage, winner filtering is 10 frames, but possibly more.
Speaker C: I mean, if channel 1, VAD gives you...
Speaker C: Yeah.
Speaker C: Okay.
Speaker C: Yeah, but in this experiment, I didn't use any VAD.
Speaker C: I just used the 21st frame to estimate the noise.
Speaker C: So I expected it to be a little bit better if I use more frames.
Speaker C: Okay, that's it for spectroscopy.
Speaker C: The second thing I was working on is to try to look at noise estimation and using some technique that doesn't need voice activity detection.
Speaker C: And for this, I simply used some code that I add from...
Speaker C: I add from Belgium, which is technique that takes a bunch of frames.
Speaker C: And for each frequency bands of this frame takes a look at the minima of the energy.
Speaker C: And then average this minima and take this as an energy estimate of the noise for this particular frequency band.
Speaker C: And there is something more to this, actually, what is done is that these minima are computed based on high-resolution spectra.
Speaker C: So I compute a FFT based on the long single frame, which is 64 milliseconds.
Speaker G: So you have one minimum per each frequency.
Speaker C: What I do actually is to take a bunch of...
Speaker C: to take a tile on the spectrogram and this tile is 500 milliseconds long and 200 hertz wide.
Speaker C: And this tile...
Speaker C: In this tile appears like the harmonics if you have a voice sound, because it's the FFT bands.
Speaker C: And when you take the minima of this tile, when you don't have speech, this minima will give you some noise level estimate.
Speaker C: If you have voiced speech, this minima will still give you some noise estimate because the minima are between the harmonics.
Speaker C: And if you have an or kind of speech sound, then it's not the case.
Speaker C: But if the time frame is long enough, like 500 milliseconds seems to be long enough, you still have portions which are very close...
Speaker C: which minima are very close to the noise energy.
Speaker E: I'm confused. You said 500 milliseconds, but you said 64 milliseconds.
Speaker E: What is that?
Speaker C: 64 milliseconds is to compute the FFT bands, the FFT.
Speaker C: Actually, it's better to use 64 milliseconds because if you use 30 milliseconds, then because of this shortwindowing and at low pitch sounds, the harmonics are not correctly separated.
Speaker C: So if you take this minima, they will overestimate the noise a lot.
Speaker E: So you take 64 milliseconds and then you average them over 500 or what do you do over 500?
Speaker C: I take a bunch of these 64 milliseconds frame to cover 500 milliseconds.
Speaker C: And then I look for the minima on a bunch of 50 frames.
Speaker C: So the interest of this is that with this technique, you can estimate some reasonable noise spectra with only 500 milliseconds of signal.
Speaker C: So if the noise varies a lot, you can track better track the noise, which is not the case if you rely on a voice activity detector.
Speaker C: So even if there are no speech poses, you can track the noise level.
Speaker C: The only requirement is that you must have in this 500 milliseconds segment, you must have voiced sound at least.
Speaker C: Because this will add you to track the noise level.
Speaker C: So what I did is just to simply replace the VAD based noise estimate by this estimate.
Speaker C: First on speech.car. Well, only on speech.car actually.
Speaker C: And it's slightly worse, like one percent relative compared to the VAD based estimates.
Speaker C: I think the reason why it's not better is that speech.car. noise is our stationary.
Speaker C: There is no need to have something that's active. Well, there are mainly stationary.
Speaker C: But I expect maybe some improvement on TI digits because in this case the noises are sometimes very variable.
Speaker C: So I have to test it.
Speaker E: But are you comparing with something a little confused again?
Speaker C: When you compare it with the VAD based. It's the French Silicon based spectra winner filtering and VAD.
Speaker C: So it's their system, but just a replace. There are noise estimate by this one.
Speaker C: Oh, you're not doing this with our system. I'm not. No.
Speaker C: Yeah, it's our system, but we're just the winner filtering from their system.
Speaker C: Actually, the best system that we still have is our system, but with their noise compensation scheme.
Speaker C: So I'm trying to improve on this and by replacing their noise estimate by something that might be better.
Speaker E: Okay, but the spectra subtraction scheme that you reported on also requires a noise estimate.
Speaker C: But could I try this for that?
Speaker C: No, because I did this in parallel. I say what's working on it.
Speaker C: For sure, I can try it also.
Speaker B: I think that new noise estimate technique on this winner filtering what I'm trying. I have some experiments running around other results.
Speaker B: So I don't estimate the noise from the time France, but you see a system.
Speaker C: Yeah, I am also implemented spectral widening idea, which is in the Ericsson proposal.
Speaker C: The idea is just to flatten the log spectrum.
Speaker C: And to flatten it more if the probability of silence is higher.
Speaker C: So in this way, you can also reduce so much reduce the musical noise.
Speaker C: And you reduce the variability if you have different noise shapes.
Speaker C: Because the spectrum becomes more flat in the silence portions.
Speaker C: Yeah, with this no improvement, but there are a lot of parameters that we can play with.
Speaker C: And actually, this could be seen as a soft version of the frame dropping.
Speaker C: Because you could just put a threshold and say that below the threshold, I will flat on completely flat on the spectrum.
Speaker C: And above this threshold, give the same spectrum.
Speaker C: So it would be like frame dropping because during the silence portions, which are below the threshold of voice activity, probability, you would have some kind of dummy frame, which is perfectly flat spectrum.
Speaker C: And this widening is something that's more soft because you widen, you just have a function.
Speaker C: The widening is a function of the speech probability, so it's not a hard decision.
Speaker C: So I think maybe it can be used together with frame dropping and we are not sure about the speech of silence.
Speaker E: I mean, in J. Rasta, we were essentially adding in white noise, depending on our estimate of the noise, when the over-awesome in the noise.
Speaker E: I think it never occurred to us to use a probability in there.
Speaker E: You can imagine that that made use of where the amount that you added in was a function of the probability of it being speech or noise.
Speaker C: Yeah, we're at noise a constant that just depending on the noise spectrum.
Speaker E: Because that brings in sort of powers of classifiers that we don't really have in the other estimates, so it could be interesting.
Speaker E: What point does the system stop recording?
Speaker E: How much?
Speaker G: It'll be a little long.
Speaker G: I just ran out of this space.
Speaker G: I think we're okay.
Speaker C: Yeah, so with this technique, there are some... I just did something exactly the same as the direction proposal, but the probability of speech is not computed the same way.
Speaker C: I think for a lot of things, actually, a good speech probability is important. For frame-dropping, you can improve from 10%.
Speaker C: As a news show, if you use the channel zero speech probabilities, for this, it might add.
Speaker C: So, yeah, the next thing I started to do is to try to develop a better voice activity detector.
Speaker C: For this, I think we can maybe try to train the neural network for voice activity detection on all the data that we have, including all these speech data.
Speaker C: So, I'm starting to obtain alignment on these databases. The way I do that is that I just use the SDK system, but I train only on the closed-talking microphone, and then I obtain the Viterbi alignment of the training utterances.
Speaker C: It seems to be... Actually, what I observe is that for Italian, it doesn't seem... No, it doesn't seem to be a problem.
Speaker B: No, it doesn't seem to be a problem.
Speaker C: When you enter the frame-dropping, right? Yeah, but actually, the VAD was trained on Italians. The current VAD that we have was trained on the spine-right, the attitudes with noise and length.
Speaker C: It seems to work on Italian, but not on the Finnish and Spanish data. So, maybe one reason is that Finnish and Spanish noise are different.
Speaker C: Actually, we listen to some of the utterances, and sometimes for Finnish, there is music in the recordings and strange things.
Speaker C: Yeah, so the idea was to train on all the databases and obtain an alignment to train on these databases.
Speaker C: Also, to try different kinds of features, as input to the VAD network, we came up with a bunch of features that we want to try.
Speaker C: Like the spec-roslow, the degree of voicing with the features that we started to develop with Kermann, with the correlation between bands, and then different kinds of features.
Speaker C: Energy, of course.
Speaker E: Okay, well, Hans Kuntel will be here next week, so I think he'll be interested in all of these things.
Speaker E: Transcript L-209, 4-6-28-893020, 4-22-08-0952, 5-075-121056, 9-37105-2768, 316-727-5311, 7-329-7204-2, 7-646-7011, 5-8152-268.
Speaker G: Transcript L-294, 6-02459-228, 05827-34, scratch that, 3746, 7991-6007-1418, 6-6-717-449, 094019-6213, 6-059-78-2606, 8-613-296973, 8-556-71576, 6.
Speaker A: Transcript L-281-364-827-3611, 5-364-549408, 1-369-5408, 4-232-35773-8, 1-996-6488-2402, 153-393-89, 9-365-78060, 6-8915-07935.
Speaker B: Transcript L-285, 7269-4416, 07604-5454, 1-884-385-387-09, 0535-393-266, 4-909-909109, 6-713-05271-236, 7-832-96, 3-385, 4-757-1276-4975.
Speaker C: Transcript L-286, 3-7545146-9, 3-5453-5309, 6-85-2, 5-8-2143-44, 8-265-5808-1, 5-396-1055338, 5-01-195-910, 5-343-117-859, 6-050-1187-391.
