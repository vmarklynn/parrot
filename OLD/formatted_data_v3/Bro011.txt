None: I am Guess so radio to
Speaker E: All right
Speaker D: It works really well people say this thing Everybody's on yeah So you guys had a meeting with Henik which I unfortunately had to miss And And I guess Chuck you weren't there either so I was there or you were there with Henik yeah, so everybody knows what happened except me Maybe somebody should tell me
None: All right Well first we discussed about some of the points That I was addressing in the mail I sent last week So yeah But well the don't sampling problem Yeah And about the feet of the filters
Speaker D: So what was the time sampling problem again I think
Speaker F: Is that the fact that there is no low pass filtering before the don't sample well there is because there is a LDA filtering but that's perhaps not the best
Speaker D: But it depends but it's a Consicure requesting this yeah so you could do a you could do a structure one maybe yeah so we discussed about this about
Speaker F: Was there any conclusion about that? Or try it yeah I guess
Speaker D: Yeah so again this is the this is the down sampling of the feature vector stream And yeah I guess the LDA filters they were doing do have It's so that the feature vectors are calculated every 10 milliseconds so The question is how far down they are 50 50 hertz Sorry at 25 hertz and so down sampling right too So does anybody know the frequency characteristic is
Speaker F: We don't have yet So yeah we should have a look for that perhaps yeah the modulation spectra So there is this there is the length of the filters So the idea of trying to find filters with shorter delays We started to work with this And the third point was the yeah the online normalization where Well the recursion for the min estimation is a filter with some kind of delay Yeah that's not taken into account right now Yeah and there again for this conclusion the fin acquiesce
Speaker D: We can try it but try try what?
Speaker F: So try to take into account the delay of the recursion for the min estimation Okay And this we've not worked on this yet And so while discussing about these LDA filters some issues appeared Like well the fact that if we look at the frequency response of these filters it's Well we don't know really what's the important part in the frequency response And there is the fact that in the very low frequency these filters don't really remove a lot Compared to the standard Rasta filter And that's probably the reason why yeah online normalization helps because Yeah to remove this maze Yeah but perhaps everything could be in the filter I mean the min normalization Yeah so Yeah so basically that's what we discussed about Good things to do also generally good stuff to do for the research And this was this LDA tuning perhaps and in fact proposed again to these traps
Speaker D: Okay I mean I guess the key thing for me is figuring out how to better coordinate between two sides Because I was talking with Henrik about it later And the sense sort of that neither group of people wanted to bother the other group too much And I don't think anybody is closed in and they're thinking or I'm willing to talk about things But I think you were sort of waiting for them to tell you that they had something for you And then they expected they would do certain things and they didn't want to bother you And we were sort of waiting for you and we ended up with this thing Where they were filling up all of the possible latency for themselves And they just hadn't thought of that Yeah I mean it's true that maybe no one really thought about that this latency thing would be such a strict issue
Speaker F: And yeah I don't know what happened really but I guess it's also the time constraints Because we discussed about this problem and they told us well we will do all that's possible to have enough space for network But then the wraps never ends But the rest was a problem of communication So we will try to talk more
Speaker D: So there's... All right well maybe we should just... I mean you're busy, other than that you folks are busy doing all the things that you're trying that we talked about before And machines are busy and you're busy Okay well let's... I mean I think that as we said before one of the things that we're imagining is that there will be in system There will be something to explicitly do something about noise in addition to the other things that we're talking about And that's probably the best thing to do And there was that one email that said that it sounded like things looked very promising up there in terms of...
Speaker D: I think they were using... approach or something in addition to... they're doing some noise removal thing
Speaker F: So yeah we will start to do this also So Carmen is just looking at the Ericsson code
Speaker A: I modified it, modified it I studied there is some code in the world To take only the first step the spectral substitution And we have some... the feature for Italian database And we will try with this feature with the filter to find the result But we have the result under this moment Yeah, sure We are working in this also and maybe try another type of spectrarset ratio
Speaker D: When you say you don't have a result yet you mean it's just that it's in focus or it finished and it didn't get a good result
Speaker A: No, no, we have... we have the experiment Or we have the feature, the feature but... this experiment... we have not made this experiment Maybe it would be good to resend all the resend... we don't know
Speaker D: Yeah So I suggest actually now we move on and tear what's happening in another areas like... what's happening with your investigations Oh, I don't know about echoes and so on
Speaker B: I haven't started running the test yet, I'm meeting with Adam today And he's going to show me the scripts he has for running recognition on the meeting recorder digits I also haven't got the code yet I haven't asked for the quiz code yet So that's how Adam Down on the pieces And I don't really understand what he's doing yet It sounded like the channel normalization part of his thesis was done in a bit of... I don't know what the word is A bit of a rough way, it sounded like he... he wasn't really fleshed out and maybe he did something that was interesting for the test situation But I'm not sure if it's what I'd want to use So I have to read it more, I don't really understand what he's doing yet
Speaker D: I haven't read it in a while so it's not going to be too much help unless I read it again So...
Speaker D: And then you're also going to be doing this echo cancelling between the question and the...
Speaker D: We're calling cheating experiment
Speaker B: I'm hoping... I'm hoping you ask me what to do
Speaker D: Oh, okay Delegate It's good to be Delegate
Speaker B: I think he's at least planning to do it for the close mic crosstalk And so maybe I can just take the other set of he has and use it
Speaker D: Great, great Yeah, actually he should...
Speaker D: Like maybe it's the analysis going to be doing a different cancellation One of the things that people working in the meeting task want to get at is We'd like to have cleaner close mic recordings So this is especially true for the lapel but even for the close mic cases We'd like to be able to have other sounds from other people on so forth removed So when someone isn't speaking you'd like to part whether or not speaking to actually So what they're talking about doing is using echo cancellation like techniques It's not the way I go but just taking the input from other mics and using an adaptive filtering approach to remove the effect of that other speech So what was it? There was some point where Eric or somebody was speaking And he had lots of silence in his channel and I was saying something to somebody else Which is in the background and it was not... it was recognizing my words which was the background speech On the close mic
Speaker C: Oh, we talked about yesterday Yeah, that was actually in my eyes I was wearing the lapel and you were sitting next to me Yeah, and I only said one thing But you were talking and it was picking up all your words
Speaker D: Yeah, so they were like clean channels And for that purpose, so let's pull it out So I think that's something that somebody was working with That's going to work out So Right And I don't know if we've talked lately about the plans You're developing and we talked about this morning If we talked about that last week or not But you just do work with Ryze and probably I guess I'm just pointing So
None: We're going to Next day It's on the way It's really hot What about the stuff that Miriam has been doing?
Speaker C: And Sean, yeah.
Speaker C: Yeah.
Speaker B: What's good about you and I think that's good.
Speaker C: So they're training up nets to try to recognize these acoustic features, I see.
Speaker D: But that's a certainly relevant study and you know what are the features that they're finding.
Speaker D: We have this problem with the overloading of the term features.
Speaker D: What are the variables we're calling this one?
Speaker D: What are the variables that they're finding useful?
Speaker C: And their targets are based on canonical mappings of phones to acoustic.
Speaker D: Right. And there's certainly one thing to do and we're going to try and do something more fine than that.
Speaker D: So I guess I was trying to remember some of the things we were saying.
Speaker D: Yeah. So some issues we were talking about was just getting a good handle on what good features are.
Speaker C: What did Larry Saul use for this sonarant detector?
Speaker C: How did he do that?
Speaker C: What was his detector?
Speaker E: Yeah. It was a tonnage.
None: It was variable.
Speaker C: It was a measure of the hand to the left.
Speaker C: A thing.
Speaker E: Actually, it was a measure of the correlation.
Speaker E: So how did he combine all these features?
Speaker C: What classifier did he use?
Speaker C: What did he use for this test?
Speaker D: What are the variables that you use?
Speaker D: You combine them using the software or something more complicated.
Speaker D: And then the other thing was where to get the targets from.
Speaker D: The initial thing is just the obvious that we're discussing is starting off with phone labels from somewhere and then doing the transformation.
Speaker D: But then the other thing is to do something better.
Speaker D: What did you tell us about this database?
Speaker D: I don't know.
Speaker C: I guess if you had people who had like, like, a computer, a computer, a computer.
Speaker C: Pierce Tongues.
Speaker D: You just mounted it to that and they wouldn't even notice.
Speaker D: Welled it.
Speaker C: That's right.
Speaker D: Okay.
Speaker C: There's a bunch of data around people who have done studies like that way, way back.
Speaker C: I can't remember where Wisconsin or someplace that used to have a big database.
Speaker C: Remember there was this guy at AT&T Randolph?
Speaker C: Researcher at AT&T, a while back that was studying, trying to do speech recognition from these kinds of features.
Speaker D: Mark Randolph.
Speaker C: Oh, is he?
Speaker F: I can't remember exactly what he was using now.
Speaker C: I just remember it had to do with, you know, positional parameters and trying to, you know, speech recognition based on them.
Speaker D: The only hesitation I had about it since I haven't seen the data is it sounds like it's continuous variables and a bunch of them.
Speaker D: And so I don't know how complicated it is to go from there, but you really want these binary labels just a few of them.
Speaker D: And maybe there's a trivial mapping.
Speaker D: I worry a little bit that this is a research project in itself.
Speaker D: Because if you did something instead that like having some manual annotation by, you know, agristic students, this would, there'd be a limited set of things that you could do as per our discussions with John before.
Speaker D: But the things that you could do like anxiety and voicing a couple other things, you probably could do reasonably well.
Speaker D: And then it would really be this binary variable.
Speaker D: Of course, then that's the other question is do you want binary variables?
Speaker D: So the other thing you could do is boot trying to get those binary variables and take the continuous variables from the data itself there.
Speaker C: But I'm not sure.
Speaker C: Could you cluster the, just do some kind of clustering?
Speaker C: Yeah, then I'm up into different categories.
Speaker D: So anyway, that's another whole direction that could be looked at.
Speaker D: I mean, in general, it's going to be for new data that you look at, it's going to be hidden variable because we're not going to get everybody sitting in these meetings to where the pellets and.
Speaker C: So you're talking about using that data to get instead of using canonical mappings of phones.
Speaker C: So you use that data to give you sort of what the true mappings are for each phone.
Speaker D: Yeah, where this fits into the rest in my mind, I guess, is that we're looking at different ways that we can combine different kinds of printed representations in order to get robustness and do difficult or even typical conditions.
Speaker D: And part of it, this robustness seems to come from multi-stream and multi-band sorts of things and Saul seems to have a reasonable way of looking at it, at least for one particular toy feature.
Speaker D: The question is, can we learn from that to change some of the other methods we have since any one of the things that's nice about what he had, I thought was that it, the decision about how strongly trained the different pieces is based on a reasonable criterion with it variables rather than just assuming that you should train every detector with equal.
Speaker D: With equal strength towards it being this phone or that phone.
Speaker D: So he's got these.
Speaker D: The ends between these different features.
Speaker D: It's a soft end, I guess, but in principle, you want to get a strong concurrence of all the different things that indicate something. And then he oars across the different soft oars across the different multi-band channels.
Speaker D: And the weight, the target for the training of the AND ended things is something that's kept as a hit variable and is learned with the app.
Speaker D: Whereas what we were doing is taking the phone target and then just back propagating from that, which means that it could be, for instance, that for a particular point in the data, you don't want to train a particular band, train the detector for a particular band, you want to ignore that band.
Speaker D: That's a band is a noisy measure. And we're still going to try to train it up in our scheme. We're going to try to train it up to do as well as it can at predicting.
Speaker D: Maybe that's not the right thing to do.
Speaker C: So he doesn't have to have truth marks?
Speaker D: Well, at the talent, he has to know where it's sonarant. But what he's not training up, what he doesn't depend on his truth is, I guess, one way of describing it would be, if a sound is sonarant, is it sonarant in this band?
Speaker D: It's hard to even answer that, what you really mean is that the whole sound is sonarant. So then it comes down to what extent should you make use of information from particular band towards making your decision?
Speaker D: I see. And we're making, in a sense, sort of this hard decision that you should use everything with equal strength. And because in the ideal case, we would be going for posterior probabilities, if we had enough data to really get posterior probabilities.
Speaker D: And if we also had enough data so that it was representative of the test data, then we would, in fact, be doing the right thing to train everything as hard as we can.
Speaker D: But this is something that's more built up along an idea of robustness from the beginning, so you don't necessarily want to train everything up towards the...
Speaker C: So where did he get his high level targets about what sonarant and what's not?
Speaker E: From canonical mappings, from a person, then it's unclear. Using timet? Using timet, right?
Speaker E: Yeah, and then he does some fine tuning.
Speaker D: I mean, we have a kind of iterative training because we do this embedded with Ruby. So there is something that's adjusted based on the data.
None: I think it's seemed like quite the same. Because then whatever that line is, is that wrong?
None: No, bands. Well, that's quite...
None: I'm trying to do something. That'll be a little more like it.
Speaker D: But it's still quite the same because then it's a target based on what you'd say the sound begins in a particular band where he's not labeling per se.
Speaker D: It might be closer, I guess, if we did a soft target embedded training, we'd have done a few times before we did a forward calculations to get the gammas and train our bells.
Speaker C: What's next? I can say a little bit about stuff I've been playing with.
Speaker C: So I wanted to do this experiment to see what happens if we try to improve the performance of the backend recognizer for the Aurora task and see how that affects things.
Speaker C: I think I sent around last week a plan I had for an experiment, this matrix, where I would take the original system.
Speaker C: So there's the original system trained on the Mel Kepstrow features and then optimize the HTK system and run that again. So look at the difference there.
Speaker C: And then do the same thing for the XE-OGI front end.
Speaker E: Which test was this?
Speaker C: If I look at it, I'm looking at the Italian right now. So as far as I've gotten, I've been able to go through from beginning to end the full HTK system for the Italian data and got the same results that Stefan had.
Speaker C: So I started looking at the point where I want to know what should I change in the HTK backend in order to try to improve it.
Speaker C: One of the first things I thought of was the fact that they use the same number of states for all of the models. And so I went online and I found a pronunciation dictionary for Italian digits and just looked at the number of phones in each one of the digits.
Speaker C: So the canonical way of setting up an HMM system is that you use three states per phone. And so then the total number of states for a word would just be the number of phones times three.
Speaker C: And so when I did that for the Italian digits, I got a number of states ranging on a low end from nine to the high end 18.
Speaker C: Now you have to really add two to that because in HTK there's an initial null and a final null. So when they use models that have 18 states, they're really 16 states. They've got this initial and final null states.
Speaker C: And so their guess of 18 states seems to be pretty well matched to the two longest words, the Italian digits, the four and five, which according to my sort of off the cuff calculation should have 18 states each. And so they had 16. So that's pretty close.
Speaker C: But for the most of the words are much shorter. So the majority of them want to have nine states. And so there's sort of twice as long. So my guess. And then if you, I printed out a confusion matrix for the well matched case.
Speaker C: And it turns out that the longest words are actually the ones that do the best. So my guess about what's happening is that if you assume a fixed the same amount of training data for each of these digits and a fixed length model for all of them.
Speaker C: But the actual words for some of them are half as long. You really have half as much training data for those models.
Speaker C: Because if you have a long word and you're training it to 18 states, you've got, you've got the same number of Gaussian's. You've got a train in each case. But for the shorter words, you know, the total number of frames is actually half as many.
Speaker C: So it could be that, you know, for the short words, there's because you have so many states, you just don't have enough data to train all those Gaussian's. So I'm going to try to create more word specific prototype HMMs to start training from.
Speaker D: Yeah, I mean, it's not at all uncommon. You do where send long words on short words and long words anyway, just because you're accumulating more evidence.
Speaker C: Yeah, so I'll, I'll, the next experiment I'm going to try is to just, you know, create models that seem to be more matched to my guess about how long they should be.
Speaker C: And as part of that, I wanted to see sort of how the, how these models were coming out, you know, when we train up the model for one, which wants to have nine states, you know, what is the, what are the transition probabilities look like in the self loops look like in those models.
Speaker C: And so I talked to Andreas and he explained to me how you can calculate the expected duration of an HMM just by looking at the transition matrix.
Speaker C: And so I wrote a little MATLAB script that calculates that. And so I'm going to sort of print those out for each of the words to see what's happening, you know, how these models are training up, you know, the long ones versus the short ones.
Speaker C: I did, quickly I did the silence model and, and that's coming out with about 1.2 seconds is its average duration. And the silence model is the one that's used at the beginning and the end of each of the string of digits.
Speaker C: Lots of silence.
Speaker C: Yeah, yeah. And so the SP model, which is what they put in between digits, I haven't calculated that for that one yet.
Speaker C: So they basically they're their model for a whole digit string is silence digit, SP digit, SP, blah blah and then silence at the end.
Speaker C: So are the SPs optional?
Speaker C: I have to look at that, but I'm not sure that they are. Now the one thing about the SP model is really it only has a single emitting state to it.
Speaker C: So if it's not optional, you know, it's it's not going to hurt a whole lot. And it's tied to the center state of the silence.
Speaker C: It's not a little, it doesn't require some training data, it just shares that state.
Speaker C: So I mean it's pretty good the way that they have it set up, but, so I want to put that a little bit more curious about looking at, you know, how these models have trained and looking at the expected durations models.
Speaker C: And I want to compare that into the well matched case, the unmatched case and see if you can get an idea just from looking at the durations of these models, you know, what's happening.
Speaker D: Yeah, I'm going to think, yeah, I'm going to think, yeah, I'm going to think, yeah, it's good to, so I'm not doing anything really tricky.
Speaker D: Not doing anything really finely. The premise is kind of if you have a good person look at this for two weeks and what you come up with.
Speaker C: And he nick when I told him about this, he had an interesting point and that was the final models that they ended up training up have, I think probably something on the order of six Gaussian per state.
Speaker C: So they're fairly, you know, hefty models and he was saying that, well, probably in a real application, you wouldn't have enough compute to handle models that are very big or complicated.
Speaker C: So in fact, what we may want are simpler models and compare how they perform to that.
Speaker C: But, you know, it depends on what the actual application is and it's really hard to know what your limits are in terms of how many Gaussian you can have.
Speaker D: Right. And at the moment, that's not the limitation. So, I mean, what I thought you were going to say, but what I was thinking was where did six come from?
Speaker D: Probably came from the same place, 18 came from.
Speaker D: Yeah. So that's another parameter, right?
Speaker D: Yeah. Maybe, you really want to do one thing.
Speaker C: If I start reducing the number of states for some of these shorter models, that's going to reduce the total number of Gaussian. So in a sense, it will be a simpler system.
Speaker D: Yeah. But I think right now, again, the idea is doing just very simple things, how much better can you make it?
Speaker D: And since there are only simple things, there's nothing that you're going to do that is going to blow up the amount of computation. So if you found that nine was better than six, that would be okay.
Speaker C: I really wasn't even going to play with that part of the system yet. I was just going to change the work with the models.
Speaker C: Yeah. Just look at the models and see what happened.
Speaker C: Yeah.
Speaker C: So.
Speaker D: Cool. Okay. So what's I guess your plan for you guys playing for the next next week is just continue on these same things you've been talking about.
Speaker F: Yeah.
Speaker F: I guess you can try to have some kind of new baseline for next week, perhaps with all these minor things.
Speaker F: And then you modify it, and then do other things play with the spectrospection and retry to a MFG.
Speaker D: Yeah. Yeah. We have a big list.
Speaker D: You have a big list of things to do.
Speaker D: So that's good. I think that after all of this confusion settles down. At some point a little later next year, there will be some sort of standard.
Speaker D: We'll get out there and hopefully a little have some effect from something that has been done by a group of people.
Speaker D: But even if it doesn't, there's going to be standards after that.
Speaker C: Does anybody know how to run MATLAB sort of in batch mode like you send it to commands to run it? Is it possible to do that?
Speaker E: I think Mike tried it.
Speaker E: Yeah.
Speaker E: And he said it was impossible. So he went to active.
Speaker E: I thought it was the Unix clone of MATLAB.
Speaker E: Which you can dodge.
Speaker C: Okay. Great. Thank you.
Speaker C: Yeah.
Speaker C: I was going crazy trying to do that.
Speaker F: Yeah. What is that?
Speaker E: It's a free software.
Speaker E: I think we have it here running somewhere.
Speaker F: Great.
Speaker F: And it does the same syntax.
Speaker F: I think it's a little behind it.
Speaker E: It's a little behind in that MATLAB went to these like, like, half-sales and you can implement object-oriented type things.
Speaker E: Active doesn't do that yet.
Speaker E: I think you've, like, active MATLAB or point something.
Speaker C: If it'll do a lot of the basic matrix and vector stuff.
Speaker C: That's perfect.
Speaker C: Great.
Speaker D: Okay. I guess we're done.
Speaker D: I don't find it.
